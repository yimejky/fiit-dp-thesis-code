{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n",
      "Dataset biggest bounding box wihtout spinal cord [56, 177, 156]\n",
      "Cut target size [72, 192, 168]\n",
      "Done Init\n"
     ]
    }
   ],
   "source": [
    "from src.consts import IN_COLAB, DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Found Google Colab')\n",
    "    !pip3 install torch torchvision torchsummary\n",
    "    !pip3 install simpleitk\n",
    "\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import datetime\n",
    "from torchio import RandomAffine, Compose, ZNormalization\n",
    "\n",
    "import src.dataset.oars_labels_consts as OARS_LABELS\n",
    "from src.helpers.threshold_calc_helpers import get_threshold_info_df\n",
    "from src.helpers.show_model_dataset_pred_preview import show_model_dataset_pred_preview\n",
    "from src.dataset.get_cut_lists import get_cut_lists\n",
    "from src.dataset.get_full_res_cut import get_full_res_cut\n",
    "from src.dataset.get_dataset import get_dataset\n",
    "from src.dataset.get_dataset_info import get_dataset_info\n",
    "from src.dataset.preview_dataset import preview_dataset\n",
    "from src.dataset.get_dataset_transform import get_dataset_transform\n",
    "from src.model_and_training.prepare_model import prepare_model\n",
    "from src.model_and_training.train_loop import train_loop\n",
    "from src.model_and_training.show_model_info import show_model_info\n",
    "from src.model_and_training.load_checkpoint_model_info import load_checkpoint_model_info\n",
    "from src.helpers.show_cuda_usage import show_cuda_usage\n",
    "from src.helpers.get_rescaled_pred import get_rescaled_preds\n",
    "from src.dataset.split_dataset import split_dataset, copy_split_dataset\n",
    "from src.helpers.compare_prediction_with_ground_true import compare_prediction_with_ground_true, compare_one_prediction_with_ground_true\n",
    "from src.helpers.get_img_outliers_pixels import get_img_outliers_pixels\n",
    "from src.helpers.get_raw_with_prediction import get_raw_with_prediction\n",
    "\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "torch.manual_seed(20)\n",
    "logging.basicConfig(filename='logs/precourse_neural_network.log', level=logging.DEBUG)\n",
    "\n",
    "\n",
    "TRAIN_LOW_MODEL=False\n",
    "TRAIN_CUT_MODEL=False\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading precourse neural network with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 16x dataset\n",
      "filtering labels\n",
      "filtering labels done\n",
      "dilatating 1x dataset\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n",
      "CUDA using 1x dataset\n",
      "filtering labels\n",
      "filtering labels done\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n",
      "Device running \"cuda\"\n",
      "max output channels 64\n",
      "Model number of params: 298881, trainable 298881\n",
      "getting cut index 0\n",
      "debug removing 10 outlier pixels from 1335\n",
      "debug box delta [21 48 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1223526 1223526\n",
      "getting cut index 1\n",
      "debug removing 0 outlier pixels from 1416\n",
      "debug box delta [24 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1326052 1326052\n",
      "getting cut index 2\n",
      "debug removing 0 outlier pixels from 1873\n",
      "debug box delta [ 20   0 -24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1890464 1890464\n",
      "getting cut index 3\n",
      "debug removing 0 outlier pixels from 1545\n",
      "debug box delta [17 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1560217 1560217\n",
      "getting cut index 4\n",
      "debug removing 9 outlier pixels from 1510\n",
      "debug box delta [20 48 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1451227 1451227\n",
      "getting cut index 5\n",
      "debug removing 0 outlier pixels from 1390\n",
      "debug box delta [22 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1262651 1262651\n",
      "getting cut index 6\n",
      "debug removing 0 outlier pixels from 1451\n",
      "debug box delta [19 16 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1566938 1566938\n",
      "getting cut index 7\n",
      "debug removing 0 outlier pixels from 958\n",
      "debug box delta [29 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 869847 869847\n",
      "getting cut index 8\n",
      "debug removing 0 outlier pixels from 1489\n",
      "debug box delta [19 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1397249 1397249\n",
      "getting cut index 9\n",
      "debug removing 0 outlier pixels from 1465\n",
      "debug box delta [21 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1350330 1350330\n",
      "getting cut index 10\n",
      "debug removing 0 outlier pixels from 1650\n",
      "debug box delta [20  0 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1635868 1635868\n",
      "getting cut index 11\n",
      "debug removing 16 outlier pixels from 1371\n",
      "debug box delta [23 48 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1283062 1283062\n",
      "getting cut index 12\n",
      "debug removing 0 outlier pixels from 1594\n",
      "debug box delta [21 32 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1518406 1518406\n",
      "getting cut index 13\n",
      "debug removing 0 outlier pixels from 1482\n",
      "debug box delta [23 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1504194 1504194\n",
      "getting cut index 14\n",
      "debug removing 0 outlier pixels from 1191\n",
      "debug box delta [25 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1084254 1084254\n",
      "getting cut index 15\n",
      "debug removing 0 outlier pixels from 1267\n",
      "debug box delta [25 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1221257 1221257\n",
      "getting cut index 16\n",
      "debug removing 2 outlier pixels from 1009\n",
      "debug box delta [21 64 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 945639 945639\n",
      "getting cut index 17\n",
      "debug removing 0 outlier pixels from 1498\n",
      "debug box delta [22 16 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1469035 1469035\n",
      "getting cut index 18\n",
      "debug removing 7 outlier pixels from 1371\n",
      "debug box delta [22 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1322571 1322571\n",
      "getting cut index 19\n",
      "debug removing 0 outlier pixels from 1608\n",
      "debug box delta [17 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1593516 1593516\n",
      "getting cut index 20\n",
      "debug removing 0 outlier pixels from 1359\n",
      "debug box delta [25 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1390348 1390348\n",
      "getting cut index 21\n",
      "debug removing 0 outlier pixels from 1536\n",
      "debug box delta [ 22  16 -24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1463017 1463017\n",
      "getting cut index 22\n",
      "debug removing 0 outlier pixels from 1231\n",
      "debug box delta [26 32 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1162215 1162215\n",
      "getting cut index 23\n",
      "debug removing 0 outlier pixels from 1154\n",
      "debug box delta [23 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1029805 1029805\n",
      "getting cut index 24\n",
      "debug removing 0 outlier pixels from 1669\n",
      "debug box delta [ 29 -16   8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1689537 1689537\n",
      "getting cut index 25\n",
      "debug removing 0 outlier pixels from 1267\n",
      "debug box delta [23 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1141739 1141739\n",
      "getting cut index 26\n",
      "debug removing 9 outlier pixels from 1289\n",
      "debug box delta [18 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1167835 1167835\n",
      "getting cut index 27\n",
      "debug removing 15 outlier pixels from 1780\n",
      "debug box delta [23  0  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1783264 1783264\n",
      "getting cut index 28\n",
      "debug removing 25 outlier pixels from 1916\n",
      "debug box delta [ 22  16 -24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1944758 1944758\n",
      "getting cut index 29\n",
      "debug removing 0 outlier pixels from 1369\n",
      "debug box delta [22 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1583396 1583396\n",
      "getting cut index 30\n",
      "debug removing 11 outlier pixels from 1390\n",
      "debug box delta [19 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1248609 1248609\n",
      "getting cut index 31\n",
      "debug removing 0 outlier pixels from 1087\n",
      "debug box delta [25 48 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 947124 947124\n",
      "getting cut index 32\n",
      "debug removing 39 outlier pixels from 1798\n",
      "debug box delta [16 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1648187 1648187\n",
      "getting cut index 33\n",
      "debug removing 0 outlier pixels from 1327\n",
      "debug box delta [20 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1214697 1214697\n",
      "getting cut index 34\n",
      "debug removing 0 outlier pixels from 1528\n",
      "debug box delta [24 16 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1445951 1445951\n",
      "getting cut index 35\n",
      "debug removing 0 outlier pixels from 1981\n",
      "debug box delta [ 14  32 -24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1963068 1963068\n",
      "getting cut index 36\n",
      "debug removing 0 outlier pixels from 1403\n",
      "debug box delta [26 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1241941 1241941\n",
      "getting cut index 37\n",
      "debug removing 0 outlier pixels from 1417\n",
      "debug box delta [19 32 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1298886 1298886\n",
      "getting cut index 38\n",
      "debug removing 0 outlier pixels from 1567\n",
      "debug box delta [21 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1731533 1731533\n",
      "getting cut index 39\n",
      "debug removing 15 outlier pixels from 1286\n",
      "debug box delta [24 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1067335 1067335\n",
      "getting cut index 40\n",
      "debug removing 0 outlier pixels from 1328\n",
      "debug box delta [27 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1311715 1311715\n",
      "getting cut index 41\n",
      "debug removing 0 outlier pixels from 1559\n",
      "debug box delta [18 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1411792 1411792\n",
      "getting cut index 42\n",
      "debug removing 10 outlier pixels from 1102\n",
      "debug box delta [27 48 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 951804 951804\n",
      "getting cut index 43\n",
      "debug removing 0 outlier pixels from 1143\n",
      "debug box delta [32 48 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1024831 1024831\n",
      "getting cut index 44\n",
      "debug removing 0 outlier pixels from 1734\n",
      "debug box delta [13 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1763923 1763923\n",
      "getting cut index 45\n",
      "debug removing 0 outlier pixels from 1156\n",
      "debug box delta [27 16 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1115633 1115633\n",
      "getting cut index 46\n",
      "debug removing 0 outlier pixels from 1657\n",
      "debug box delta [21 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1670156 1670156\n",
      "getting cut index 47\n",
      "debug removing 0 outlier pixels from 1436\n",
      "debug box delta [20 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1413179 1413179\n",
      "getting cut index 48\n",
      "debug removing 0 outlier pixels from 1002\n",
      "debug box delta [28 64 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 781269 781269\n",
      "getting cut index 49\n",
      "debug removing 0 outlier pixels from 1705\n",
      "debug box delta [20 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1756965 1756965\n"
     ]
    }
   ],
   "source": [
    "datasets_params = ['train_dataset', 'valid_dataset', 'test_dataset']\n",
    "filter_labels = OARS_LABELS.OARS_LABELS_LIST\n",
    "if OARS_LABELS.SPINAL_CORD in filter_labels:\n",
    "    filter_labels.remove(OARS_LABELS.SPINAL_CORD)\n",
    "\n",
    "# low res\n",
    "low_res_dataset = get_dataset(dataset_size=50, shrink_factor=16, filter_labels=filter_labels, unify_labels=True)\n",
    "low_res_dataset.dilatate_labels(repeat=1)\n",
    "low_res_dataset.to_numpy()\n",
    "low_res_split_dataset_obj = split_dataset(low_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "train_low_res_dataset, valid_low_res_dataset, test_low_res_dataset = itemgetter(*datasets_params)(low_res_split_dataset_obj)\n",
    "\n",
    "# full res\n",
    "full_res_dataset = get_dataset(dataset_size=50, shrink_factor=1, filter_labels=filter_labels, unify_labels=False)\n",
    "full_res_dataset.to_numpy()\n",
    "full_res_split_dataset_obj = copy_split_dataset(full_res_dataset, low_res_split_dataset_obj)\n",
    "\n",
    "# low res model - precourse model\n",
    "epoch = 500\n",
    "log_date = datetime.datetime(year=2020, month=10, day=27, hour=11, minute=45, second=30).strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f'{log_date}_3d_unet_PRECOURSE'\n",
    "\n",
    "low_res_model_info = load_checkpoint_model_info(model_name, epoch, train_low_res_dataset, valid_low_res_dataset, test_low_res_dataset)\n",
    "show_model_info(low_res_model_info)\n",
    "low_res_model_info['model'] = low_res_model_info['model'].to('cpu')\n",
    "low_res_model_info['model'].eval()\n",
    "low_res_model = low_res_model_info['model']\n",
    "\n",
    "# cut res\n",
    "cut_full_res_dataset = full_res_dataset.copy(copy_lists=False)\n",
    "cut_full_res_dataset = get_cut_lists(low_res_model, low_res_dataset, full_res_dataset, cut_full_res_dataset, low_res_mask_threshold=0.5)\n",
    "cut_full_res_dataset.set_output_label(None)\n",
    "cut_split_dataset_obj = copy_split_dataset(cut_full_res_dataset, low_res_split_dataset_obj)\n",
    "cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*datasets_params)(cut_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "get_dataset_info(low_res_dataset, low_res_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device running \"cuda\"\n",
      "max output channels 128\n",
      "Model number of params: 1193537, trainable 1193537\n"
     ]
    }
   ],
   "source": [
    "# high res model - eyes_model\n",
    "epoch = 100\n",
    "log_date_dict = {\n",
    "    \"year\": 2020, \n",
    "    \"month\": 10, \n",
    "    \"day\": 27, \n",
    "    \"hour\": 14, \n",
    "    \"minute\": 14, \n",
    "    \"second\": 51\n",
    "}\n",
    "log_date = datetime.datetime(**log_date_dict).strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f'{log_date}_3d_unet_EYES'\n",
    "\n",
    "cut_model_info = load_checkpoint_model_info(model_name, epoch, cut_train_dataset, cut_valid_dataset, cut_test_dataset)\n",
    "show_model_info(cut_model_info)\n",
    "cut_model_info['model'] = cut_model_info['model'].to('cpu')\n",
    "cut_model_info['model'].eval()\n",
    "cut_model = cut_model_info['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 3071, min -1024\n",
      "label max 22, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f34dc81e41a45d7a14e2de8f9e0b5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c51aa78f13a47448ee428e0c1288b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_dataset(cut_full_res_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training all organs models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset label 'PITUITARY', \t value '11'\n",
      "dataset label 'PAROTID_GLAND_L', \t value '12'\n",
      "dataset label 'PAROTID_GLAND_R', \t value '13'\n",
      "dataset label 'INNER_EAR_L', \t value '14'\n",
      "dataset label 'INNER_EAR_R', \t value '15'\n",
      "dataset label 'MID_EAR_L', \t value '16'\n",
      "dataset label 'MID_EAR_R', \t value '17'\n",
      "dataset label 'T_M_JOINT_L', \t value '18'\n",
      "dataset label 'T_M_JOINT_R', \t value '19'\n",
      "dataset label 'MANDIBLE_L', \t value '21'\n",
      "dataset label 'MANDIBLE_R', \t value '22'\n"
     ]
    }
   ],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels['SPINAL_CORD']\n",
    "\n",
    "\n",
    "for OAR_KEY, OAR_VALUE in list(filter_labels.items())[10:]:\n",
    "    cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "    print(f'dataset label \\'{OAR_KEY}\\', \\t value \\'{OAR_VALUE}\\'')\n",
    "    \n",
    "    # Preview one image form dataset  \n",
    "#     tmp_data, tmp_label = cut_train_dataset[0]\n",
    "#     tmp_idx = np.where(tmp_label > 0)[0]\n",
    "#     tmp_slice = tmp_idx[int(np.median(tmp_idx))]\n",
    "#     tmp_slice = 46\n",
    "\n",
    "#     plt.figure(figsize=(16, 16))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.imshow(tmp_data[0, tmp_slice], cmap=\"gray\")\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.imshow(tmp_label[tmp_slice])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with dataset label 'PITUITARY', value '11'\n",
      "folder '20201103-105237_3d_unet_PITUITARY'\n",
      "Device running \"cuda\"\n",
      "max output channels 128\n",
      "Model number of params: 1193537, trainable 1193537\n",
      "\n",
      "\n",
      "\n",
      "Running training loop\n",
      "Batch train [1] loss 0.99993, dsc 0.00007\n",
      "Batch train [2] loss 0.99984, dsc 0.00016\n",
      "Batch train [3] loss 0.99973, dsc 0.00027\n",
      "Batch train [4] loss 0.99987, dsc 0.00013\n",
      "Batch train [5] loss 0.99976, dsc 0.00024\n",
      "Batch train [6] loss 0.99990, dsc 0.00010\n",
      "Batch train [7] loss 0.99971, dsc 0.00029\n",
      "Batch train [8] loss 0.99990, dsc 0.00010\n",
      "Batch train [9] loss 0.99981, dsc 0.00019\n",
      "Batch train [10] loss 0.99939, dsc 0.00061\n",
      "Batch train [11] loss 0.99976, dsc 0.00024\n",
      "Batch train [12] loss 0.99994, dsc 0.00006\n",
      "Batch train [13] loss 0.99975, dsc 0.00025\n",
      "Batch train [14] loss 0.99964, dsc 0.00036\n",
      "Batch train [15] loss 0.99967, dsc 0.00033\n",
      "Batch train [16] loss 0.99968, dsc 0.00032\n",
      "Batch train [17] loss 0.99962, dsc 0.00038\n",
      "Batch train [18] loss 0.99942, dsc 0.00058\n",
      "Batch train [19] loss 0.99961, dsc 0.00039\n",
      "Batch train [20] loss 0.99966, dsc 0.00034\n",
      "Epoch [1] train done\n",
      "Batch eval [1] loss 0.99995, dsc 0.00005\n",
      "Batch eval [2] loss 0.99997, dsc 0.00003\n",
      "Batch eval [3] loss 0.99990, dsc 0.00010\n",
      "Batch eval [4] loss 0.99982, dsc 0.00018\n",
      "Batch eval [5] loss 0.99987, dsc 0.00013\n",
      "Epoch [1] valid done\n",
      "Epoch [1] T 62.29s, deltaT 62.29s, loss: train 0.99973, valid 0.99990, dsc: train 0.00027, valid 0.00010\n",
      "Batch train [1] loss 0.99954, dsc 0.00046\n",
      "Batch train [2] loss 0.99959, dsc 0.00041\n",
      "Batch train [3] loss 0.99977, dsc 0.00023\n",
      "Batch train [4] loss 0.99958, dsc 0.00042\n",
      "Batch train [5] loss 0.99960, dsc 0.00040\n",
      "Batch train [6] loss 0.99971, dsc 0.00029\n",
      "Batch train [7] loss 0.99971, dsc 0.00029\n",
      "Batch train [8] loss 0.99957, dsc 0.00043\n",
      "Batch train [9] loss 0.99980, dsc 0.00020\n",
      "Batch train [10] loss 0.99925, dsc 0.00075\n",
      "Batch train [11] loss 0.99955, dsc 0.00045\n",
      "Batch train [12] loss 0.99975, dsc 0.00025\n",
      "Batch train [13] loss 0.99981, dsc 0.00019\n",
      "Batch train [14] loss 0.99979, dsc 0.00021\n",
      "Batch train [15] loss 0.99935, dsc 0.00065\n",
      "Batch train [16] loss 0.99984, dsc 0.00016\n",
      "Batch train [17] loss 0.99962, dsc 0.00038\n",
      "Batch train [18] loss 0.99972, dsc 0.00028\n",
      "Batch train [19] loss 0.99981, dsc 0.00019\n",
      "Batch train [20] loss 0.99958, dsc 0.00042\n",
      "Epoch [2] train done\n",
      "Batch eval [1] loss 0.99988, dsc 0.00012\n",
      "Batch eval [2] loss 0.99996, dsc 0.00004\n",
      "Batch eval [3] loss 0.99990, dsc 0.00010\n",
      "Batch eval [4] loss 0.99967, dsc 0.00033\n",
      "Batch eval [5] loss 0.99978, dsc 0.00022\n",
      "Epoch [2] valid done\n",
      "Epoch [2] T 126.31s, deltaT 64.02s, loss: train 0.99965, valid 0.99984, dsc: train 0.00035, valid 0.00016\n",
      "Batch train [1] loss 0.99964, dsc 0.00036\n",
      "Batch train [2] loss 0.99955, dsc 0.00045\n",
      "Batch train [3] loss 0.99985, dsc 0.00015\n",
      "Batch train [4] loss 0.99967, dsc 0.00033\n",
      "Batch train [5] loss 0.99982, dsc 0.00018\n",
      "Batch train [6] loss 0.99971, dsc 0.00029\n",
      "Batch train [7] loss 0.99927, dsc 0.00073\n",
      "Batch train [8] loss 0.99946, dsc 0.00054\n",
      "Batch train [9] loss 0.99964, dsc 0.00036\n",
      "Batch train [10] loss 0.99976, dsc 0.00024\n",
      "Batch train [11] loss 0.99965, dsc 0.00035\n",
      "Batch train [12] loss 0.99977, dsc 0.00023\n",
      "Batch train [13] loss 0.99974, dsc 0.00026\n",
      "Batch train [14] loss 0.99955, dsc 0.00045\n",
      "Batch train [15] loss 0.99983, dsc 0.00017\n",
      "Batch train [16] loss 0.99966, dsc 0.00034\n",
      "Batch train [17] loss 0.99960, dsc 0.00040\n",
      "Batch train [18] loss 0.99965, dsc 0.00035\n",
      "Batch train [19] loss 0.99930, dsc 0.00070\n",
      "Batch train [20] loss 0.99957, dsc 0.00043\n",
      "Epoch [3] train done\n",
      "Batch eval [1] loss 0.99989, dsc 0.00011\n",
      "Batch eval [2] loss 0.99993, dsc 0.00007\n",
      "Batch eval [3] loss 0.99977, dsc 0.00023\n",
      "Batch eval [4] loss 0.99976, dsc 0.00024\n",
      "Batch eval [5] loss 0.99987, dsc 0.00013\n",
      "Epoch [3] valid done\n",
      "Epoch [3] T 190.03s, deltaT 63.72s, loss: train 0.99963, valid 0.99985, dsc: train 0.00037, valid 0.00015\n",
      "Batch train [1] loss 0.99972, dsc 0.00028\n",
      "Batch train [2] loss 0.99961, dsc 0.00039\n",
      "Batch train [3] loss 0.99979, dsc 0.00021\n",
      "Batch train [4] loss 0.99912, dsc 0.00088\n",
      "Batch train [5] loss 0.99971, dsc 0.00029\n",
      "Batch train [6] loss 0.99972, dsc 0.00028\n",
      "Batch train [7] loss 0.99966, dsc 0.00034\n",
      "Batch train [8] loss 0.99982, dsc 0.00018\n",
      "Batch train [9] loss 0.99956, dsc 0.00044\n",
      "Batch train [10] loss 0.99972, dsc 0.00028\n",
      "Batch train [11] loss 0.99943, dsc 0.00057\n",
      "Batch train [12] loss 0.99960, dsc 0.00040\n",
      "Batch train [13] loss 0.99983, dsc 0.00017\n",
      "Batch train [14] loss 0.99980, dsc 0.00020\n",
      "Batch train [15] loss 0.99917, dsc 0.00083\n",
      "Batch train [16] loss 0.99967, dsc 0.00033\n",
      "Batch train [17] loss 0.99961, dsc 0.00039\n",
      "Batch train [18] loss 0.99955, dsc 0.00045\n",
      "Batch train [19] loss 0.99968, dsc 0.00032\n",
      "Batch train [20] loss 0.99947, dsc 0.00053\n",
      "Epoch [4] train done\n",
      "Batch eval [1] loss 0.99988, dsc 0.00012\n",
      "Batch eval [2] loss 0.99992, dsc 0.00008\n",
      "Batch eval [3] loss 0.99972, dsc 0.00028\n",
      "Batch eval [4] loss 0.99959, dsc 0.00041\n",
      "Batch eval [5] loss 0.99971, dsc 0.00029\n",
      "Epoch [4] valid done\n",
      "Epoch [4] T 254.97s, deltaT 64.94s, loss: train 0.99961, valid 0.99976, dsc: train 0.00039, valid 0.00024\n",
      "Batch train [1] loss 0.99978, dsc 0.00022\n",
      "Batch train [2] loss 0.99968, dsc 0.00032\n",
      "Batch train [3] loss 0.99959, dsc 0.00041\n",
      "Batch train [4] loss 0.99974, dsc 0.00026\n",
      "Batch train [5] loss 0.99983, dsc 0.00017\n",
      "Batch train [6] loss 0.99962, dsc 0.00038\n",
      "Batch train [7] loss 0.99979, dsc 0.00021\n",
      "Batch train [8] loss 0.99976, dsc 0.00024\n",
      "Batch train [9] loss 0.99977, dsc 0.00023\n",
      "Batch train [10] loss 0.99947, dsc 0.00053\n",
      "Batch train [11] loss 0.99925, dsc 0.00075\n",
      "Batch train [12] loss 0.99936, dsc 0.00064\n",
      "Batch train [13] loss 0.99967, dsc 0.00033\n",
      "Batch train [14] loss 0.99969, dsc 0.00031\n",
      "Batch train [15] loss 0.99926, dsc 0.00074\n",
      "Batch train [16] loss 0.99905, dsc 0.00095\n",
      "Batch train [17] loss 0.99984, dsc 0.00016\n",
      "Batch train [18] loss 0.99971, dsc 0.00029\n",
      "Batch train [19] loss 0.99947, dsc 0.00053\n",
      "Batch train [20] loss 0.99961, dsc 0.00039\n",
      "Epoch [5] train done\n",
      "Batch eval [1] loss 0.99987, dsc 0.00013\n",
      "Batch eval [2] loss 0.99992, dsc 0.00008\n",
      "Batch eval [3] loss 0.99971, dsc 0.00029\n",
      "Batch eval [4] loss 0.99958, dsc 0.00042\n",
      "Batch eval [5] loss 0.99970, dsc 0.00030\n",
      "Epoch [5] valid done\n",
      "Epoch [5] T 319.15s, deltaT 64.17s, loss: train 0.99960, valid 0.99975, dsc: train 0.00040, valid 0.00025\n",
      "Batch train [1] loss 0.99963, dsc 0.00037\n",
      "Batch train [2] loss 0.99962, dsc 0.00038\n",
      "Batch train [3] loss 0.99939, dsc 0.00061\n",
      "Batch train [4] loss 0.99946, dsc 0.00054\n",
      "Batch train [5] loss 0.99984, dsc 0.00016\n",
      "Batch train [6] loss 0.99972, dsc 0.00028\n",
      "Batch train [7] loss 0.99906, dsc 0.00094\n",
      "Batch train [8] loss 0.99963, dsc 0.00037\n",
      "Batch train [9] loss 0.99924, dsc 0.00076\n",
      "Batch train [10] loss 0.99935, dsc 0.00065\n",
      "Batch train [11] loss 0.99972, dsc 0.00028\n",
      "Batch train [12] loss 0.99965, dsc 0.00035\n",
      "Batch train [13] loss 0.99986, dsc 0.00014\n",
      "Batch train [14] loss 0.99969, dsc 0.00031\n",
      "Batch train [15] loss 0.99970, dsc 0.00030\n",
      "Batch train [16] loss 0.99982, dsc 0.00018\n",
      "Batch train [17] loss 0.99950, dsc 0.00050\n",
      "Batch train [18] loss 0.99959, dsc 0.00041\n",
      "Batch train [19] loss 0.99963, dsc 0.00037\n",
      "Batch train [20] loss 0.99960, dsc 0.00040\n",
      "Epoch [6] train done\n",
      "Batch eval [1] loss 0.99987, dsc 0.00013\n",
      "Batch eval [2] loss 0.99991, dsc 0.00009\n",
      "Batch eval [3] loss 0.99970, dsc 0.00030\n",
      "Batch eval [4] loss 0.99956, dsc 0.00044\n",
      "Batch eval [5] loss 0.99969, dsc 0.00031\n",
      "Epoch [6] valid done\n",
      "Epoch [6] T 384.44s, deltaT 65.29s, loss: train 0.99958, valid 0.99975, dsc: train 0.00042, valid 0.00025\n",
      "Batch train [1] loss 0.99960, dsc 0.00040\n",
      "Batch train [2] loss 0.99912, dsc 0.00088\n",
      "Batch train [3] loss 0.99944, dsc 0.00056\n",
      "Batch train [4] loss 0.99955, dsc 0.00045\n",
      "Batch train [5] loss 0.99970, dsc 0.00030\n",
      "Batch train [6] loss 0.99951, dsc 0.00049\n",
      "Batch train [7] loss 0.99944, dsc 0.00056\n",
      "Batch train [8] loss 0.99961, dsc 0.00039\n",
      "Batch train [9] loss 0.99978, dsc 0.00022\n",
      "Batch train [10] loss 0.99953, dsc 0.00047\n",
      "Batch train [11] loss 0.99981, dsc 0.00019\n",
      "Batch train [12] loss 0.99922, dsc 0.00078\n",
      "Batch train [13] loss 0.99969, dsc 0.00031\n",
      "Batch train [14] loss 0.99960, dsc 0.00040\n",
      "Batch train [15] loss 0.99974, dsc 0.00026\n",
      "Batch train [16] loss 0.99951, dsc 0.00049\n",
      "Batch train [17] loss 0.99986, dsc 0.00014\n",
      "Batch train [18] loss 0.99953, dsc 0.00047\n",
      "Batch train [19] loss 0.99961, dsc 0.00039\n",
      "Batch train [20] loss 0.99950, dsc 0.00050\n",
      "Epoch [7] train done\n",
      "Batch eval [1] loss 0.99986, dsc 0.00014\n",
      "Batch eval [2] loss 0.99991, dsc 0.00009\n",
      "Batch eval [3] loss 0.99969, dsc 0.00031\n",
      "Batch eval [4] loss 0.99955, dsc 0.00045\n",
      "Batch eval [5] loss 0.99968, dsc 0.00032\n",
      "Epoch [7] valid done\n",
      "Epoch [7] T 448.39s, deltaT 63.95s, loss: train 0.99957, valid 0.99974, dsc: train 0.00043, valid 0.00026\n",
      "Batch train [1] loss 0.99964, dsc 0.00036\n",
      "Batch train [2] loss 0.99950, dsc 0.00050\n",
      "Batch train [3] loss 0.99981, dsc 0.00019\n",
      "Batch train [4] loss 0.99980, dsc 0.00020\n",
      "Batch train [5] loss 0.99918, dsc 0.00082\n",
      "Batch train [6] loss 0.99980, dsc 0.00020\n",
      "Batch train [7] loss 0.99961, dsc 0.00039\n",
      "Batch train [8] loss 0.99908, dsc 0.00092\n",
      "Batch train [9] loss 0.99963, dsc 0.00037\n",
      "Batch train [10] loss 0.99963, dsc 0.00037\n",
      "Batch train [11] loss 0.99945, dsc 0.00055\n",
      "Batch train [12] loss 0.99957, dsc 0.00043\n",
      "Batch train [13] loss 0.99975, dsc 0.00025\n",
      "Batch train [14] loss 0.99984, dsc 0.00016\n",
      "Batch train [15] loss 0.99945, dsc 0.00055\n",
      "Batch train [16] loss 0.99937, dsc 0.00063\n",
      "Batch train [17] loss 0.99941, dsc 0.00059\n",
      "Batch train [18] loss 0.99961, dsc 0.00039\n",
      "Batch train [19] loss 0.99957, dsc 0.00043\n",
      "Batch train [20] loss 0.99939, dsc 0.00061\n",
      "Epoch [8] train done\n",
      "Batch eval [1] loss 0.99986, dsc 0.00014\n",
      "Batch eval [2] loss 0.99991, dsc 0.00009\n",
      "Batch eval [3] loss 0.99969, dsc 0.00031\n",
      "Batch eval [4] loss 0.99953, dsc 0.00047\n",
      "Batch eval [5] loss 0.99967, dsc 0.00033\n",
      "Epoch [8] valid done\n",
      "Epoch [8] T 512.93s, deltaT 64.54s, loss: train 0.99955, valid 0.99973, dsc: train 0.00045, valid 0.00027\n",
      "Batch train [1] loss 0.99941, dsc 0.00059\n",
      "Batch train [2] loss 0.99915, dsc 0.00085\n",
      "Batch train [3] loss 0.99978, dsc 0.00022\n",
      "Batch train [4] loss 0.99945, dsc 0.00055\n",
      "Batch train [5] loss 0.99953, dsc 0.00047\n",
      "Batch train [6] loss 0.99988, dsc 0.00012\n",
      "Batch train [7] loss 0.99940, dsc 0.00060\n",
      "Batch train [8] loss 0.99952, dsc 0.00048\n",
      "Batch train [9] loss 0.99936, dsc 0.00064\n",
      "Batch train [10] loss 0.99942, dsc 0.00058\n",
      "Batch train [11] loss 0.99925, dsc 0.00075\n",
      "Batch train [12] loss 0.99931, dsc 0.00069\n",
      "Batch train [13] loss 0.99960, dsc 0.00040\n",
      "Batch train [14] loss 0.99959, dsc 0.00041\n",
      "Batch train [15] loss 0.99963, dsc 0.00037\n",
      "Batch train [16] loss 0.99979, dsc 0.00021\n",
      "Batch train [17] loss 0.99961, dsc 0.00039\n",
      "Batch train [18] loss 0.99960, dsc 0.00040\n",
      "Batch train [19] loss 0.99962, dsc 0.00038\n",
      "Batch train [20] loss 0.99988, dsc 0.00012\n",
      "Epoch [9] train done\n",
      "Batch eval [1] loss 0.99985, dsc 0.00015\n",
      "Batch eval [2] loss 0.99991, dsc 0.00009\n",
      "Batch eval [3] loss 0.99968, dsc 0.00032\n",
      "Batch eval [4] loss 0.99952, dsc 0.00048\n",
      "Batch eval [5] loss 0.99966, dsc 0.00034\n",
      "Epoch [9] valid done\n",
      "Epoch [9] T 578.76s, deltaT 65.83s, loss: train 0.99954, valid 0.99972, dsc: train 0.00046, valid 0.00028\n",
      "Batch train [1] loss 0.99981, dsc 0.00019\n",
      "Batch train [2] loss 0.99982, dsc 0.00018\n",
      "Batch train [3] loss 0.99921, dsc 0.00079\n",
      "Batch train [4] loss 0.99931, dsc 0.00069\n",
      "Batch train [5] loss 0.99964, dsc 0.00036\n",
      "Batch train [6] loss 0.99970, dsc 0.00030\n",
      "Batch train [7] loss 0.99947, dsc 0.00053\n",
      "Batch train [8] loss 0.99965, dsc 0.00035\n",
      "Batch train [9] loss 0.99961, dsc 0.00039\n",
      "Batch train [10] loss 0.99889, dsc 0.00111\n",
      "Batch train [11] loss 0.99899, dsc 0.00101\n",
      "Batch train [12] loss 0.99968, dsc 0.00032\n",
      "Batch train [13] loss 0.99990, dsc 0.00010\n",
      "Batch train [14] loss 0.99976, dsc 0.00024\n",
      "Batch train [15] loss 0.99969, dsc 0.00031\n",
      "Batch train [16] loss 0.99954, dsc 0.00046\n",
      "Batch train [17] loss 0.99959, dsc 0.00041\n",
      "Batch train [18] loss 0.99927, dsc 0.00073\n",
      "Batch train [19] loss 0.99960, dsc 0.00040\n",
      "Batch train [20] loss 0.99936, dsc 0.00064\n",
      "Epoch [10] train done\n",
      "Batch eval [1] loss 0.99985, dsc 0.00015\n",
      "Batch eval [2] loss 0.99990, dsc 0.00010\n",
      "Batch eval [3] loss 0.99966, dsc 0.00034\n",
      "Batch eval [4] loss 0.99950, dsc 0.00050\n",
      "Batch eval [5] loss 0.99965, dsc 0.00035\n",
      "Epoch [10] valid done\n",
      "Epoch [10] T 645.39s, deltaT 66.63s, loss: train 0.99952, valid 0.99971, dsc: train 0.00048, valid 0.00029\n",
      "Batch train [1] loss 0.99940, dsc 0.00060\n",
      "Batch train [2] loss 0.99952, dsc 0.00048\n",
      "Batch train [3] loss 0.99914, dsc 0.00086\n",
      "Batch train [4] loss 0.99962, dsc 0.00038\n",
      "Batch train [5] loss 0.99950, dsc 0.00050\n",
      "Batch train [6] loss 0.99940, dsc 0.00060\n",
      "Batch train [7] loss 0.99960, dsc 0.00040\n",
      "Batch train [8] loss 0.99930, dsc 0.00070\n",
      "Batch train [9] loss 0.99979, dsc 0.00021\n",
      "Batch train [10] loss 0.99978, dsc 0.00022\n",
      "Batch train [11] loss 0.99987, dsc 0.00013\n",
      "Batch train [12] loss 0.99961, dsc 0.00039\n",
      "Batch train [13] loss 0.99953, dsc 0.00047\n",
      "Batch train [14] loss 0.99974, dsc 0.00026\n",
      "Batch train [15] loss 0.99920, dsc 0.00080\n",
      "Batch train [16] loss 0.99889, dsc 0.00111\n",
      "Batch train [17] loss 0.99956, dsc 0.00044\n",
      "Batch train [18] loss 0.99955, dsc 0.00045\n",
      "Batch train [19] loss 0.99945, dsc 0.00055\n",
      "Batch train [20] loss 0.99966, dsc 0.00034\n",
      "Epoch [11] train done\n",
      "Batch eval [1] loss 0.99985, dsc 0.00015\n",
      "Batch eval [2] loss 0.99990, dsc 0.00010\n",
      "Batch eval [3] loss 0.99965, dsc 0.00035\n",
      "Batch eval [4] loss 0.99949, dsc 0.00051\n",
      "Batch eval [5] loss 0.99964, dsc 0.00036\n",
      "Epoch [11] valid done\n",
      "Epoch [11] T 710.54s, deltaT 65.15s, loss: train 0.99950, valid 0.99970, dsc: train 0.00050, valid 0.00030\n",
      "Batch train [1] loss 0.99980, dsc 0.00020\n",
      "Batch train [2] loss 0.99967, dsc 0.00033\n",
      "Batch train [3] loss 0.99942, dsc 0.00058\n",
      "Batch train [4] loss 0.99957, dsc 0.00043\n",
      "Batch train [5] loss 0.99977, dsc 0.00023\n",
      "Batch train [6] loss 0.99971, dsc 0.00029\n",
      "Batch train [7] loss 0.99953, dsc 0.00047\n",
      "Batch train [8] loss 0.99935, dsc 0.00065\n",
      "Batch train [9] loss 0.99903, dsc 0.00097\n",
      "Batch train [10] loss 0.99942, dsc 0.00058\n",
      "Batch train [11] loss 0.99965, dsc 0.00035\n",
      "Batch train [12] loss 0.99986, dsc 0.00014\n",
      "Batch train [13] loss 0.99961, dsc 0.00039\n",
      "Batch train [14] loss 0.99974, dsc 0.00026\n",
      "Batch train [15] loss 0.99923, dsc 0.00077\n",
      "Batch train [16] loss 0.99963, dsc 0.00037\n",
      "Batch train [17] loss 0.99935, dsc 0.00065\n",
      "Batch train [18] loss 0.99925, dsc 0.00075\n",
      "Batch train [19] loss 0.99845, dsc 0.00155\n",
      "Batch train [20] loss 0.99963, dsc 0.00037\n",
      "Epoch [12] train done\n",
      "Batch eval [1] loss 0.99984, dsc 0.00016\n",
      "Batch eval [2] loss 0.99989, dsc 0.00011\n",
      "Batch eval [3] loss 0.99963, dsc 0.00037\n",
      "Batch eval [4] loss 0.99946, dsc 0.00054\n",
      "Batch eval [5] loss 0.99962, dsc 0.00038\n",
      "Epoch [12] valid done\n",
      "Epoch [12] T 775.18s, deltaT 64.64s, loss: train 0.99948, valid 0.99969, dsc: train 0.00052, valid 0.00031\n",
      "Batch train [1] loss 0.99970, dsc 0.00030\n",
      "Batch train [2] loss 0.99967, dsc 0.00033\n",
      "Batch train [3] loss 0.99931, dsc 0.00069\n",
      "Batch train [4] loss 0.99925, dsc 0.00075\n",
      "Batch train [5] loss 0.99970, dsc 0.00030\n",
      "Batch train [6] loss 0.99921, dsc 0.00079\n",
      "Batch train [7] loss 0.99952, dsc 0.00048\n",
      "Batch train [8] loss 0.99842, dsc 0.00158\n",
      "Batch train [9] loss 0.99933, dsc 0.00067\n",
      "Batch train [10] loss 0.99972, dsc 0.00028\n",
      "Batch train [11] loss 0.99978, dsc 0.00022\n",
      "Batch train [12] loss 0.99944, dsc 0.00056\n",
      "Batch train [13] loss 0.99934, dsc 0.00066\n",
      "Batch train [14] loss 0.99953, dsc 0.00047\n",
      "Batch train [15] loss 0.99933, dsc 0.00067\n",
      "Batch train [16] loss 0.99973, dsc 0.00027\n",
      "Batch train [17] loss 0.99949, dsc 0.00051\n",
      "Batch train [18] loss 0.99954, dsc 0.00046\n",
      "Batch train [19] loss 0.99986, dsc 0.00014\n",
      "Batch train [20] loss 0.99942, dsc 0.00058\n",
      "Epoch [13] train done\n",
      "Batch eval [1] loss 0.99984, dsc 0.00016\n",
      "Batch eval [2] loss 0.99989, dsc 0.00011\n",
      "Batch eval [3] loss 0.99962, dsc 0.00038\n",
      "Batch eval [4] loss 0.99946, dsc 0.00054\n",
      "Batch eval [5] loss 0.99961, dsc 0.00039\n",
      "Epoch [13] valid done\n",
      "Epoch [13] T 839.99s, deltaT 64.81s, loss: train 0.99946, valid 0.99969, dsc: train 0.00054, valid 0.00031\n",
      "Batch train [1] loss 0.99899, dsc 0.00101\n",
      "Batch train [2] loss 0.99943, dsc 0.00057\n",
      "Batch train [3] loss 0.99977, dsc 0.00023\n",
      "Batch train [4] loss 0.99931, dsc 0.00069\n",
      "Batch train [5] loss 0.99885, dsc 0.00115\n",
      "Batch train [6] loss 0.99924, dsc 0.00076\n",
      "Batch train [7] loss 0.99951, dsc 0.00049\n",
      "Batch train [8] loss 0.99963, dsc 0.00037\n",
      "Batch train [9] loss 0.99953, dsc 0.00047\n",
      "Batch train [10] loss 0.99953, dsc 0.00047\n",
      "Batch train [11] loss 0.99953, dsc 0.00047\n",
      "Batch train [12] loss 0.99980, dsc 0.00020\n",
      "Batch train [13] loss 0.99942, dsc 0.00058\n",
      "Batch train [14] loss 0.99959, dsc 0.00041\n",
      "Batch train [15] loss 0.99962, dsc 0.00038\n",
      "Batch train [16] loss 0.99940, dsc 0.00060\n",
      "Batch train [17] loss 0.99986, dsc 0.00014\n",
      "Batch train [18] loss 0.99948, dsc 0.00052\n",
      "Batch train [19] loss 0.99938, dsc 0.00062\n",
      "Batch train [20] loss 0.99893, dsc 0.00107\n",
      "Epoch [14] train done\n",
      "Batch eval [1] loss 0.99983, dsc 0.00017\n",
      "Batch eval [2] loss 0.99989, dsc 0.00011\n",
      "Batch eval [3] loss 0.99961, dsc 0.00039\n",
      "Batch eval [4] loss 0.99943, dsc 0.00057\n",
      "Batch eval [5] loss 0.99959, dsc 0.00041\n",
      "Epoch [14] valid done\n",
      "Epoch [14] T 906.77s, deltaT 66.77s, loss: train 0.99944, valid 0.99967, dsc: train 0.00056, valid 0.00033\n",
      "Batch train [1] loss 0.99919, dsc 0.00081\n",
      "Batch train [2] loss 0.99916, dsc 0.00084\n",
      "Batch train [3] loss 0.99972, dsc 0.00028\n",
      "Batch train [4] loss 0.99951, dsc 0.00049\n",
      "Batch train [5] loss 0.99919, dsc 0.00081\n",
      "Batch train [6] loss 0.99929, dsc 0.00071\n",
      "Batch train [7] loss 0.99974, dsc 0.00026\n",
      "Batch train [8] loss 0.99941, dsc 0.00059\n",
      "Batch train [9] loss 0.99960, dsc 0.00040\n",
      "Batch train [10] loss 0.99941, dsc 0.00059\n",
      "Batch train [11] loss 0.99974, dsc 0.00026\n",
      "Batch train [12] loss 0.99933, dsc 0.00067\n",
      "Batch train [13] loss 0.99966, dsc 0.00034\n",
      "Batch train [14] loss 0.99962, dsc 0.00038\n",
      "Batch train [15] loss 0.99971, dsc 0.00029\n",
      "Batch train [16] loss 0.99937, dsc 0.00063\n",
      "Batch train [17] loss 0.99895, dsc 0.00105\n",
      "Batch train [18] loss 0.99942, dsc 0.00058\n",
      "Batch train [19] loss 0.99900, dsc 0.00100\n",
      "Batch train [20] loss 0.99957, dsc 0.00043\n",
      "Epoch [15] train done\n",
      "Batch eval [1] loss 0.99985, dsc 0.00015\n",
      "Batch eval [2] loss 0.99989, dsc 0.00011\n",
      "Batch eval [3] loss 0.99964, dsc 0.00036\n",
      "Batch eval [4] loss 0.99947, dsc 0.00053\n",
      "Batch eval [5] loss 0.99962, dsc 0.00038\n",
      "Epoch [15] valid done\n",
      "Epoch [15] T 972.25s, deltaT 65.48s, loss: train 0.99943, valid 0.99969, dsc: train 0.00057, valid 0.00031\n",
      "Batch train [1] loss 0.99978, dsc 0.00022\n",
      "Batch train [2] loss 0.99938, dsc 0.00062\n",
      "Batch train [3] loss 0.99957, dsc 0.00043\n",
      "Batch train [4] loss 0.99950, dsc 0.00050\n",
      "Batch train [5] loss 0.99960, dsc 0.00040\n",
      "Batch train [6] loss 0.99967, dsc 0.00033\n",
      "Batch train [7] loss 0.99971, dsc 0.00029\n",
      "Batch train [8] loss 0.99931, dsc 0.00069\n",
      "Batch train [9] loss 0.99955, dsc 0.00045\n",
      "Batch train [10] loss 0.99949, dsc 0.00051\n",
      "Batch train [11] loss 0.99937, dsc 0.00063\n",
      "Batch train [12] loss 0.99936, dsc 0.00064\n",
      "Batch train [13] loss 0.99885, dsc 0.00115\n",
      "Batch train [14] loss 0.99928, dsc 0.00072\n",
      "Batch train [15] loss 0.99949, dsc 0.00051\n",
      "Batch train [16] loss 0.99940, dsc 0.00060\n",
      "Batch train [17] loss 0.99931, dsc 0.00069\n",
      "Batch train [18] loss 0.99904, dsc 0.00096\n",
      "Batch train [19] loss 0.99915, dsc 0.00085\n",
      "Batch train [20] loss 0.99954, dsc 0.00046\n",
      "Epoch [16] train done\n",
      "Batch eval [1] loss 0.99980, dsc 0.00020\n",
      "Batch eval [2] loss 0.99987, dsc 0.00013\n",
      "Batch eval [3] loss 0.99955, dsc 0.00045\n",
      "Batch eval [4] loss 0.99935, dsc 0.00065\n",
      "Batch eval [5] loss 0.99954, dsc 0.00046\n",
      "Epoch [16] valid done\n",
      "Epoch [16] T 1038.80s, deltaT 66.55s, loss: train 0.99942, valid 0.99962, dsc: train 0.00058, valid 0.00038\n",
      "Batch train [1] loss 0.99819, dsc 0.00181\n",
      "Batch train [2] loss 0.99960, dsc 0.00040\n",
      "Batch train [3] loss 0.99922, dsc 0.00078\n",
      "Batch train [4] loss 0.99964, dsc 0.00036\n",
      "Batch train [5] loss 0.99919, dsc 0.00081\n",
      "Batch train [6] loss 0.99922, dsc 0.00078\n",
      "Batch train [7] loss 0.99918, dsc 0.00082\n",
      "Batch train [8] loss 0.99938, dsc 0.00062\n",
      "Batch train [9] loss 0.99948, dsc 0.00052\n",
      "Batch train [10] loss 0.99954, dsc 0.00046\n",
      "Batch train [11] loss 0.99970, dsc 0.00030\n",
      "Batch train [12] loss 0.99954, dsc 0.00046\n",
      "Batch train [13] loss 0.99947, dsc 0.00053\n",
      "Batch train [14] loss 0.99932, dsc 0.00068\n",
      "Batch train [15] loss 0.99931, dsc 0.00069\n",
      "Batch train [16] loss 0.99967, dsc 0.00033\n",
      "Batch train [17] loss 0.99903, dsc 0.00097\n",
      "Batch train [18] loss 0.99962, dsc 0.00038\n",
      "Batch train [19] loss 0.99939, dsc 0.00061\n",
      "Batch train [20] loss 0.99970, dsc 0.00030\n",
      "Epoch [17] train done\n",
      "Batch eval [1] loss 0.99979, dsc 0.00021\n",
      "Batch eval [2] loss 0.99987, dsc 0.00013\n",
      "Batch eval [3] loss 0.99953, dsc 0.00047\n",
      "Batch eval [4] loss 0.99933, dsc 0.00067\n",
      "Batch eval [5] loss 0.99952, dsc 0.00048\n",
      "Epoch [17] valid done\n",
      "Epoch [17] T 1103.81s, deltaT 65.01s, loss: train 0.99937, valid 0.99961, dsc: train 0.00063, valid 0.00039\n",
      "Batch train [1] loss 0.99945, dsc 0.00055\n",
      "Batch train [2] loss 0.99937, dsc 0.00063\n",
      "Batch train [3] loss 0.99974, dsc 0.00026\n",
      "Batch train [4] loss 0.99928, dsc 0.00072\n",
      "Batch train [5] loss 0.99970, dsc 0.00030\n",
      "Batch train [6] loss 0.99905, dsc 0.00095\n",
      "Batch train [7] loss 0.99984, dsc 0.00016\n",
      "Batch train [8] loss 0.99944, dsc 0.00056\n",
      "Batch train [9] loss 0.99932, dsc 0.00068\n",
      "Batch train [10] loss 0.99935, dsc 0.00065\n",
      "Batch train [11] loss 0.99873, dsc 0.00127\n",
      "Batch train [12] loss 0.99973, dsc 0.00027\n",
      "Batch train [13] loss 0.99875, dsc 0.00125\n",
      "Batch train [14] loss 0.99920, dsc 0.00080\n",
      "Batch train [15] loss 0.99981, dsc 0.00019\n",
      "Batch train [16] loss 0.99925, dsc 0.00075\n",
      "Batch train [17] loss 0.99852, dsc 0.00148\n",
      "Batch train [18] loss 0.99955, dsc 0.00045\n",
      "Batch train [19] loss 0.99926, dsc 0.00074\n",
      "Batch train [20] loss 0.99911, dsc 0.00089\n",
      "Epoch [18] train done\n",
      "Batch eval [1] loss 0.99978, dsc 0.00022\n",
      "Batch eval [2] loss 0.99986, dsc 0.00014\n",
      "Batch eval [3] loss 0.99950, dsc 0.00050\n",
      "Batch eval [4] loss 0.99927, dsc 0.00073\n",
      "Batch eval [5] loss 0.99949, dsc 0.00051\n",
      "Epoch [18] valid done\n",
      "Epoch [18] T 1168.88s, deltaT 65.07s, loss: train 0.99932, valid 0.99958, dsc: train 0.00068, valid 0.00042\n",
      "Batch train [1] loss 0.99913, dsc 0.00087\n",
      "Batch train [2] loss 0.99918, dsc 0.00082\n",
      "Batch train [3] loss 0.99902, dsc 0.00098\n",
      "Batch train [4] loss 0.99976, dsc 0.00024\n",
      "Batch train [5] loss 0.99925, dsc 0.00075\n",
      "Batch train [6] loss 0.99881, dsc 0.00119\n",
      "Batch train [7] loss 0.99942, dsc 0.00058\n",
      "Batch train [8] loss 0.99962, dsc 0.00038\n",
      "Batch train [9] loss 0.99937, dsc 0.00063\n",
      "Batch train [10] loss 0.99948, dsc 0.00052\n",
      "Batch train [11] loss 0.99957, dsc 0.00043\n",
      "Batch train [12] loss 0.99934, dsc 0.00066\n",
      "Batch train [13] loss 0.99964, dsc 0.00036\n",
      "Batch train [14] loss 0.99927, dsc 0.00073\n",
      "Batch train [15] loss 0.99935, dsc 0.00065\n",
      "Batch train [16] loss 0.99974, dsc 0.00026\n",
      "Batch train [17] loss 0.99828, dsc 0.00172\n",
      "Batch train [18] loss 0.99922, dsc 0.00078\n",
      "Batch train [19] loss 0.99909, dsc 0.00091\n",
      "Batch train [20] loss 0.99906, dsc 0.00094\n",
      "Epoch [19] train done\n",
      "Batch eval [1] loss 0.99977, dsc 0.00023\n",
      "Batch eval [2] loss 0.99985, dsc 0.00015\n",
      "Batch eval [3] loss 0.99947, dsc 0.00053\n",
      "Batch eval [4] loss 0.99923, dsc 0.00077\n",
      "Batch eval [5] loss 0.99945, dsc 0.00055\n",
      "Epoch [19] valid done\n",
      "Epoch [19] T 1234.69s, deltaT 65.81s, loss: train 0.99928, valid 0.99956, dsc: train 0.00072, valid 0.00044\n",
      "Batch train [1] loss 0.99914, dsc 0.00086\n",
      "Batch train [2] loss 0.99843, dsc 0.00157\n",
      "Batch train [3] loss 0.99925, dsc 0.00075\n",
      "Batch train [4] loss 0.99951, dsc 0.00049\n",
      "Batch train [5] loss 0.99913, dsc 0.00087\n",
      "Batch train [6] loss 0.99942, dsc 0.00058\n",
      "Batch train [7] loss 0.99929, dsc 0.00071\n",
      "Batch train [8] loss 0.99916, dsc 0.00084\n",
      "Batch train [9] loss 0.99941, dsc 0.00059\n",
      "Batch train [10] loss 0.99944, dsc 0.00056\n",
      "Batch train [11] loss 0.99937, dsc 0.00063\n",
      "Batch train [12] loss 0.99954, dsc 0.00046\n",
      "Batch train [13] loss 0.99945, dsc 0.00055\n",
      "Batch train [14] loss 0.99950, dsc 0.00050\n",
      "Batch train [15] loss 0.99861, dsc 0.00139\n",
      "Batch train [16] loss 0.99972, dsc 0.00028\n",
      "Batch train [17] loss 0.99928, dsc 0.00072\n",
      "Batch train [18] loss 0.99965, dsc 0.00035\n",
      "Batch train [19] loss 0.99860, dsc 0.00140\n",
      "Batch train [20] loss 0.99875, dsc 0.00125\n",
      "Epoch [20] train done\n",
      "Batch eval [1] loss 0.99975, dsc 0.00025\n",
      "Batch eval [2] loss 0.99984, dsc 0.00016\n",
      "Batch eval [3] loss 0.99944, dsc 0.00056\n",
      "Batch eval [4] loss 0.99917, dsc 0.00083\n",
      "Batch eval [5] loss 0.99941, dsc 0.00059\n",
      "Epoch [20] valid done\n",
      "Epoch [20] T 1299.80s, deltaT 65.11s, loss: train 0.99923, valid 0.99952, dsc: train 0.00077, valid 0.00048\n",
      "Batch train [1] loss 0.99964, dsc 0.00036\n",
      "Batch train [2] loss 0.99931, dsc 0.00069\n",
      "Batch train [3] loss 0.99930, dsc 0.00070\n",
      "Batch train [4] loss 0.99935, dsc 0.00065\n",
      "Batch train [5] loss 0.99923, dsc 0.00077\n",
      "Batch train [6] loss 0.99882, dsc 0.00118\n",
      "Batch train [7] loss 0.99922, dsc 0.00078\n",
      "Batch train [8] loss 0.99944, dsc 0.00056\n",
      "Batch train [9] loss 0.99906, dsc 0.00094\n",
      "Batch train [10] loss 0.99927, dsc 0.00073\n",
      "Batch train [11] loss 0.99925, dsc 0.00075\n",
      "Batch train [12] loss 0.99827, dsc 0.00173\n",
      "Batch train [13] loss 0.99867, dsc 0.00133\n",
      "Batch train [14] loss 0.99883, dsc 0.00117\n",
      "Batch train [15] loss 0.99969, dsc 0.00031\n",
      "Batch train [16] loss 0.99895, dsc 0.00105\n",
      "Batch train [17] loss 0.99914, dsc 0.00086\n",
      "Batch train [18] loss 0.99928, dsc 0.00072\n",
      "Batch train [19] loss 0.99977, dsc 0.00023\n",
      "Batch train [20] loss 0.99907, dsc 0.00093\n",
      "Epoch [21] train done\n",
      "Batch eval [1] loss 0.99973, dsc 0.00027\n",
      "Batch eval [2] loss 0.99982, dsc 0.00018\n",
      "Batch eval [3] loss 0.99939, dsc 0.00061\n",
      "Batch eval [4] loss 0.99911, dsc 0.00089\n",
      "Batch eval [5] loss 0.99936, dsc 0.00064\n",
      "Epoch [21] valid done\n",
      "Epoch [21] T 1366.19s, deltaT 66.38s, loss: train 0.99918, valid 0.99948, dsc: train 0.00082, valid 0.00052\n",
      "Batch train [1] loss 0.99945, dsc 0.00055\n",
      "Batch train [2] loss 0.99943, dsc 0.00057\n",
      "Batch train [3] loss 0.99919, dsc 0.00081\n",
      "Batch train [4] loss 0.99955, dsc 0.00045\n",
      "Batch train [5] loss 0.99882, dsc 0.00118\n",
      "Batch train [6] loss 0.99950, dsc 0.00050\n",
      "Batch train [7] loss 0.99869, dsc 0.00131\n",
      "Batch train [8] loss 0.99907, dsc 0.00093\n",
      "Batch train [9] loss 0.99916, dsc 0.00084\n",
      "Batch train [10] loss 0.99947, dsc 0.00053\n",
      "Batch train [11] loss 0.99973, dsc 0.00027\n",
      "Batch train [12] loss 0.99886, dsc 0.00114\n",
      "Batch train [13] loss 0.99832, dsc 0.00168\n",
      "Batch train [14] loss 0.99882, dsc 0.00118\n",
      "Batch train [15] loss 0.99924, dsc 0.00076\n",
      "Batch train [16] loss 0.99918, dsc 0.00082\n",
      "Batch train [17] loss 0.99859, dsc 0.00141\n",
      "Batch train [18] loss 0.99948, dsc 0.00052\n",
      "Batch train [19] loss 0.99917, dsc 0.00083\n",
      "Batch train [20] loss 0.99862, dsc 0.00138\n",
      "Epoch [22] train done\n",
      "Batch eval [1] loss 0.99971, dsc 0.00029\n",
      "Batch eval [2] loss 0.99981, dsc 0.00019\n",
      "Batch eval [3] loss 0.99934, dsc 0.00066\n",
      "Batch eval [4] loss 0.99903, dsc 0.00097\n",
      "Batch eval [5] loss 0.99931, dsc 0.00069\n",
      "Epoch [22] valid done\n",
      "Epoch [22] T 1432.05s, deltaT 65.86s, loss: train 0.99912, valid 0.99944, dsc: train 0.00088, valid 0.00056\n",
      "Batch train [1] loss 0.99898, dsc 0.00102\n",
      "Batch train [2] loss 0.99918, dsc 0.00082\n",
      "Batch train [3] loss 0.99913, dsc 0.00087\n",
      "Batch train [4] loss 0.99728, dsc 0.00272\n",
      "Batch train [5] loss 0.99923, dsc 0.00077\n",
      "Batch train [6] loss 0.99960, dsc 0.00040\n",
      "Batch train [7] loss 0.99932, dsc 0.00068\n",
      "Batch train [8] loss 0.99953, dsc 0.00047\n",
      "Batch train [9] loss 0.99909, dsc 0.00091\n",
      "Batch train [10] loss 0.99932, dsc 0.00068\n",
      "Batch train [11] loss 0.99875, dsc 0.00125\n",
      "Batch train [12] loss 0.99950, dsc 0.00050\n",
      "Batch train [13] loss 0.99882, dsc 0.00118\n",
      "Batch train [14] loss 0.99929, dsc 0.00071\n",
      "Batch train [15] loss 0.99869, dsc 0.00131\n",
      "Batch train [16] loss 0.99867, dsc 0.00133\n",
      "Batch train [17] loss 0.99888, dsc 0.00112\n",
      "Batch train [18] loss 0.99964, dsc 0.00036\n",
      "Batch train [19] loss 0.99926, dsc 0.00074\n",
      "Batch train [20] loss 0.99884, dsc 0.00116\n",
      "Epoch [23] train done\n",
      "Batch eval [1] loss 0.99970, dsc 0.00030\n",
      "Batch eval [2] loss 0.99980, dsc 0.00020\n",
      "Batch eval [3] loss 0.99931, dsc 0.00069\n",
      "Batch eval [4] loss 0.99900, dsc 0.00100\n",
      "Batch eval [5] loss 0.99930, dsc 0.00070\n",
      "Epoch [23] valid done\n",
      "Epoch [23] T 1497.16s, deltaT 65.11s, loss: train 0.99905, valid 0.99942, dsc: train 0.00095, valid 0.00058\n",
      "Batch train [1] loss 0.99887, dsc 0.00113\n",
      "Batch train [2] loss 0.99907, dsc 0.00093\n",
      "Batch train [3] loss 0.99790, dsc 0.00210\n",
      "Batch train [4] loss 0.99864, dsc 0.00136\n",
      "Batch train [5] loss 0.99843, dsc 0.00157\n",
      "Batch train [6] loss 0.99910, dsc 0.00090\n",
      "Batch train [7] loss 0.99947, dsc 0.00053\n",
      "Batch train [8] loss 0.99910, dsc 0.00090\n",
      "Batch train [9] loss 0.99913, dsc 0.00087\n",
      "Batch train [10] loss 0.99917, dsc 0.00083\n",
      "Batch train [11] loss 0.99978, dsc 0.00022\n",
      "Batch train [12] loss 0.99873, dsc 0.00127\n",
      "Batch train [13] loss 0.99950, dsc 0.00050\n",
      "Batch train [14] loss 0.99899, dsc 0.00101\n",
      "Batch train [15] loss 0.99910, dsc 0.00090\n",
      "Batch train [16] loss 0.99926, dsc 0.00074\n",
      "Batch train [17] loss 0.99817, dsc 0.00183\n",
      "Batch train [18] loss 0.99852, dsc 0.00148\n",
      "Batch train [19] loss 0.99954, dsc 0.00046\n",
      "Batch train [20] loss 0.99901, dsc 0.00099\n",
      "Epoch [24] train done\n",
      "Batch eval [1] loss 0.99968, dsc 0.00032\n",
      "Batch eval [2] loss 0.99979, dsc 0.00021\n",
      "Batch eval [3] loss 0.99928, dsc 0.00072\n",
      "Batch eval [4] loss 0.99892, dsc 0.00108\n",
      "Batch eval [5] loss 0.99924, dsc 0.00076\n",
      "Epoch [24] valid done\n",
      "Epoch [24] T 1561.97s, deltaT 64.81s, loss: train 0.99897, valid 0.99938, dsc: train 0.00103, valid 0.00062\n",
      "Batch train [1] loss 0.99897, dsc 0.00103\n",
      "Batch train [2] loss 0.99927, dsc 0.00073\n",
      "Batch train [3] loss 0.99858, dsc 0.00142\n",
      "Batch train [4] loss 0.99867, dsc 0.00133\n",
      "Batch train [5] loss 0.99814, dsc 0.00186\n",
      "Batch train [6] loss 0.99895, dsc 0.00105\n",
      "Batch train [7] loss 0.99812, dsc 0.00188\n",
      "Batch train [8] loss 0.99944, dsc 0.00056\n",
      "Batch train [9] loss 0.99898, dsc 0.00102\n",
      "Batch train [10] loss 0.99896, dsc 0.00104\n",
      "Batch train [11] loss 0.99905, dsc 0.00095\n",
      "Batch train [12] loss 0.99908, dsc 0.00092\n",
      "Batch train [13] loss 0.99822, dsc 0.00178\n",
      "Batch train [14] loss 0.99911, dsc 0.00089\n",
      "Batch train [15] loss 0.99934, dsc 0.00066\n",
      "Batch train [16] loss 0.99860, dsc 0.00140\n",
      "Batch train [17] loss 0.99780, dsc 0.00220\n",
      "Batch train [18] loss 0.99940, dsc 0.00060\n",
      "Batch train [19] loss 0.99896, dsc 0.00104\n",
      "Batch train [20] loss 0.99975, dsc 0.00025\n",
      "Epoch [25] train done\n",
      "Batch eval [1] loss 0.99965, dsc 0.00035\n",
      "Batch eval [2] loss 0.99977, dsc 0.00023\n",
      "Batch eval [3] loss 0.99919, dsc 0.00081\n",
      "Batch eval [4] loss 0.99883, dsc 0.00117\n",
      "Batch eval [5] loss 0.99916, dsc 0.00084\n",
      "Epoch [25] valid done\n",
      "Epoch [25] T 1626.86s, deltaT 64.89s, loss: train 0.99887, valid 0.99932, dsc: train 0.00113, valid 0.00068\n",
      "Batch train [1] loss 0.99854, dsc 0.00146\n",
      "Batch train [2] loss 0.99882, dsc 0.00118\n",
      "Batch train [3] loss 0.99878, dsc 0.00122\n",
      "Batch train [4] loss 0.99879, dsc 0.00121\n",
      "Batch train [5] loss 0.99931, dsc 0.00069\n",
      "Batch train [6] loss 0.99923, dsc 0.00077\n",
      "Batch train [7] loss 0.99903, dsc 0.00097\n",
      "Batch train [8] loss 0.99850, dsc 0.00150\n",
      "Batch train [9] loss 0.99908, dsc 0.00092\n",
      "Batch train [10] loss 0.99903, dsc 0.00097\n",
      "Batch train [11] loss 0.99878, dsc 0.00122\n",
      "Batch train [12] loss 0.99903, dsc 0.00097\n",
      "Batch train [13] loss 0.99764, dsc 0.00236\n",
      "Batch train [14] loss 0.99898, dsc 0.00102\n",
      "Batch train [15] loss 0.99897, dsc 0.00103\n",
      "Batch train [16] loss 0.99894, dsc 0.00106\n",
      "Batch train [17] loss 0.99882, dsc 0.00118\n",
      "Batch train [18] loss 0.99931, dsc 0.00069\n",
      "Batch train [19] loss 0.99702, dsc 0.00298\n",
      "Batch train [20] loss 0.99856, dsc 0.00144\n",
      "Epoch [26] train done\n",
      "Batch eval [1] loss 0.99961, dsc 0.00039\n",
      "Batch eval [2] loss 0.99974, dsc 0.00026\n",
      "Batch eval [3] loss 0.99914, dsc 0.00086\n",
      "Batch eval [4] loss 0.99872, dsc 0.00128\n",
      "Batch eval [5] loss 0.99909, dsc 0.00091\n",
      "Epoch [26] valid done\n",
      "Epoch [26] T 1691.73s, deltaT 64.86s, loss: train 0.99876, valid 0.99926, dsc: train 0.00124, valid 0.00074\n",
      "Batch train [1] loss 0.99890, dsc 0.00110\n",
      "Batch train [2] loss 0.99903, dsc 0.00097\n",
      "Batch train [3] loss 0.99829, dsc 0.00171\n",
      "Batch train [4] loss 0.99775, dsc 0.00225\n",
      "Batch train [5] loss 0.99974, dsc 0.00026\n",
      "Batch train [6] loss 0.99904, dsc 0.00096\n",
      "Batch train [7] loss 0.99876, dsc 0.00124\n",
      "Batch train [8] loss 0.99813, dsc 0.00187\n",
      "Batch train [9] loss 0.99804, dsc 0.00196\n",
      "Batch train [10] loss 0.99790, dsc 0.00210\n",
      "Batch train [11] loss 0.99884, dsc 0.00116\n",
      "Batch train [12] loss 0.99815, dsc 0.00185\n",
      "Batch train [13] loss 0.99868, dsc 0.00132\n",
      "Batch train [14] loss 0.99860, dsc 0.00140\n",
      "Batch train [15] loss 0.99891, dsc 0.00109\n",
      "Batch train [16] loss 0.99911, dsc 0.00089\n",
      "Batch train [17] loss 0.99938, dsc 0.00062\n",
      "Batch train [18] loss 0.99890, dsc 0.00110\n",
      "Batch train [19] loss 0.99932, dsc 0.00068\n",
      "Batch train [20] loss 0.99792, dsc 0.00208\n",
      "Epoch [27] train done\n",
      "Batch eval [1] loss 0.99952, dsc 0.00048\n",
      "Batch eval [2] loss 0.99968, dsc 0.00032\n",
      "Batch eval [3] loss 0.99887, dsc 0.00113\n",
      "Batch eval [4] loss 0.99838, dsc 0.00162\n",
      "Batch eval [5] loss 0.99887, dsc 0.00113\n",
      "Epoch [27] valid done\n",
      "Epoch [27] T 1756.57s, deltaT 64.84s, loss: train 0.99867, valid 0.99906, dsc: train 0.00133, valid 0.00094\n",
      "Batch train [1] loss 0.99727, dsc 0.00273\n",
      "Batch train [2] loss 0.99830, dsc 0.00170\n",
      "Batch train [3] loss 0.99844, dsc 0.00156\n",
      "Batch train [4] loss 0.99929, dsc 0.00071\n",
      "Batch train [5] loss 0.99900, dsc 0.00100\n",
      "Batch train [6] loss 0.99908, dsc 0.00092\n",
      "Batch train [7] loss 0.99918, dsc 0.00082\n",
      "Batch train [8] loss 0.99883, dsc 0.00117\n",
      "Batch train [9] loss 0.99854, dsc 0.00146\n",
      "Batch train [10] loss 0.99822, dsc 0.00178\n",
      "Batch train [11] loss 0.99888, dsc 0.00112\n",
      "Batch train [12] loss 0.99913, dsc 0.00087\n",
      "Batch train [13] loss 0.99934, dsc 0.00066\n",
      "Batch train [14] loss 0.99816, dsc 0.00184\n",
      "Batch train [15] loss 0.99686, dsc 0.00314\n",
      "Batch train [16] loss 0.99866, dsc 0.00134\n",
      "Batch train [17] loss 0.99927, dsc 0.00073\n",
      "Batch train [18] loss 0.99823, dsc 0.00177\n",
      "Batch train [19] loss 0.99764, dsc 0.00236\n",
      "Batch train [20] loss 0.99819, dsc 0.00181\n",
      "Epoch [28] train done\n",
      "Batch eval [1] loss 0.99946, dsc 0.00054\n",
      "Batch eval [2] loss 0.99964, dsc 0.00036\n",
      "Batch eval [3] loss 0.99877, dsc 0.00123\n",
      "Batch eval [4] loss 0.99818, dsc 0.00182\n",
      "Batch eval [5] loss 0.99873, dsc 0.00127\n",
      "Epoch [28] valid done\n",
      "Epoch [28] T 1821.32s, deltaT 64.75s, loss: train 0.99853, valid 0.99896, dsc: train 0.00147, valid 0.00104\n",
      "Batch train [1] loss 0.99754, dsc 0.00246\n",
      "Batch train [2] loss 0.99846, dsc 0.00154\n",
      "Batch train [3] loss 0.99796, dsc 0.00204\n",
      "Batch train [4] loss 0.99878, dsc 0.00122\n",
      "Batch train [5] loss 0.99915, dsc 0.00085\n",
      "Batch train [6] loss 0.99621, dsc 0.00379\n",
      "Batch train [7] loss 0.99846, dsc 0.00154\n",
      "Batch train [8] loss 0.99906, dsc 0.00094\n",
      "Batch train [9] loss 0.99921, dsc 0.00079\n",
      "Batch train [10] loss 0.99749, dsc 0.00251\n",
      "Batch train [11] loss 0.99804, dsc 0.00196\n",
      "Batch train [12] loss 0.99939, dsc 0.00061\n",
      "Batch train [13] loss 0.99869, dsc 0.00131\n",
      "Batch train [14] loss 0.99877, dsc 0.00123\n",
      "Batch train [15] loss 0.99840, dsc 0.00160\n",
      "Batch train [16] loss 0.99872, dsc 0.00128\n",
      "Batch train [17] loss 0.99810, dsc 0.00190\n",
      "Batch train [18] loss 0.99850, dsc 0.00150\n",
      "Batch train [19] loss 0.99775, dsc 0.00225\n",
      "Batch train [20] loss 0.99797, dsc 0.00203\n",
      "Epoch [29] train done\n",
      "Batch eval [1] loss 0.99946, dsc 0.00054\n",
      "Batch eval [2] loss 0.99966, dsc 0.00034\n",
      "Batch eval [3] loss 0.99884, dsc 0.00116\n",
      "Batch eval [4] loss 0.99826, dsc 0.00174\n",
      "Batch eval [5] loss 0.99880, dsc 0.00120\n",
      "Epoch [29] valid done\n",
      "Epoch [29] T 1886.93s, deltaT 65.61s, loss: train 0.99833, valid 0.99900, dsc: train 0.00167, valid 0.00100\n",
      "Batch train [1] loss 0.99894, dsc 0.00106\n",
      "Batch train [2] loss 0.99928, dsc 0.00072\n",
      "Batch train [3] loss 0.99813, dsc 0.00187\n",
      "Batch train [4] loss 0.99727, dsc 0.00273\n",
      "Batch train [5] loss 0.99891, dsc 0.00109\n",
      "Batch train [6] loss 0.99812, dsc 0.00188\n",
      "Batch train [7] loss 0.99689, dsc 0.00311\n",
      "Batch train [8] loss 0.99901, dsc 0.00099\n",
      "Batch train [9] loss 0.99950, dsc 0.00050\n",
      "Batch train [10] loss 0.99771, dsc 0.00229\n",
      "Batch train [11] loss 0.99826, dsc 0.00174\n",
      "Batch train [12] loss 0.99790, dsc 0.00210\n",
      "Batch train [13] loss 0.99875, dsc 0.00125\n",
      "Batch train [14] loss 0.99873, dsc 0.00127\n",
      "Batch train [15] loss 0.99830, dsc 0.00170\n",
      "Batch train [16] loss 0.99589, dsc 0.00411\n",
      "Batch train [17] loss 0.99799, dsc 0.00201\n",
      "Batch train [18] loss 0.99647, dsc 0.00353\n",
      "Batch train [19] loss 0.99839, dsc 0.00161\n",
      "Batch train [20] loss 0.99736, dsc 0.00264\n",
      "Epoch [30] train done\n",
      "Batch eval [1] loss 0.99970, dsc 0.00030\n",
      "Batch eval [2] loss 0.99982, dsc 0.00018\n",
      "Batch eval [3] loss 0.99857, dsc 0.00143\n",
      "Batch eval [4] loss 0.99858, dsc 0.00142\n",
      "Batch eval [5] loss 0.99983, dsc 0.00017\n",
      "Epoch [30] valid done\n",
      "Epoch [30] T 1951.14s, deltaT 64.21s, loss: train 0.99809, valid 0.99930, dsc: train 0.00191, valid 0.00070\n",
      "Batch train [1] loss 0.99585, dsc 0.00415\n",
      "Batch train [2] loss 0.99591, dsc 0.00409\n",
      "Batch train [3] loss 0.99714, dsc 0.00286\n",
      "Batch train [4] loss 0.99726, dsc 0.00274\n",
      "Batch train [5] loss 0.99804, dsc 0.00196\n",
      "Batch train [6] loss 0.99827, dsc 0.00173\n",
      "Batch train [7] loss 0.99822, dsc 0.00178\n",
      "Batch train [8] loss 0.99814, dsc 0.00186\n",
      "Batch train [9] loss 0.99893, dsc 0.00107\n",
      "Batch train [10] loss 0.99907, dsc 0.00093\n",
      "Batch train [11] loss 0.99860, dsc 0.00140\n",
      "Batch train [12] loss 0.99931, dsc 0.00069\n",
      "Batch train [13] loss 0.99826, dsc 0.00174\n",
      "Batch train [14] loss 0.99808, dsc 0.00192\n",
      "Batch train [15] loss 0.99789, dsc 0.00211\n",
      "Batch train [16] loss 0.99734, dsc 0.00266\n",
      "Batch train [17] loss 0.99808, dsc 0.00192\n",
      "Batch train [18] loss 0.99756, dsc 0.00244\n",
      "Batch train [19] loss 0.99880, dsc 0.00120\n",
      "Batch train [20] loss 0.99639, dsc 0.00361\n",
      "Epoch [31] train done\n",
      "Batch eval [1] loss 0.99907, dsc 0.00093\n",
      "Batch eval [2] loss 0.99945, dsc 0.00055\n",
      "Batch eval [3] loss 0.99804, dsc 0.00197\n",
      "Batch eval [4] loss 0.99707, dsc 0.00293\n",
      "Batch eval [5] loss 0.99812, dsc 0.00188\n",
      "Epoch [31] valid done\n",
      "Epoch [31] T 2017.80s, deltaT 66.65s, loss: train 0.99786, valid 0.99835, dsc: train 0.00214, valid 0.00165\n",
      "Batch train [1] loss 0.99638, dsc 0.00362\n",
      "Batch train [2] loss 0.99864, dsc 0.00136\n",
      "Batch train [3] loss 0.99711, dsc 0.00289\n",
      "Batch train [4] loss 0.99744, dsc 0.00256\n",
      "Batch train [5] loss 0.99868, dsc 0.00132\n",
      "Batch train [6] loss 0.99832, dsc 0.00168\n",
      "Batch train [7] loss 0.99860, dsc 0.00140\n",
      "Batch train [8] loss 0.99676, dsc 0.00324\n",
      "Batch train [9] loss 0.99858, dsc 0.00142\n",
      "Batch train [10] loss 0.99828, dsc 0.00172\n",
      "Batch train [11] loss 0.99817, dsc 0.00183\n",
      "Batch train [12] loss 0.99770, dsc 0.00230\n",
      "Batch train [13] loss 0.99853, dsc 0.00147\n",
      "Batch train [14] loss 0.99714, dsc 0.00286\n",
      "Batch train [15] loss 0.99768, dsc 0.00232\n",
      "Batch train [16] loss 0.99515, dsc 0.00485\n",
      "Batch train [17] loss 0.99756, dsc 0.00244\n",
      "Batch train [18] loss 0.99801, dsc 0.00199\n",
      "Batch train [19] loss 0.99424, dsc 0.00576\n",
      "Batch train [20] loss 0.99754, dsc 0.00246\n",
      "Epoch [32] train done\n",
      "Batch eval [1] loss 0.99900, dsc 0.00100\n",
      "Batch eval [2] loss 0.99934, dsc 0.00066\n",
      "Batch eval [3] loss 0.99767, dsc 0.00233\n",
      "Batch eval [4] loss 0.99675, dsc 0.00325\n",
      "Batch eval [5] loss 0.99760, dsc 0.00240\n",
      "Epoch [32] valid done\n",
      "Epoch [32] T 2083.46s, deltaT 65.66s, loss: train 0.99753, valid 0.99807, dsc: train 0.00247, valid 0.00193\n",
      "Batch train [1] loss 0.99771, dsc 0.00229\n",
      "Batch train [2] loss 0.99689, dsc 0.00311\n",
      "Batch train [3] loss 0.99678, dsc 0.00322\n",
      "Batch train [4] loss 0.99728, dsc 0.00272\n",
      "Batch train [5] loss 0.99710, dsc 0.00290\n",
      "Batch train [6] loss 0.99510, dsc 0.00490\n",
      "Batch train [7] loss 0.99574, dsc 0.00426\n",
      "Batch train [8] loss 0.99778, dsc 0.00222\n",
      "Batch train [9] loss 0.99923, dsc 0.00077\n",
      "Batch train [10] loss 0.99905, dsc 0.00095\n",
      "Batch train [11] loss 0.99708, dsc 0.00292\n",
      "Batch train [12] loss 0.99492, dsc 0.00508\n",
      "Batch train [13] loss 0.99626, dsc 0.00374\n",
      "Batch train [14] loss 0.99691, dsc 0.00309\n",
      "Batch train [15] loss 0.99811, dsc 0.00189\n",
      "Batch train [16] loss 0.99755, dsc 0.00245\n",
      "Batch train [17] loss 0.99809, dsc 0.00191\n",
      "Batch train [18] loss 0.99665, dsc 0.00335\n",
      "Batch train [19] loss 0.99769, dsc 0.00231\n",
      "Batch train [20] loss 0.99819, dsc 0.00181\n",
      "Epoch [33] train done\n",
      "Batch eval [1] loss 0.99880, dsc 0.00120\n",
      "Batch eval [2] loss 0.99923, dsc 0.00077\n",
      "Batch eval [3] loss 0.99757, dsc 0.00243\n",
      "Batch eval [4] loss 0.99625, dsc 0.00375\n",
      "Batch eval [5] loss 0.99735, dsc 0.00265\n",
      "Epoch [33] valid done\n",
      "Epoch [33] T 2150.61s, deltaT 67.15s, loss: train 0.99721, valid 0.99784, dsc: train 0.00279, valid 0.00216\n",
      "Batch train [1] loss 0.99833, dsc 0.00167\n",
      "Batch train [2] loss 0.99709, dsc 0.00291\n",
      "Batch train [3] loss 0.99415, dsc 0.00585\n",
      "Batch train [4] loss 0.99731, dsc 0.00269\n",
      "Batch train [5] loss 0.99808, dsc 0.00192\n",
      "Batch train [6] loss 0.99815, dsc 0.00185\n",
      "Batch train [7] loss 0.99958, dsc 0.00042\n",
      "Batch train [8] loss 0.99631, dsc 0.00369\n",
      "Batch train [9] loss 0.99705, dsc 0.00295\n",
      "Batch train [10] loss 0.99709, dsc 0.00291\n",
      "Batch train [11] loss 0.99714, dsc 0.00286\n",
      "Batch train [12] loss 0.99774, dsc 0.00226\n",
      "Batch train [13] loss 0.99772, dsc 0.00228\n",
      "Batch train [14] loss 0.99471, dsc 0.00529\n",
      "Batch train [15] loss 0.99589, dsc 0.00411\n",
      "Batch train [16] loss 0.99417, dsc 0.00583\n",
      "Batch train [17] loss 0.99708, dsc 0.00292\n",
      "Batch train [18] loss 0.99346, dsc 0.00654\n",
      "Batch train [19] loss 0.99653, dsc 0.00347\n",
      "Batch train [20] loss 0.99569, dsc 0.00431\n",
      "Epoch [34] train done\n",
      "Batch eval [1] loss 0.99902, dsc 0.00098\n",
      "Batch eval [2] loss 0.99925, dsc 0.00075\n",
      "Batch eval [3] loss 0.99778, dsc 0.00222\n",
      "Batch eval [4] loss 0.99617, dsc 0.00383\n",
      "Batch eval [5] loss 0.99729, dsc 0.00271\n",
      "Epoch [34] valid done\n",
      "Epoch [34] T 2217.84s, deltaT 67.23s, loss: train 0.99666, valid 0.99790, dsc: train 0.00334, valid 0.00210\n",
      "Batch train [1] loss 0.99671, dsc 0.00329\n",
      "Batch train [2] loss 0.99688, dsc 0.00312\n",
      "Batch train [3] loss 0.99616, dsc 0.00384\n",
      "Batch train [4] loss 0.99383, dsc 0.00617\n",
      "Batch train [5] loss 0.99502, dsc 0.00498\n",
      "Batch train [6] loss 0.99828, dsc 0.00172\n",
      "Batch train [7] loss 0.99725, dsc 0.00275\n",
      "Batch train [8] loss 0.99759, dsc 0.00241\n",
      "Batch train [9] loss 0.99626, dsc 0.00374\n",
      "Batch train [10] loss 0.99485, dsc 0.00515\n",
      "Batch train [11] loss 0.99288, dsc 0.00712\n",
      "Batch train [12] loss 0.99872, dsc 0.00128\n",
      "Batch train [13] loss 0.99611, dsc 0.00389\n",
      "Batch train [14] loss 0.99659, dsc 0.00341\n",
      "Batch train [15] loss 0.99626, dsc 0.00374\n",
      "Batch train [16] loss 0.99486, dsc 0.00514\n",
      "Batch train [17] loss 0.99658, dsc 0.00342\n",
      "Batch train [18] loss 0.99262, dsc 0.00738\n",
      "Batch train [19] loss 0.99681, dsc 0.00319\n",
      "Batch train [20] loss 0.99824, dsc 0.00176\n",
      "Epoch [35] train done\n",
      "Batch eval [1] loss 0.99852, dsc 0.00148\n",
      "Batch eval [2] loss 0.99904, dsc 0.00096\n",
      "Batch eval [3] loss 0.99824, dsc 0.00176\n",
      "Batch eval [4] loss 0.99507, dsc 0.00493\n",
      "Batch eval [5] loss 0.99658, dsc 0.00342\n",
      "Epoch [35] valid done\n",
      "Epoch [35] T 2288.15s, deltaT 70.31s, loss: train 0.99612, valid 0.99749, dsc: train 0.00388, valid 0.00251\n",
      "Batch train [1] loss 0.99842, dsc 0.00158\n",
      "Batch train [2] loss 0.99326, dsc 0.00674\n",
      "Batch train [3] loss 0.99121, dsc 0.00879\n",
      "Batch train [4] loss 0.99514, dsc 0.00486\n",
      "Batch train [5] loss 0.99780, dsc 0.00220\n",
      "Batch train [6] loss 0.99574, dsc 0.00426\n",
      "Batch train [7] loss 0.99646, dsc 0.00354\n",
      "Batch train [8] loss 0.99671, dsc 0.00329\n",
      "Batch train [9] loss 0.99650, dsc 0.00350\n",
      "Batch train [10] loss 0.99603, dsc 0.00397\n",
      "Batch train [11] loss 0.99512, dsc 0.00488\n",
      "Batch train [12] loss 0.99403, dsc 0.00597\n",
      "Batch train [13] loss 0.99685, dsc 0.00315\n",
      "Batch train [14] loss 0.99719, dsc 0.00281\n",
      "Batch train [15] loss 0.99448, dsc 0.00552\n",
      "Batch train [16] loss 0.99702, dsc 0.00298\n",
      "Batch train [17] loss 0.99564, dsc 0.00436\n",
      "Batch train [18] loss 0.99320, dsc 0.00680\n",
      "Batch train [19] loss 0.99527, dsc 0.00473\n",
      "Batch train [20] loss 0.99582, dsc 0.00418\n",
      "Epoch [36] train done\n",
      "Batch eval [1] loss 0.99805, dsc 0.00195\n",
      "Batch eval [2] loss 0.99875, dsc 0.00125\n",
      "Batch eval [3] loss 0.99559, dsc 0.00441\n",
      "Batch eval [4] loss 0.99379, dsc 0.00621\n",
      "Batch eval [5] loss 0.99554, dsc 0.00446\n",
      "Epoch [36] valid done\n",
      "Epoch [36] T 2358.99s, deltaT 70.83s, loss: train 0.99559, valid 0.99634, dsc: train 0.00441, valid 0.00366\n",
      "Batch train [1] loss 0.99647, dsc 0.00353\n",
      "Batch train [2] loss 0.99482, dsc 0.00518\n",
      "Batch train [3] loss 0.99158, dsc 0.00842\n",
      "Batch train [4] loss 0.99676, dsc 0.00324\n",
      "Batch train [5] loss 0.99411, dsc 0.00589\n",
      "Batch train [6] loss 0.99694, dsc 0.00306\n",
      "Batch train [7] loss 0.99408, dsc 0.00592\n",
      "Batch train [8] loss 0.99814, dsc 0.00186\n",
      "Batch train [9] loss 0.99353, dsc 0.00647\n",
      "Batch train [10] loss 0.99738, dsc 0.00262\n",
      "Batch train [11] loss 0.99650, dsc 0.00350\n",
      "Batch train [12] loss 0.99825, dsc 0.00175\n",
      "Batch train [13] loss 0.99238, dsc 0.00762\n",
      "Batch train [14] loss 0.98889, dsc 0.01111\n",
      "Batch train [15] loss 0.99335, dsc 0.00665\n",
      "Batch train [16] loss 0.99525, dsc 0.00475\n",
      "Batch train [17] loss 0.99337, dsc 0.00663\n",
      "Batch train [18] loss 0.99453, dsc 0.00547\n",
      "Batch train [19] loss 0.99210, dsc 0.00790\n",
      "Batch train [20] loss 0.99390, dsc 0.00610\n",
      "Epoch [37] train done\n",
      "Batch eval [1] loss 0.99793, dsc 0.00207\n",
      "Batch eval [2] loss 0.99865, dsc 0.00135\n",
      "Batch eval [3] loss 0.99525, dsc 0.00475\n",
      "Batch eval [4] loss 0.99317, dsc 0.00683\n",
      "Batch eval [5] loss 0.99525, dsc 0.00475\n",
      "Epoch [37] valid done\n",
      "Epoch [37] T 2424.42s, deltaT 65.43s, loss: train 0.99462, valid 0.99605, dsc: train 0.00538, valid 0.00395\n",
      "Batch train [1] loss 0.99347, dsc 0.00653\n",
      "Batch train [2] loss 0.98859, dsc 0.01141\n",
      "Batch train [3] loss 0.99239, dsc 0.00761\n",
      "Batch train [4] loss 0.99465, dsc 0.00535\n",
      "Batch train [5] loss 0.99094, dsc 0.00906\n",
      "Batch train [6] loss 0.99591, dsc 0.00409\n",
      "Batch train [7] loss 0.99726, dsc 0.00274\n",
      "Batch train [8] loss 0.99362, dsc 0.00638\n",
      "Batch train [9] loss 0.99278, dsc 0.00722\n",
      "Batch train [10] loss 0.99197, dsc 0.00803\n",
      "Batch train [11] loss 0.99052, dsc 0.00948\n",
      "Batch train [12] loss 0.99357, dsc 0.00643\n",
      "Batch train [13] loss 0.99414, dsc 0.00586\n",
      "Batch train [14] loss 0.99425, dsc 0.00575\n",
      "Batch train [15] loss 0.99808, dsc 0.00192\n",
      "Batch train [16] loss 0.99370, dsc 0.00630\n",
      "Batch train [17] loss 0.98617, dsc 0.01383\n",
      "Batch train [18] loss 0.99210, dsc 0.00790\n",
      "Batch train [19] loss 0.99721, dsc 0.00279\n",
      "Batch train [20] loss 0.99517, dsc 0.00483\n",
      "Epoch [38] train done\n",
      "Batch eval [1] loss 0.99741, dsc 0.00259\n",
      "Batch eval [2] loss 0.99833, dsc 0.00167\n",
      "Batch eval [3] loss 0.99502, dsc 0.00498\n",
      "Batch eval [4] loss 0.99146, dsc 0.00854\n",
      "Batch eval [5] loss 0.99412, dsc 0.00588\n",
      "Epoch [38] valid done\n",
      "Epoch [38] T 2489.40s, deltaT 64.98s, loss: train 0.99332, valid 0.99527, dsc: train 0.00668, valid 0.00473\n",
      "Batch train [1] loss 0.99758, dsc 0.00242\n",
      "Batch train [2] loss 0.98281, dsc 0.01719\n",
      "Batch train [3] loss 0.99605, dsc 0.00395\n",
      "Batch train [4] loss 0.99396, dsc 0.00604\n",
      "Batch train [5] loss 0.99786, dsc 0.00214\n",
      "Batch train [6] loss 0.99442, dsc 0.00558\n",
      "Batch train [7] loss 0.99078, dsc 0.00922\n",
      "Batch train [8] loss 0.99073, dsc 0.00927\n",
      "Batch train [9] loss 0.99167, dsc 0.00833\n",
      "Batch train [10] loss 0.98466, dsc 0.01534\n",
      "Batch train [11] loss 0.99370, dsc 0.00630\n",
      "Batch train [12] loss 0.99414, dsc 0.00586\n",
      "Batch train [13] loss 0.99230, dsc 0.00770\n",
      "Batch train [14] loss 0.99249, dsc 0.00751\n",
      "Batch train [15] loss 0.98765, dsc 0.01235\n",
      "Batch train [16] loss 0.99703, dsc 0.00297\n",
      "Batch train [17] loss 0.98578, dsc 0.01422\n",
      "Batch train [18] loss 0.99561, dsc 0.00439\n",
      "Batch train [19] loss 0.98654, dsc 0.01346\n",
      "Batch train [20] loss 0.98922, dsc 0.01078\n",
      "Epoch [39] train done\n",
      "Batch eval [1] loss 0.99684, dsc 0.00316\n",
      "Batch eval [2] loss 0.99799, dsc 0.00201\n",
      "Batch eval [3] loss 0.99276, dsc 0.00724\n",
      "Batch eval [4] loss 0.98952, dsc 0.01048\n",
      "Batch eval [5] loss 0.99347, dsc 0.00653\n",
      "Epoch [39] valid done\n",
      "Epoch [39] T 2556.91s, deltaT 67.50s, loss: train 0.99175, valid 0.99411, dsc: train 0.00825, valid 0.00589\n",
      "Batch train [1] loss 0.99118, dsc 0.00882\n",
      "Batch train [2] loss 0.99160, dsc 0.00840\n",
      "Batch train [3] loss 0.99082, dsc 0.00918\n",
      "Batch train [4] loss 0.98533, dsc 0.01467\n",
      "Batch train [5] loss 0.99022, dsc 0.00978\n",
      "Batch train [6] loss 0.99688, dsc 0.00312\n",
      "Batch train [7] loss 0.98996, dsc 0.01004\n",
      "Batch train [8] loss 0.99278, dsc 0.00722\n",
      "Batch train [9] loss 0.99488, dsc 0.00512\n",
      "Batch train [10] loss 0.97971, dsc 0.02029\n",
      "Batch train [11] loss 0.99511, dsc 0.00489\n",
      "Batch train [12] loss 0.99201, dsc 0.00799\n",
      "Batch train [13] loss 0.99112, dsc 0.00888\n",
      "Batch train [14] loss 0.97429, dsc 0.02571\n",
      "Batch train [15] loss 0.99643, dsc 0.00357\n",
      "Batch train [16] loss 0.99608, dsc 0.00392\n",
      "Batch train [17] loss 0.99100, dsc 0.00900\n",
      "Batch train [18] loss 0.97882, dsc 0.02118\n",
      "Batch train [19] loss 0.98564, dsc 0.01436\n",
      "Batch train [20] loss 0.98909, dsc 0.01091\n",
      "Epoch [40] train done\n",
      "Batch eval [1] loss 0.99609, dsc 0.00391\n",
      "Batch eval [2] loss 0.99738, dsc 0.00262\n",
      "Batch eval [3] loss 0.99247, dsc 0.00753\n",
      "Batch eval [4] loss 0.98678, dsc 0.01322\n",
      "Batch eval [5] loss 0.99073, dsc 0.00927\n",
      "Epoch [40] valid done\n",
      "Epoch [40] T 2624.29s, deltaT 67.38s, loss: train 0.98965, valid 0.99269, dsc: train 0.01035, valid 0.00731\n",
      "Batch train [1] loss 0.99212, dsc 0.00788\n",
      "Batch train [2] loss 0.97513, dsc 0.02487\n",
      "Batch train [3] loss 0.99287, dsc 0.00712\n",
      "Batch train [4] loss 0.99214, dsc 0.00786\n",
      "Batch train [5] loss 0.99389, dsc 0.00611\n",
      "Batch train [6] loss 0.99130, dsc 0.00870\n",
      "Batch train [7] loss 0.98920, dsc 0.01080\n",
      "Batch train [8] loss 0.98682, dsc 0.01318\n",
      "Batch train [9] loss 0.98641, dsc 0.01359\n",
      "Batch train [10] loss 0.98884, dsc 0.01116\n",
      "Batch train [11] loss 0.99464, dsc 0.00536\n",
      "Batch train [12] loss 0.98434, dsc 0.01566\n",
      "Batch train [13] loss 0.98364, dsc 0.01636\n",
      "Batch train [14] loss 0.98235, dsc 0.01765\n",
      "Batch train [15] loss 0.97926, dsc 0.02074\n",
      "Batch train [16] loss 0.99275, dsc 0.00725\n",
      "Batch train [17] loss 0.97064, dsc 0.02936\n",
      "Batch train [18] loss 0.98658, dsc 0.01342\n",
      "Batch train [19] loss 0.98430, dsc 0.01570\n",
      "Batch train [20] loss 0.98824, dsc 0.01176\n",
      "Epoch [41] train done\n",
      "Batch eval [1] loss 0.99387, dsc 0.00613\n",
      "Batch eval [2] loss 0.99626, dsc 0.00374\n",
      "Batch eval [3] loss 0.98732, dsc 0.01268\n",
      "Batch eval [4] loss 0.98107, dsc 0.01893\n",
      "Batch eval [5] loss 0.98686, dsc 0.01314\n",
      "Epoch [41] valid done\n",
      "Epoch [41] T 2690.33s, deltaT 66.04s, loss: train 0.98677, valid 0.98908, dsc: train 0.01323, valid 0.01092\n",
      "Batch train [1] loss 0.98593, dsc 0.01407\n",
      "Batch train [2] loss 0.97562, dsc 0.02438\n",
      "Batch train [3] loss 0.97956, dsc 0.02044\n",
      "Batch train [4] loss 0.99130, dsc 0.00870\n",
      "Batch train [5] loss 0.99200, dsc 0.00800\n",
      "Batch train [6] loss 0.96933, dsc 0.03067\n",
      "Batch train [7] loss 0.98752, dsc 0.01248\n",
      "Batch train [8] loss 0.99236, dsc 0.00764\n",
      "Batch train [9] loss 0.98842, dsc 0.01158\n",
      "Batch train [10] loss 0.98937, dsc 0.01063\n",
      "Batch train [11] loss 0.98652, dsc 0.01348\n",
      "Batch train [12] loss 0.97061, dsc 0.02939\n",
      "Batch train [13] loss 0.97895, dsc 0.02105\n",
      "Batch train [14] loss 0.98667, dsc 0.01333\n",
      "Batch train [15] loss 0.99023, dsc 0.00977\n",
      "Batch train [16] loss 0.98957, dsc 0.01043\n",
      "Batch train [17] loss 0.97626, dsc 0.02374\n",
      "Batch train [18] loss 0.98872, dsc 0.01128\n",
      "Batch train [19] loss 0.98861, dsc 0.01139\n",
      "Batch train [20] loss 0.98812, dsc 0.01188\n",
      "Epoch [42] train done\n",
      "Batch eval [1] loss 0.99646, dsc 0.00354\n",
      "Batch eval [2] loss 0.99772, dsc 0.00228\n",
      "Batch eval [3] loss 0.99271, dsc 0.00729\n",
      "Batch eval [4] loss 0.98790, dsc 0.01210\n",
      "Batch eval [5] loss 0.99220, dsc 0.00780\n",
      "Epoch [42] valid done\n",
      "Epoch [42] T 2757.15s, deltaT 66.82s, loss: train 0.98478, valid 0.99340, dsc: train 0.01522, valid 0.00660\n",
      "Batch train [1] loss 0.97903, dsc 0.02097\n",
      "Batch train [2] loss 0.99449, dsc 0.00551\n",
      "Batch train [3] loss 0.99179, dsc 0.00821\n",
      "Batch train [4] loss 0.98485, dsc 0.01515\n",
      "Batch train [5] loss 0.98838, dsc 0.01162\n",
      "Batch train [6] loss 0.98619, dsc 0.01381\n",
      "Batch train [7] loss 0.98650, dsc 0.01350\n",
      "Batch train [8] loss 0.98466, dsc 0.01534\n",
      "Batch train [9] loss 0.98153, dsc 0.01847\n",
      "Batch train [10] loss 0.97714, dsc 0.02286\n",
      "Batch train [11] loss 0.98765, dsc 0.01235\n",
      "Batch train [12] loss 0.97689, dsc 0.02311\n",
      "Batch train [13] loss 0.98254, dsc 0.01746\n",
      "Batch train [14] loss 0.97272, dsc 0.02728\n",
      "Batch train [15] loss 0.97671, dsc 0.02329\n",
      "Batch train [16] loss 0.97212, dsc 0.02788\n",
      "Batch train [17] loss 0.99083, dsc 0.00917\n",
      "Batch train [18] loss 0.98823, dsc 0.01177\n",
      "Batch train [19] loss 0.96272, dsc 0.03728\n",
      "Batch train [20] loss 0.97687, dsc 0.02313\n",
      "Epoch [43] train done\n",
      "Batch eval [1] loss 0.99195, dsc 0.00805\n",
      "Batch eval [2] loss 0.99488, dsc 0.00512\n",
      "Batch eval [3] loss 0.98145, dsc 0.01855\n",
      "Batch eval [4] loss 0.97545, dsc 0.02455\n",
      "Batch eval [5] loss 0.98253, dsc 0.01747\n",
      "Epoch [43] valid done\n",
      "Epoch [43] T 2823.45s, deltaT 66.31s, loss: train 0.98209, valid 0.98525, dsc: train 0.01791, valid 0.01475\n",
      "Batch train [1] loss 0.97594, dsc 0.02406\n",
      "Batch train [2] loss 0.98421, dsc 0.01579\n",
      "Batch train [3] loss 0.96195, dsc 0.03805\n",
      "Batch train [4] loss 0.97829, dsc 0.02171\n",
      "Batch train [5] loss 0.97899, dsc 0.02101\n",
      "Batch train [6] loss 0.98919, dsc 0.01081\n",
      "Batch train [7] loss 0.99481, dsc 0.00519\n",
      "Batch train [8] loss 0.97151, dsc 0.02849\n",
      "Batch train [9] loss 0.96543, dsc 0.03457\n",
      "Batch train [10] loss 0.97667, dsc 0.02333\n",
      "Batch train [11] loss 0.98066, dsc 0.01934\n",
      "Batch train [12] loss 0.98824, dsc 0.01176\n",
      "Batch train [13] loss 0.97689, dsc 0.02311\n",
      "Batch train [14] loss 0.95580, dsc 0.04420\n",
      "Batch train [15] loss 0.97598, dsc 0.02402\n",
      "Batch train [16] loss 0.97671, dsc 0.02329\n",
      "Batch train [17] loss 0.97722, dsc 0.02278\n",
      "Batch train [18] loss 0.97318, dsc 0.02682\n",
      "Batch train [19] loss 0.97761, dsc 0.02239\n",
      "Batch train [20] loss 0.96618, dsc 0.03382\n",
      "Epoch [44] train done\n",
      "Batch eval [1] loss 0.99026, dsc 0.00974\n",
      "Batch eval [2] loss 0.99271, dsc 0.00729\n",
      "Batch eval [3] loss 0.98399, dsc 0.01601\n",
      "Batch eval [4] loss 0.96515, dsc 0.03485\n",
      "Batch eval [5] loss 0.97471, dsc 0.02529\n",
      "Epoch [44] valid done\n",
      "Epoch [44] T 2888.47s, deltaT 65.01s, loss: train 0.97627, valid 0.98136, dsc: train 0.02373, valid 0.01864\n",
      "Batch train [1] loss 0.97831, dsc 0.02169\n",
      "Batch train [2] loss 0.99316, dsc 0.00684\n",
      "Batch train [3] loss 0.94168, dsc 0.05832\n",
      "Batch train [4] loss 0.98787, dsc 0.01213\n",
      "Batch train [5] loss 0.96551, dsc 0.03449\n",
      "Batch train [6] loss 0.97702, dsc 0.02298\n",
      "Batch train [7] loss 0.95654, dsc 0.04346\n",
      "Batch train [8] loss 0.97337, dsc 0.02663\n",
      "Batch train [9] loss 0.97198, dsc 0.02802\n",
      "Batch train [10] loss 0.98115, dsc 0.01885\n",
      "Batch train [11] loss 0.96966, dsc 0.03034\n",
      "Batch train [12] loss 0.96953, dsc 0.03047\n",
      "Batch train [13] loss 0.96997, dsc 0.03003\n",
      "Batch train [14] loss 0.96522, dsc 0.03478\n",
      "Batch train [15] loss 0.96380, dsc 0.03620\n",
      "Batch train [16] loss 0.95993, dsc 0.04007\n",
      "Batch train [17] loss 0.95850, dsc 0.04150\n",
      "Batch train [18] loss 0.95893, dsc 0.04107\n",
      "Batch train [19] loss 0.97628, dsc 0.02372\n",
      "Batch train [20] loss 0.93570, dsc 0.06430\n",
      "Epoch [45] train done\n",
      "Batch eval [1] loss 0.98886, dsc 0.01114\n",
      "Batch eval [2] loss 0.99155, dsc 0.00845\n",
      "Batch eval [3] loss 0.99732, dsc 0.00268\n",
      "Batch eval [4] loss 0.96015, dsc 0.03985\n",
      "Batch eval [5] loss 0.99163, dsc 0.00837\n",
      "Epoch [45] valid done\n",
      "Epoch [45] T 2954.19s, deltaT 65.72s, loss: train 0.96771, valid 0.98590, dsc: train 0.03229, valid 0.01410\n",
      "Batch train [1] loss 0.98969, dsc 0.01031\n",
      "Batch train [2] loss 0.96937, dsc 0.03063\n",
      "Batch train [3] loss 0.94799, dsc 0.05201\n",
      "Batch train [4] loss 0.98659, dsc 0.01341\n",
      "Batch train [5] loss 0.95586, dsc 0.04414\n",
      "Batch train [6] loss 0.96404, dsc 0.03596\n",
      "Batch train [7] loss 0.98444, dsc 0.01556\n",
      "Batch train [8] loss 0.98998, dsc 0.01002\n",
      "Batch train [9] loss 0.96515, dsc 0.03485\n",
      "Batch train [10] loss 0.96464, dsc 0.03536\n",
      "Batch train [11] loss 0.96824, dsc 0.03176\n",
      "Batch train [12] loss 0.93611, dsc 0.06389\n",
      "Batch train [13] loss 0.95091, dsc 0.04909\n",
      "Batch train [14] loss 0.97968, dsc 0.02032\n",
      "Batch train [15] loss 0.95908, dsc 0.04092\n",
      "Batch train [16] loss 0.93706, dsc 0.06294\n",
      "Batch train [17] loss 0.92852, dsc 0.07148\n",
      "Batch train [18] loss 0.96281, dsc 0.03719\n",
      "Batch train [19] loss 0.94711, dsc 0.05289\n",
      "Batch train [20] loss 0.97085, dsc 0.02915\n",
      "Epoch [46] train done\n",
      "Batch eval [1] loss 0.99055, dsc 0.00945\n",
      "Batch eval [2] loss 0.99675, dsc 0.00325\n",
      "Batch eval [3] loss 0.99106, dsc 0.00894\n",
      "Batch eval [4] loss 0.95621, dsc 0.04379\n",
      "Batch eval [5] loss 0.99987, dsc 0.00013\n",
      "Epoch [46] valid done\n",
      "Epoch [46] T 3022.60s, deltaT 68.41s, loss: train 0.96291, valid 0.98689, dsc: train 0.03709, valid 0.01311\n",
      "Batch train [1] loss 0.91783, dsc 0.08217\n",
      "Batch train [2] loss 0.94405, dsc 0.05595\n",
      "Batch train [3] loss 0.97343, dsc 0.02657\n",
      "Batch train [4] loss 0.98470, dsc 0.01530\n",
      "Batch train [5] loss 0.95904, dsc 0.04096\n",
      "Batch train [6] loss 0.97024, dsc 0.02976\n",
      "Batch train [7] loss 0.95716, dsc 0.04284\n",
      "Batch train [8] loss 0.94695, dsc 0.05305\n",
      "Batch train [9] loss 0.97166, dsc 0.02834\n",
      "Batch train [10] loss 0.97590, dsc 0.02410\n",
      "Batch train [11] loss 0.95617, dsc 0.04383\n",
      "Batch train [12] loss 0.90974, dsc 0.09026\n",
      "Batch train [13] loss 0.93983, dsc 0.06017\n",
      "Batch train [14] loss 0.95457, dsc 0.04543\n",
      "Batch train [15] loss 0.95912, dsc 0.04088\n",
      "Batch train [16] loss 0.92241, dsc 0.07759\n",
      "Batch train [17] loss 0.96642, dsc 0.03358\n",
      "Batch train [18] loss 0.93589, dsc 0.06411\n",
      "Batch train [19] loss 0.88541, dsc 0.11459\n",
      "Batch train [20] loss 0.92839, dsc 0.07161\n",
      "Epoch [47] train done\n",
      "Batch eval [1] loss 0.98263, dsc 0.01737\n",
      "Batch eval [2] loss 0.98858, dsc 0.01142\n",
      "Batch eval [3] loss 0.96198, dsc 0.03802\n",
      "Batch eval [4] loss 0.94648, dsc 0.05352\n",
      "Batch eval [5] loss 0.95990, dsc 0.04010\n",
      "Epoch [47] valid done\n",
      "Epoch [47] T 3090.88s, deltaT 68.28s, loss: train 0.94795, valid 0.96791, dsc: train 0.05205, valid 0.03209\n",
      "Batch train [1] loss 0.92789, dsc 0.07211\n",
      "Batch train [2] loss 0.96883, dsc 0.03117\n",
      "Batch train [3] loss 0.92751, dsc 0.07249\n",
      "Batch train [4] loss 0.92557, dsc 0.07443\n",
      "Batch train [5] loss 0.96588, dsc 0.03412\n",
      "Batch train [6] loss 0.91659, dsc 0.08341\n",
      "Batch train [7] loss 0.90461, dsc 0.09539\n",
      "Batch train [8] loss 0.90663, dsc 0.09337\n",
      "Batch train [9] loss 0.91829, dsc 0.08171\n",
      "Batch train [10] loss 0.90557, dsc 0.09443\n",
      "Batch train [11] loss 0.94822, dsc 0.05178\n",
      "Batch train [12] loss 0.98564, dsc 0.01436\n",
      "Batch train [13] loss 0.92863, dsc 0.07137\n",
      "Batch train [14] loss 0.86211, dsc 0.13789\n",
      "Batch train [15] loss 0.91573, dsc 0.08427\n",
      "Batch train [16] loss 0.92084, dsc 0.07916\n",
      "Batch train [17] loss 0.98347, dsc 0.01653\n",
      "Batch train [18] loss 0.93475, dsc 0.06525\n",
      "Batch train [19] loss 0.95868, dsc 0.04132\n",
      "Batch train [20] loss 0.93041, dsc 0.06959\n",
      "Epoch [48] train done\n",
      "Batch eval [1] loss 0.97107, dsc 0.02893\n",
      "Batch eval [2] loss 0.97886, dsc 0.02114\n",
      "Batch eval [3] loss 0.94622, dsc 0.05378\n",
      "Batch eval [4] loss 0.91386, dsc 0.08614\n",
      "Batch eval [5] loss 0.93461, dsc 0.06539\n",
      "Epoch [48] valid done\n",
      "Epoch [48] T 3158.92s, deltaT 68.03s, loss: train 0.93179, valid 0.94893, dsc: train 0.06821, valid 0.05107\n",
      "Batch train [1] loss 0.94457, dsc 0.05543\n",
      "Batch train [2] loss 0.91907, dsc 0.08093\n",
      "Batch train [3] loss 0.93119, dsc 0.06881\n",
      "Batch train [4] loss 0.92456, dsc 0.07544\n",
      "Batch train [5] loss 0.94426, dsc 0.05574\n",
      "Batch train [6] loss 0.90189, dsc 0.09811\n",
      "Batch train [7] loss 0.93867, dsc 0.06133\n",
      "Batch train [8] loss 0.97188, dsc 0.02812\n",
      "Batch train [9] loss 0.84546, dsc 0.15454\n",
      "Batch train [10] loss 0.89417, dsc 0.10583\n",
      "Batch train [11] loss 0.95349, dsc 0.04651\n",
      "Batch train [12] loss 0.95892, dsc 0.04108\n",
      "Batch train [13] loss 0.91479, dsc 0.08521\n",
      "Batch train [14] loss 0.91438, dsc 0.08562\n",
      "Batch train [15] loss 0.87447, dsc 0.12553\n",
      "Batch train [16] loss 0.85151, dsc 0.14849\n",
      "Batch train [17] loss 0.94918, dsc 0.05082\n",
      "Batch train [18] loss 0.89357, dsc 0.10643\n",
      "Batch train [19] loss 0.93603, dsc 0.06397\n",
      "Batch train [20] loss 0.81699, dsc 0.18301\n",
      "Epoch [49] train done\n",
      "Batch eval [1] loss 0.94597, dsc 0.05403\n",
      "Batch eval [2] loss 0.96455, dsc 0.03545\n",
      "Batch eval [3] loss 0.97433, dsc 0.02567\n",
      "Batch eval [4] loss 0.85461, dsc 0.14539\n",
      "Batch eval [5] loss 0.92813, dsc 0.07187\n",
      "Epoch [49] valid done\n",
      "Epoch [49] T 3225.64s, deltaT 66.72s, loss: train 0.91395, valid 0.93352, dsc: train 0.08605, valid 0.06648\n",
      "Batch train [1] loss 0.95355, dsc 0.04645\n",
      "Batch train [2] loss 0.94135, dsc 0.05865\n",
      "Batch train [3] loss 0.92240, dsc 0.07760\n",
      "Batch train [4] loss 0.80110, dsc 0.19890\n",
      "Batch train [5] loss 0.97228, dsc 0.02772\n",
      "Batch train [6] loss 0.92309, dsc 0.07691\n",
      "Batch train [7] loss 0.86919, dsc 0.13081\n",
      "Batch train [8] loss 0.87842, dsc 0.12158\n",
      "Batch train [9] loss 0.85136, dsc 0.14864\n",
      "Batch train [10] loss 0.90352, dsc 0.09648\n",
      "Batch train [11] loss 0.82868, dsc 0.17132\n",
      "Batch train [12] loss 0.94059, dsc 0.05941\n",
      "Batch train [13] loss 0.82315, dsc 0.17685\n",
      "Batch train [14] loss 0.89315, dsc 0.10685\n",
      "Batch train [15] loss 0.85160, dsc 0.14840\n",
      "Batch train [16] loss 0.91331, dsc 0.08669\n",
      "Batch train [17] loss 0.85174, dsc 0.14826\n",
      "Batch train [18] loss 0.89974, dsc 0.10026\n",
      "Batch train [19] loss 0.83447, dsc 0.16553\n",
      "Batch train [20] loss 0.84753, dsc 0.15247\n",
      "Epoch [50] train done\n",
      "Batch eval [1] loss 0.95517, dsc 0.04483\n",
      "Batch eval [2] loss 0.98995, dsc 0.01005\n",
      "Batch eval [3] loss 0.99860, dsc 0.00140\n",
      "Batch eval [4] loss 0.84636, dsc 0.15364\n",
      "Batch eval [5] loss 0.93481, dsc 0.06519\n",
      "Epoch [50] valid done\n",
      "Epoch [50] T 3292.44s, deltaT 66.80s, loss: train 0.88501, valid 0.94498, dsc: train 0.11499, valid 0.05502\n",
      "Batch train [1] loss 0.88612, dsc 0.11388\n",
      "Batch train [2] loss 0.85896, dsc 0.14104\n",
      "Batch train [3] loss 0.85747, dsc 0.14253\n",
      "Batch train [4] loss 0.89014, dsc 0.10986\n",
      "Batch train [5] loss 0.74626, dsc 0.25374\n",
      "Batch train [6] loss 0.86324, dsc 0.13676\n",
      "Batch train [7] loss 0.85331, dsc 0.14669\n",
      "Batch train [8] loss 0.85543, dsc 0.14457\n",
      "Batch train [9] loss 0.88985, dsc 0.11015\n",
      "Batch train [10] loss 0.82939, dsc 0.17061\n",
      "Batch train [11] loss 0.83481, dsc 0.16519\n",
      "Batch train [12] loss 0.90326, dsc 0.09674\n",
      "Batch train [13] loss 0.93581, dsc 0.06419\n",
      "Batch train [14] loss 0.86546, dsc 0.13454\n",
      "Batch train [15] loss 0.83953, dsc 0.16047\n",
      "Batch train [16] loss 0.92072, dsc 0.07928\n",
      "Batch train [17] loss 0.87313, dsc 0.12687\n",
      "Batch train [18] loss 0.82265, dsc 0.17735\n",
      "Batch train [19] loss 0.83996, dsc 0.16004\n",
      "Batch train [20] loss 0.81578, dsc 0.18422\n",
      "Epoch [51] train done\n",
      "Batch eval [1] loss 0.91603, dsc 0.08397\n",
      "Batch eval [2] loss 0.94907, dsc 0.05093\n",
      "Batch eval [3] loss 0.90992, dsc 0.09008\n",
      "Batch eval [4] loss 0.80830, dsc 0.19170\n",
      "Batch eval [5] loss 0.85364, dsc 0.14636\n",
      "Epoch [51] valid done\n",
      "Epoch [51] T 3361.17s, deltaT 68.72s, loss: train 0.85906, valid 0.88739, dsc: train 0.14094, valid 0.11261\n",
      "Batch train [1] loss 0.91281, dsc 0.08719\n",
      "Batch train [2] loss 0.76734, dsc 0.23266\n",
      "Batch train [3] loss 0.70475, dsc 0.29525\n",
      "Batch train [4] loss 0.90731, dsc 0.09269\n",
      "Batch train [5] loss 0.77836, dsc 0.22164\n",
      "Batch train [6] loss 0.82996, dsc 0.17004\n",
      "Batch train [7] loss 0.77700, dsc 0.22300\n",
      "Batch train [8] loss 0.78596, dsc 0.21404\n",
      "Batch train [9] loss 0.88602, dsc 0.11398\n",
      "Batch train [10] loss 0.75440, dsc 0.24560\n",
      "Batch train [11] loss 0.88051, dsc 0.11949\n",
      "Batch train [12] loss 0.84302, dsc 0.15698\n",
      "Batch train [13] loss 0.87921, dsc 0.12079\n",
      "Batch train [14] loss 0.87285, dsc 0.12715\n",
      "Batch train [15] loss 0.82752, dsc 0.17248\n",
      "Batch train [16] loss 0.86870, dsc 0.13130\n",
      "Batch train [17] loss 0.86074, dsc 0.13926\n",
      "Batch train [18] loss 0.75701, dsc 0.24299\n",
      "Batch train [19] loss 0.81321, dsc 0.18679\n",
      "Batch train [20] loss 0.87159, dsc 0.12841\n",
      "Epoch [52] train done\n",
      "Batch eval [1] loss 0.97358, dsc 0.02642\n",
      "Batch eval [2] loss 0.98878, dsc 0.01122\n",
      "Batch eval [3] loss 0.99989, dsc 0.00011\n",
      "Batch eval [4] loss 0.83436, dsc 0.16564\n",
      "Batch eval [5] loss 0.92087, dsc 0.07913\n",
      "Epoch [52] valid done\n",
      "Epoch [52] T 3431.34s, deltaT 70.17s, loss: train 0.82891, valid 0.94349, dsc: train 0.17109, valid 0.05651\n",
      "Batch train [1] loss 0.82663, dsc 0.17337\n",
      "Batch train [2] loss 0.98071, dsc 0.01929\n",
      "Batch train [3] loss 0.84686, dsc 0.15314\n",
      "Batch train [4] loss 0.84100, dsc 0.15900\n",
      "Batch train [5] loss 0.70237, dsc 0.29763\n",
      "Batch train [6] loss 0.88503, dsc 0.11497\n",
      "Batch train [7] loss 0.68701, dsc 0.31299\n",
      "Batch train [8] loss 0.86934, dsc 0.13066\n",
      "Batch train [9] loss 0.70038, dsc 0.29962\n",
      "Batch train [10] loss 0.86796, dsc 0.13204\n",
      "Batch train [11] loss 0.74569, dsc 0.25431\n",
      "Batch train [12] loss 0.84235, dsc 0.15765\n",
      "Batch train [13] loss 0.75352, dsc 0.24648\n",
      "Batch train [14] loss 0.75348, dsc 0.24652\n",
      "Batch train [15] loss 0.80741, dsc 0.19259\n",
      "Batch train [16] loss 0.84911, dsc 0.15089\n",
      "Batch train [17] loss 0.57841, dsc 0.42159\n",
      "Batch train [18] loss 0.63507, dsc 0.36493\n",
      "Batch train [19] loss 0.70499, dsc 0.29501\n",
      "Batch train [20] loss 0.76204, dsc 0.23796\n",
      "Epoch [53] train done\n",
      "Batch eval [1] loss 0.89824, dsc 0.10176\n",
      "Batch eval [2] loss 0.91175, dsc 0.08825\n",
      "Batch eval [3] loss 0.99506, dsc 0.00494\n",
      "Batch eval [4] loss 0.66498, dsc 0.33502\n",
      "Batch eval [5] loss 0.77914, dsc 0.22086\n",
      "Epoch [53] valid done\n",
      "Epoch [53] T 3497.67s, deltaT 66.32s, loss: train 0.78197, valid 0.84983, dsc: train 0.21803, valid 0.15017\n",
      "Batch train [1] loss 0.83142, dsc 0.16858\n",
      "Batch train [2] loss 0.74726, dsc 0.25274\n",
      "Batch train [3] loss 0.68707, dsc 0.31293\n",
      "Batch train [4] loss 0.75660, dsc 0.24340\n",
      "Batch train [5] loss 0.60914, dsc 0.39086\n",
      "Batch train [6] loss 0.75861, dsc 0.24139\n",
      "Batch train [7] loss 0.68280, dsc 0.31720\n",
      "Batch train [8] loss 0.78599, dsc 0.21401\n",
      "Batch train [9] loss 0.78125, dsc 0.21875\n",
      "Batch train [10] loss 0.71956, dsc 0.28044\n",
      "Batch train [11] loss 0.58744, dsc 0.41256\n",
      "Batch train [12] loss 0.66960, dsc 0.33040\n",
      "Batch train [13] loss 0.87153, dsc 0.12847\n",
      "Batch train [14] loss 0.77285, dsc 0.22715\n",
      "Batch train [15] loss 0.81826, dsc 0.18174\n",
      "Batch train [16] loss 0.71134, dsc 0.28866\n",
      "Batch train [17] loss 0.79963, dsc 0.20037\n",
      "Batch train [18] loss 0.63241, dsc 0.36759\n",
      "Batch train [19] loss 0.82923, dsc 0.17077\n",
      "Batch train [20] loss 0.79720, dsc 0.20280\n",
      "Epoch [54] train done\n",
      "Batch eval [1] loss 0.85903, dsc 0.14097\n",
      "Batch eval [2] loss 0.89105, dsc 0.10895\n",
      "Batch eval [3] loss 0.98695, dsc 0.01305\n",
      "Batch eval [4] loss 0.70212, dsc 0.29788\n",
      "Batch eval [5] loss 0.75162, dsc 0.24838\n",
      "Epoch [54] valid done\n",
      "Epoch [54] T 3563.17s, deltaT 65.50s, loss: train 0.74246, valid 0.83815, dsc: train 0.25754, valid 0.16185\n",
      "Batch train [1] loss 0.69178, dsc 0.30822\n",
      "Batch train [2] loss 0.73089, dsc 0.26911\n",
      "Batch train [3] loss 0.56211, dsc 0.43789\n",
      "Batch train [4] loss 0.75196, dsc 0.24804\n",
      "Batch train [5] loss 0.75205, dsc 0.24795\n",
      "Batch train [6] loss 0.74390, dsc 0.25610\n",
      "Batch train [7] loss 0.78299, dsc 0.21701\n",
      "Batch train [8] loss 0.83468, dsc 0.16532\n",
      "Batch train [9] loss 0.87690, dsc 0.12310\n",
      "Batch train [10] loss 0.64996, dsc 0.35004\n",
      "Batch train [11] loss 0.68044, dsc 0.31956\n",
      "Batch train [12] loss 0.86878, dsc 0.13122\n",
      "Batch train [13] loss 0.58869, dsc 0.41131\n",
      "Batch train [14] loss 0.80582, dsc 0.19418\n",
      "Batch train [15] loss 0.56999, dsc 0.43001\n",
      "Batch train [16] loss 0.51348, dsc 0.48652\n",
      "Batch train [17] loss 0.62483, dsc 0.37517\n",
      "Batch train [18] loss 0.57433, dsc 0.42567\n",
      "Batch train [19] loss 0.76138, dsc 0.23862\n",
      "Batch train [20] loss 0.63318, dsc 0.36682\n",
      "Epoch [55] train done\n",
      "Batch eval [1] loss 0.88588, dsc 0.11412\n",
      "Batch eval [2] loss 0.96103, dsc 0.03897\n",
      "Batch eval [3] loss 0.99996, dsc 0.00004\n",
      "Batch eval [4] loss 0.54288, dsc 0.45712\n",
      "Batch eval [5] loss 0.77645, dsc 0.22355\n",
      "Epoch [55] valid done\n",
      "Epoch [55] T 3633.13s, deltaT 69.95s, loss: train 0.69991, valid 0.83324, dsc: train 0.30009, valid 0.16676\n",
      "Batch train [1] loss 0.54505, dsc 0.45495\n",
      "Batch train [2] loss 0.77779, dsc 0.22221\n",
      "Batch train [3] loss 0.85467, dsc 0.14533\n",
      "Batch train [4] loss 0.67622, dsc 0.32378\n",
      "Batch train [5] loss 0.65402, dsc 0.34598\n",
      "Batch train [6] loss 0.49361, dsc 0.50639\n",
      "Batch train [7] loss 0.93688, dsc 0.06312\n",
      "Batch train [8] loss 0.52572, dsc 0.47428\n",
      "Batch train [9] loss 0.60690, dsc 0.39310\n",
      "Batch train [10] loss 0.65805, dsc 0.34195\n",
      "Batch train [11] loss 0.65163, dsc 0.34837\n",
      "Batch train [12] loss 0.70565, dsc 0.29435\n",
      "Batch train [13] loss 0.68136, dsc 0.31864\n",
      "Batch train [14] loss 0.79055, dsc 0.20945\n",
      "Batch train [15] loss 0.62596, dsc 0.37404\n",
      "Batch train [16] loss 0.54370, dsc 0.45630\n",
      "Batch train [17] loss 0.62329, dsc 0.37671\n",
      "Batch train [18] loss 0.55820, dsc 0.44180\n",
      "Batch train [19] loss 0.65954, dsc 0.34046\n",
      "Batch train [20] loss 0.70241, dsc 0.29759\n",
      "Epoch [56] train done\n",
      "Batch eval [1] loss 0.91702, dsc 0.08298\n",
      "Batch eval [2] loss 0.99993, dsc 0.00007\n",
      "Batch eval [3] loss 0.99891, dsc 0.00109\n",
      "Batch eval [4] loss 0.72918, dsc 0.27082\n",
      "Batch eval [5] loss 0.81200, dsc 0.18800\n",
      "Epoch [56] valid done\n",
      "Epoch [56] T 3703.17s, deltaT 70.04s, loss: train 0.66356, valid 0.89141, dsc: train 0.33644, valid 0.10859\n",
      "Batch train [1] loss 0.76633, dsc 0.23367\n",
      "Batch train [2] loss 0.60636, dsc 0.39364\n",
      "Batch train [3] loss 0.86121, dsc 0.13879\n",
      "Batch train [4] loss 0.56040, dsc 0.43960\n",
      "Batch train [5] loss 0.54065, dsc 0.45935\n",
      "Batch train [6] loss 0.57977, dsc 0.42023\n",
      "Batch train [7] loss 0.39717, dsc 0.60283\n",
      "Batch train [8] loss 0.54439, dsc 0.45561\n",
      "Batch train [9] loss 0.60687, dsc 0.39313\n",
      "Batch train [10] loss 0.71380, dsc 0.28620\n",
      "Batch train [11] loss 0.49638, dsc 0.50362\n",
      "Batch train [12] loss 0.48375, dsc 0.51625\n",
      "Batch train [13] loss 0.56759, dsc 0.43241\n",
      "Batch train [14] loss 0.65700, dsc 0.34300\n",
      "Batch train [15] loss 0.55281, dsc 0.44719\n",
      "Batch train [16] loss 0.68548, dsc 0.31452\n",
      "Batch train [17] loss 0.65409, dsc 0.34591\n",
      "Batch train [18] loss 0.65688, dsc 0.34312\n",
      "Batch train [19] loss 0.75723, dsc 0.24277\n",
      "Batch train [20] loss 0.65606, dsc 0.34394\n",
      "Epoch [57] train done\n",
      "Batch eval [1] loss 0.84977, dsc 0.15023\n",
      "Batch eval [2] loss 0.93306, dsc 0.06694\n",
      "Batch eval [3] loss 0.99987, dsc 0.00013\n",
      "Batch eval [4] loss 0.46967, dsc 0.53033\n",
      "Batch eval [5] loss 0.70033, dsc 0.29967\n",
      "Epoch [57] valid done\n",
      "Epoch [57] T 3773.02s, deltaT 69.85s, loss: train 0.61721, valid 0.79054, dsc: train 0.38279, valid 0.20946\n",
      "Batch train [1] loss 0.63797, dsc 0.36203\n",
      "Batch train [2] loss 0.61074, dsc 0.38926\n",
      "Batch train [3] loss 0.60014, dsc 0.39986\n",
      "Batch train [4] loss 0.45755, dsc 0.54245\n",
      "Batch train [5] loss 0.75433, dsc 0.24567\n",
      "Batch train [6] loss 0.50782, dsc 0.49218\n",
      "Batch train [7] loss 0.67134, dsc 0.32866\n",
      "Batch train [8] loss 0.50102, dsc 0.49898\n",
      "Batch train [9] loss 0.79313, dsc 0.20687\n",
      "Batch train [10] loss 0.64201, dsc 0.35799\n",
      "Batch train [11] loss 0.43357, dsc 0.56643\n",
      "Batch train [12] loss 0.51374, dsc 0.48626\n",
      "Batch train [13] loss 0.66893, dsc 0.33107\n",
      "Batch train [14] loss 0.32563, dsc 0.67437\n",
      "Batch train [15] loss 0.53836, dsc 0.46164\n",
      "Batch train [16] loss 0.56157, dsc 0.43843\n",
      "Batch train [17] loss 0.58953, dsc 0.41047\n",
      "Batch train [18] loss 0.56765, dsc 0.43235\n",
      "Batch train [19] loss 0.49844, dsc 0.50156\n",
      "Batch train [20] loss 0.65628, dsc 0.34372\n",
      "Epoch [58] train done\n",
      "Batch eval [1] loss 0.80092, dsc 0.19908\n",
      "Batch eval [2] loss 0.84527, dsc 0.15473\n",
      "Batch eval [3] loss 0.99460, dsc 0.00540\n",
      "Batch eval [4] loss 0.37344, dsc 0.62656\n",
      "Batch eval [5] loss 0.61043, dsc 0.38957\n",
      "Epoch [58] valid done\n",
      "Epoch [58] T 3843.00s, deltaT 69.98s, loss: train 0.57649, valid 0.72493, dsc: train 0.42351, valid 0.27507\n",
      "Batch train [1] loss 0.47269, dsc 0.52731\n",
      "Batch train [2] loss 0.45031, dsc 0.54969\n",
      "Batch train [3] loss 0.46036, dsc 0.53964\n",
      "Batch train [4] loss 0.91713, dsc 0.08287\n",
      "Batch train [5] loss 0.48219, dsc 0.51781\n",
      "Batch train [6] loss 0.59669, dsc 0.40331\n",
      "Batch train [7] loss 0.47483, dsc 0.52517\n",
      "Batch train [8] loss 0.68670, dsc 0.31330\n",
      "Batch train [9] loss 0.35962, dsc 0.64038\n",
      "Batch train [10] loss 0.55662, dsc 0.44338\n",
      "Batch train [11] loss 0.53276, dsc 0.46724\n",
      "Batch train [12] loss 0.47782, dsc 0.52218\n",
      "Batch train [13] loss 0.56857, dsc 0.43143\n",
      "Batch train [14] loss 0.46208, dsc 0.53792\n",
      "Batch train [15] loss 0.66062, dsc 0.33938\n",
      "Batch train [16] loss 0.51177, dsc 0.48823\n",
      "Batch train [17] loss 0.71531, dsc 0.28469\n",
      "Batch train [18] loss 0.56609, dsc 0.43391\n",
      "Batch train [19] loss 0.67574, dsc 0.32426\n",
      "Batch train [20] loss 0.34976, dsc 0.65024\n",
      "Epoch [59] train done\n",
      "Batch eval [1] loss 0.76763, dsc 0.23237\n",
      "Batch eval [2] loss 0.72871, dsc 0.27129\n",
      "Batch eval [3] loss 0.97039, dsc 0.02961\n",
      "Batch eval [4] loss 0.40468, dsc 0.59532\n",
      "Batch eval [5] loss 0.59279, dsc 0.40721\n",
      "Epoch [59] valid done\n",
      "Epoch [59] T 3913.09s, deltaT 70.09s, loss: train 0.54888, valid 0.69284, dsc: train 0.45112, valid 0.30716\n",
      "Batch train [1] loss 0.58458, dsc 0.41542\n",
      "Batch train [2] loss 0.51445, dsc 0.48555\n",
      "Batch train [3] loss 0.34067, dsc 0.65933\n",
      "Batch train [4] loss 0.60399, dsc 0.39601\n",
      "Batch train [5] loss 0.74060, dsc 0.25940\n",
      "Batch train [6] loss 0.50009, dsc 0.49991\n",
      "Batch train [7] loss 0.37060, dsc 0.62940\n",
      "Batch train [8] loss 0.44930, dsc 0.55070\n",
      "Batch train [9] loss 0.51257, dsc 0.48743\n",
      "Batch train [10] loss 0.50915, dsc 0.49085\n",
      "Batch train [11] loss 0.48951, dsc 0.51049\n",
      "Batch train [12] loss 0.48721, dsc 0.51279\n",
      "Batch train [13] loss 0.58536, dsc 0.41464\n",
      "Batch train [14] loss 0.82007, dsc 0.17993\n",
      "Batch train [15] loss 0.49699, dsc 0.50301\n",
      "Batch train [16] loss 0.41574, dsc 0.58426\n",
      "Batch train [17] loss 0.28755, dsc 0.71245\n",
      "Batch train [18] loss 0.50270, dsc 0.49730\n",
      "Batch train [19] loss 0.42691, dsc 0.57309\n",
      "Batch train [20] loss 0.52462, dsc 0.47538\n",
      "Epoch [60] train done\n",
      "Batch eval [1] loss 0.56087, dsc 0.43913\n",
      "Batch eval [2] loss 0.83168, dsc 0.16832\n",
      "Batch eval [3] loss 0.99977, dsc 0.00023\n",
      "Batch eval [4] loss 0.40491, dsc 0.59509\n",
      "Batch eval [5] loss 0.79410, dsc 0.20590\n",
      "Epoch [60] valid done\n",
      "Epoch [60] T 3982.83s, deltaT 69.74s, loss: train 0.50813, valid 0.71826, dsc: train 0.49187, valid 0.28174\n",
      "Batch train [1] loss 0.55854, dsc 0.44146\n",
      "Batch train [2] loss 0.55153, dsc 0.44847\n",
      "Batch train [3] loss 0.59973, dsc 0.40027\n",
      "Batch train [4] loss 0.61300, dsc 0.38700\n",
      "Batch train [5] loss 0.47622, dsc 0.52378\n",
      "Batch train [6] loss 0.40416, dsc 0.59584\n",
      "Batch train [7] loss 0.65275, dsc 0.34725\n",
      "Batch train [8] loss 0.56805, dsc 0.43195\n",
      "Batch train [9] loss 0.26780, dsc 0.73220\n",
      "Batch train [10] loss 0.45634, dsc 0.54366\n",
      "Batch train [11] loss 0.46825, dsc 0.53175\n",
      "Batch train [12] loss 0.37231, dsc 0.62769\n",
      "Batch train [13] loss 0.46974, dsc 0.53026\n",
      "Batch train [14] loss 0.53822, dsc 0.46178\n",
      "Batch train [15] loss 0.48654, dsc 0.51346\n",
      "Batch train [16] loss 0.44293, dsc 0.55707\n",
      "Batch train [17] loss 0.38295, dsc 0.61705\n",
      "Batch train [18] loss 0.36357, dsc 0.63643\n",
      "Batch train [19] loss 0.48654, dsc 0.51346\n",
      "Batch train [20] loss 0.43210, dsc 0.56790\n",
      "Epoch [61] train done\n",
      "Batch eval [1] loss 0.74275, dsc 0.25725\n",
      "Batch eval [2] loss 0.80843, dsc 0.19157\n",
      "Batch eval [3] loss 0.99883, dsc 0.00117\n",
      "Batch eval [4] loss 0.32556, dsc 0.67444\n",
      "Batch eval [5] loss 0.49507, dsc 0.50493\n",
      "Epoch [61] valid done\n",
      "Epoch [61] T 4051.84s, deltaT 69.01s, loss: train 0.47956, valid 0.67413, dsc: train 0.52044, valid 0.32587\n",
      "Batch train [1] loss 0.36176, dsc 0.63824\n",
      "Batch train [2] loss 0.61049, dsc 0.38951\n",
      "Batch train [3] loss 0.53465, dsc 0.46535\n",
      "Batch train [4] loss 0.33919, dsc 0.66081\n",
      "Batch train [5] loss 0.64344, dsc 0.35656\n",
      "Batch train [6] loss 0.46350, dsc 0.53650\n",
      "Batch train [7] loss 0.39545, dsc 0.60455\n",
      "Batch train [8] loss 0.53967, dsc 0.46033\n",
      "Batch train [9] loss 0.43881, dsc 0.56119\n",
      "Batch train [10] loss 0.44663, dsc 0.55337\n",
      "Batch train [11] loss 0.44242, dsc 0.55758\n",
      "Batch train [12] loss 0.30822, dsc 0.69178\n",
      "Batch train [13] loss 0.78478, dsc 0.21522\n",
      "Batch train [14] loss 0.45080, dsc 0.54920\n",
      "Batch train [15] loss 0.23886, dsc 0.76114\n",
      "Batch train [16] loss 0.34280, dsc 0.65720\n",
      "Batch train [17] loss 0.47083, dsc 0.52917\n",
      "Batch train [18] loss 0.58978, dsc 0.41022\n",
      "Batch train [19] loss 0.52578, dsc 0.47422\n",
      "Batch train [20] loss 0.72337, dsc 0.27663\n",
      "Epoch [62] train done\n",
      "Batch eval [1] loss 0.70212, dsc 0.29788\n",
      "Batch eval [2] loss 0.86301, dsc 0.13699\n",
      "Batch eval [3] loss 0.95521, dsc 0.04479\n",
      "Batch eval [4] loss 0.32991, dsc 0.67009\n",
      "Batch eval [5] loss 0.70271, dsc 0.29729\n",
      "Epoch [62] valid done\n",
      "Epoch [62] T 4119.07s, deltaT 67.23s, loss: train 0.48256, valid 0.71059, dsc: train 0.51744, valid 0.28941\n",
      "Batch train [1] loss 0.36268, dsc 0.63732\n",
      "Batch train [2] loss 0.49225, dsc 0.50775\n",
      "Batch train [3] loss 0.49055, dsc 0.50945\n",
      "Batch train [4] loss 0.41358, dsc 0.58642\n",
      "Batch train [5] loss 0.35315, dsc 0.64685\n",
      "Batch train [6] loss 0.35619, dsc 0.64381\n",
      "Batch train [7] loss 0.45861, dsc 0.54139\n",
      "Batch train [8] loss 0.35902, dsc 0.64098\n",
      "Batch train [9] loss 0.45308, dsc 0.54692\n",
      "Batch train [10] loss 0.47959, dsc 0.52041\n",
      "Batch train [11] loss 0.81521, dsc 0.18479\n",
      "Batch train [12] loss 0.61838, dsc 0.38162\n",
      "Batch train [13] loss 0.43502, dsc 0.56498\n",
      "Batch train [14] loss 0.48285, dsc 0.51715\n",
      "Batch train [15] loss 0.32164, dsc 0.67836\n",
      "Batch train [16] loss 0.45908, dsc 0.54092\n",
      "Batch train [17] loss 0.39812, dsc 0.60188\n",
      "Batch train [18] loss 0.48261, dsc 0.51739\n",
      "Batch train [19] loss 0.44609, dsc 0.55391\n",
      "Batch train [20] loss 0.39558, dsc 0.60442\n",
      "Epoch [63] train done\n",
      "Batch eval [1] loss 0.65832, dsc 0.34168\n",
      "Batch eval [2] loss 0.67682, dsc 0.32318\n",
      "Batch eval [3] loss 0.87309, dsc 0.12691\n",
      "Batch eval [4] loss 0.24904, dsc 0.75096\n",
      "Batch eval [5] loss 0.53641, dsc 0.46359\n",
      "Epoch [63] valid done\n",
      "Epoch [63] T 4186.35s, deltaT 67.27s, loss: train 0.45366, valid 0.59874, dsc: train 0.54634, valid 0.40126\n",
      "Batch train [1] loss 0.30700, dsc 0.69300\n",
      "Batch train [2] loss 0.57031, dsc 0.42969\n",
      "Batch train [3] loss 0.37562, dsc 0.62438\n",
      "Batch train [4] loss 0.67181, dsc 0.32819\n",
      "Batch train [5] loss 0.39184, dsc 0.60816\n",
      "Batch train [6] loss 0.31991, dsc 0.68009\n",
      "Batch train [7] loss 0.42069, dsc 0.57931\n",
      "Batch train [8] loss 0.38471, dsc 0.61529\n",
      "Batch train [9] loss 0.36578, dsc 0.63422\n",
      "Batch train [10] loss 0.34974, dsc 0.65026\n",
      "Batch train [11] loss 0.62867, dsc 0.37133\n",
      "Batch train [12] loss 0.41074, dsc 0.58926\n",
      "Batch train [13] loss 0.34682, dsc 0.65318\n",
      "Batch train [14] loss 0.38895, dsc 0.61105\n",
      "Batch train [15] loss 0.67240, dsc 0.32760\n",
      "Batch train [16] loss 0.39552, dsc 0.60448\n",
      "Batch train [17] loss 0.67950, dsc 0.32050\n",
      "Batch train [18] loss 0.36510, dsc 0.63490\n",
      "Batch train [19] loss 0.67902, dsc 0.32098\n",
      "Batch train [20] loss 0.64791, dsc 0.35209\n",
      "Epoch [64] train done\n",
      "Batch eval [1] loss 0.99786, dsc 0.00214\n",
      "Batch eval [2] loss 0.62677, dsc 0.37323\n",
      "Batch eval [3] loss 0.99994, dsc 0.00006\n",
      "Batch eval [4] loss 0.34959, dsc 0.65041\n",
      "Batch eval [5] loss 0.46904, dsc 0.53096\n",
      "Epoch [64] valid done\n",
      "Epoch [64] T 4251.48s, deltaT 65.13s, loss: train 0.46860, valid 0.68864, dsc: train 0.53140, valid 0.31136\n",
      "Batch train [1] loss 0.62554, dsc 0.37446\n",
      "Batch train [2] loss 0.44579, dsc 0.55421\n",
      "Batch train [3] loss 0.42239, dsc 0.57761\n",
      "Batch train [4] loss 0.22437, dsc 0.77563\n",
      "Batch train [5] loss 0.56482, dsc 0.43518\n",
      "Batch train [6] loss 0.43406, dsc 0.56594\n",
      "Batch train [7] loss 0.75211, dsc 0.24789\n",
      "Batch train [8] loss 0.33344, dsc 0.66656\n",
      "Batch train [9] loss 0.37987, dsc 0.62013\n",
      "Batch train [10] loss 0.55714, dsc 0.44286\n",
      "Batch train [11] loss 0.43249, dsc 0.56751\n",
      "Batch train [12] loss 0.38989, dsc 0.61011\n",
      "Batch train [13] loss 0.42489, dsc 0.57511\n",
      "Batch train [14] loss 0.38792, dsc 0.61208\n",
      "Batch train [15] loss 0.71558, dsc 0.28442\n",
      "Batch train [16] loss 0.36298, dsc 0.63702\n",
      "Batch train [17] loss 0.59595, dsc 0.40405\n",
      "Batch train [18] loss 0.58929, dsc 0.41071\n",
      "Batch train [19] loss 0.38859, dsc 0.61141\n",
      "Batch train [20] loss 0.28059, dsc 0.71941\n",
      "Epoch [65] train done\n",
      "Batch eval [1] loss 0.92626, dsc 0.07374\n",
      "Batch eval [2] loss 0.93811, dsc 0.06189\n",
      "Batch eval [3] loss 0.99997, dsc 0.00003\n",
      "Batch eval [4] loss 0.27820, dsc 0.72180\n",
      "Batch eval [5] loss 0.74247, dsc 0.25753\n",
      "Epoch [65] valid done\n",
      "Epoch [65] T 4316.97s, deltaT 65.49s, loss: train 0.46539, valid 0.77700, dsc: train 0.53461, valid 0.22300\n",
      "Batch train [1] loss 0.52676, dsc 0.47324\n",
      "Batch train [2] loss 0.41723, dsc 0.58277\n",
      "Batch train [3] loss 0.57411, dsc 0.42589\n",
      "Batch train [4] loss 0.54827, dsc 0.45173\n",
      "Batch train [5] loss 0.31654, dsc 0.68346\n",
      "Batch train [6] loss 0.25342, dsc 0.74658\n",
      "Batch train [7] loss 0.33150, dsc 0.66850\n",
      "Batch train [8] loss 0.77084, dsc 0.22916\n",
      "Batch train [9] loss 0.34000, dsc 0.66000\n",
      "Batch train [10] loss 0.58559, dsc 0.41441\n",
      "Batch train [11] loss 0.60429, dsc 0.39571\n",
      "Batch train [12] loss 0.32588, dsc 0.67412\n",
      "Batch train [13] loss 0.43783, dsc 0.56217\n",
      "Batch train [14] loss 0.21941, dsc 0.78059\n",
      "Batch train [15] loss 0.59183, dsc 0.40817\n",
      "Batch train [16] loss 0.36659, dsc 0.63341\n",
      "Batch train [17] loss 0.53812, dsc 0.46188\n",
      "Batch train [18] loss 0.42033, dsc 0.57967\n",
      "Batch train [19] loss 0.37278, dsc 0.62722\n",
      "Batch train [20] loss 0.44671, dsc 0.55329\n",
      "Epoch [66] train done\n",
      "Batch eval [1] loss 0.71479, dsc 0.28521\n",
      "Batch eval [2] loss 0.69271, dsc 0.30729\n",
      "Batch eval [3] loss 0.99917, dsc 0.00083\n",
      "Batch eval [4] loss 0.24923, dsc 0.75077\n",
      "Batch eval [5] loss 0.43609, dsc 0.56391\n",
      "Epoch [66] valid done\n",
      "Epoch [66] T 4382.84s, deltaT 65.87s, loss: train 0.44940, valid 0.61840, dsc: train 0.55060, valid 0.38160\n",
      "Batch train [1] loss 0.57523, dsc 0.42477\n",
      "Batch train [2] loss 0.24538, dsc 0.75462\n",
      "Batch train [3] loss 0.40127, dsc 0.59873\n",
      "Batch train [4] loss 0.26172, dsc 0.73828\n",
      "Batch train [5] loss 0.34806, dsc 0.65194\n",
      "Batch train [6] loss 0.35777, dsc 0.64223\n",
      "Batch train [7] loss 0.32085, dsc 0.67915\n",
      "Batch train [8] loss 0.52035, dsc 0.47965\n",
      "Batch train [9] loss 0.61206, dsc 0.38794\n",
      "Batch train [10] loss 0.39639, dsc 0.60361\n",
      "Batch train [11] loss 0.37497, dsc 0.62503\n",
      "Batch train [12] loss 0.31150, dsc 0.68850\n",
      "Batch train [13] loss 0.32429, dsc 0.67571\n",
      "Batch train [14] loss 0.40878, dsc 0.59122\n",
      "Batch train [15] loss 0.47315, dsc 0.52685\n",
      "Batch train [16] loss 0.58428, dsc 0.41572\n",
      "Batch train [17] loss 0.36817, dsc 0.63183\n",
      "Batch train [18] loss 0.45299, dsc 0.54701\n",
      "Batch train [19] loss 0.40542, dsc 0.59458\n",
      "Batch train [20] loss 0.46499, dsc 0.53501\n",
      "Epoch [67] train done\n",
      "Batch eval [1] loss 0.46535, dsc 0.53465\n",
      "Batch eval [2] loss 0.64530, dsc 0.35470\n",
      "Batch eval [3] loss 0.91427, dsc 0.08573\n",
      "Batch eval [4] loss 0.27384, dsc 0.72616\n",
      "Batch eval [5] loss 0.36309, dsc 0.63691\n",
      "Epoch [67] valid done\n",
      "Epoch [67] T 4448.22s, deltaT 65.38s, loss: train 0.41038, valid 0.53237, dsc: train 0.58962, valid 0.46763\n",
      "Batch train [1] loss 0.54546, dsc 0.45454\n",
      "Batch train [2] loss 0.33533, dsc 0.66467\n",
      "Batch train [3] loss 0.36147, dsc 0.63853\n",
      "Batch train [4] loss 0.28203, dsc 0.71797\n",
      "Batch train [5] loss 0.50161, dsc 0.49839\n",
      "Batch train [6] loss 0.30646, dsc 0.69354\n",
      "Batch train [7] loss 0.37971, dsc 0.62029\n",
      "Batch train [8] loss 0.33673, dsc 0.66327\n",
      "Batch train [9] loss 0.29033, dsc 0.70967\n",
      "Batch train [10] loss 0.36140, dsc 0.63860\n",
      "Batch train [11] loss 0.29613, dsc 0.70387\n",
      "Batch train [12] loss 0.65334, dsc 0.34666\n",
      "Batch train [13] loss 0.39612, dsc 0.60388\n",
      "Batch train [14] loss 0.51963, dsc 0.48037\n",
      "Batch train [15] loss 0.33447, dsc 0.66553\n",
      "Batch train [16] loss 0.48407, dsc 0.51593\n",
      "Batch train [17] loss 0.33099, dsc 0.66901\n",
      "Batch train [18] loss 0.39461, dsc 0.60539\n",
      "Batch train [19] loss 0.35869, dsc 0.64131\n",
      "Batch train [20] loss 0.41908, dsc 0.58092\n",
      "Epoch [68] train done\n",
      "Batch eval [1] loss 0.59190, dsc 0.40810\n",
      "Batch eval [2] loss 0.60726, dsc 0.39274\n",
      "Batch eval [3] loss 0.99962, dsc 0.00038\n",
      "Batch eval [4] loss 0.35172, dsc 0.64828\n",
      "Batch eval [5] loss 0.73791, dsc 0.26209\n",
      "Epoch [68] valid done\n",
      "Epoch [68] T 4513.50s, deltaT 65.28s, loss: train 0.39438, valid 0.65768, dsc: train 0.60562, valid 0.34232\n",
      "Batch train [1] loss 0.48328, dsc 0.51672\n",
      "Batch train [2] loss 0.55923, dsc 0.44077\n",
      "Batch train [3] loss 0.27953, dsc 0.72047\n",
      "Batch train [4] loss 0.39620, dsc 0.60380\n",
      "Batch train [5] loss 0.45364, dsc 0.54636\n",
      "Batch train [6] loss 0.29013, dsc 0.70987\n",
      "Batch train [7] loss 0.57145, dsc 0.42855\n",
      "Batch train [8] loss 0.46158, dsc 0.53842\n",
      "Batch train [9] loss 0.34734, dsc 0.65266\n",
      "Batch train [10] loss 0.31138, dsc 0.68862\n",
      "Batch train [11] loss 0.28224, dsc 0.71776\n",
      "Batch train [12] loss 0.39146, dsc 0.60854\n",
      "Batch train [13] loss 0.32272, dsc 0.67728\n",
      "Batch train [14] loss 0.50977, dsc 0.49023\n",
      "Batch train [15] loss 0.26634, dsc 0.73366\n",
      "Batch train [16] loss 0.40100, dsc 0.59900\n",
      "Batch train [17] loss 0.31265, dsc 0.68735\n",
      "Batch train [18] loss 0.30985, dsc 0.69015\n",
      "Batch train [19] loss 0.27276, dsc 0.72724\n",
      "Batch train [20] loss 0.46935, dsc 0.53065\n",
      "Epoch [69] train done\n",
      "Batch eval [1] loss 0.66796, dsc 0.33204\n",
      "Batch eval [2] loss 0.63299, dsc 0.36701\n",
      "Batch eval [3] loss 0.80256, dsc 0.19744\n",
      "Batch eval [4] loss 0.22283, dsc 0.77717\n",
      "Batch eval [5] loss 0.46446, dsc 0.53554\n",
      "Epoch [69] valid done\n",
      "Epoch [69] T 4577.65s, deltaT 64.16s, loss: train 0.38460, valid 0.55816, dsc: train 0.61540, valid 0.44184\n",
      "Batch train [1] loss 0.55601, dsc 0.44399\n",
      "Batch train [2] loss 0.34670, dsc 0.65330\n",
      "Batch train [3] loss 0.45723, dsc 0.54277\n",
      "Batch train [4] loss 0.52471, dsc 0.47529\n",
      "Batch train [5] loss 0.27346, dsc 0.72654\n",
      "Batch train [6] loss 0.22688, dsc 0.77312\n",
      "Batch train [7] loss 0.41243, dsc 0.58757\n",
      "Batch train [8] loss 0.41131, dsc 0.58869\n",
      "Batch train [9] loss 0.64850, dsc 0.35150\n",
      "Batch train [10] loss 0.27843, dsc 0.72157\n",
      "Batch train [11] loss 0.34361, dsc 0.65639\n",
      "Batch train [12] loss 0.31113, dsc 0.68887\n",
      "Batch train [13] loss 0.48297, dsc 0.51703\n",
      "Batch train [14] loss 0.31795, dsc 0.68205\n",
      "Batch train [15] loss 0.80498, dsc 0.19502\n",
      "Batch train [16] loss 0.31001, dsc 0.68999\n",
      "Batch train [17] loss 0.29955, dsc 0.70045\n",
      "Batch train [18] loss 0.38337, dsc 0.61663\n",
      "Batch train [19] loss 0.36778, dsc 0.63222\n",
      "Batch train [20] loss 0.25051, dsc 0.74949\n",
      "Epoch [70] train done\n",
      "Batch eval [1] loss 0.71802, dsc 0.28198\n",
      "Batch eval [2] loss 0.74690, dsc 0.25310\n",
      "Batch eval [3] loss 0.54590, dsc 0.45410\n",
      "Batch eval [4] loss 0.24815, dsc 0.75185\n",
      "Batch eval [5] loss 0.48441, dsc 0.51559\n",
      "Epoch [70] valid done\n",
      "Epoch [70] T 4644.19s, deltaT 66.54s, loss: train 0.40038, valid 0.54868, dsc: train 0.59962, valid 0.45132\n",
      "Batch train [1] loss 0.35635, dsc 0.64365\n",
      "Batch train [2] loss 0.36775, dsc 0.63225\n",
      "Batch train [3] loss 0.37140, dsc 0.62860\n",
      "Batch train [4] loss 0.29501, dsc 0.70499\n",
      "Batch train [5] loss 0.44971, dsc 0.55029\n",
      "Batch train [6] loss 0.45039, dsc 0.54961\n",
      "Batch train [7] loss 0.32846, dsc 0.67154\n",
      "Batch train [8] loss 0.69736, dsc 0.30264\n",
      "Batch train [9] loss 0.55496, dsc 0.44504\n",
      "Batch train [10] loss 0.25614, dsc 0.74386\n",
      "Batch train [11] loss 0.34024, dsc 0.65976\n",
      "Batch train [12] loss 0.30437, dsc 0.69563\n",
      "Batch train [13] loss 0.39689, dsc 0.60311\n",
      "Batch train [14] loss 0.65382, dsc 0.34618\n",
      "Batch train [15] loss 0.36952, dsc 0.63048\n",
      "Batch train [16] loss 0.37625, dsc 0.62375\n",
      "Batch train [17] loss 0.69079, dsc 0.30921\n",
      "Batch train [18] loss 0.39526, dsc 0.60474\n",
      "Batch train [19] loss 0.36573, dsc 0.63427\n",
      "Batch train [20] loss 0.49093, dsc 0.50907\n",
      "Epoch [71] train done\n",
      "Batch eval [1] loss 0.62935, dsc 0.37065\n",
      "Batch eval [2] loss 0.99649, dsc 0.00351\n",
      "Batch eval [3] loss 0.99361, dsc 0.00639\n",
      "Batch eval [4] loss 0.33748, dsc 0.66252\n",
      "Batch eval [5] loss 0.65170, dsc 0.34830\n",
      "Epoch [71] valid done\n",
      "Epoch [71] T 4708.66s, deltaT 64.47s, loss: train 0.42557, valid 0.72172, dsc: train 0.57443, valid 0.27828\n",
      "Batch train [1] loss 0.33226, dsc 0.66774\n",
      "Batch train [2] loss 0.26129, dsc 0.73871\n",
      "Batch train [3] loss 0.32316, dsc 0.67684\n",
      "Batch train [4] loss 0.53418, dsc 0.46582\n",
      "Batch train [5] loss 0.25321, dsc 0.74679\n",
      "Batch train [6] loss 0.39674, dsc 0.60326\n",
      "Batch train [7] loss 0.54264, dsc 0.45736\n",
      "Batch train [8] loss 0.31682, dsc 0.68318\n",
      "Batch train [9] loss 0.40640, dsc 0.59360\n",
      "Batch train [10] loss 0.33860, dsc 0.66140\n",
      "Batch train [11] loss 0.26450, dsc 0.73550\n",
      "Batch train [12] loss 0.59801, dsc 0.40199\n",
      "Batch train [13] loss 0.25981, dsc 0.74019\n",
      "Batch train [14] loss 0.35333, dsc 0.64667\n",
      "Batch train [15] loss 0.59970, dsc 0.40030\n",
      "Batch train [16] loss 0.33007, dsc 0.66993\n",
      "Batch train [17] loss 0.41310, dsc 0.58690\n",
      "Batch train [18] loss 0.20458, dsc 0.79542\n",
      "Batch train [19] loss 0.34268, dsc 0.65732\n",
      "Batch train [20] loss 0.48580, dsc 0.51420\n",
      "Epoch [72] train done\n",
      "Batch eval [1] loss 0.53489, dsc 0.46511\n",
      "Batch eval [2] loss 0.47932, dsc 0.52068\n",
      "Batch eval [3] loss 0.99790, dsc 0.00210\n",
      "Batch eval [4] loss 0.17827, dsc 0.82173\n",
      "Batch eval [5] loss 0.67866, dsc 0.32134\n",
      "Epoch [72] valid done\n",
      "Epoch [72] T 4772.21s, deltaT 63.55s, loss: train 0.37784, valid 0.57381, dsc: train 0.62216, valid 0.42619\n",
      "Batch train [1] loss 0.57182, dsc 0.42818\n",
      "Batch train [2] loss 0.34937, dsc 0.65063\n",
      "Batch train [3] loss 0.46833, dsc 0.53167\n",
      "Batch train [4] loss 0.51776, dsc 0.48224\n",
      "Batch train [5] loss 0.28138, dsc 0.71862\n",
      "Batch train [6] loss 0.13839, dsc 0.86161\n",
      "Batch train [7] loss 0.54479, dsc 0.45521\n",
      "Batch train [8] loss 0.18293, dsc 0.81707\n",
      "Batch train [9] loss 0.36868, dsc 0.63132\n",
      "Batch train [10] loss 0.40703, dsc 0.59297\n",
      "Batch train [11] loss 0.25472, dsc 0.74528\n",
      "Batch train [12] loss 0.28825, dsc 0.71175\n",
      "Batch train [13] loss 0.31531, dsc 0.68469\n",
      "Batch train [14] loss 0.28561, dsc 0.71439\n",
      "Batch train [15] loss 0.28824, dsc 0.71176\n",
      "Batch train [16] loss 0.51102, dsc 0.48898\n",
      "Batch train [17] loss 0.51602, dsc 0.48398\n",
      "Batch train [18] loss 0.30047, dsc 0.69953\n",
      "Batch train [19] loss 0.48955, dsc 0.51045\n",
      "Batch train [20] loss 0.42564, dsc 0.57436\n",
      "Epoch [73] train done\n",
      "Batch eval [1] loss 0.59859, dsc 0.40141\n",
      "Batch eval [2] loss 0.57111, dsc 0.42889\n",
      "Batch eval [3] loss 0.95988, dsc 0.04012\n",
      "Batch eval [4] loss 0.23136, dsc 0.76864\n",
      "Batch eval [5] loss 0.51406, dsc 0.48594\n",
      "Epoch [73] valid done\n",
      "Epoch [73] T 4835.32s, deltaT 63.10s, loss: train 0.37527, valid 0.57500, dsc: train 0.62473, valid 0.42500\n",
      "Batch train [1] loss 0.29869, dsc 0.70131\n",
      "Batch train [2] loss 0.59086, dsc 0.40914\n",
      "Batch train [3] loss 0.45489, dsc 0.54511\n",
      "Batch train [4] loss 0.41682, dsc 0.58318\n",
      "Batch train [5] loss 0.32509, dsc 0.67491\n",
      "Batch train [6] loss 0.29441, dsc 0.70559\n",
      "Batch train [7] loss 0.22355, dsc 0.77645\n",
      "Batch train [8] loss 0.42234, dsc 0.57766\n",
      "Batch train [9] loss 0.20363, dsc 0.79637\n",
      "Batch train [10] loss 0.40968, dsc 0.59032\n",
      "Batch train [11] loss 0.54167, dsc 0.45833\n",
      "Batch train [12] loss 0.30937, dsc 0.69063\n",
      "Batch train [13] loss 0.25546, dsc 0.74454\n",
      "Batch train [14] loss 0.45411, dsc 0.54589\n",
      "Batch train [15] loss 0.27107, dsc 0.72893\n",
      "Batch train [16] loss 0.24625, dsc 0.75375\n",
      "Batch train [17] loss 0.32797, dsc 0.67203\n",
      "Batch train [18] loss 0.22007, dsc 0.77993\n",
      "Batch train [19] loss 0.29326, dsc 0.70674\n",
      "Batch train [20] loss 0.35278, dsc 0.64722\n",
      "Epoch [74] train done\n",
      "Batch eval [1] loss 0.67275, dsc 0.32725\n",
      "Batch eval [2] loss 0.68041, dsc 0.31959\n",
      "Batch eval [3] loss 0.80842, dsc 0.19158\n",
      "Batch eval [4] loss 0.19963, dsc 0.80037\n",
      "Batch eval [5] loss 0.48876, dsc 0.51124\n",
      "Epoch [74] valid done\n",
      "Epoch [74] T 4897.76s, deltaT 62.44s, loss: train 0.34560, valid 0.57000, dsc: train 0.65440, valid 0.43000\n",
      "Batch train [1] loss 0.18255, dsc 0.81745\n",
      "Batch train [2] loss 0.20648, dsc 0.79352\n",
      "Batch train [3] loss 0.40159, dsc 0.59841\n",
      "Batch train [4] loss 0.25966, dsc 0.74034\n",
      "Batch train [5] loss 0.13030, dsc 0.86970\n",
      "Batch train [6] loss 0.20579, dsc 0.79421\n",
      "Batch train [7] loss 0.38725, dsc 0.61275\n",
      "Batch train [8] loss 0.67082, dsc 0.32918\n",
      "Batch train [9] loss 0.38843, dsc 0.61157\n",
      "Batch train [10] loss 0.32382, dsc 0.67618\n",
      "Batch train [11] loss 0.29775, dsc 0.70225\n",
      "Batch train [12] loss 0.41130, dsc 0.58870\n",
      "Batch train [13] loss 0.32316, dsc 0.67684\n",
      "Batch train [14] loss 0.25245, dsc 0.74755\n",
      "Batch train [15] loss 0.55876, dsc 0.44124\n",
      "Batch train [16] loss 0.26420, dsc 0.73580\n",
      "Batch train [17] loss 0.37245, dsc 0.62755\n",
      "Batch train [18] loss 0.27894, dsc 0.72106\n",
      "Batch train [19] loss 0.54729, dsc 0.45271\n",
      "Batch train [20] loss 0.32601, dsc 0.67399\n",
      "Epoch [75] train done\n",
      "Batch eval [1] loss 0.64893, dsc 0.35107\n",
      "Batch eval [2] loss 0.66534, dsc 0.33466\n",
      "Batch eval [3] loss 0.99859, dsc 0.00141\n",
      "Batch eval [4] loss 0.21965, dsc 0.78035\n",
      "Batch eval [5] loss 0.32472, dsc 0.67528\n",
      "Epoch [75] valid done\n",
      "Epoch [75] T 4960.21s, deltaT 62.45s, loss: train 0.33945, valid 0.57145, dsc: train 0.66055, valid 0.42855\n",
      "Elapsed time 1:22:40\n",
      "\n",
      "\n",
      "\n",
      "Training model with dataset label 'PAROTID_GLAND_L', value '12'\n",
      "folder '20201103-121517_3d_unet_PAROTID_GLAND_L'\n",
      "Device running \"cuda\"\n",
      "max output channels 128\n",
      "Model number of params: 1193537, trainable 1193537\n",
      "\n",
      "\n",
      "\n",
      "Running training loop\n",
      "Batch train [1] loss 0.99427, dsc 0.00573\n",
      "Batch train [2] loss 0.99475, dsc 0.00525\n",
      "Batch train [3] loss 0.99451, dsc 0.00549\n",
      "Batch train [4] loss 0.99165, dsc 0.00835\n",
      "Batch train [5] loss 0.99245, dsc 0.00755\n",
      "Batch train [6] loss 0.98824, dsc 0.01176\n",
      "Batch train [7] loss 0.98380, dsc 0.01620\n",
      "Batch train [8] loss 0.99033, dsc 0.00967\n",
      "Batch train [9] loss 0.99160, dsc 0.00840\n",
      "Batch train [10] loss 0.98493, dsc 0.01507\n",
      "Batch train [11] loss 0.98310, dsc 0.01690\n",
      "Batch train [12] loss 0.98293, dsc 0.01707\n",
      "Batch train [13] loss 0.98572, dsc 0.01428\n",
      "Batch train [14] loss 0.97908, dsc 0.02092\n",
      "Batch train [15] loss 0.98360, dsc 0.01640\n",
      "Batch train [16] loss 0.98348, dsc 0.01652\n",
      "Batch train [17] loss 0.98546, dsc 0.01454\n",
      "Batch train [18] loss 0.98209, dsc 0.01791\n",
      "Batch train [19] loss 0.98060, dsc 0.01940\n",
      "Batch train [20] loss 0.98012, dsc 0.01988\n",
      "Epoch [1] train done\n",
      "Batch eval [1] loss 0.98986, dsc 0.01014\n",
      "Batch eval [2] loss 0.99287, dsc 0.00712\n",
      "Batch eval [3] loss 0.98933, dsc 0.01067\n",
      "Batch eval [4] loss 0.99229, dsc 0.00772\n",
      "Batch eval [5] loss 0.99439, dsc 0.00561\n",
      "Epoch [1] valid done\n",
      "Epoch [1] T 62.45s, deltaT 62.45s, loss: train 0.98664, valid 0.99175, dsc: train 0.01336, valid 0.00825\n",
      "Batch train [1] loss 0.98345, dsc 0.01655\n",
      "Batch train [2] loss 0.97064, dsc 0.02936\n",
      "Batch train [3] loss 0.97871, dsc 0.02129\n",
      "Batch train [4] loss 0.97898, dsc 0.02102\n",
      "Batch train [5] loss 0.98369, dsc 0.01631\n",
      "Batch train [6] loss 0.97632, dsc 0.02368\n",
      "Batch train [7] loss 0.98243, dsc 0.01757\n",
      "Batch train [8] loss 0.98689, dsc 0.01311\n",
      "Batch train [9] loss 0.98407, dsc 0.01593\n",
      "Batch train [10] loss 0.97922, dsc 0.02078\n",
      "Batch train [11] loss 0.97800, dsc 0.02200\n",
      "Batch train [12] loss 0.97869, dsc 0.02131\n",
      "Batch train [13] loss 0.98545, dsc 0.01455\n",
      "Batch train [14] loss 0.98410, dsc 0.01590\n",
      "Batch train [15] loss 0.97966, dsc 0.02034\n",
      "Batch train [16] loss 0.98478, dsc 0.01522\n",
      "Batch train [17] loss 0.96852, dsc 0.03148\n",
      "Batch train [18] loss 0.97227, dsc 0.02773\n",
      "Batch train [19] loss 0.97590, dsc 0.02410\n",
      "Batch train [20] loss 0.98250, dsc 0.01750\n",
      "Epoch [2] train done\n",
      "Batch eval [1] loss 0.98409, dsc 0.01591\n",
      "Batch eval [2] loss 0.98813, dsc 0.01187\n",
      "Batch eval [3] loss 0.98475, dsc 0.01525\n",
      "Batch eval [4] loss 0.98623, dsc 0.01377\n",
      "Batch eval [5] loss 0.98743, dsc 0.01257\n",
      "Epoch [2] valid done\n",
      "Epoch [2] T 125.42s, deltaT 62.97s, loss: train 0.97971, valid 0.98613, dsc: train 0.02029, valid 0.01387\n",
      "Batch train [1] loss 0.97948, dsc 0.02052\n",
      "Batch train [2] loss 0.97790, dsc 0.02210\n",
      "Batch train [3] loss 0.97108, dsc 0.02892\n",
      "Batch train [4] loss 0.97499, dsc 0.02501\n",
      "Batch train [5] loss 0.98537, dsc 0.01463\n",
      "Batch train [6] loss 0.98037, dsc 0.01963\n",
      "Batch train [7] loss 0.97162, dsc 0.02838\n",
      "Batch train [8] loss 0.97260, dsc 0.02740\n",
      "Batch train [9] loss 0.97617, dsc 0.02383\n",
      "Batch train [10] loss 0.98007, dsc 0.01993\n",
      "Batch train [11] loss 0.97311, dsc 0.02689\n",
      "Batch train [12] loss 0.98264, dsc 0.01736\n",
      "Batch train [13] loss 0.97270, dsc 0.02730\n",
      "Batch train [14] loss 0.98210, dsc 0.01790\n",
      "Batch train [15] loss 0.98767, dsc 0.01233\n",
      "Batch train [16] loss 0.97327, dsc 0.02673\n",
      "Batch train [17] loss 0.97999, dsc 0.02001\n",
      "Batch train [18] loss 0.98200, dsc 0.01800\n",
      "Batch train [19] loss 0.97648, dsc 0.02352\n",
      "Batch train [20] loss 0.97635, dsc 0.02365\n",
      "Epoch [3] train done\n",
      "Batch eval [1] loss 0.96937, dsc 0.03063\n",
      "Batch eval [2] loss 0.97768, dsc 0.02232\n",
      "Batch eval [3] loss 0.96739, dsc 0.03261\n",
      "Batch eval [4] loss 0.97556, dsc 0.02444\n",
      "Batch eval [5] loss 0.98224, dsc 0.01776\n",
      "Epoch [3] valid done\n",
      "Epoch [3] T 190.15s, deltaT 64.72s, loss: train 0.97780, valid 0.97445, dsc: train 0.02220, valid 0.02555\n",
      "Batch train [1] loss 0.98314, dsc 0.01686\n",
      "Batch train [2] loss 0.98169, dsc 0.01831\n",
      "Batch train [3] loss 0.98226, dsc 0.01774\n",
      "Batch train [4] loss 0.98165, dsc 0.01835\n",
      "Batch train [5] loss 0.98021, dsc 0.01979\n",
      "Batch train [6] loss 0.97415, dsc 0.02585\n",
      "Batch train [7] loss 0.97229, dsc 0.02771\n",
      "Batch train [8] loss 0.97891, dsc 0.02109\n",
      "Batch train [9] loss 0.97756, dsc 0.02244\n",
      "Batch train [10] loss 0.97354, dsc 0.02646\n",
      "Batch train [11] loss 0.97431, dsc 0.02569\n",
      "Batch train [12] loss 0.96248, dsc 0.03752\n",
      "Batch train [13] loss 0.97030, dsc 0.02970\n",
      "Batch train [14] loss 0.98027, dsc 0.01973\n",
      "Batch train [15] loss 0.97642, dsc 0.02358\n",
      "Batch train [16] loss 0.97675, dsc 0.02325\n",
      "Batch train [17] loss 0.96795, dsc 0.03205\n",
      "Batch train [18] loss 0.97942, dsc 0.02058\n",
      "Batch train [19] loss 0.97953, dsc 0.02047\n",
      "Batch train [20] loss 0.97944, dsc 0.02056\n",
      "Epoch [4] train done\n",
      "Batch eval [1] loss 0.96810, dsc 0.03190\n",
      "Batch eval [2] loss 0.97672, dsc 0.02328\n",
      "Batch eval [3] loss 0.96645, dsc 0.03355\n",
      "Batch eval [4] loss 0.97459, dsc 0.02541\n",
      "Batch eval [5] loss 0.98150, dsc 0.01850\n",
      "Epoch [4] valid done\n",
      "Epoch [4] T 256.49s, deltaT 66.35s, loss: train 0.97661, valid 0.97347, dsc: train 0.02339, valid 0.02653\n",
      "Batch train [1] loss 0.97885, dsc 0.02115\n",
      "Batch train [2] loss 0.97063, dsc 0.02937\n",
      "Batch train [3] loss 0.97233, dsc 0.02767\n",
      "Batch train [4] loss 0.97435, dsc 0.02565\n",
      "Batch train [5] loss 0.97660, dsc 0.02340\n",
      "Batch train [6] loss 0.98066, dsc 0.01934\n",
      "Batch train [7] loss 0.97706, dsc 0.02294\n",
      "Batch train [8] loss 0.96905, dsc 0.03095\n",
      "Batch train [9] loss 0.97215, dsc 0.02785\n",
      "Batch train [10] loss 0.97992, dsc 0.02008\n",
      "Batch train [11] loss 0.97871, dsc 0.02129\n",
      "Batch train [12] loss 0.97339, dsc 0.02661\n",
      "Batch train [13] loss 0.97211, dsc 0.02789\n",
      "Batch train [14] loss 0.97468, dsc 0.02532\n",
      "Batch train [15] loss 0.96514, dsc 0.03486\n",
      "Batch train [16] loss 0.98155, dsc 0.01845\n",
      "Batch train [17] loss 0.96252, dsc 0.03748\n",
      "Batch train [18] loss 0.98617, dsc 0.01383\n",
      "Batch train [19] loss 0.97993, dsc 0.02007\n",
      "Batch train [20] loss 0.97856, dsc 0.02144\n",
      "Epoch [5] train done\n",
      "Batch eval [1] loss 0.96971, dsc 0.03029\n",
      "Batch eval [2] loss 0.97695, dsc 0.02305\n",
      "Batch eval [3] loss 0.96813, dsc 0.03187\n",
      "Batch eval [4] loss 0.97548, dsc 0.02452\n",
      "Batch eval [5] loss 0.98149, dsc 0.01851\n",
      "Epoch [5] valid done\n",
      "Epoch [5] T 322.89s, deltaT 66.39s, loss: train 0.97522, valid 0.97435, dsc: train 0.02478, valid 0.02565\n",
      "Batch train [1] loss 0.98024, dsc 0.01976\n",
      "Batch train [2] loss 0.96849, dsc 0.03151\n",
      "Batch train [3] loss 0.97058, dsc 0.02942\n",
      "Batch train [4] loss 0.97972, dsc 0.02028\n",
      "Batch train [5] loss 0.97351, dsc 0.02649\n",
      "Batch train [6] loss 0.98336, dsc 0.01664\n",
      "Batch train [7] loss 0.97297, dsc 0.02703\n",
      "Batch train [8] loss 0.96989, dsc 0.03011\n",
      "Batch train [9] loss 0.95950, dsc 0.04050\n",
      "Batch train [10] loss 0.97436, dsc 0.02564\n",
      "Batch train [11] loss 0.97782, dsc 0.02218\n",
      "Batch train [12] loss 0.96261, dsc 0.03739\n",
      "Batch train [13] loss 0.97425, dsc 0.02575\n",
      "Batch train [14] loss 0.98255, dsc 0.01745\n",
      "Batch train [15] loss 0.97195, dsc 0.02805\n",
      "Batch train [16] loss 0.97849, dsc 0.02151\n",
      "Batch train [17] loss 0.96663, dsc 0.03337\n",
      "Batch train [18] loss 0.97114, dsc 0.02886\n",
      "Batch train [19] loss 0.98161, dsc 0.01839\n",
      "Batch train [20] loss 0.97677, dsc 0.02323\n",
      "Epoch [6] train done\n",
      "Batch eval [1] loss 0.96391, dsc 0.03609\n",
      "Batch eval [2] loss 0.97361, dsc 0.02639\n",
      "Batch eval [3] loss 0.96220, dsc 0.03780\n",
      "Batch eval [4] loss 0.97132, dsc 0.02868\n",
      "Batch eval [5] loss 0.97929, dsc 0.02071\n",
      "Epoch [6] valid done\n",
      "Epoch [6] T 388.44s, deltaT 65.55s, loss: train 0.97382, valid 0.97007, dsc: train 0.02618, valid 0.02993\n",
      "Batch train [1] loss 0.97774, dsc 0.02226\n",
      "Batch train [2] loss 0.97451, dsc 0.02549\n",
      "Batch train [3] loss 0.97327, dsc 0.02673\n",
      "Batch train [4] loss 0.95921, dsc 0.04079\n",
      "Batch train [5] loss 0.96927, dsc 0.03073\n",
      "Batch train [6] loss 0.97270, dsc 0.02730\n",
      "Batch train [7] loss 0.96995, dsc 0.03005\n",
      "Batch train [8] loss 0.97170, dsc 0.02830\n",
      "Batch train [9] loss 0.96557, dsc 0.03443\n",
      "Batch train [10] loss 0.98282, dsc 0.01718\n",
      "Batch train [11] loss 0.98524, dsc 0.01476\n",
      "Batch train [12] loss 0.97176, dsc 0.02824\n",
      "Batch train [13] loss 0.97875, dsc 0.02124\n",
      "Batch train [14] loss 0.96473, dsc 0.03527\n",
      "Batch train [15] loss 0.96532, dsc 0.03468\n",
      "Batch train [16] loss 0.96946, dsc 0.03054\n",
      "Batch train [17] loss 0.97052, dsc 0.02948\n",
      "Batch train [18] loss 0.97593, dsc 0.02407\n",
      "Batch train [19] loss 0.96892, dsc 0.03108\n",
      "Batch train [20] loss 0.97556, dsc 0.02444\n",
      "Epoch [7] train done\n",
      "Batch eval [1] loss 0.96202, dsc 0.03798\n",
      "Batch eval [2] loss 0.97209, dsc 0.02791\n",
      "Batch eval [3] loss 0.95999, dsc 0.04001\n",
      "Batch eval [4] loss 0.97058, dsc 0.02942\n",
      "Batch eval [5] loss 0.97848, dsc 0.02152\n",
      "Epoch [7] valid done\n",
      "Epoch [7] T 453.61s, deltaT 65.17s, loss: train 0.97215, valid 0.96863, dsc: train 0.02785, valid 0.03137\n",
      "Batch train [1] loss 0.96136, dsc 0.03864\n",
      "Batch train [2] loss 0.97368, dsc 0.02632\n",
      "Batch train [3] loss 0.96730, dsc 0.03270\n",
      "Batch train [4] loss 0.96722, dsc 0.03278\n",
      "Batch train [5] loss 0.96409, dsc 0.03591\n",
      "Batch train [6] loss 0.96530, dsc 0.03470\n",
      "Batch train [7] loss 0.97674, dsc 0.02326\n",
      "Batch train [8] loss 0.97295, dsc 0.02705\n",
      "Batch train [9] loss 0.96667, dsc 0.03333\n",
      "Batch train [10] loss 0.97497, dsc 0.02503\n",
      "Batch train [11] loss 0.98257, dsc 0.01743\n",
      "Batch train [12] loss 0.97134, dsc 0.02866\n",
      "Batch train [13] loss 0.96711, dsc 0.03289\n",
      "Batch train [14] loss 0.97951, dsc 0.02049\n",
      "Batch train [15] loss 0.96153, dsc 0.03847\n",
      "Batch train [16] loss 0.97277, dsc 0.02723\n",
      "Batch train [17] loss 0.97003, dsc 0.02997\n",
      "Batch train [18] loss 0.97518, dsc 0.02482\n",
      "Batch train [19] loss 0.96261, dsc 0.03739\n",
      "Batch train [20] loss 0.97287, dsc 0.02713\n",
      "Epoch [8] train done\n",
      "Batch eval [1] loss 0.95733, dsc 0.04267\n",
      "Batch eval [2] loss 0.96879, dsc 0.03121\n",
      "Batch eval [3] loss 0.95513, dsc 0.04487\n",
      "Batch eval [4] loss 0.96599, dsc 0.03401\n",
      "Batch eval [5] loss 0.97525, dsc 0.02475\n",
      "Epoch [8] valid done\n",
      "Epoch [8] T 520.28s, deltaT 66.67s, loss: train 0.97029, valid 0.96450, dsc: train 0.02971, valid 0.03550\n",
      "Batch train [1] loss 0.96471, dsc 0.03529\n",
      "Batch train [2] loss 0.97372, dsc 0.02628\n",
      "Batch train [3] loss 0.96402, dsc 0.03598\n",
      "Batch train [4] loss 0.96251, dsc 0.03749\n",
      "Batch train [5] loss 0.98016, dsc 0.01984\n",
      "Batch train [6] loss 0.97485, dsc 0.02515\n",
      "Batch train [7] loss 0.97337, dsc 0.02663\n",
      "Batch train [8] loss 0.97885, dsc 0.02115\n",
      "Batch train [9] loss 0.96105, dsc 0.03895\n",
      "Batch train [10] loss 0.96345, dsc 0.03655\n",
      "Batch train [11] loss 0.98104, dsc 0.01896\n",
      "Batch train [12] loss 0.95107, dsc 0.04893\n",
      "Batch train [13] loss 0.96394, dsc 0.03606\n",
      "Batch train [14] loss 0.97430, dsc 0.02570\n",
      "Batch train [15] loss 0.96533, dsc 0.03467\n",
      "Batch train [16] loss 0.96284, dsc 0.03716\n",
      "Batch train [17] loss 0.95963, dsc 0.04037\n",
      "Batch train [18] loss 0.96411, dsc 0.03589\n",
      "Batch train [19] loss 0.97416, dsc 0.02584\n",
      "Batch train [20] loss 0.97375, dsc 0.02625\n",
      "Epoch [9] train done\n",
      "Batch eval [1] loss 0.95539, dsc 0.04461\n",
      "Batch eval [2] loss 0.96674, dsc 0.03326\n",
      "Batch eval [3] loss 0.95345, dsc 0.04655\n",
      "Batch eval [4] loss 0.96377, dsc 0.03623\n",
      "Batch eval [5] loss 0.97407, dsc 0.02593\n",
      "Epoch [9] valid done\n",
      "Epoch [9] T 587.38s, deltaT 67.09s, loss: train 0.96834, valid 0.96268, dsc: train 0.03166, valid 0.03732\n",
      "Batch train [1] loss 0.94807, dsc 0.05193\n",
      "Batch train [2] loss 0.96596, dsc 0.03404\n",
      "Batch train [3] loss 0.97398, dsc 0.02602\n",
      "Batch train [4] loss 0.97036, dsc 0.02964\n",
      "Batch train [5] loss 0.97595, dsc 0.02405\n",
      "Batch train [6] loss 0.97651, dsc 0.02349\n",
      "Batch train [7] loss 0.96309, dsc 0.03691\n",
      "Batch train [8] loss 0.96143, dsc 0.03857\n",
      "Batch train [9] loss 0.97806, dsc 0.02194\n",
      "Batch train [10] loss 0.95557, dsc 0.04443\n",
      "Batch train [11] loss 0.96936, dsc 0.03064\n",
      "Batch train [12] loss 0.97376, dsc 0.02624\n",
      "Batch train [13] loss 0.96357, dsc 0.03643\n",
      "Batch train [14] loss 0.96072, dsc 0.03928\n",
      "Batch train [15] loss 0.96435, dsc 0.03565\n",
      "Batch train [16] loss 0.95508, dsc 0.04492\n",
      "Batch train [17] loss 0.94768, dsc 0.05232\n",
      "Batch train [18] loss 0.97221, dsc 0.02779\n",
      "Batch train [19] loss 0.97244, dsc 0.02756\n",
      "Batch train [20] loss 0.97416, dsc 0.02584\n",
      "Epoch [10] train done\n",
      "Batch eval [1] loss 0.95209, dsc 0.04791\n",
      "Batch eval [2] loss 0.96476, dsc 0.03524\n",
      "Batch eval [3] loss 0.95040, dsc 0.04960\n",
      "Batch eval [4] loss 0.96161, dsc 0.03839\n",
      "Batch eval [5] loss 0.97360, dsc 0.02640\n",
      "Epoch [10] valid done\n",
      "Epoch [10] T 654.89s, deltaT 67.51s, loss: train 0.96612, valid 0.96049, dsc: train 0.03388, valid 0.03951\n",
      "Batch train [1] loss 0.96006, dsc 0.03994\n",
      "Batch train [2] loss 0.96896, dsc 0.03104\n",
      "Batch train [3] loss 0.96587, dsc 0.03413\n",
      "Batch train [4] loss 0.97808, dsc 0.02192\n",
      "Batch train [5] loss 0.96555, dsc 0.03445\n",
      "Batch train [6] loss 0.96353, dsc 0.03647\n",
      "Batch train [7] loss 0.95594, dsc 0.04406\n",
      "Batch train [8] loss 0.97319, dsc 0.02681\n",
      "Batch train [9] loss 0.96619, dsc 0.03381\n",
      "Batch train [10] loss 0.95557, dsc 0.04443\n",
      "Batch train [11] loss 0.96029, dsc 0.03971\n",
      "Batch train [12] loss 0.95638, dsc 0.04362\n",
      "Batch train [13] loss 0.96945, dsc 0.03055\n",
      "Batch train [14] loss 0.94946, dsc 0.05054\n",
      "Batch train [15] loss 0.97064, dsc 0.02936\n",
      "Batch train [16] loss 0.95939, dsc 0.04061\n",
      "Batch train [17] loss 0.95626, dsc 0.04374\n",
      "Batch train [18] loss 0.96131, dsc 0.03869\n",
      "Batch train [19] loss 0.96217, dsc 0.03783\n",
      "Batch train [20] loss 0.97036, dsc 0.02964\n",
      "Epoch [11] train done\n",
      "Batch eval [1] loss 0.94521, dsc 0.05479\n",
      "Batch eval [2] loss 0.96058, dsc 0.03942\n",
      "Batch eval [3] loss 0.94194, dsc 0.05806\n",
      "Batch eval [4] loss 0.95735, dsc 0.04265\n",
      "Batch eval [5] loss 0.96872, dsc 0.03128\n",
      "Epoch [11] valid done\n",
      "Epoch [11] T 720.17s, deltaT 65.28s, loss: train 0.96343, valid 0.95476, dsc: train 0.03657, valid 0.04524\n",
      "Batch train [1] loss 0.96595, dsc 0.03405\n",
      "Batch train [2] loss 0.97050, dsc 0.02950\n",
      "Batch train [3] loss 0.97937, dsc 0.02063\n",
      "Batch train [4] loss 0.96125, dsc 0.03875\n",
      "Batch train [5] loss 0.96203, dsc 0.03797\n",
      "Batch train [6] loss 0.96576, dsc 0.03424\n",
      "Batch train [7] loss 0.95445, dsc 0.04555\n",
      "Batch train [8] loss 0.96337, dsc 0.03663\n",
      "Batch train [9] loss 0.96040, dsc 0.03960\n",
      "Batch train [10] loss 0.94524, dsc 0.05476\n",
      "Batch train [11] loss 0.94855, dsc 0.05145\n",
      "Batch train [12] loss 0.95923, dsc 0.04077\n",
      "Batch train [13] loss 0.96371, dsc 0.03629\n",
      "Batch train [14] loss 0.95439, dsc 0.04561\n",
      "Batch train [15] loss 0.96785, dsc 0.03215\n",
      "Batch train [16] loss 0.95130, dsc 0.04870\n",
      "Batch train [17] loss 0.97281, dsc 0.02719\n",
      "Batch train [18] loss 0.95346, dsc 0.04654\n",
      "Batch train [19] loss 0.95735, dsc 0.04265\n",
      "Batch train [20] loss 0.95566, dsc 0.04434\n",
      "Epoch [12] train done\n",
      "Batch eval [1] loss 0.94244, dsc 0.05756\n",
      "Batch eval [2] loss 0.95867, dsc 0.04133\n",
      "Batch eval [3] loss 0.93939, dsc 0.06061\n",
      "Batch eval [4] loss 0.95387, dsc 0.04613\n",
      "Batch eval [5] loss 0.96787, dsc 0.03213\n",
      "Epoch [12] valid done\n",
      "Epoch [12] T 785.81s, deltaT 65.63s, loss: train 0.96063, valid 0.95245, dsc: train 0.03937, valid 0.04755\n",
      "Batch train [1] loss 0.96559, dsc 0.03441\n",
      "Batch train [2] loss 0.96509, dsc 0.03491\n",
      "Batch train [3] loss 0.95472, dsc 0.04528\n",
      "Batch train [4] loss 0.95252, dsc 0.04748\n",
      "Batch train [5] loss 0.96111, dsc 0.03889\n",
      "Batch train [6] loss 0.96432, dsc 0.03568\n",
      "Batch train [7] loss 0.97504, dsc 0.02496\n",
      "Batch train [8] loss 0.95551, dsc 0.04449\n",
      "Batch train [9] loss 0.96250, dsc 0.03750\n",
      "Batch train [10] loss 0.96024, dsc 0.03976\n",
      "Batch train [11] loss 0.94592, dsc 0.05408\n",
      "Batch train [12] loss 0.95208, dsc 0.04792\n",
      "Batch train [13] loss 0.95285, dsc 0.04715\n",
      "Batch train [14] loss 0.95049, dsc 0.04951\n",
      "Batch train [15] loss 0.94992, dsc 0.05008\n",
      "Batch train [16] loss 0.94601, dsc 0.05399\n",
      "Batch train [17] loss 0.93547, dsc 0.06453\n",
      "Batch train [18] loss 0.96785, dsc 0.03215\n",
      "Batch train [19] loss 0.95794, dsc 0.04206\n",
      "Batch train [20] loss 0.96453, dsc 0.03547\n",
      "Epoch [13] train done\n",
      "Batch eval [1] loss 0.93530, dsc 0.06470\n",
      "Batch eval [2] loss 0.95245, dsc 0.04755\n",
      "Batch eval [3] loss 0.93079, dsc 0.06921\n",
      "Batch eval [4] loss 0.94760, dsc 0.05240\n",
      "Batch eval [5] loss 0.96300, dsc 0.03700\n",
      "Epoch [13] valid done\n",
      "Epoch [13] T 850.81s, deltaT 65.01s, loss: train 0.95699, valid 0.94583, dsc: train 0.04301, valid 0.05417\n",
      "Batch train [1] loss 0.94932, dsc 0.05068\n",
      "Batch train [2] loss 0.94660, dsc 0.05340\n",
      "Batch train [3] loss 0.95589, dsc 0.04411\n",
      "Batch train [4] loss 0.93986, dsc 0.06014\n",
      "Batch train [5] loss 0.95141, dsc 0.04859\n",
      "Batch train [6] loss 0.94702, dsc 0.05298\n",
      "Batch train [7] loss 0.95726, dsc 0.04274\n",
      "Batch train [8] loss 0.94838, dsc 0.05162\n",
      "Batch train [9] loss 0.95570, dsc 0.04430\n",
      "Batch train [10] loss 0.96207, dsc 0.03793\n",
      "Batch train [11] loss 0.93006, dsc 0.06994\n",
      "Batch train [12] loss 0.94049, dsc 0.05951\n",
      "Batch train [13] loss 0.96079, dsc 0.03921\n",
      "Batch train [14] loss 0.95038, dsc 0.04962\n",
      "Batch train [15] loss 0.96436, dsc 0.03564\n",
      "Batch train [16] loss 0.95672, dsc 0.04328\n",
      "Batch train [17] loss 0.96630, dsc 0.03370\n",
      "Batch train [18] loss 0.96683, dsc 0.03317\n",
      "Batch train [19] loss 0.97112, dsc 0.02888\n",
      "Batch train [20] loss 0.94991, dsc 0.05009\n",
      "Epoch [14] train done\n",
      "Batch eval [1] loss 0.93296, dsc 0.06704\n",
      "Batch eval [2] loss 0.95033, dsc 0.04967\n",
      "Batch eval [3] loss 0.92954, dsc 0.07046\n",
      "Batch eval [4] loss 0.94614, dsc 0.05386\n",
      "Batch eval [5] loss 0.95986, dsc 0.04014\n",
      "Epoch [14] valid done\n",
      "Epoch [14] T 915.54s, deltaT 64.72s, loss: train 0.95352, valid 0.94377, dsc: train 0.04648, valid 0.05623\n",
      "Batch train [1] loss 0.94055, dsc 0.05945\n",
      "Batch train [2] loss 0.96340, dsc 0.03660\n",
      "Batch train [3] loss 0.95718, dsc 0.04282\n",
      "Batch train [4] loss 0.94612, dsc 0.05388\n",
      "Batch train [5] loss 0.92513, dsc 0.07487\n",
      "Batch train [6] loss 0.93526, dsc 0.06474\n",
      "Batch train [7] loss 0.95467, dsc 0.04533\n",
      "Batch train [8] loss 0.95570, dsc 0.04430\n",
      "Batch train [9] loss 0.96081, dsc 0.03919\n",
      "Batch train [10] loss 0.95540, dsc 0.04460\n",
      "Batch train [11] loss 0.96506, dsc 0.03494\n",
      "Batch train [12] loss 0.93390, dsc 0.06610\n",
      "Batch train [13] loss 0.95017, dsc 0.04983\n",
      "Batch train [14] loss 0.93440, dsc 0.06560\n",
      "Batch train [15] loss 0.95909, dsc 0.04091\n",
      "Batch train [16] loss 0.96008, dsc 0.03992\n",
      "Batch train [17] loss 0.93467, dsc 0.06533\n",
      "Batch train [18] loss 0.94804, dsc 0.05196\n",
      "Batch train [19] loss 0.95362, dsc 0.04638\n",
      "Batch train [20] loss 0.94721, dsc 0.05279\n",
      "Epoch [15] train done\n",
      "Batch eval [1] loss 0.92110, dsc 0.07890\n",
      "Batch eval [2] loss 0.94018, dsc 0.05982\n",
      "Batch eval [3] loss 0.91736, dsc 0.08264\n",
      "Batch eval [4] loss 0.93604, dsc 0.06396\n",
      "Batch eval [5] loss 0.95427, dsc 0.04573\n",
      "Epoch [15] valid done\n",
      "Epoch [15] T 980.54s, deltaT 65.00s, loss: train 0.94902, valid 0.93379, dsc: train 0.05098, valid 0.06621\n",
      "Batch train [1] loss 0.94748, dsc 0.05252\n",
      "Batch train [2] loss 0.95685, dsc 0.04315\n",
      "Batch train [3] loss 0.95507, dsc 0.04493\n",
      "Batch train [4] loss 0.94356, dsc 0.05644\n",
      "Batch train [5] loss 0.95719, dsc 0.04281\n",
      "Batch train [6] loss 0.94617, dsc 0.05383\n",
      "Batch train [7] loss 0.93855, dsc 0.06145\n",
      "Batch train [8] loss 0.91788, dsc 0.08212\n",
      "Batch train [9] loss 0.91199, dsc 0.08801\n",
      "Batch train [10] loss 0.93524, dsc 0.06476\n",
      "Batch train [11] loss 0.94762, dsc 0.05238\n",
      "Batch train [12] loss 0.94995, dsc 0.05005\n",
      "Batch train [13] loss 0.94836, dsc 0.05164\n",
      "Batch train [14] loss 0.95013, dsc 0.04987\n",
      "Batch train [15] loss 0.95885, dsc 0.04115\n",
      "Batch train [16] loss 0.94123, dsc 0.05877\n",
      "Batch train [17] loss 0.94584, dsc 0.05416\n",
      "Batch train [18] loss 0.96212, dsc 0.03788\n",
      "Batch train [19] loss 0.94066, dsc 0.05934\n",
      "Batch train [20] loss 0.92374, dsc 0.07626\n",
      "Epoch [16] train done\n",
      "Batch eval [1] loss 0.91994, dsc 0.08006\n",
      "Batch eval [2] loss 0.94077, dsc 0.05923\n",
      "Batch eval [3] loss 0.91470, dsc 0.08530\n",
      "Batch eval [4] loss 0.93422, dsc 0.06578\n",
      "Batch eval [5] loss 0.95299, dsc 0.04701\n",
      "Epoch [16] valid done\n",
      "Epoch [16] T 1045.65s, deltaT 65.11s, loss: train 0.94392, valid 0.93252, dsc: train 0.05608, valid 0.06748\n",
      "Batch train [1] loss 0.95307, dsc 0.04693\n",
      "Batch train [2] loss 0.93343, dsc 0.06657\n",
      "Batch train [3] loss 0.92870, dsc 0.07130\n",
      "Batch train [4] loss 0.91529, dsc 0.08471\n",
      "Batch train [5] loss 0.94793, dsc 0.05207\n",
      "Batch train [6] loss 0.95584, dsc 0.04416\n",
      "Batch train [7] loss 0.94481, dsc 0.05519\n",
      "Batch train [8] loss 0.93975, dsc 0.06025\n",
      "Batch train [9] loss 0.95670, dsc 0.04330\n",
      "Batch train [10] loss 0.92739, dsc 0.07261\n",
      "Batch train [11] loss 0.91356, dsc 0.08644\n",
      "Batch train [12] loss 0.94845, dsc 0.05155\n",
      "Batch train [13] loss 0.93623, dsc 0.06377\n",
      "Batch train [14] loss 0.95214, dsc 0.04786\n",
      "Batch train [15] loss 0.93788, dsc 0.06212\n",
      "Batch train [16] loss 0.93458, dsc 0.06542\n",
      "Batch train [17] loss 0.93783, dsc 0.06217\n",
      "Batch train [18] loss 0.90787, dsc 0.09213\n",
      "Batch train [19] loss 0.92469, dsc 0.07531\n",
      "Batch train [20] loss 0.94832, dsc 0.05168\n",
      "Epoch [17] train done\n",
      "Batch eval [1] loss 0.91010, dsc 0.08990\n",
      "Batch eval [2] loss 0.93296, dsc 0.06704\n",
      "Batch eval [3] loss 0.90394, dsc 0.09606\n",
      "Batch eval [4] loss 0.92547, dsc 0.07453\n",
      "Batch eval [5] loss 0.94629, dsc 0.05371\n",
      "Epoch [17] valid done\n",
      "Epoch [17] T 1110.44s, deltaT 64.79s, loss: train 0.93722, valid 0.92375, dsc: train 0.06278, valid 0.07625\n",
      "Batch train [1] loss 0.94439, dsc 0.05561\n",
      "Batch train [2] loss 0.94229, dsc 0.05771\n",
      "Batch train [3] loss 0.91762, dsc 0.08238\n",
      "Batch train [4] loss 0.95311, dsc 0.04689\n",
      "Batch train [5] loss 0.92103, dsc 0.07897\n",
      "Batch train [6] loss 0.95411, dsc 0.04589\n",
      "Batch train [7] loss 0.94960, dsc 0.05040\n",
      "Batch train [8] loss 0.92855, dsc 0.07145\n",
      "Batch train [9] loss 0.91673, dsc 0.08327\n",
      "Batch train [10] loss 0.93199, dsc 0.06801\n",
      "Batch train [11] loss 0.92097, dsc 0.07903\n",
      "Batch train [12] loss 0.90236, dsc 0.09764\n",
      "Batch train [13] loss 0.89992, dsc 0.10008\n",
      "Batch train [14] loss 0.92694, dsc 0.07306\n",
      "Batch train [15] loss 0.92510, dsc 0.07490\n",
      "Batch train [16] loss 0.94938, dsc 0.05062\n",
      "Batch train [17] loss 0.90823, dsc 0.09177\n",
      "Batch train [18] loss 0.92559, dsc 0.07441\n",
      "Batch train [19] loss 0.94101, dsc 0.05899\n",
      "Batch train [20] loss 0.92972, dsc 0.07028\n",
      "Epoch [18] train done\n",
      "Batch eval [1] loss 0.89772, dsc 0.10228\n",
      "Batch eval [2] loss 0.92495, dsc 0.07505\n",
      "Batch eval [3] loss 0.89388, dsc 0.10612\n",
      "Batch eval [4] loss 0.91585, dsc 0.08415\n",
      "Batch eval [5] loss 0.94060, dsc 0.05940\n",
      "Epoch [18] valid done\n",
      "Epoch [18] T 1175.22s, deltaT 64.78s, loss: train 0.92943, valid 0.91460, dsc: train 0.07057, valid 0.08540\n",
      "Batch train [1] loss 0.94062, dsc 0.05938\n",
      "Batch train [2] loss 0.94867, dsc 0.05133\n",
      "Batch train [3] loss 0.91721, dsc 0.08279\n",
      "Batch train [4] loss 0.92457, dsc 0.07543\n",
      "Batch train [5] loss 0.94360, dsc 0.05640\n",
      "Batch train [6] loss 0.90590, dsc 0.09410\n",
      "Batch train [7] loss 0.92271, dsc 0.07729\n",
      "Batch train [8] loss 0.90560, dsc 0.09440\n",
      "Batch train [9] loss 0.91477, dsc 0.08523\n",
      "Batch train [10] loss 0.92115, dsc 0.07885\n",
      "Batch train [11] loss 0.93293, dsc 0.06707\n",
      "Batch train [12] loss 0.90361, dsc 0.09639\n",
      "Batch train [13] loss 0.91344, dsc 0.08656\n",
      "Batch train [14] loss 0.89550, dsc 0.10450\n",
      "Batch train [15] loss 0.93211, dsc 0.06789\n",
      "Batch train [16] loss 0.90661, dsc 0.09339\n",
      "Batch train [17] loss 0.94119, dsc 0.05881\n",
      "Batch train [18] loss 0.90843, dsc 0.09157\n",
      "Batch train [19] loss 0.89562, dsc 0.10438\n",
      "Batch train [20] loss 0.93721, dsc 0.06279\n",
      "Epoch [19] train done\n",
      "Batch eval [1] loss 0.88196, dsc 0.11804\n",
      "Batch eval [2] loss 0.91254, dsc 0.08746\n",
      "Batch eval [3] loss 0.87144, dsc 0.12856\n",
      "Batch eval [4] loss 0.90222, dsc 0.09778\n",
      "Batch eval [5] loss 0.92978, dsc 0.07022\n",
      "Epoch [19] valid done\n",
      "Epoch [19] T 1240.48s, deltaT 65.26s, loss: train 0.92057, valid 0.89959, dsc: train 0.07943, valid 0.10041\n",
      "Batch train [1] loss 0.93530, dsc 0.06470\n",
      "Batch train [2] loss 0.91945, dsc 0.08055\n",
      "Batch train [3] loss 0.90659, dsc 0.09341\n",
      "Batch train [4] loss 0.94743, dsc 0.05257\n",
      "Batch train [5] loss 0.91072, dsc 0.08928\n",
      "Batch train [6] loss 0.89684, dsc 0.10316\n",
      "Batch train [7] loss 0.89763, dsc 0.10237\n",
      "Batch train [8] loss 0.90797, dsc 0.09203\n",
      "Batch train [9] loss 0.94378, dsc 0.05622\n",
      "Batch train [10] loss 0.88488, dsc 0.11512\n",
      "Batch train [11] loss 0.90438, dsc 0.09562\n",
      "Batch train [12] loss 0.92224, dsc 0.07776\n",
      "Batch train [13] loss 0.92737, dsc 0.07263\n",
      "Batch train [14] loss 0.87566, dsc 0.12434\n",
      "Batch train [15] loss 0.87609, dsc 0.12391\n",
      "Batch train [16] loss 0.93144, dsc 0.06856\n",
      "Batch train [17] loss 0.90250, dsc 0.09750\n",
      "Batch train [18] loss 0.91846, dsc 0.08154\n",
      "Batch train [19] loss 0.90583, dsc 0.09417\n",
      "Batch train [20] loss 0.88926, dsc 0.11074\n",
      "Epoch [20] train done\n",
      "Batch eval [1] loss 0.86046, dsc 0.13954\n",
      "Batch eval [2] loss 0.89752, dsc 0.10248\n",
      "Batch eval [3] loss 0.85737, dsc 0.14263\n",
      "Batch eval [4] loss 0.88774, dsc 0.11226\n",
      "Batch eval [5] loss 0.91999, dsc 0.08001\n",
      "Epoch [20] valid done\n",
      "Epoch [20] T 1306.01s, deltaT 65.54s, loss: train 0.91019, valid 0.88462, dsc: train 0.08981, valid 0.11538\n",
      "Batch train [1] loss 0.93599, dsc 0.06401\n",
      "Batch train [2] loss 0.90575, dsc 0.09425\n",
      "Batch train [3] loss 0.91972, dsc 0.08028\n",
      "Batch train [4] loss 0.89467, dsc 0.10533\n",
      "Batch train [5] loss 0.91210, dsc 0.08790\n",
      "Batch train [6] loss 0.88803, dsc 0.11197\n",
      "Batch train [7] loss 0.90881, dsc 0.09119\n",
      "Batch train [8] loss 0.85682, dsc 0.14318\n",
      "Batch train [9] loss 0.84438, dsc 0.15562\n",
      "Batch train [10] loss 0.90172, dsc 0.09828\n",
      "Batch train [11] loss 0.91197, dsc 0.08803\n",
      "Batch train [12] loss 0.92175, dsc 0.07825\n",
      "Batch train [13] loss 0.92345, dsc 0.07655\n",
      "Batch train [14] loss 0.87878, dsc 0.12122\n",
      "Batch train [15] loss 0.90063, dsc 0.09937\n",
      "Batch train [16] loss 0.87063, dsc 0.12937\n",
      "Batch train [17] loss 0.88525, dsc 0.11475\n",
      "Batch train [18] loss 0.86328, dsc 0.13672\n",
      "Batch train [19] loss 0.93276, dsc 0.06724\n",
      "Batch train [20] loss 0.90855, dsc 0.09145\n",
      "Epoch [21] train done\n",
      "Batch eval [1] loss 0.85435, dsc 0.14565\n",
      "Batch eval [2] loss 0.88680, dsc 0.11320\n",
      "Batch eval [3] loss 0.84852, dsc 0.15148\n",
      "Batch eval [4] loss 0.87889, dsc 0.12111\n",
      "Batch eval [5] loss 0.91051, dsc 0.08949\n",
      "Epoch [21] valid done\n",
      "Epoch [21] T 1370.92s, deltaT 64.91s, loss: train 0.89825, valid 0.87581, dsc: train 0.10175, valid 0.12419\n",
      "Batch train [1] loss 0.83646, dsc 0.16354\n",
      "Batch train [2] loss 0.88101, dsc 0.11899\n",
      "Batch train [3] loss 0.91036, dsc 0.08964\n",
      "Batch train [4] loss 0.87391, dsc 0.12609\n",
      "Batch train [5] loss 0.90260, dsc 0.09740\n",
      "Batch train [6] loss 0.82806, dsc 0.17194\n",
      "Batch train [7] loss 0.86366, dsc 0.13634\n",
      "Batch train [8] loss 0.86059, dsc 0.13941\n",
      "Batch train [9] loss 0.90572, dsc 0.09428\n",
      "Batch train [10] loss 0.92314, dsc 0.07686\n",
      "Batch train [11] loss 0.89238, dsc 0.10762\n",
      "Batch train [12] loss 0.91345, dsc 0.08655\n",
      "Batch train [13] loss 0.91415, dsc 0.08585\n",
      "Batch train [14] loss 0.88013, dsc 0.11987\n",
      "Batch train [15] loss 0.84158, dsc 0.15842\n",
      "Batch train [16] loss 0.91072, dsc 0.08928\n",
      "Batch train [17] loss 0.90594, dsc 0.09406\n",
      "Batch train [18] loss 0.89151, dsc 0.10849\n",
      "Batch train [19] loss 0.89206, dsc 0.10794\n",
      "Batch train [20] loss 0.86871, dsc 0.13129\n",
      "Epoch [22] train done\n",
      "Batch eval [1] loss 0.82122, dsc 0.17878\n",
      "Batch eval [2] loss 0.86760, dsc 0.13240\n",
      "Batch eval [3] loss 0.81834, dsc 0.18166\n",
      "Batch eval [4] loss 0.85400, dsc 0.14600\n",
      "Batch eval [5] loss 0.89044, dsc 0.10956\n",
      "Epoch [22] valid done\n",
      "Epoch [22] T 1435.89s, deltaT 64.97s, loss: train 0.88481, valid 0.85032, dsc: train 0.11519, valid 0.14968\n",
      "Batch train [1] loss 0.88371, dsc 0.11629\n",
      "Batch train [2] loss 0.89409, dsc 0.10591\n",
      "Batch train [3] loss 0.90171, dsc 0.09829\n",
      "Batch train [4] loss 0.87781, dsc 0.12219\n",
      "Batch train [5] loss 0.91455, dsc 0.08545\n",
      "Batch train [6] loss 0.81709, dsc 0.18291\n",
      "Batch train [7] loss 0.88701, dsc 0.11299\n",
      "Batch train [8] loss 0.91048, dsc 0.08952\n",
      "Batch train [9] loss 0.87501, dsc 0.12499\n",
      "Batch train [10] loss 0.86725, dsc 0.13275\n",
      "Batch train [11] loss 0.85156, dsc 0.14844\n",
      "Batch train [12] loss 0.84995, dsc 0.15005\n",
      "Batch train [13] loss 0.83288, dsc 0.16712\n",
      "Batch train [14] loss 0.82407, dsc 0.17593\n",
      "Batch train [15] loss 0.83819, dsc 0.16181\n",
      "Batch train [16] loss 0.87305, dsc 0.12695\n",
      "Batch train [17] loss 0.85306, dsc 0.14694\n",
      "Batch train [18] loss 0.88917, dsc 0.11083\n",
      "Batch train [19] loss 0.86960, dsc 0.13040\n",
      "Batch train [20] loss 0.85313, dsc 0.14687\n",
      "Epoch [23] train done\n",
      "Batch eval [1] loss 0.79251, dsc 0.20749\n",
      "Batch eval [2] loss 0.84115, dsc 0.15885\n",
      "Batch eval [3] loss 0.77967, dsc 0.22033\n",
      "Batch eval [4] loss 0.82607, dsc 0.17393\n",
      "Batch eval [5] loss 0.87097, dsc 0.12903\n",
      "Epoch [23] valid done\n",
      "Epoch [23] T 1503.30s, deltaT 67.40s, loss: train 0.86817, valid 0.82207, dsc: train 0.13183, valid 0.17793\n",
      "Batch train [1] loss 0.88749, dsc 0.11251\n",
      "Batch train [2] loss 0.89517, dsc 0.10483\n",
      "Batch train [3] loss 0.88209, dsc 0.11791\n",
      "Batch train [4] loss 0.81552, dsc 0.18448\n",
      "Batch train [5] loss 0.88342, dsc 0.11658\n",
      "Batch train [6] loss 0.84973, dsc 0.15027\n",
      "Batch train [7] loss 0.86931, dsc 0.13069\n",
      "Batch train [8] loss 0.85741, dsc 0.14259\n",
      "Batch train [9] loss 0.91227, dsc 0.08773\n",
      "Batch train [10] loss 0.87352, dsc 0.12648\n",
      "Batch train [11] loss 0.79094, dsc 0.20906\n",
      "Batch train [12] loss 0.81872, dsc 0.18128\n",
      "Batch train [13] loss 0.79155, dsc 0.20845\n",
      "Batch train [14] loss 0.85059, dsc 0.14941\n",
      "Batch train [15] loss 0.80996, dsc 0.19004\n",
      "Batch train [16] loss 0.81937, dsc 0.18063\n",
      "Batch train [17] loss 0.83706, dsc 0.16294\n",
      "Batch train [18] loss 0.80772, dsc 0.19228\n",
      "Batch train [19] loss 0.88897, dsc 0.11103\n",
      "Batch train [20] loss 0.78943, dsc 0.21057\n",
      "Epoch [24] train done\n",
      "Batch eval [1] loss 0.76675, dsc 0.23325\n",
      "Batch eval [2] loss 0.81735, dsc 0.18265\n",
      "Batch eval [3] loss 0.75091, dsc 0.24909\n",
      "Batch eval [4] loss 0.80347, dsc 0.19653\n",
      "Batch eval [5] loss 0.85751, dsc 0.14249\n",
      "Epoch [24] valid done\n",
      "Epoch [24] T 1571.60s, deltaT 68.30s, loss: train 0.84651, valid 0.79920, dsc: train 0.15349, valid 0.20080\n",
      "Batch train [1] loss 0.86453, dsc 0.13547\n",
      "Batch train [2] loss 0.77302, dsc 0.22698\n",
      "Batch train [3] loss 0.82983, dsc 0.17017\n",
      "Batch train [4] loss 0.85137, dsc 0.14863\n",
      "Batch train [5] loss 0.81213, dsc 0.18787\n",
      "Batch train [6] loss 0.79561, dsc 0.20439\n",
      "Batch train [7] loss 0.84504, dsc 0.15496\n",
      "Batch train [8] loss 0.85248, dsc 0.14752\n",
      "Batch train [9] loss 0.82174, dsc 0.17826\n",
      "Batch train [10] loss 0.83950, dsc 0.16050\n",
      "Batch train [11] loss 0.80683, dsc 0.19317\n",
      "Batch train [12] loss 0.84750, dsc 0.15250\n",
      "Batch train [13] loss 0.81187, dsc 0.18813\n",
      "Batch train [14] loss 0.85329, dsc 0.14671\n",
      "Batch train [15] loss 0.78155, dsc 0.21845\n",
      "Batch train [16] loss 0.83719, dsc 0.16281\n",
      "Batch train [17] loss 0.85403, dsc 0.14597\n",
      "Batch train [18] loss 0.78221, dsc 0.21779\n",
      "Batch train [19] loss 0.77522, dsc 0.22478\n",
      "Batch train [20] loss 0.79081, dsc 0.20919\n",
      "Epoch [25] train done\n",
      "Batch eval [1] loss 0.73924, dsc 0.26076\n",
      "Batch eval [2] loss 0.79508, dsc 0.20492\n",
      "Batch eval [3] loss 0.72367, dsc 0.27633\n",
      "Batch eval [4] loss 0.77133, dsc 0.22867\n",
      "Batch eval [5] loss 0.83066, dsc 0.16934\n",
      "Epoch [25] valid done\n",
      "Epoch [25] T 1639.24s, deltaT 67.63s, loss: train 0.82129, valid 0.77200, dsc: train 0.17871, valid 0.22800\n",
      "Batch train [1] loss 0.86525, dsc 0.13475\n",
      "Batch train [2] loss 0.72679, dsc 0.27321\n",
      "Batch train [3] loss 0.75688, dsc 0.24312\n",
      "Batch train [4] loss 0.79948, dsc 0.20052\n",
      "Batch train [5] loss 0.85075, dsc 0.14925\n",
      "Batch train [6] loss 0.79656, dsc 0.20344\n",
      "Batch train [7] loss 0.83369, dsc 0.16631\n",
      "Batch train [8] loss 0.85120, dsc 0.14880\n",
      "Batch train [9] loss 0.81924, dsc 0.18076\n",
      "Batch train [10] loss 0.83009, dsc 0.16991\n",
      "Batch train [11] loss 0.76756, dsc 0.23244\n",
      "Batch train [12] loss 0.79873, dsc 0.20127\n",
      "Batch train [13] loss 0.72627, dsc 0.27373\n",
      "Batch train [14] loss 0.81682, dsc 0.18318\n",
      "Batch train [15] loss 0.74896, dsc 0.25104\n",
      "Batch train [16] loss 0.79758, dsc 0.20242\n",
      "Batch train [17] loss 0.78916, dsc 0.21084\n",
      "Batch train [18] loss 0.70877, dsc 0.29123\n",
      "Batch train [19] loss 0.79276, dsc 0.20724\n",
      "Batch train [20] loss 0.83670, dsc 0.16330\n",
      "Epoch [26] train done\n",
      "Batch eval [1] loss 0.76228, dsc 0.23772\n",
      "Batch eval [2] loss 0.80855, dsc 0.19145\n",
      "Batch eval [3] loss 0.73662, dsc 0.26338\n",
      "Batch eval [4] loss 0.78675, dsc 0.21325\n",
      "Batch eval [5] loss 0.84805, dsc 0.15195\n",
      "Epoch [26] valid done\n",
      "Epoch [26] T 1705.59s, deltaT 66.36s, loss: train 0.79566, valid 0.78845, dsc: train 0.20434, valid 0.21155\n",
      "Batch train [1] loss 0.77147, dsc 0.22853\n",
      "Batch train [2] loss 0.75643, dsc 0.24357\n",
      "Batch train [3] loss 0.72240, dsc 0.27760\n",
      "Batch train [4] loss 0.82897, dsc 0.17103\n",
      "Batch train [5] loss 0.84166, dsc 0.15834\n",
      "Batch train [6] loss 0.69757, dsc 0.30243\n",
      "Batch train [7] loss 0.71382, dsc 0.28618\n",
      "Batch train [8] loss 0.83037, dsc 0.16963\n",
      "Batch train [9] loss 0.77493, dsc 0.22507\n",
      "Batch train [10] loss 0.74635, dsc 0.25365\n",
      "Batch train [11] loss 0.78818, dsc 0.21182\n",
      "Batch train [12] loss 0.83282, dsc 0.16718\n",
      "Batch train [13] loss 0.79389, dsc 0.20611\n",
      "Batch train [14] loss 0.74379, dsc 0.25621\n",
      "Batch train [15] loss 0.77455, dsc 0.22545\n",
      "Batch train [16] loss 0.75225, dsc 0.24775\n",
      "Batch train [17] loss 0.77719, dsc 0.22281\n",
      "Batch train [18] loss 0.66259, dsc 0.33741\n",
      "Batch train [19] loss 0.69989, dsc 0.30011\n",
      "Batch train [20] loss 0.77864, dsc 0.22136\n",
      "Epoch [27] train done\n",
      "Batch eval [1] loss 0.69836, dsc 0.30164\n",
      "Batch eval [2] loss 0.75069, dsc 0.24931\n",
      "Batch eval [3] loss 0.67320, dsc 0.32680\n",
      "Batch eval [4] loss 0.72572, dsc 0.27428\n",
      "Batch eval [5] loss 0.80159, dsc 0.19841\n",
      "Epoch [27] valid done\n",
      "Epoch [27] T 1771.37s, deltaT 65.78s, loss: train 0.76439, valid 0.72991, dsc: train 0.23561, valid 0.27009\n",
      "Batch train [1] loss 0.75398, dsc 0.24602\n",
      "Batch train [2] loss 0.66918, dsc 0.33082\n",
      "Batch train [3] loss 0.76631, dsc 0.23369\n",
      "Batch train [4] loss 0.75459, dsc 0.24541\n",
      "Batch train [5] loss 0.74951, dsc 0.25049\n",
      "Batch train [6] loss 0.73378, dsc 0.26622\n",
      "Batch train [7] loss 0.66254, dsc 0.33746\n",
      "Batch train [8] loss 0.78575, dsc 0.21425\n",
      "Batch train [9] loss 0.68181, dsc 0.31819\n",
      "Batch train [10] loss 0.69797, dsc 0.30203\n",
      "Batch train [11] loss 0.79413, dsc 0.20587\n",
      "Batch train [12] loss 0.66111, dsc 0.33889\n",
      "Batch train [13] loss 0.72697, dsc 0.27303\n",
      "Batch train [14] loss 0.68060, dsc 0.31940\n",
      "Batch train [15] loss 0.82523, dsc 0.17477\n",
      "Batch train [16] loss 0.76698, dsc 0.23302\n",
      "Batch train [17] loss 0.73485, dsc 0.26515\n",
      "Batch train [18] loss 0.74223, dsc 0.25777\n",
      "Batch train [19] loss 0.68820, dsc 0.31180\n",
      "Batch train [20] loss 0.67343, dsc 0.32657\n",
      "Epoch [28] train done\n",
      "Batch eval [1] loss 0.63432, dsc 0.36568\n",
      "Batch eval [2] loss 0.70695, dsc 0.29305\n",
      "Batch eval [3] loss 0.61960, dsc 0.38040\n",
      "Batch eval [4] loss 0.68145, dsc 0.31855\n",
      "Batch eval [5] loss 0.75977, dsc 0.24023\n",
      "Epoch [28] valid done\n",
      "Epoch [28] T 1839.69s, deltaT 68.32s, loss: train 0.72746, valid 0.68042, dsc: train 0.27254, valid 0.31958\n",
      "Batch train [1] loss 0.70991, dsc 0.29009\n",
      "Batch train [2] loss 0.63286, dsc 0.36714\n",
      "Batch train [3] loss 0.71222, dsc 0.28778\n",
      "Batch train [4] loss 0.73107, dsc 0.26893\n",
      "Batch train [5] loss 0.75055, dsc 0.24945\n",
      "Batch train [6] loss 0.72458, dsc 0.27542\n",
      "Batch train [7] loss 0.72722, dsc 0.27278\n",
      "Batch train [8] loss 0.65246, dsc 0.34754\n",
      "Batch train [9] loss 0.65914, dsc 0.34086\n",
      "Batch train [10] loss 0.65700, dsc 0.34300\n",
      "Batch train [11] loss 0.80093, dsc 0.19907\n",
      "Batch train [12] loss 0.63810, dsc 0.36190\n",
      "Batch train [13] loss 0.74853, dsc 0.25147\n",
      "Batch train [14] loss 0.71618, dsc 0.28382\n",
      "Batch train [15] loss 0.70342, dsc 0.29658\n",
      "Batch train [16] loss 0.73210, dsc 0.26790\n",
      "Batch train [17] loss 0.63340, dsc 0.36660\n",
      "Batch train [18] loss 0.58436, dsc 0.41564\n",
      "Batch train [19] loss 0.63460, dsc 0.36540\n",
      "Batch train [20] loss 0.60463, dsc 0.39537\n",
      "Epoch [29] train done\n",
      "Batch eval [1] loss 0.55897, dsc 0.44103\n",
      "Batch eval [2] loss 0.63676, dsc 0.36324\n",
      "Batch eval [3] loss 0.53854, dsc 0.46146\n",
      "Batch eval [4] loss 0.61813, dsc 0.38187\n",
      "Batch eval [5] loss 0.69419, dsc 0.30581\n",
      "Epoch [29] valid done\n",
      "Epoch [29] T 1908.17s, deltaT 68.48s, loss: train 0.68766, valid 0.60932, dsc: train 0.31234, valid 0.39068\n",
      "Batch train [1] loss 0.67252, dsc 0.32748\n",
      "Batch train [2] loss 0.57674, dsc 0.42326\n",
      "Batch train [3] loss 0.73258, dsc 0.26742\n",
      "Batch train [4] loss 0.67398, dsc 0.32602\n",
      "Batch train [5] loss 0.69302, dsc 0.30698\n",
      "Batch train [6] loss 0.71267, dsc 0.28733\n",
      "Batch train [7] loss 0.73769, dsc 0.26231\n",
      "Batch train [8] loss 0.70359, dsc 0.29641\n",
      "Batch train [9] loss 0.69744, dsc 0.30256\n",
      "Batch train [10] loss 0.60901, dsc 0.39099\n",
      "Batch train [11] loss 0.59126, dsc 0.40874\n",
      "Batch train [12] loss 0.60585, dsc 0.39415\n",
      "Batch train [13] loss 0.54107, dsc 0.45893\n",
      "Batch train [14] loss 0.64827, dsc 0.35173\n",
      "Batch train [15] loss 0.67461, dsc 0.32539\n",
      "Batch train [16] loss 0.69990, dsc 0.30010\n",
      "Batch train [17] loss 0.61678, dsc 0.38322\n",
      "Batch train [18] loss 0.68305, dsc 0.31695\n",
      "Batch train [19] loss 0.58520, dsc 0.41480\n",
      "Batch train [20] loss 0.53044, dsc 0.46956\n",
      "Epoch [30] train done\n",
      "Batch eval [1] loss 0.52283, dsc 0.47717\n",
      "Batch eval [2] loss 0.58260, dsc 0.41740\n",
      "Batch eval [3] loss 0.49472, dsc 0.50528\n",
      "Batch eval [4] loss 0.54952, dsc 0.45048\n",
      "Batch eval [5] loss 0.65330, dsc 0.34670\n",
      "Epoch [30] valid done\n",
      "Epoch [30] T 1973.81s, deltaT 65.64s, loss: train 0.64928, valid 0.56059, dsc: train 0.35072, valid 0.43941\n",
      "Batch train [1] loss 0.66615, dsc 0.33385\n",
      "Batch train [2] loss 0.67473, dsc 0.32527\n",
      "Batch train [3] loss 0.70434, dsc 0.29566\n",
      "Batch train [4] loss 0.61036, dsc 0.38964\n",
      "Batch train [5] loss 0.57639, dsc 0.42361\n",
      "Batch train [6] loss 0.55567, dsc 0.44433\n",
      "Batch train [7] loss 0.59745, dsc 0.40255\n",
      "Batch train [8] loss 0.54347, dsc 0.45653\n",
      "Batch train [9] loss 0.60897, dsc 0.39103\n",
      "Batch train [10] loss 0.61434, dsc 0.38566\n",
      "Batch train [11] loss 0.56326, dsc 0.43674\n",
      "Batch train [12] loss 0.60542, dsc 0.39458\n",
      "Batch train [13] loss 0.55614, dsc 0.44386\n",
      "Batch train [14] loss 0.65387, dsc 0.34613\n",
      "Batch train [15] loss 0.55235, dsc 0.44765\n",
      "Batch train [16] loss 0.57657, dsc 0.42343\n",
      "Batch train [17] loss 0.61780, dsc 0.38220\n",
      "Batch train [18] loss 0.66026, dsc 0.33974\n",
      "Batch train [19] loss 0.66398, dsc 0.33602\n",
      "Batch train [20] loss 0.51691, dsc 0.48309\n",
      "Epoch [31] train done\n",
      "Batch eval [1] loss 0.49728, dsc 0.50272\n",
      "Batch eval [2] loss 0.56071, dsc 0.43929\n",
      "Batch eval [3] loss 0.47680, dsc 0.52320\n",
      "Batch eval [4] loss 0.53815, dsc 0.46185\n",
      "Batch eval [5] loss 0.62348, dsc 0.37652\n",
      "Epoch [31] valid done\n",
      "Epoch [31] T 2036.48s, deltaT 62.67s, loss: train 0.60592, valid 0.53928, dsc: train 0.39408, valid 0.46072\n",
      "Batch train [1] loss 0.59552, dsc 0.40448\n",
      "Batch train [2] loss 0.64603, dsc 0.35397\n",
      "Batch train [3] loss 0.58072, dsc 0.41928\n",
      "Batch train [4] loss 0.59820, dsc 0.40180\n",
      "Batch train [5] loss 0.54543, dsc 0.45457\n",
      "Batch train [6] loss 0.49339, dsc 0.50661\n",
      "Batch train [7] loss 0.51231, dsc 0.48769\n",
      "Batch train [8] loss 0.51196, dsc 0.48804\n",
      "Batch train [9] loss 0.59188, dsc 0.40812\n",
      "Batch train [10] loss 0.55450, dsc 0.44550\n",
      "Batch train [11] loss 0.63451, dsc 0.36549\n",
      "Batch train [12] loss 0.49334, dsc 0.50666\n",
      "Batch train [13] loss 0.58108, dsc 0.41892\n",
      "Batch train [14] loss 0.46536, dsc 0.53464\n",
      "Batch train [15] loss 0.54654, dsc 0.45346\n",
      "Batch train [16] loss 0.56696, dsc 0.43304\n",
      "Batch train [17] loss 0.52586, dsc 0.47414\n",
      "Batch train [18] loss 0.60435, dsc 0.39565\n",
      "Batch train [19] loss 0.52513, dsc 0.47487\n",
      "Batch train [20] loss 0.55792, dsc 0.44208\n",
      "Epoch [32] train done\n",
      "Batch eval [1] loss 0.45265, dsc 0.54735\n",
      "Batch eval [2] loss 0.52294, dsc 0.47706\n",
      "Batch eval [3] loss 0.43564, dsc 0.56436\n",
      "Batch eval [4] loss 0.49793, dsc 0.50207\n",
      "Batch eval [5] loss 0.60110, dsc 0.39890\n",
      "Epoch [32] valid done\n",
      "Epoch [32] T 2099.36s, deltaT 62.87s, loss: train 0.55655, valid 0.50205, dsc: train 0.44345, valid 0.49795\n",
      "Batch train [1] loss 0.48128, dsc 0.51872\n",
      "Batch train [2] loss 0.60986, dsc 0.39014\n",
      "Batch train [3] loss 0.55849, dsc 0.44151\n",
      "Batch train [4] loss 0.48503, dsc 0.51497\n",
      "Batch train [5] loss 0.57602, dsc 0.42398\n",
      "Batch train [6] loss 0.57716, dsc 0.42284\n",
      "Batch train [7] loss 0.49644, dsc 0.50356\n",
      "Batch train [8] loss 0.48745, dsc 0.51255\n",
      "Batch train [9] loss 0.42196, dsc 0.57804\n",
      "Batch train [10] loss 0.53131, dsc 0.46869\n",
      "Batch train [11] loss 0.49368, dsc 0.50632\n",
      "Batch train [12] loss 0.44614, dsc 0.55386\n",
      "Batch train [13] loss 0.56672, dsc 0.43328\n",
      "Batch train [14] loss 0.47769, dsc 0.52231\n",
      "Batch train [15] loss 0.53177, dsc 0.46823\n",
      "Batch train [16] loss 0.45458, dsc 0.54542\n",
      "Batch train [17] loss 0.53604, dsc 0.46396\n",
      "Batch train [18] loss 0.39357, dsc 0.60643\n",
      "Batch train [19] loss 0.49147, dsc 0.50853\n",
      "Batch train [20] loss 0.55995, dsc 0.44005\n",
      "Epoch [33] train done\n",
      "Batch eval [1] loss 0.44718, dsc 0.55282\n",
      "Batch eval [2] loss 0.49480, dsc 0.50520\n",
      "Batch eval [3] loss 0.44861, dsc 0.55139\n",
      "Batch eval [4] loss 0.49960, dsc 0.50040\n",
      "Batch eval [5] loss 0.54197, dsc 0.45803\n",
      "Epoch [33] valid done\n",
      "Epoch [33] T 2162.56s, deltaT 63.20s, loss: train 0.50883, valid 0.48643, dsc: train 0.49117, valid 0.51357\n",
      "Batch train [1] loss 0.48831, dsc 0.51169\n",
      "Batch train [2] loss 0.49552, dsc 0.50448\n",
      "Batch train [3] loss 0.45083, dsc 0.54917\n",
      "Batch train [4] loss 0.47851, dsc 0.52149\n",
      "Batch train [5] loss 0.50148, dsc 0.49852\n",
      "Batch train [6] loss 0.45104, dsc 0.54896\n",
      "Batch train [7] loss 0.54319, dsc 0.45681\n",
      "Batch train [8] loss 0.51644, dsc 0.48356\n",
      "Batch train [9] loss 0.51594, dsc 0.48406\n",
      "Batch train [10] loss 0.43594, dsc 0.56406\n",
      "Batch train [11] loss 0.52000, dsc 0.48000\n",
      "Batch train [12] loss 0.38512, dsc 0.61488\n",
      "Batch train [13] loss 0.49140, dsc 0.50860\n",
      "Batch train [14] loss 0.50715, dsc 0.49285\n",
      "Batch train [15] loss 0.39553, dsc 0.60447\n",
      "Batch train [16] loss 0.46684, dsc 0.53316\n",
      "Batch train [17] loss 0.45534, dsc 0.54466\n",
      "Batch train [18] loss 0.44487, dsc 0.55513\n",
      "Batch train [19] loss 0.36314, dsc 0.63686\n",
      "Batch train [20] loss 0.55217, dsc 0.44783\n",
      "Epoch [34] train done\n",
      "Batch eval [1] loss 0.38767, dsc 0.61233\n",
      "Batch eval [2] loss 0.42431, dsc 0.57569\n",
      "Batch eval [3] loss 0.36435, dsc 0.63565\n",
      "Batch eval [4] loss 0.39902, dsc 0.60098\n",
      "Batch eval [5] loss 0.51453, dsc 0.48547\n",
      "Epoch [34] valid done\n",
      "Epoch [34] T 2230.82s, deltaT 68.26s, loss: train 0.47294, valid 0.41798, dsc: train 0.52706, valid 0.58202\n",
      "Batch train [1] loss 0.48373, dsc 0.51627\n",
      "Batch train [2] loss 0.52793, dsc 0.47207\n",
      "Batch train [3] loss 0.40541, dsc 0.59459\n",
      "Batch train [4] loss 0.42974, dsc 0.57026\n",
      "Batch train [5] loss 0.47266, dsc 0.52734\n",
      "Batch train [6] loss 0.49466, dsc 0.50534\n",
      "Batch train [7] loss 0.38654, dsc 0.61346\n",
      "Batch train [8] loss 0.40103, dsc 0.59897\n",
      "Batch train [9] loss 0.44897, dsc 0.55103\n",
      "Batch train [10] loss 0.40877, dsc 0.59123\n",
      "Batch train [11] loss 0.32082, dsc 0.67918\n",
      "Batch train [12] loss 0.44734, dsc 0.55266\n",
      "Batch train [13] loss 0.46465, dsc 0.53535\n",
      "Batch train [14] loss 0.39680, dsc 0.60320\n",
      "Batch train [15] loss 0.43129, dsc 0.56871\n",
      "Batch train [16] loss 0.36205, dsc 0.63795\n",
      "Batch train [17] loss 0.41452, dsc 0.58548\n",
      "Batch train [18] loss 0.48564, dsc 0.51436\n",
      "Batch train [19] loss 0.45378, dsc 0.54622\n",
      "Batch train [20] loss 0.39060, dsc 0.60940\n",
      "Epoch [35] train done\n",
      "Batch eval [1] loss 0.33588, dsc 0.66412\n",
      "Batch eval [2] loss 0.39849, dsc 0.60151\n",
      "Batch eval [3] loss 0.33709, dsc 0.66291\n",
      "Batch eval [4] loss 0.36603, dsc 0.63397\n",
      "Batch eval [5] loss 0.47403, dsc 0.52597\n",
      "Epoch [35] valid done\n",
      "Epoch [35] T 2298.52s, deltaT 67.70s, loss: train 0.43135, valid 0.38230, dsc: train 0.56865, valid 0.61770\n",
      "Batch train [1] loss 0.45903, dsc 0.54097\n",
      "Batch train [2] loss 0.42092, dsc 0.57908\n",
      "Batch train [3] loss 0.47655, dsc 0.52345\n",
      "Batch train [4] loss 0.45810, dsc 0.54190\n",
      "Batch train [5] loss 0.30568, dsc 0.69432\n",
      "Batch train [6] loss 0.40732, dsc 0.59268\n",
      "Batch train [7] loss 0.43192, dsc 0.56808\n",
      "Batch train [8] loss 0.41470, dsc 0.58530\n",
      "Batch train [9] loss 0.37086, dsc 0.62914\n",
      "Batch train [10] loss 0.32940, dsc 0.67060\n",
      "Batch train [11] loss 0.32655, dsc 0.67345\n",
      "Batch train [12] loss 0.40833, dsc 0.59167\n",
      "Batch train [13] loss 0.44983, dsc 0.55017\n",
      "Batch train [14] loss 0.40504, dsc 0.59496\n",
      "Batch train [15] loss 0.40522, dsc 0.59478\n",
      "Batch train [16] loss 0.41444, dsc 0.58556\n",
      "Batch train [17] loss 0.35625, dsc 0.64375\n",
      "Batch train [18] loss 0.36195, dsc 0.63805\n",
      "Batch train [19] loss 0.38400, dsc 0.61600\n",
      "Batch train [20] loss 0.42646, dsc 0.57354\n",
      "Epoch [36] train done\n",
      "Batch eval [1] loss 0.32781, dsc 0.67219\n",
      "Batch eval [2] loss 0.38038, dsc 0.61962\n",
      "Batch eval [3] loss 0.31842, dsc 0.68158\n",
      "Batch eval [4] loss 0.35756, dsc 0.64244\n",
      "Batch eval [5] loss 0.45235, dsc 0.54765\n",
      "Epoch [36] valid done\n",
      "Epoch [36] T 2367.03s, deltaT 68.51s, loss: train 0.40063, valid 0.36731, dsc: train 0.59937, valid 0.63269\n",
      "Batch train [1] loss 0.43794, dsc 0.56206\n",
      "Batch train [2] loss 0.43764, dsc 0.56236\n",
      "Batch train [3] loss 0.31454, dsc 0.68546\n",
      "Batch train [4] loss 0.35326, dsc 0.64674\n",
      "Batch train [5] loss 0.36484, dsc 0.63516\n",
      "Batch train [6] loss 0.39739, dsc 0.60261\n",
      "Batch train [7] loss 0.36122, dsc 0.63878\n",
      "Batch train [8] loss 0.32869, dsc 0.67131\n",
      "Batch train [9] loss 0.43354, dsc 0.56646\n",
      "Batch train [10] loss 0.37901, dsc 0.62099\n",
      "Batch train [11] loss 0.42991, dsc 0.57009\n",
      "Batch train [12] loss 0.35623, dsc 0.64377\n",
      "Batch train [13] loss 0.33755, dsc 0.66245\n",
      "Batch train [14] loss 0.41369, dsc 0.58631\n",
      "Batch train [15] loss 0.40132, dsc 0.59868\n",
      "Batch train [16] loss 0.31288, dsc 0.68712\n",
      "Batch train [17] loss 0.33727, dsc 0.66273\n"
     ]
    }
   ],
   "source": [
    "TRAIN_MODELS = True\n",
    "if TRAIN_MODELS:\n",
    "    filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "    if 'SPINAL_CORD' in filter_labels:\n",
    "        del filter_labels['SPINAL_CORD']\n",
    "\n",
    "\n",
    "    for OAR_KEY, OAR_VALUE in list(filter_labels.items())[10:]:\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        log_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        model_name = f'{log_date}_3d_unet_{OAR_KEY}'\n",
    "\n",
    "        print(f'Training model with dataset label \\'{OAR_KEY}\\', value \\'{OAR_VALUE}\\'')\n",
    "        print(f'folder \\'{model_name}\\'')\n",
    "        cut_model_info = prepare_model(epochs=75,\n",
    "                                       learning_rate=3e-4,\n",
    "                                       in_channels=8,\n",
    "                                       dropout_rate=0.2,\n",
    "                                       train_batch_size=2,\n",
    "                                       model_name=model_name,\n",
    "                                       train_dataset=cut_train_dataset, \n",
    "                                       valid_dataset=cut_valid_dataset, \n",
    "                                       test_dataset=cut_test_dataset)\n",
    "        show_model_info(cut_model_info)\n",
    "        print('\\n\\n')\n",
    "        train_loop(cut_model_info)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # clearing memory\n",
    "        del cut_model_info\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    #     # Preview one image form dataset  \n",
    "    #     tmp_data, tmp_label = cut_train_dataset[0]\n",
    "    #     tmp_idx = np.where(tmp_label > 0)[0]\n",
    "    #     tmp_slice = tmp_idx[int(np.median(tmp_idx))]\n",
    "    #     plt.figure(figsize=(16, 16))\n",
    "    #     plt.subplot(1, 2, 1)\n",
    "    #     plt.imshow(tmp_data[0, tmp_slice], cmap=\"gray\")\n",
    "    #     plt.subplot(1, 2, 2)\n",
    "    #     plt.imshow(tmp_label[tmp_slice])\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview organ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading organ model\n",
    "# log_date_dict = {\n",
    "#     \"year\": 2020, \n",
    "#     \"month\": 11, \n",
    "#     \"day\": 2, \n",
    "#     \"hour\": 15, \n",
    "#     \"minute\": 19, \n",
    "#     \"second\": 45\n",
    "# }\n",
    "# log_date = datetime.datetime(**log_date_dict).strftime(\"%Y%m%d-%H%M%S\")\n",
    "# model_name = f'{log_date}_3d_unet_{OAR_KEY}'\n",
    "\n",
    "# OAR_KEY = 'EYE_L'\n",
    "# epoch = 75\n",
    "\n",
    "OAR_VALUE = OARS_LABELS.OPT_NERVE_L\n",
    "OAR_KEY = OARS_LABELS.OARS_LABELS_R_DICT[OAR_VALUE]\n",
    "epoch = 75\n",
    "possible_models = [folder_name for folder_name in os.listdir('./models') if OAR_KEY in folder_name]\n",
    "model_name = possible_models[0]\n",
    "\n",
    "\n",
    "# loading model checkpoint\n",
    "cut_model_info = load_checkpoint_model_info(model_name, epoch, cut_train_dataset, cut_valid_dataset, cut_test_dataset)\n",
    "\n",
    "# moving model to cpu with eval mode\n",
    "cut_model_info['device'] = 'cpu'\n",
    "cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "cut_model_info['model'].eval()\n",
    "\n",
    "# preparing dataset for comparison\n",
    "OAR_VALUE = OARS_LABELS.OARS_LABELS_DICT[OAR_KEY]\n",
    "cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "\n",
    "# train\n",
    "rnd_train_idx = low_res_split_dataset_obj['train_dataset'].indices[0]\n",
    "print(f'Train index {rnd_train_idx}')\n",
    "raw_data, raw_label, raw_prediction = get_raw_with_prediction(cut_model_info['model'], cut_full_res_dataset, cut_model_info[\"device\"], rnd_train_idx)\n",
    "compare_one_prediction_with_ground_true(raw_data,\n",
    "                                        raw_label,\n",
    "                                        raw_prediction,\n",
    "                                        pred_threshold=0.5)\n",
    "\n",
    "# valid\n",
    "rnd_valid_idx = low_res_split_dataset_obj['valid_dataset'].indices[0]\n",
    "print(f'Valid index {rnd_valid_idx}')\n",
    "raw_data, raw_label, raw_prediction = get_raw_with_prediction(cut_model_info['model'], cut_full_res_dataset, cut_model_info[\"device\"], rnd_valid_idx)\n",
    "compare_one_prediction_with_ground_true(raw_data,\n",
    "                                        raw_label,\n",
    "                                        raw_prediction,\n",
    "                                        pred_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
