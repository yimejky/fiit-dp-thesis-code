{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n",
      "Dataset biggest bounding box wihtout spinal cord [56, 177, 156]\n",
      "Cut target size [72, 192, 168]\n",
      "Done Init\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/dp_tomastik/code')\n",
    "    !bash \"/content/drive/My Drive/dp_tomastik/code/scripts/install_libs.sh\"\n",
    "    \n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "from torchio import RandomAffine, Compose, ZNormalization\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from src.helpers import preview_3d_image\n",
    "from src.helpers import show_cuda_usage, preview_model_dataset_pred, preview_dataset\n",
    "from src.helpers import get_threshold_info_df, get_rescaled_preds\n",
    "from src.helpers import compare_prediction_with_ground_true, compare_one_prediction_with_ground_true\n",
    "from src.helpers import get_img_outliers_pixels, get_raw_with_prediction\n",
    "from src.helpers import get_rescaled_pred\n",
    "from src.helpers import get_transformed_label_np, create_regis_trans_list, trans_list\n",
    "\n",
    "from src.dataset import HaNOarsDataset, transform_input_with_registration, get_norm_transform\n",
    "from src.dataset import get_full_res_cut, get_cut_lists, OARS_LABELS, get_dataset, get_dataset_info, get_dataset_transform\n",
    "from src.dataset import split_dataset, copy_split_dataset\n",
    "\n",
    "from src.model_and_training import prepare_model, train_loop, show_model_info, load_checkpoint_model_info\n",
    "from src.model_and_training import iterate_model_v3v2\n",
    "from src.model_and_training.getters.get_device import get_device\n",
    "from src.model_and_training.architectures.unet_architecture_v3v2 import UNetV3v2\n",
    "\n",
    "from src.consts import DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "  \n",
    "torch.manual_seed(20)\n",
    "logging.basicConfig(filename='logs/model3v2_all_organs_jupyter.log', level=logging.DEBUG)\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_models(oar_key):\n",
    "    possible_models = [folder_name for folder_name in os.listdir('./models') if oar_key in folder_name]    \n",
    "    \n",
    "    return possible_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'GeForce RTX 2070')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training all organs models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting random indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'./data/HaN_OAR_cut_all_maps_reg'\n",
    "example_cut_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "example_cut_dataset.load_from_file(data_path)\n",
    "example_cut_dataset_obj = split_dataset(example_cut_dataset, train_size=40, valid_size=5, test_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing dataset input example\n",
    "# data_path = f'./data/HaN_OAR_cut_left_parotid_reg'\n",
    "# cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "# cut_full_res_dataset.load_from_file(data_path)\n",
    "# cut_full_res_dataset.set_output_label(OARS_LABELS.PAROTID_GLAND_R)\n",
    "                                     \n",
    "# preview_3d_image(cut_full_res_dataset[0][0][0], figsize=(4,4))\n",
    "# preview_3d_image(cut_full_res_dataset[0][0][1], figsize=(4,4))\n",
    "# preview_3d_image(cut_full_res_dataset[0][1], figsize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brainstem, PITUITARY, 11\n"
     ]
    }
   ],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels['SPINAL_CORD']\n",
    "\n",
    "tmp_list = list(filter_labels.items())\n",
    "labels_list = list()\n",
    "\n",
    "# creating registration dataset and organ segmentation pairs\n",
    "# dataset use inverted labeling of left and right\n",
    "# labels_list.append(('left_parotid', *tmp_list[12]))\n",
    "# labels_list.append(('right_parotid', *tmp_list[11]))\n",
    "\n",
    "labels_list.append(('brainstem', *tmp_list[10]))\n",
    "labels_list.append(('parotids', *tmp_list[10]))\n",
    "labels_list.append(('all_maps', *tmp_list[10]))\n",
    "\n",
    "for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "    print(f\"{DATASET_REG_NAME}, {OAR_KEY}, {OAR_VALUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_MODELS = False\n",
    "if TRAIN_MODELS:\n",
    "    for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "        # loading dataset\n",
    "        data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "        cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "        cut_full_res_dataset.load_from_file(data_path)\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        \n",
    "        cut_full_res_dataset_obj = copy_split_dataset(cut_full_res_dataset, example_cut_dataset_obj)\n",
    "        cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "\n",
    "        # preparing model name\n",
    "        log_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        model_name = f'{log_date}_3d_unet_lowres_model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg'\n",
    "\n",
    "        print(f'Training model with dataset label \\'{OAR_KEY}\\', value \\'{OAR_VALUE}\\'')\n",
    "        print(f'folder \\'{model_name}\\'')\n",
    "        cut_model_info = prepare_model(epochs=75,\n",
    "                                       learning_rate=3e-4,\n",
    "                                       in_channels=8,\n",
    "                                       input_data_channels=1,\n",
    "                                       output_label_channels=1,\n",
    "                                       dropout_rate=0.2,\n",
    "                                       train_batch_size=2,\n",
    "                                       model_name=model_name,\n",
    "                                       train_dataset=cut_train_dataset, \n",
    "                                       valid_dataset=cut_valid_dataset, \n",
    "                                       test_dataset=cut_test_dataset,\n",
    "                                       model_class=UNetV3v2)\n",
    "        show_model_info(cut_model_info)\n",
    "        print('\\n\\n')\n",
    "        train_loop(cut_model_info, iterate_model_fn=iterate_model_v3v2)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # clearing memory\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_and_training.getters.get_loaders import get_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with dataset label 'PITUITARY', value '11'\n",
      "folder '20210420-140656_3d_unet_lowres_model3v2__cloud-PITUITARY-brainstem_reg'\n",
      "Model: Loading model 20210417-183504_3d_unet_lowres_model3v2__cloud-PITUITARY-brainstem_reg\n",
      "Model number of params: 1221609, trainable 1221609\n",
      "\n",
      "\n",
      "\n",
      "Running training loop\n",
      "DEBUG: Writing to tensorboard before epoch True, 75, step 0\n",
      "DEBUG: Writing to tensorboard after epoch True,  75, step 0\n",
      "Batch train [1] loss 0.87406, dsc 0.12594\n",
      "Batch train [1] loss 0.81033, dsc 0.18967\n",
      "Batch train [1] loss 0.51317, dsc 0.48683\n",
      "Batch train [1] loss 0.62090, dsc 0.37910\n",
      "Batch train [1] loss 0.99839, dsc 0.00161\n",
      "Batch train [1] loss 0.97970, dsc 0.02030\n",
      "Batch train [1] loss 0.95374, dsc 0.04626\n",
      "Batch train [1] loss 0.80466, dsc 0.19534\n",
      "Batch train [1] loss 0.58118, dsc 0.41882\n",
      "Batch train [1] loss 0.56174, dsc 0.43826\n",
      "Batch train [1] loss 0.88566, dsc 0.11434\n",
      "Batch train [1] loss 0.50360, dsc 0.49640\n",
      "Batch train [1] loss 0.66871, dsc 0.33129\n",
      "Batch train [1] loss 0.75223, dsc 0.24777\n",
      "Batch train [1] loss 0.38068, dsc 0.61932\n",
      "Batch train [1] loss 0.43652, dsc 0.56348\n",
      "Batch train [1] loss 0.74279, dsc 0.25721\n",
      "Batch train [1] loss 0.47677, dsc 0.52323\n",
      "Batch train [1] loss 0.87232, dsc 0.12768\n",
      "Batch train [1] loss 0.75991, dsc 0.24009\n",
      "Batch train [1] loss 0.54839, dsc 0.45161\n",
      "Batch train [1] loss 0.49404, dsc 0.50596\n",
      "Batch train [1] loss 0.47206, dsc 0.52794\n",
      "Batch train [1] loss 0.92230, dsc 0.07770\n",
      "Batch train [1] loss 0.56909, dsc 0.43091\n",
      "Batch train [1] loss 0.63880, dsc 0.36120\n",
      "Batch train [1] loss 0.38186, dsc 0.61814\n",
      "Batch train [1] loss 0.70641, dsc 0.29359\n",
      "Batch train [1] loss 0.80183, dsc 0.19817\n",
      "Batch train [1] loss 0.51157, dsc 0.48843\n",
      "Batch train [1] loss 0.56620, dsc 0.43380\n",
      "Batch train [1] loss 0.51364, dsc 0.48636\n",
      "Batch train [1] loss 0.61855, dsc 0.38145\n",
      "Batch train [1] loss 0.83634, dsc 0.16366\n",
      "Batch train [1] loss 0.47465, dsc 0.52535\n",
      "Batch train [1] loss 0.52737, dsc 0.47263\n",
      "Batch train [1] loss 0.58444, dsc 0.41556\n",
      "Batch train [1] loss 0.64183, dsc 0.35817\n",
      "Batch train [1] loss 0.45252, dsc 0.54748\n",
      "Batch train [1] loss 0.50679, dsc 0.49321\n",
      "Epoch [76] train done\n",
      "DEBUG: Writing to tensorboard before epoch False, 75, step 0\n",
      "DEBUG: Writing to tensorboard after epoch False,  75, step 0\n",
      "Batch eval [1] loss 0.78436, dsc 0.21564\n",
      "Batch eval [1] loss 0.99608, dsc 0.00392\n",
      "Batch eval [1] loss 0.99967, dsc 0.00033\n",
      "Batch eval [1] loss 0.44306, dsc 0.55694\n",
      "Batch eval [1] loss 0.98268, dsc 0.01732\n",
      "Epoch [76] valid done\n",
      "Epoch [76] T 122.61s, deltaT 122.61s, loss: train 0.64864, valid 0.84117, dsc: train 0.35136, valid 0.15883\n",
      "DEBUG: Writing to tensorboard before epoch True, 76, step 0\n",
      "DEBUG: Writing to tensorboard after epoch True,  76, step 0\n",
      "Batch train [1] loss 0.50210, dsc 0.49790\n",
      "Batch train [1] loss 0.92238, dsc 0.07762\n",
      "Batch train [1] loss 0.45181, dsc 0.54819\n",
      "Batch train [1] loss 0.43841, dsc 0.56159\n",
      "Batch train [1] loss 0.84416, dsc 0.15584\n",
      "Batch train [1] loss 0.72278, dsc 0.27722\n",
      "Batch train [1] loss 0.27953, dsc 0.72047\n",
      "Batch train [1] loss 0.40297, dsc 0.59703\n",
      "Batch train [1] loss 0.67923, dsc 0.32077\n",
      "Batch train [1] loss 0.72621, dsc 0.27379\n",
      "Batch train [1] loss 0.43077, dsc 0.56923\n",
      "Batch train [1] loss 0.39438, dsc 0.60562\n",
      "Batch train [1] loss 0.68010, dsc 0.31990\n",
      "Batch train [1] loss 0.69654, dsc 0.30346\n",
      "Batch train [1] loss 0.79446, dsc 0.20554\n",
      "Batch train [1] loss 0.39963, dsc 0.60037\n",
      "Batch train [1] loss 0.50575, dsc 0.49425\n",
      "Batch train [1] loss 0.76415, dsc 0.23585\n",
      "Batch train [1] loss 0.38047, dsc 0.61953\n",
      "Batch train [1] loss 0.87286, dsc 0.12714\n",
      "Batch train [1] loss 0.61781, dsc 0.38219\n",
      "Batch train [1] loss 0.40631, dsc 0.59369\n",
      "Batch train [1] loss 0.44626, dsc 0.55374\n",
      "Batch train [1] loss 0.65874, dsc 0.34126\n",
      "Batch train [1] loss 0.54022, dsc 0.45978\n",
      "Batch train [1] loss 0.59443, dsc 0.40557\n",
      "Batch train [1] loss 0.54037, dsc 0.45963\n",
      "Batch train [1] loss 0.40424, dsc 0.59576\n",
      "Batch train [1] loss 0.44616, dsc 0.55384\n",
      "Batch train [1] loss 0.65213, dsc 0.34787\n",
      "Batch train [1] loss 0.43610, dsc 0.56390\n",
      "Batch train [1] loss 0.50022, dsc 0.49978\n",
      "Batch train [1] loss 0.41424, dsc 0.58576\n",
      "Batch train [1] loss 0.71530, dsc 0.28470\n",
      "Batch train [1] loss 0.34405, dsc 0.65595\n",
      "Batch train [1] loss 0.56873, dsc 0.43127\n",
      "Batch train [1] loss 0.74386, dsc 0.25614\n",
      "Batch train [1] loss 0.46755, dsc 0.53245\n",
      "Batch train [1] loss 0.56825, dsc 0.43175\n",
      "Batch train [1] loss 0.50599, dsc 0.49401\n",
      "Epoch [77] train done\n",
      "DEBUG: Writing to tensorboard before epoch False, 76, step 0\n",
      "DEBUG: Writing to tensorboard after epoch False,  76, step 0\n",
      "Batch eval [1] loss 0.87629, dsc 0.12371\n",
      "Batch eval [1] loss 0.99992, dsc 0.00008\n",
      "Batch eval [1] loss 0.99994, dsc 0.00006\n",
      "Batch eval [1] loss 0.48846, dsc 0.51154\n",
      "Batch eval [1] loss 0.93885, dsc 0.06115\n",
      "Epoch [77] valid done\n",
      "Epoch [77] T 245.81s, deltaT 123.20s, loss: train 0.56149, valid 0.86069, dsc: train 0.43851, valid 0.13931\n",
      "DEBUG: Writing to tensorboard before epoch True, 77, step 0\n",
      "DEBUG: Writing to tensorboard after epoch True,  77, step 0\n",
      "Batch train [1] loss 0.60489, dsc 0.39511\n",
      "Batch train [1] loss 0.50921, dsc 0.49079\n",
      "Batch train [1] loss 0.92663, dsc 0.07337\n",
      "Batch train [1] loss 0.33501, dsc 0.66499\n",
      "Batch train [1] loss 0.44212, dsc 0.55788\n",
      "Batch train [1] loss 0.39532, dsc 0.60468\n",
      "Batch train [1] loss 0.36304, dsc 0.63696\n",
      "Batch train [1] loss 0.36036, dsc 0.63964\n",
      "Batch train [1] loss 0.62513, dsc 0.37487\n",
      "Batch train [1] loss 0.37420, dsc 0.62580\n",
      "Batch train [1] loss 0.47057, dsc 0.52943\n",
      "Batch train [1] loss 0.65565, dsc 0.34435\n",
      "Batch train [1] loss 0.76051, dsc 0.23949\n",
      "Batch train [1] loss 0.31688, dsc 0.68312\n",
      "Batch train [1] loss 0.41883, dsc 0.58117\n",
      "Batch train [1] loss 0.36147, dsc 0.63853\n",
      "Batch train [1] loss 0.67455, dsc 0.32545\n",
      "Batch train [1] loss 0.63036, dsc 0.36964\n",
      "Batch train [1] loss 0.56901, dsc 0.43099\n",
      "Batch train [1] loss 0.65788, dsc 0.34212\n",
      "Batch train [1] loss 0.42011, dsc 0.57989\n",
      "Batch train [1] loss 0.59100, dsc 0.40900\n",
      "Batch train [1] loss 0.82541, dsc 0.17459\n",
      "Batch train [1] loss 0.32194, dsc 0.67806\n",
      "Batch train [1] loss 0.45524, dsc 0.54476\n",
      "Batch train [1] loss 0.51595, dsc 0.48405\n",
      "Batch train [1] loss 0.70560, dsc 0.29440\n",
      "Batch train [1] loss 0.38946, dsc 0.61054\n",
      "Batch train [1] loss 0.39965, dsc 0.60035\n",
      "Batch train [1] loss 0.34137, dsc 0.65863\n",
      "Batch train [1] loss 0.34699, dsc 0.65301\n",
      "Batch train [1] loss 0.28534, dsc 0.71466\n",
      "Batch train [1] loss 0.40752, dsc 0.59248\n",
      "Batch train [1] loss 0.44818, dsc 0.55182\n",
      "Batch train [1] loss 0.53257, dsc 0.46743\n",
      "Batch train [1] loss 0.36367, dsc 0.63633\n",
      "Batch train [1] loss 0.37956, dsc 0.62044\n",
      "Batch train [1] loss 0.83740, dsc 0.16260\n",
      "Batch train [1] loss 0.88186, dsc 0.11814\n",
      "Batch train [1] loss 0.45228, dsc 0.54772\n",
      "Epoch [78] train done\n",
      "DEBUG: Writing to tensorboard before epoch False, 77, step 0\n",
      "DEBUG: Writing to tensorboard after epoch False,  77, step 0\n",
      "Batch eval [1] loss 0.98074, dsc 0.01926\n",
      "Batch eval [1] loss 1.00000, dsc 0.00000\n",
      "Batch eval [1] loss 0.99999, dsc 0.00001\n",
      "Batch eval [1] loss 0.75172, dsc 0.24828\n",
      "Batch eval [1] loss 0.90805, dsc 0.09195\n",
      "Epoch [78] valid done\n",
      "Epoch [78] T 368.93s, deltaT 123.12s, loss: train 0.50882, valid 0.92810, dsc: train 0.49118, valid 0.07190\n",
      "DEBUG: Writing to tensorboard before epoch True, 78, step 0\n",
      "DEBUG: Writing to tensorboard after epoch True,  78, step 0\n",
      "Batch train [1] loss 0.95828, dsc 0.04172\n",
      "Batch train [1] loss 0.52972, dsc 0.47028\n",
      "Batch train [1] loss 0.57174, dsc 0.42826\n",
      "Batch train [1] loss 0.63748, dsc 0.36252\n",
      "Batch train [1] loss 0.40670, dsc 0.59330\n",
      "Batch train [1] loss 0.67527, dsc 0.32473\n",
      "Batch train [1] loss 0.39924, dsc 0.60076\n",
      "Batch train [1] loss 0.37748, dsc 0.62252\n",
      "Batch train [1] loss 0.37329, dsc 0.62671\n",
      "Batch train [1] loss 0.31063, dsc 0.68937\n",
      "Batch train [1] loss 0.20559, dsc 0.79441\n",
      "Batch train [1] loss 0.24669, dsc 0.75331\n",
      "Batch train [1] loss 0.36803, dsc 0.63197\n",
      "Batch train [1] loss 0.36023, dsc 0.63977\n",
      "Batch train [1] loss 0.28915, dsc 0.71085\n",
      "Batch train [1] loss 0.69294, dsc 0.30706\n",
      "Batch train [1] loss 0.31779, dsc 0.68221\n",
      "Batch train [1] loss 0.49091, dsc 0.50909\n",
      "Batch train [1] loss 0.66840, dsc 0.33160\n",
      "Batch train [1] loss 0.32715, dsc 0.67285\n",
      "Batch train [1] loss 0.40307, dsc 0.59693\n",
      "Batch train [1] loss 0.24185, dsc 0.75815\n",
      "Batch train [1] loss 0.67233, dsc 0.32767\n",
      "Batch train [1] loss 0.38937, dsc 0.61063\n",
      "Batch train [1] loss 0.52089, dsc 0.47911\n",
      "Batch train [1] loss 0.72817, dsc 0.27183\n",
      "Batch train [1] loss 0.63083, dsc 0.36917\n",
      "Batch train [1] loss 0.33383, dsc 0.66617\n",
      "Batch train [1] loss 0.61076, dsc 0.38924\n",
      "Batch train [1] loss 0.68310, dsc 0.31690\n",
      "Batch train [1] loss 0.42153, dsc 0.57847\n",
      "Batch train [1] loss 0.45730, dsc 0.54270\n",
      "Batch train [1] loss 0.83911, dsc 0.16089\n",
      "Batch train [1] loss 0.44552, dsc 0.55448\n",
      "Batch train [1] loss 0.36741, dsc 0.63259\n",
      "Batch train [1] loss 0.43278, dsc 0.56722\n",
      "Batch train [1] loss 0.33957, dsc 0.66043\n",
      "Batch train [1] loss 0.34138, dsc 0.65862\n",
      "Batch train [1] loss 0.89805, dsc 0.10195\n",
      "Batch train [1] loss 0.56800, dsc 0.43200\n",
      "Epoch [79] train done\n",
      "DEBUG: Writing to tensorboard before epoch False, 78, step 0\n",
      "DEBUG: Writing to tensorboard after epoch False,  78, step 0\n",
      "Batch eval [1] loss 0.78355, dsc 0.21645\n",
      "Batch eval [1] loss 0.99993, dsc 0.00007\n",
      "Batch eval [1] loss 1.00000, dsc 0.00000\n",
      "Batch eval [1] loss 0.45516, dsc 0.54484\n",
      "Batch eval [1] loss 0.76316, dsc 0.23684\n",
      "Epoch [79] valid done\n",
      "Epoch [79] T 492.75s, deltaT 123.82s, loss: train 0.48829, valid 0.80036, dsc: train 0.51171, valid 0.19964\n",
      "DEBUG: Writing to tensorboard before epoch True, 79, step 0\n",
      "DEBUG: Writing to tensorboard after epoch True,  79, step 0\n",
      "Batch train [1] loss 0.39210, dsc 0.60790\n",
      "Batch train [1] loss 0.80322, dsc 0.19678\n",
      "Batch train [1] loss 0.58374, dsc 0.41626\n",
      "Batch train [1] loss 0.28366, dsc 0.71634\n",
      "Batch train [1] loss 0.38213, dsc 0.61787\n",
      "Batch train [1] loss 0.29844, dsc 0.70156\n",
      "Batch train [1] loss 0.47069, dsc 0.52931\n",
      "Batch train [1] loss 0.44700, dsc 0.55300\n",
      "Batch train [1] loss 0.40837, dsc 0.59163\n",
      "Batch train [1] loss 0.47063, dsc 0.52937\n",
      "Batch train [1] loss 0.34786, dsc 0.65214\n",
      "Batch train [1] loss 0.34108, dsc 0.65892\n",
      "Batch train [1] loss 0.29292, dsc 0.70708\n",
      "Batch train [1] loss 0.40525, dsc 0.59475\n",
      "Batch train [1] loss 0.32794, dsc 0.67206\n",
      "Batch train [1] loss 0.63579, dsc 0.36421\n",
      "Batch train [1] loss 0.32829, dsc 0.67171\n",
      "Batch train [1] loss 0.60220, dsc 0.39780\n",
      "Batch train [1] loss 0.74229, dsc 0.25771\n",
      "Batch train [1] loss 0.52540, dsc 0.47460\n",
      "Batch train [1] loss 0.52201, dsc 0.47799\n",
      "Batch train [1] loss 0.65157, dsc 0.34843\n",
      "Batch train [1] loss 0.26146, dsc 0.73854\n",
      "Batch train [1] loss 0.78358, dsc 0.21642\n",
      "Batch train [1] loss 0.44867, dsc 0.55133\n",
      "Batch train [1] loss 0.45259, dsc 0.54741\n",
      "Batch train [1] loss 0.64599, dsc 0.35401\n",
      "Batch train [1] loss 0.84769, dsc 0.15231\n",
      "Batch train [1] loss 0.54158, dsc 0.45842\n",
      "Batch train [1] loss 0.67148, dsc 0.32852\n",
      "Batch train [1] loss 0.49169, dsc 0.50831\n",
      "Batch train [1] loss 0.46468, dsc 0.53532\n",
      "Batch train [1] loss 0.36170, dsc 0.63830\n",
      "Batch train [1] loss 0.29759, dsc 0.70241\n",
      "Batch train [1] loss 0.37958, dsc 0.62042\n",
      "Batch train [1] loss 0.35086, dsc 0.64914\n",
      "Batch train [1] loss 0.22317, dsc 0.77683\n",
      "Batch train [1] loss 0.61565, dsc 0.38435\n",
      "Batch train [1] loss 0.46046, dsc 0.53954\n",
      "Batch train [1] loss 0.86843, dsc 0.13157\n",
      "Epoch [80] train done\n",
      "DEBUG: Writing to tensorboard before epoch False, 79, step 0\n",
      "DEBUG: Writing to tensorboard after epoch False,  79, step 0\n",
      "Batch eval [1] loss 0.74171, dsc 0.25829\n",
      "Batch eval [1] loss 0.99887, dsc 0.00113\n",
      "Batch eval [1] loss 0.99999, dsc 0.00001\n",
      "Batch eval [1] loss 0.38401, dsc 0.61599\n",
      "Batch eval [1] loss 0.79821, dsc 0.20179\n",
      "Epoch [80] valid done\n",
      "Epoch [80] T 615.18s, deltaT 122.42s, loss: train 0.48574, valid 0.78456, dsc: train 0.51426, valid 0.21544\n",
      "Elapsed time 0:10:15\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RETRAIN_MODELS = True\n",
    "if RETRAIN_MODELS:\n",
    "    for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "        # loading dataset\n",
    "        data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "        cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "        cut_full_res_dataset.load_from_file(data_path)\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        \n",
    "        cut_full_res_dataset_obj = copy_split_dataset(cut_full_res_dataset, example_cut_dataset_obj)\n",
    "        cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "\n",
    "        # preparing model name\n",
    "        log_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        model_name = f'{log_date}_3d_unet_lowres_model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg'\n",
    "\n",
    "        print(f'Training model with dataset label \\'{OAR_KEY}\\', value \\'{OAR_VALUE}\\'')\n",
    "        print(f'folder \\'{model_name}\\'')\n",
    "        \n",
    "        # getting possible models\n",
    "        possible_models = get_possible_models(f\"model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg\")\n",
    "        if len(possible_models) <= 0:\n",
    "            print(f'{OAR_KEY} Model: No avaiable model')\n",
    "            continue\n",
    "\n",
    "        model_name = possible_models[0]\n",
    "        print(f'Model: Loading model {model_name}')\n",
    "\n",
    "        # loading model checkpoint\n",
    "        epoch = 75\n",
    "        cut_model_info = load_checkpoint_model_info(model_name, epoch, cut_train_dataset, cut_valid_dataset, cut_test_dataset, model_class=UNetV3v2)\n",
    "        cut_model_info['epochs'] = 125\n",
    "\n",
    "        # train_batch_size = 1\n",
    "        # train_dataloader, valid_dataloader, test_dataloader = get_loaders(train_batch_size, cut_train_dataset, cut_valid_dataset, cut_test_dataset)\n",
    "        # cut_model_info[\"train_dataloader\"] = train_dataloader\n",
    "        # cut_model_info[\"valid_dataloader\"] = valid_dataloader\n",
    "        # cut_model_info[\"test_dataloader\"] = test_dataloader\n",
    "        \n",
    "        show_model_info(cut_model_info)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        # training\n",
    "        train_loop(cut_model_info, iterate_model_fn=iterate_model_v3v2, start_epoch=epoch)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # clearing memory\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_parotid, PAROTID_GLAND_R, 13\n",
      "right_parotid, PAROTID_GLAND_L, 12\n",
      "brainstem, PITUITARY, 11\n",
      "parotids, PITUITARY, 11\n",
      "all_maps, PITUITARY, 11\n"
     ]
    }
   ],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels['SPINAL_CORD']\n",
    "\n",
    "tmp_list = list(filter_labels.items())\n",
    "labels_list = list()\n",
    "\n",
    "# creating registration dataset and organ segmentation pairs\n",
    "# dataset use inverted labeling of left and right\n",
    "labels_list.append(('left_parotid', *tmp_list[12]))\n",
    "labels_list.append(('right_parotid', *tmp_list[11]))\n",
    "\n",
    "labels_list.append(('brainstem', *tmp_list[10]))\n",
    "labels_list.append(('parotids', *tmp_list[10]))\n",
    "labels_list.append(('all_maps', *tmp_list[10]))\n",
    "\n",
    "for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "    print(f\"{DATASET_REG_NAME}, {OAR_KEY}, {OAR_VALUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading models to CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAROTID_GLAND_R Model: No avaiable model\n",
      "PAROTID_GLAND_L-right_parotid Model: Loading model 20210417-110036_3d_unet_lowres_model3v2__cloud-PAROTID_GLAND_L-right_parotid_reg\n",
      "PITUITARY-brainstem Model: Loading model 20210417-183504_3d_unet_lowres_model3v2__cloud-PITUITARY-brainstem_reg\n",
      "PITUITARY-parotids Model: Loading model 20210418-020905_3d_unet_lowres_model3v2__cloud-PITUITARY-parotids_reg\n",
      "PITUITARY-all_maps Model: Loading model 20210418-100654_3d_unet_lowres_model3v2__cloud-PITUITARY-all_maps_reg\n"
     ]
    }
   ],
   "source": [
    "models = dict()\n",
    "for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "    model_reg_name = f'{OAR_KEY}-{DATASET_REG_NAME}'\n",
    "    \n",
    "    # dataset loading\n",
    "    data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "    cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "    cut_full_res_dataset.load_from_file(data_path)\n",
    "    cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "    \n",
    "    cut_full_res_dataset_obj = copy_split_dataset(cut_full_res_dataset, example_cut_dataset_obj)\n",
    "    cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "    \n",
    "    # cut_full_res_dataset_obj = split_dataset(cut_full_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "    # cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "\n",
    "    possible_models = get_possible_models(f\"model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg\")\n",
    "    if len(possible_models) <= 0:\n",
    "        print(f'{OAR_KEY} Model: No avaiable model')\n",
    "        continue\n",
    "\n",
    "    model_name = possible_models[0]\n",
    "    print(f'{model_reg_name} Model: Loading model {model_name}')\n",
    "\n",
    "    # loading model checkpoint\n",
    "    epoch = 75\n",
    "    cut_model_info = load_checkpoint_model_info(model_name, epoch, cut_train_dataset, cut_valid_dataset, cut_test_dataset, model_class=UNetV3v2)\n",
    "\n",
    "    # moving model to cpu/cuda with eval mode\n",
    "    cut_model_info['device'] = 'cpu'\n",
    "    cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "    cut_model_info['model'].eval()\n",
    "    cut_model_info['model'].disable_tensorboard_writing = True\n",
    "    \n",
    "    models[model_reg_name] = cut_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['PAROTID_GLAND_L-right_parotid', 'PITUITARY-brainstem', 'PITUITARY-parotids', 'PITUITARY-all_maps'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Eval vs Train Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(train_dataloader):\n",
    "#     if i == 0:\n",
    "#         print(data[0][0][0].shape)\n",
    "#         preview_3d_image(data[0][0][0], figsize=(4, 4))\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch eval [2] loss 0.65127, dsc 0.34873\n",
      "Batch eval [2] loss 0.69104, dsc 0.30896\n",
      "Batch eval [2] loss 0.77400, dsc 0.22600\n",
      "Batch eval [2] loss 0.87350, dsc 0.12650\n",
      "Batch eval [2] loss 0.62450, dsc 0.37550\n",
      "Batch eval [2] loss 0.78698, dsc 0.21302\n",
      "Batch eval [2] loss 0.54430, dsc 0.45570\n",
      "Batch eval [2] loss 0.82854, dsc 0.17146\n",
      "Batch eval [2] loss 0.80717, dsc 0.19283\n",
      "Batch eval [2] loss 0.86497, dsc 0.13503\n",
      "Batch eval [2] loss 0.72086, dsc 0.27914\n",
      "Batch eval [2] loss 0.72491, dsc 0.27509\n",
      "Batch eval [2] loss 0.67294, dsc 0.32706\n",
      "Batch eval [2] loss 0.81407, dsc 0.18593\n",
      "Batch eval [2] loss 0.72586, dsc 0.27414\n",
      "Batch eval [2] loss 0.81476, dsc 0.18524\n",
      "Batch eval [2] loss 0.75063, dsc 0.24937\n",
      "Batch eval [2] loss 0.59830, dsc 0.40170\n",
      "Batch eval [2] loss 0.81488, dsc 0.18512\n",
      "Batch eval [2] loss 0.59100, dsc 0.40900\n",
      "0.7337235987186432 0.26627640426158905\n"
     ]
    }
   ],
   "source": [
    "cut_full_res_dataset.set_output_label(OARS_LABELS.PITUITARY)\n",
    "cut_model_info = models['PITUITARY-all_maps'] # list(models.keys())\n",
    "cut_model_info['device'] = get_device()\n",
    "cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "cut_model_info['model'].disable_tensorboard_writing = True\n",
    "    \n",
    "model, model_name, optimizer, criterion = itemgetter('model', 'model_name', 'optimizer', 'criterion')(cut_model_info)\n",
    "epochs, device, tensorboard_writer = itemgetter('epochs', 'device', 'tensorboard_writer')(cut_model_info)\n",
    "train_dataloader, valid_dataloader, test_dataloader = itemgetter('train_dataloader',\n",
    "                                                                 'valid_dataloader',\n",
    "                                                                 'test_dataloader')(cut_model_info)\n",
    "model.actual_epoch = 100\n",
    "\n",
    "valid_loss, valid_dsc = iterate_model_v3v2(train_dataloader, model, optimizer, criterion, device, is_eval=True)\n",
    "print(valid_loss, valid_dsc)\n",
    "\n",
    "cut_model_info['model'].disable_tensorboard_writing = True\n",
    "cut_model_info['device'] = 'cpu'\n",
    "cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = get_rescaled_pred(cut_model_info['model'], cut_full_res_dataset, 'cpu', 0, transform_input_fn=transform_input_with_registration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAROTID_GLAND_R-left_parotid Model: No avaiable model\n",
      "PAROTID_GLAND_L-right_parotid Model: DSC train 0.8686 valid 0.8447\n",
      "PITUITARY-brainstem Model: DSC train 0.3757 valid 0.3703\n",
      "PITUITARY-parotids Model: DSC train 0.5009 valid 0.5368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-766460e81a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# calculating dsc predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         info_df, preds, rescaled_preds = get_threshold_info_df(\n\u001b[0m\u001b[1;32m     33\u001b[0m                                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcut_model_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcut_full_res_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fiit-dp-thesis-code/src/helpers/threshold_helpers.py\u001b[0m in \u001b[0;36mget_threshold_info_df\u001b[0;34m(model, dataset, device, train_indices, valid_indices, test_indices, step, transform_input_fn)\u001b[0m\n\u001b[1;32m     55\u001b[0m                           transform_input_fn=transform_input):\n\u001b[1;32m     56\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get_threshold_info_df0 calculating all predictions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     preds, rescaled_preds = get_rescaled_preds(model, dataset, device,\n\u001b[0m\u001b[1;32m     58\u001b[0m                                                transform_input_fn=transform_input_fn)\n\u001b[1;32m     59\u001b[0m     \u001b[0minfo_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fiit-dp-thesis-code/src/helpers/prediction_helpers.py\u001b[0m in \u001b[0;36mget_rescaled_preds\u001b[0;34m(model, dataset, device, transform_input_fn)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mrescaled_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         prediction, rescaled_pred = get_rescaled_pred(model, dataset, device, index,\n\u001b[0m\u001b[1;32m     38\u001b[0m                                                       transform_input_fn=transform_input_fn)\n\u001b[1;32m     39\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fiit-dp-thesis-code/src/helpers/prediction_helpers.py\u001b[0m in \u001b[0;36mget_rescaled_pred\u001b[0;34m(model, dataset, device, index, use_only_one_dimension, transform_input_fn)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdata_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnorm_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_only_one_dimension\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnorm_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdata_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# data_input.shape => batch, channel, slices, x, y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SHOW_DSC_INFO = True\n",
    "if SHOW_DSC_INFO:\n",
    "    info_per_organs_df = {}\n",
    "    models_info = list()\n",
    "    for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "        model_name = f'{OAR_KEY}-{DATASET_REG_NAME}'\n",
    "        \n",
    "        if model_name not in models:\n",
    "            print(f'{model_name} Model: No avaiable model')\n",
    "            continue\n",
    "\n",
    "        # getting model to gpu\n",
    "        cut_model_info = models[model_name]\n",
    "        cut_model_info['device'] = get_device()\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "        cut_model_info['model'].eval()\n",
    "        cut_model_info['model'].disable_tensorboard_writing = True\n",
    "\n",
    "        # preparing dataset for comparison\n",
    "        # dataset loading\n",
    "        data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "        cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "        cut_full_res_dataset.load_from_file(data_path)\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        \n",
    "        cut_full_res_dataset_obj = copy_split_dataset(cut_full_res_dataset, example_cut_dataset_obj)\n",
    "        cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "        # preview_3d_image(cut_train_dataset[0][0][0], figsize=(5, 5))\n",
    "        # preview_3d_image(cut_train_dataset[0][0][0], figsize=(5, 5))\n",
    "        \n",
    "        # calculating dsc predictions        \n",
    "        info_df, preds, rescaled_preds = get_threshold_info_df(\n",
    "                                    model=cut_model_info['model'], \n",
    "                                    dataset=cut_full_res_dataset, \n",
    "                                    device=cut_model_info['device'], \n",
    "                                    train_indices=cut_train_dataset.indices, \n",
    "                                    valid_indices=cut_valid_dataset.indices, \n",
    "                                    test_indices=cut_test_dataset.indices,\n",
    "                                    step=0.5,\n",
    "                                    transform_input_fn=transform_input_with_registration)\n",
    "        info_per_organs_df[model_name] = info_df\n",
    "\n",
    "        # moving model back to cpu\n",
    "        cut_model_info['device'] = 'cpu'\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "\n",
    "        # parsing data\n",
    "        best_threshold_col = 'thres_rescaled_dsc_0.50'\n",
    "        train_tmp_df = info_df[info_df['is_train']][best_threshold_col]\n",
    "        valid_tmp_df = info_df[info_df['is_valid']][best_threshold_col]\n",
    "        train_dsc = train_tmp_df.mean()\n",
    "        valid_dsc = valid_tmp_df.mean()\n",
    "        print(f'{model_name} Model: DSC train {round(train_dsc, 4)} valid {round(valid_dsc, 4)}')\n",
    "\n",
    "        models_info.append({\n",
    "            'oar_key': OAR_KEY,\n",
    "            'model_name': model_name,\n",
    "            # Train\n",
    "            'train_dsc_mean': train_dsc,\n",
    "            'train_dsc_std': train_tmp_df.std(),\n",
    "            'train_dsc_median': train_tmp_df.median(),\n",
    "            'train_dsc_min': train_tmp_df.min(),\n",
    "            'train_dsc_max': train_tmp_df.max(),\n",
    "            # Valid\n",
    "            'valid_dsc_mean': valid_dsc,\n",
    "            'valid_dsc_std': valid_tmp_df.std(),\n",
    "            'valid_dsc_median': valid_tmp_df.median(),\n",
    "            'valid_dsc_min': valid_tmp_df.min(),\n",
    "            'valid_dsc_max': valid_tmp_df.max(),\n",
    "            # Both\n",
    "            'train_valid_mean_delta': train_dsc - valid_dsc\n",
    "        })\n",
    "\n",
    "    models_info_df = pd.DataFrame(models_info)\n",
    "    \n",
    "    tmp_df = models_info_df[['model_name', 'train_dsc_mean', 'train_dsc_std', 'valid_dsc_mean', 'valid_dsc_std']].copy()\n",
    "    tmp_df['train_dsc_mean'] = (tmp_df['train_dsc_mean'] * 100).round(2)\n",
    "    tmp_df['valid_dsc_mean'] = (tmp_df['valid_dsc_mean'] * 100).round(2)\n",
    "    tmp_df['train_dsc_std'] = (tmp_df['train_dsc_std'] * 100).round(2)\n",
    "    tmp_df['valid_dsc_std'] = (tmp_df['valid_dsc_std'] * 100).round(2)\n",
    "    \n",
    "    display(tmp_df.mean().round(2))\n",
    "    display(tmp_df.round(2))\n",
    "    display(tmp_df.sort_values(by=['train_dsc_std']).round(2))\n",
    "    display(models_info_df.sort_values(by=['train_dsc_mean']).round(2))\n",
    "    display(models_info_df.sort_values(by=['train_valid_mean_delta']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_DSC_INFO:\n",
    "    tmp_column = 'is_train'\n",
    "    \n",
    "    try:\n",
    "        print('OARS_LABELS.PAROTID_GLAND_R')\n",
    "        tmp_df = info_per_organs_df['PAROTID_GLAND_L-right_parotid']\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:   \n",
    "        print('OARS_LABELS.PAROTID_GLAND_L')\n",
    "        tmp_df = info_per_organs_df['PITUITARY-brainstem Model']\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try: \n",
    "        print('OARS_LABELS.OPT_NERVE_L')\n",
    "        tmp_df = info_per_organs_df['PITUITARY-parotids']\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try: \n",
    "        print('OARS_LABELS.PITUITARY')\n",
    "        tmp_df = info_per_organs_df['PITUITARY-all_maps']\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions merging and checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels_dict = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels_dict['SPINAL_CORD']\n",
    "\n",
    "cut_full_res_dataset.set_output_label(filter_labels_dict)\n",
    "preview_dataset(cut_full_res_dataset, preview_index=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSE_CUT_DATASET = True\n",
    "if PARSE_CUT_DATASET:\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    prediction_threshold = 0.5\n",
    "    cut_dataset_predictions = defaultdict(lambda: defaultdict(lambda: np.zeros(cut_full_res_dataset[0][0][0].shape)))\n",
    "    \n",
    "    # for each label\n",
    "    for label_index, val in enumerate(labels_list[:]):\n",
    "        DATASET_REG_NAME, OAR_KEY, OAR_VALUE = val\n",
    "        model_name = f'{OAR_KEY}-{DATASET_REG_NAME}'\n",
    "        \n",
    "        # loading model\n",
    "        if model_name not in models:\n",
    "            print(f'{label_index+1}/{len(labels_list)}: {model_name} Model: No avaiable model')\n",
    "            continue\n",
    "        print(f'{label_index+1}/{len(labels_list)}: {model_name} Model: got model {datetime.datetime.now()}')\n",
    "\n",
    "        # getting model to gpu\n",
    "        cut_model_info = models[model_name]\n",
    "        cut_model_info['device'] = get_device()\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "        cut_model_info['model'].eval()\n",
    "        cut_model_info['model'].disable_tensorboard_writing = True\n",
    "\n",
    "        # for label in whole dataset\n",
    "        for index in range(len(cut_full_res_dataset)):\n",
    "            prediction, rescaled_pred = get_rescaled_pred(cut_model_info['model'], cut_full_res_dataset, \n",
    "                                                          cut_model_info['device'], index, use_only_one_dimension=False)\n",
    "    \n",
    "            cut_dataset_predictions[index][OAR_VALUE] = prediction[0]\n",
    "            # extended_cut_full_res_dataset.data_list[index][label_index + 1] = prediction\n",
    "            # extended_cut_full_res_dataset.data_list[index][label_index + 1] = ((rescaled_pred > prediction_threshold) * 1).astype(np.int8)\n",
    "\n",
    "        # moving model back to cpu\n",
    "        cut_model_info['device'] = 'cpu'\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels_dict = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels_dict['SPINAL_CORD']\n",
    "\n",
    "cut_full_res_dataset.set_output_label(filter_labels_dict)\n",
    "preview_dataset(cut_full_res_dataset, preview_index=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARSE_CUT_DATASET:\n",
    "    def custom_preview_dataset(dataset, predictions, preview_index=0, show_hist=False, use_transform=False):\n",
    "        data, label = dataset.get_raw_item_with_label_filter(preview_index)  # equivalent dataset[preview_index]\n",
    "        if use_transform:\n",
    "            transform = get_dataset_transform()\n",
    "            data, label = transform_input(data, label, transform)\n",
    "\n",
    "        prediction = predictions[preview_index]\n",
    "        max_channels = label.shape[0]\n",
    "        max_slices = label.shape[1]\n",
    "\n",
    "        print(f'data max {data.max()}, min {data.min()}')\n",
    "        print(f'label max {label.max()}, min {label.min()}')\n",
    "        print(f'data {data.shape}, label {label.shape}')\n",
    "\n",
    "        def f(slice_index, label_channel):\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(data[0, slice_index], cmap=\"gray\")\n",
    "            plt.title('data')\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(prediction[label_channel+1][slice_index])\n",
    "            plt.title(f'prediction {label_channel}')\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(label[label_channel, slice_index])\n",
    "            plt.title(f'label {label_channel}')\n",
    "            plt.show()\n",
    "\n",
    "            if show_hist:\n",
    "                plt.figure(figsize=(20, 10))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.hist(data.flatten(), 128)\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.hist(label.flatten(), 128)\n",
    "                plt.show()\n",
    "\n",
    "        sliceSlider = widgets.IntSlider(min=0, max=max_slices - 1, step=1, value=(max_slices - 1) / 2)\n",
    "        labelChannelSlider = widgets.IntSlider(min=0, max=max_channels - 1, step=1, value=(max_channels - 1) / 2)\n",
    "        ui = widgets.VBox([widgets.HBox([sliceSlider, labelChannelSlider])])\n",
    "        out = widgets.interactive_output(f, {'slice_index': sliceSlider, 'label_channel': labelChannelSlider})\n",
    "        # noinspection PyTypeChecker\n",
    "        display(ui, out)\n",
    "    \n",
    "    index = cut_valid_dataset.indices[3]\n",
    "    index = 35\n",
    "    custom_preview_dataset(cut_full_res_dataset, cut_dataset_predictions, preview_index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
