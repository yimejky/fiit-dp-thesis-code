{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n",
      "Dataset biggest bounding box wihtout spinal cord [56, 177, 156]\n",
      "Cut target size [72, 192, 168]\n",
      "Done Init\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/dp_tomastik/code')\n",
    "    !bash \"/content/drive/My Drive/dp_tomastik/code/scripts/install_libs.sh\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "from torchio import RandomAffine, Compose, ZNormalization\n",
    "\n",
    "import src.dataset.oars_labels_consts as OARS_LABELS\n",
    "from src.consts import DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "from src.helpers.threshold_calc_helpers import get_threshold_info_df\n",
    "from src.helpers.show_model_dataset_pred_preview import show_model_dataset_pred_preview\n",
    "from src.dataset.get_cut_lists import get_cut_lists\n",
    "from src.dataset.get_full_res_cut import get_full_res_cut\n",
    "from src.dataset.get_dataset import get_dataset\n",
    "from src.dataset.get_dataset_info import get_dataset_info\n",
    "from src.dataset.preview_dataset import preview_dataset\n",
    "from src.dataset.get_dataset_transform import get_dataset_transform\n",
    "from src.model_and_training.prepare_model import prepare_model\n",
    "from src.model_and_training.train_loop import train_loop\n",
    "from src.model_and_training.show_model_info import show_model_info\n",
    "from src.model_and_training.load_checkpoint_model_info import load_checkpoint_model_info\n",
    "from src.helpers.show_cuda_usage import show_cuda_usage\n",
    "from src.helpers.get_rescaled_pred import get_rescaled_preds\n",
    "from src.dataset.split_dataset import split_dataset, copy_split_dataset\n",
    "from src.helpers.compare_prediction_with_ground_true import compare_prediction_with_ground_true, compare_one_prediction_with_ground_true\n",
    "from src.helpers.get_img_outliers_pixels import get_img_outliers_pixels\n",
    "from src.helpers.get_raw_with_prediction import get_raw_with_prediction\n",
    "from src.model_and_training.getters.get_device import get_device\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "torch.manual_seed(20)\n",
    "logging.basicConfig(filename='logs/pdd_data_check.log', level=logging.DEBUG)\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_REGISTRATION=False\n",
    "TRANSFORM_REGISTRATION=False\n",
    "DISPLAY_REGISTRATION=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDDCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_3d_image(img):\n",
    "    if type(img) is sitk.SimpleITK.Image:\n",
    "        img = sitk.GetArrayFromImage(img)\n",
    "\n",
    "    max_slices = img.shape[0]\n",
    "    def f(slice_index):    \n",
    "        plt.figure(figsize=(16, 16))\n",
    "        plt.imshow(img[slice_index])\n",
    "        plt.show()\n",
    "        print(f\"debug: {img.min()}, {img.max()}\")\n",
    "        print(f\"debug: unique {np.unique(img[slice_index])}\")\n",
    "\n",
    "    sliceSlider = widgets.IntSlider(min=0, max=max_slices - 1, step=1, value=(max_slices - 1) / 2)\n",
    "    ui = widgets.VBox([widgets.HBox([sliceSlider])])\n",
    "    out = widgets.interactive_output(f, {'slice_index': sliceSlider})\n",
    "    # noinspection PyTypeChecker\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import SimpleITK as sitk\n",
    "import nrrd\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "LOAD_PDDCA = False\n",
    "if LOAD_PDDCA:\n",
    "    # PDDCA\n",
    "    d =\"./data/PDDCA-1.4.1\"\n",
    "    pddca_dir_items = sorted([o for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))])\n",
    "\n",
    "    ignore_items = ['0522c0014', '0522c0077', '0522c0079', '0522c0147', '0522c0159', '0522c0161', '0522c0190', '0522c0226', \n",
    "                    '0522c0329', '0522c0330', '0522c0427', '0522c0433', '0522c0441', '0522c0455', '0522c0457', '0522c0479']\n",
    "    print(f'Loading {len(pddca_dir_items) - len(ignore_items)} items')\n",
    "\n",
    "    pddca_items = list() \n",
    "    for item_id in pddca_dir_items:\n",
    "        if item_id in ignore_items:\n",
    "            # print(f\"pddca {item_id}: ignoring\")\n",
    "            continue\n",
    "\n",
    "        # parsing data\n",
    "        data_filepath = Path.joinpath(Path(d), f'./{item_id}/img.nrrd')\n",
    "        pddca_data, header = nrrd.read(data_filepath)\n",
    "        pddca_data = pddca_data.astype(np.int16)\n",
    "        pddca_data = np.transpose(pddca_data, axes=[2, 0, 1]).swapaxes(-2,-1)[...,::-1]\n",
    "\n",
    "        # parsing labels\n",
    "        oar_labels = [\"BrainStem\", \"Chiasm\", \"Mandible\", \"OpticNerve_L\", \"OpticNerve_R\", \"Parotid_L\", \"Parotid_R\", \"Submandibular_L\", \"Submandibular_R\"]\n",
    "        pddca_label = np.zeros(pddca_data.shape, dtype=np.int8)\n",
    "\n",
    "        for OAR_INDEX, OAR_KEY in enumerate(oar_labels):\n",
    "            label_filepath = Path.joinpath(Path(d), f'./{item_id}/structures/{OAR_KEY}.nrrd')\n",
    "            oar_pddca_label, header = nrrd.read(label_filepath)\n",
    "            oar_pddca_label = oar_pddca_label.astype(np.int8)\n",
    "            oar_pddca_label = np.transpose(oar_pddca_label, axes=[2, 0, 1]).swapaxes(-2,-1)[...,::-1]\n",
    "            pddca_label += oar_pddca_label*(OAR_INDEX+1)\n",
    "\n",
    "        # appending\n",
    "        pddca_items.append((pddca_data, pddca_label))\n",
    "        print(f\"pddca {item_id}: {pddca_data.max()}, {pddca_data.min()}, {pddca_label.max()}, {pddca_label.min()}, {pddca_data.dtype}, {pddca_label.dtype}, {pddca_data.shape}, {pddca_label.shape}\")\n",
    "\n",
    "    print('Done loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PDDCA = False\n",
    "if LOAD_PDDCA:\n",
    "    item_index = 1\n",
    "    pddca_data, pddca_label = pddca_items[item_index]\n",
    "\n",
    "    max_slices = pddca_data.shape[0]\n",
    "    def f(slice_index):    \n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.imshow(pddca_data[slice_index], cmap=\"gray\")\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.imshow(pddca_label[slice_index])\n",
    "        plt.subplot(2, 2, 3)\n",
    "\n",
    "        tmp_combine = np.stack((pddca_data[slice_index],) * 3, axis=-1)\n",
    "        tmp_combine -= tmp_combine.min()\n",
    "        tmp_combine = tmp_combine / tmp_combine.max()    \n",
    "        tmp = (pddca_label[slice_index] > 1) * 1\n",
    "        tmp_cond = tmp > 0\n",
    "        tmp_combine[tmp_cond, 0] = tmp[tmp_cond]\n",
    "\n",
    "        plt.imshow(tmp_combine)\n",
    "        plt.show()\n",
    "        print(f\"debug: {pddca_data.min()}, {pddca_data.max()}\")\n",
    "        print(f\"debug: {tmp_combine.min()}, {tmp_combine.max()}\")\n",
    "        print(f\"debug: unique {np.unique(pddca_label[slice_index])}\")\n",
    "\n",
    "    sliceSlider = widgets.IntSlider(min=0, max=max_slices - 1, step=1, value=(max_slices - 1) / 2)\n",
    "    ui = widgets.VBox([widgets.HBox([sliceSlider])])\n",
    "    out = widgets.interactive_output(f, {'slice_index': sliceSlider})\n",
    "    # noinspection PyTypeChecker\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRUCT SEG 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 1x dataset\n",
      "filtering labels\n",
      "filtering labels done\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.dataset.han_oars_dataset.HaNOarsDataset at 0x7fc5dfc1a850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_LIST\n",
    "if OARS_LABELS.SPINAL_CORD in filter_labels:\n",
    "    filter_labels.remove(OARS_LABELS.SPINAL_CORD)\n",
    "\n",
    "full_res_dataset = get_dataset(dataset_size=50, shrink_factor=1, filter_labels=filter_labels, unify_labels=False)\n",
    "full_res_dataset.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_registration_transform_sitk(fixed, moving, show=True):\n",
    "    \"\"\"https://simpleitk.readthedocs.io/en/master/link_ImageRegistrationMethod3_docs.html\"\"\"\n",
    "    def command_iteration(method):\n",
    "        if (method.GetOptimizerIteration() == 0):\n",
    "            print(\"Estimated Scales: \", method.GetOptimizerScales())\n",
    "        print(\"{0:3} = {1:7.5f} : {2}\".format(method.GetOptimizerIteration(),\n",
    "                                              method.GetMetricValue(),\n",
    "                                              method.GetOptimizerPosition()))\n",
    "\n",
    "    R = sitk.ImageRegistrationMethod()\n",
    "    R.SetMetricAsCorrelation()\n",
    "    R.SetOptimizerAsRegularStepGradientDescent(learningRate=2.0,\n",
    "                                               minStep=1e-4,\n",
    "                                               numberOfIterations=500,\n",
    "                                               gradientMagnitudeTolerance=1e-6)\n",
    "    R.SetOptimizerScalesFromIndexShift()\n",
    "    tx = sitk.CenteredTransformInitializer(fixed, moving, sitk.Similarity3DTransform())\n",
    "    R.SetInitialTransform(tx)\n",
    "    R.SetInterpolator(sitk.sitkLinear)\n",
    "    R.AddCommand(sitk.sitkIterationEvent, lambda: command_iteration(R))\n",
    "\n",
    "    output_transform = R.Execute(fixed, moving)\n",
    "\n",
    "    print(\"-------\")\n",
    "    print(output_transform)\n",
    "    print(\"Optimizer stop condition: {0}\".format(R.GetOptimizerStopConditionDescription()))\n",
    "    print(\" Iteration: {0}\".format(R.GetOptimizerIteration()))\n",
    "    print(\" Metric value: {0}\".format(R.GetMetricValue()))\n",
    "\n",
    "    if show:\n",
    "        resampler = sitk.ResampleImageFilter()\n",
    "        resampler.SetReferenceImage(fixed)\n",
    "        resampler.SetInterpolator(sitk.sitkLinear)\n",
    "        resampler.SetDefaultPixelValue(1)\n",
    "        resampler.SetTransform(output_transform)\n",
    "\n",
    "        out = resampler.Execute(moving)\n",
    "\n",
    "        simg1 = sitk.Cast(sitk.RescaleIntensity(fixed), sitk.sitkUInt8)\n",
    "        simg2 = sitk.Cast(sitk.RescaleIntensity(out), sitk.sitkUInt8)\n",
    "        cimg = sitk.Compose(simg1, simg2, simg1 // 2. + simg2 // 2.)\n",
    "        preview_3d_image(cimg)\n",
    "\n",
    "    return output_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TRAIN_REGISTRATION:\n",
    "    fixed_data, fixed_label = pddca_items[0]\n",
    "    moving_data, moving_label = full_res_dataset.get_raw_item_with_label_filter(0)\n",
    "\n",
    "    fixed_data = fixed_data.astype(np.float32)\n",
    "    moving_data = moving_data.astype(np.float32)[0]\n",
    "    print(fixed_data.dtype, moving_data.dtype, fixed_data.shape, moving_data.shape)\n",
    "\n",
    "    fixed_sitk = sitk.GetImageFromArray(fixed_data)\n",
    "    moving_sitk = sitk.GetImageFromArray(moving_data)\n",
    "\n",
    "    output_transform = get_registration_transform_sitk(fixed_sitk, moving_sitk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFORM_REGISTRATION:\n",
    "    def transform(image, transform):\n",
    "        ref_image = image\n",
    "        interpolator = sitk.sitkNearestNeighbor\n",
    "        default_value = 0\n",
    "        return sitk.Resample(image, ref_image, transform, interpolator, default_value)\n",
    "\n",
    "    fixed_label_sitk = sitk.GetImageFromArray(fixed_label)\n",
    "    trans_fixed_label = transform(fixed_label_sitk, output_transform.GetInverse())\n",
    "    trans_fixed_label_np = sitk.GetArrayFromImage(trans_fixed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISPLAY_REGISTRATION:\n",
    "    max_slices = trans_fixed_label_np.shape[0]\n",
    "\n",
    "    def f(slice_index):\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.subplot(2, 2, 1).title.set_text(\"Transformed label from atlas\")\n",
    "        plt.imshow(trans_fixed_label_np[slice_index])\n",
    "        plt.subplot(2, 2, 2).title.set_text(\"Dataset label\")\n",
    "        plt.imshow(moving_label[0, slice_index])\n",
    "        plt.show()\n",
    "\n",
    "    sliceSlider = widgets.IntSlider(min=0, max=max_slices - 1, step=1, value=(max_slices - 1) / 2)\n",
    "    ui = widgets.VBox([widgets.HBox([sliceSlider])])\n",
    "    out = widgets.interactive_output(f, {'slice_index': sliceSlider})\n",
    "    # noinspection PyTypeChecker\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlas loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136, 120, 219), (136, 120, 219), (136, 120, 219), (136, 120, 219))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlas_ri = sitk.GetArrayFromImage(sitk.ReadImage('./data/PDDCA-1.2-atlas/probabilistic_atlas/RI.mhd'))\n",
    "atlas_brainstem_map = sitk.GetArrayFromImage(sitk.ReadImage('./data/PDDCA-1.2-atlas/probabilistic_atlas/brain_stem_map.mhd'))\n",
    "atlas_left_parotid_map = sitk.GetArrayFromImage(sitk.ReadImage('./data/PDDCA-1.2-atlas/probabilistic_atlas/left_parotid_map.mhd'))\n",
    "atlas_right_parotid_map = sitk.GetArrayFromImage(sitk.ReadImage('./data/PDDCA-1.2-atlas/probabilistic_atlas/right_parotid_map.mhd'))\n",
    "\n",
    "atlas_ri.shape, atlas_brainstem_map.shape, atlas_left_parotid_map.shape, atlas_right_parotid_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77bc152436e4a07ab6048ddccf83e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=67, max=135),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab5b8fceab94d83b551542f501ebbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_slices = atlas_ri.shape[0]\n",
    "def f(slice_index):\n",
    "    plt.figure(figsize=(30, 16))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(atlas_ri[slice_index], cmap=\"gray\")\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(atlas_brainstem_map[slice_index], cmap=\"gray\")\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(atlas_left_parotid_map[slice_index], cmap=\"gray\")\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(atlas_right_parotid_map[slice_index], cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "sliceSlider = widgets.IntSlider(min=0, max=max_slices - 1, step=1, value=(max_slices - 1) / 2)\n",
    "ui = widgets.VBox([widgets.HBox([sliceSlider])])\n",
    "out = widgets.interactive_output(f, {'slice_index': sliceSlider})\n",
    "# noinspection PyTypeChecker\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: register atlas to NN input, think about speeding up because of data augmentation\n",
    "## TODO: implement architecture CRNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136, 120, 219), [72, 192, 168])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlas_ri.shape, DESIRE_BOUNDING_BOX_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 8x dataset\n",
      "filtering labels\n",
      "filtering labels done\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n",
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "# filter_labels = [OARS_LABELS.EYE_L, OARS_LABELS.EYE_R, OARS_LABELS.LENS_L, OARS_LABELS.LENS_R]\n",
    "filter_labels = OARS_LABELS.OARS_LABELS_LIST\n",
    "if OARS_LABELS.SPINAL_CORD in filter_labels:\n",
    "    filter_labels.remove(OARS_LABELS.SPINAL_CORD)\n",
    "dataset = get_dataset(shrink_factor=8, filter_labels=filter_labels)\n",
    "\n",
    "dataset.to_numpy()\n",
    "split_dataset_obj = split_dataset(dataset)\n",
    "\n",
    "get_dataset_info(dataset, split_dataset_obj)\n",
    "train_dataset, valid_dataset, test_dataset = itemgetter('train_dataset', 'valid_dataset', 'test_dataset')(split_dataset_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 3071, min -1024\n",
      "label max 1, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fe9046537e43069655dcca0ab5d192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159), IntSlider(value=0, max=0))),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68cbdf5cc284a28a592ebd41ce57f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_dataset(dataset, preview_index=2, show_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with dataset MODEL3\n",
      "folder '20210301-162727_3d_unet_model3'\n",
      "DEBUG: Writing to tensorboard epoch 0, step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolas/fiit-dp-thesis-code/src/model_and_training/unet_architecture_v3v1.py:282: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  tmp = ap3.data[0].detach().cpu().numpy()\n",
      "/home/nikolas/fiit-dp-thesis-code/src/model_and_training/unet_architecture_v3v1.py:288: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  tmp = h.data[0].detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Writing to tensorboard epoch 0, step 0\n",
      "DEBUG: Writing to tensorboard epoch 0, step 0\n",
      "Model number of params: 1221604, trainable 1221604\n",
      "\n",
      "\n",
      "\n",
      "Running training loop\n",
      "DEBUG: Writing to tensorboard epoch 0, step 0\n",
      "Batch train [1] loss 0.99353, dsc 0.00647\n",
      "Batch train [2] loss 0.98598, dsc 0.01402\n",
      "Batch train [3] loss 0.97813, dsc 0.02187\n",
      "Batch train [4] loss 0.97462, dsc 0.02538\n",
      "Batch train [5] loss 0.97382, dsc 0.02618\n",
      "Batch train [6] loss 0.95530, dsc 0.04470\n",
      "Batch train [7] loss 0.95183, dsc 0.04817\n",
      "Batch train [8] loss 0.94297, dsc 0.05703\n",
      "Batch train [9] loss 0.92282, dsc 0.07718\n",
      "Batch train [10] loss 0.93229, dsc 0.06771\n",
      "Batch train [11] loss 0.89345, dsc 0.10655\n",
      "Batch train [12] loss 0.84246, dsc 0.15754\n",
      "Batch train [13] loss 0.85142, dsc 0.14858\n",
      "Batch train [14] loss 0.81784, dsc 0.18216\n",
      "Batch train [15] loss 0.80402, dsc 0.19598\n",
      "Batch train [16] loss 0.79422, dsc 0.20578\n",
      "Batch train [17] loss 0.81648, dsc 0.18352\n",
      "Batch train [18] loss 0.73975, dsc 0.26025\n",
      "Batch train [19] loss 0.70168, dsc 0.29832\n",
      "Batch train [20] loss 0.77056, dsc 0.22944\n",
      "Batch train [21] loss 0.71414, dsc 0.28586\n",
      "Batch train [22] loss 0.66064, dsc 0.33936\n",
      "Batch train [23] loss 0.73059, dsc 0.26941\n",
      "Batch train [24] loss 0.71639, dsc 0.28361\n",
      "Batch train [25] loss 0.67633, dsc 0.32367\n",
      "Batch train [26] loss 0.63047, dsc 0.36953\n",
      "Batch train [27] loss 0.73128, dsc 0.26872\n",
      "Batch train [28] loss 0.64474, dsc 0.35526\n",
      "Batch train [29] loss 0.64472, dsc 0.35528\n",
      "Batch train [30] loss 0.60834, dsc 0.39166\n",
      "Batch train [31] loss 0.66946, dsc 0.33054\n",
      "Batch train [32] loss 0.64167, dsc 0.35833\n",
      "Batch train [33] loss 0.68566, dsc 0.31434\n",
      "Batch train [34] loss 0.63073, dsc 0.36927\n",
      "Batch train [35] loss 0.57207, dsc 0.42793\n",
      "Batch train [36] loss 0.60492, dsc 0.39508\n",
      "Batch train [37] loss 0.67752, dsc 0.32248\n",
      "Batch train [38] loss 0.57200, dsc 0.42800\n",
      "Batch train [39] loss 0.58505, dsc 0.41495\n",
      "Batch train [40] loss 0.56098, dsc 0.43902\n",
      "Epoch [1] train done\n",
      "DEBUG: Writing to tensorboard epoch 0, step 0\n",
      "Batch eval [1] loss 0.89956, dsc 0.10044\n",
      "Batch eval [2] loss 0.88196, dsc 0.11804\n",
      "Batch eval [3] loss 0.87357, dsc 0.12643\n",
      "Batch eval [4] loss 0.89510, dsc 0.10490\n",
      "Batch eval [5] loss 0.85937, dsc 0.14063\n",
      "Epoch [1] valid done\n",
      "Epoch [1] T 44.39s, deltaT 44.39s, loss: train 0.76502, valid 0.88191, dsc: train 0.23498, valid 0.11809\n",
      "DEBUG: Writing to tensorboard epoch 1, step 0\n",
      "Batch train [1] loss 0.56152, dsc 0.43848\n",
      "Batch train [2] loss 0.58368, dsc 0.41632\n",
      "Batch train [3] loss 0.56412, dsc 0.43588\n",
      "Batch train [4] loss 0.60133, dsc 0.39867\n",
      "Batch train [5] loss 0.52633, dsc 0.47367\n",
      "Batch train [6] loss 0.56134, dsc 0.43866\n",
      "Batch train [7] loss 0.69165, dsc 0.30835\n",
      "Batch train [8] loss 0.56955, dsc 0.43045\n",
      "Batch train [9] loss 0.59897, dsc 0.40103\n",
      "Batch train [10] loss 0.73938, dsc 0.26062\n",
      "Batch train [11] loss 0.52246, dsc 0.47754\n",
      "Batch train [12] loss 0.51784, dsc 0.48216\n",
      "Batch train [13] loss 0.53920, dsc 0.46080\n",
      "Batch train [14] loss 0.62475, dsc 0.37525\n",
      "Batch train [15] loss 0.51199, dsc 0.48801\n",
      "Batch train [16] loss 0.50538, dsc 0.49462\n",
      "Batch train [17] loss 0.59630, dsc 0.40370\n",
      "Batch train [18] loss 0.59322, dsc 0.40678\n",
      "Batch train [19] loss 0.51063, dsc 0.48937\n",
      "Batch train [20] loss 0.52355, dsc 0.47645\n",
      "Batch train [21] loss 0.57184, dsc 0.42816\n",
      "Batch train [22] loss 0.59746, dsc 0.40254\n",
      "Batch train [23] loss 0.57903, dsc 0.42097\n",
      "Batch train [24] loss 0.55354, dsc 0.44646\n",
      "Batch train [25] loss 0.58150, dsc 0.41850\n",
      "Batch train [26] loss 0.54978, dsc 0.45022\n",
      "Batch train [27] loss 0.51567, dsc 0.48433\n",
      "Batch train [28] loss 0.53834, dsc 0.46166\n",
      "Batch train [29] loss 0.54769, dsc 0.45231\n",
      "Batch train [30] loss 0.47358, dsc 0.52642\n",
      "Batch train [31] loss 0.53801, dsc 0.46199\n",
      "Batch train [32] loss 0.58014, dsc 0.41986\n",
      "Batch train [33] loss 0.52828, dsc 0.47172\n",
      "Batch train [34] loss 0.50909, dsc 0.49091\n",
      "Batch train [35] loss 0.45768, dsc 0.54232\n",
      "Batch train [36] loss 0.49324, dsc 0.50676\n",
      "Batch train [37] loss 0.48983, dsc 0.51017\n",
      "Batch train [38] loss 0.50146, dsc 0.49854\n",
      "Batch train [39] loss 0.55850, dsc 0.44150\n",
      "Batch train [40] loss 0.49733, dsc 0.50267\n",
      "Epoch [2] train done\n",
      "DEBUG: Writing to tensorboard epoch 1, step 0\n",
      "Batch eval [1] loss 0.60556, dsc 0.39444\n",
      "Batch eval [2] loss 0.55898, dsc 0.44102\n",
      "Batch eval [3] loss 0.62465, dsc 0.37535\n",
      "Batch eval [4] loss 0.60694, dsc 0.39306\n",
      "Batch eval [5] loss 0.53255, dsc 0.46745\n",
      "Epoch [2] valid done\n",
      "Epoch [2] T 88.58s, deltaT 44.19s, loss: train 0.55263, valid 0.58574, dsc: train 0.44737, valid 0.41426\n",
      "DEBUG: Writing to tensorboard epoch 2, step 0\n",
      "Batch train [1] loss 0.49172, dsc 0.50828\n",
      "Batch train [2] loss 0.51301, dsc 0.48699\n",
      "Batch train [3] loss 0.56905, dsc 0.43095\n",
      "Batch train [4] loss 0.50600, dsc 0.49400\n",
      "Batch train [5] loss 0.49536, dsc 0.50464\n",
      "Batch train [6] loss 0.47587, dsc 0.52413\n",
      "Batch train [7] loss 0.54413, dsc 0.45587\n",
      "Batch train [8] loss 0.46901, dsc 0.53099\n",
      "Batch train [9] loss 0.49591, dsc 0.50409\n",
      "Batch train [10] loss 0.51487, dsc 0.48513\n",
      "Batch train [11] loss 0.59917, dsc 0.40083\n",
      "Batch train [12] loss 0.60262, dsc 0.39738\n",
      "Batch train [13] loss 0.48046, dsc 0.51954\n",
      "Batch train [14] loss 0.57669, dsc 0.42331\n",
      "Batch train [15] loss 0.44677, dsc 0.55323\n",
      "Batch train [16] loss 0.50052, dsc 0.49948\n",
      "Batch train [17] loss 0.56004, dsc 0.43996\n",
      "Batch train [18] loss 0.54267, dsc 0.45733\n",
      "Batch train [19] loss 0.48830, dsc 0.51170\n",
      "Batch train [20] loss 0.43855, dsc 0.56145\n",
      "Batch train [21] loss 0.44856, dsc 0.55144\n",
      "Batch train [22] loss 0.43961, dsc 0.56039\n",
      "Batch train [23] loss 0.46652, dsc 0.53348\n",
      "Batch train [24] loss 0.47838, dsc 0.52162\n",
      "Batch train [25] loss 0.44528, dsc 0.55472\n",
      "Batch train [26] loss 0.53333, dsc 0.46667\n",
      "Batch train [27] loss 0.47331, dsc 0.52669\n",
      "Batch train [28] loss 0.53531, dsc 0.46469\n",
      "Batch train [29] loss 0.48403, dsc 0.51597\n",
      "Batch train [30] loss 0.42957, dsc 0.57043\n",
      "Batch train [31] loss 0.47484, dsc 0.52516\n",
      "Batch train [32] loss 0.48022, dsc 0.51978\n",
      "Batch train [33] loss 0.48051, dsc 0.51949\n",
      "Batch train [34] loss 0.47407, dsc 0.52593\n",
      "Batch train [35] loss 0.44234, dsc 0.55766\n",
      "Batch train [36] loss 0.47423, dsc 0.52577\n",
      "Batch train [37] loss 0.49544, dsc 0.50456\n",
      "Batch train [38] loss 0.47876, dsc 0.52124\n",
      "Batch train [39] loss 0.47074, dsc 0.52926\n",
      "Batch train [40] loss 0.41932, dsc 0.58068\n",
      "Epoch [3] train done\n",
      "DEBUG: Writing to tensorboard epoch 2, step 0\n",
      "Batch eval [1] loss 0.55139, dsc 0.44861\n",
      "Batch eval [2] loss 0.50961, dsc 0.49039\n",
      "Batch eval [3] loss 0.55152, dsc 0.44848\n",
      "Batch eval [4] loss 0.50561, dsc 0.49439\n",
      "Batch eval [5] loss 0.46318, dsc 0.53682\n",
      "Epoch [3] valid done\n",
      "Epoch [3] T 132.93s, deltaT 44.34s, loss: train 0.49338, valid 0.51626, dsc: train 0.50662, valid 0.48374\n",
      "DEBUG: Writing to tensorboard epoch 3, step 0\n",
      "Batch train [1] loss 0.42694, dsc 0.57306\n",
      "Batch train [2] loss 0.43982, dsc 0.56018\n",
      "Batch train [3] loss 0.52978, dsc 0.47022\n",
      "Batch train [4] loss 0.43823, dsc 0.56177\n",
      "Batch train [5] loss 0.47272, dsc 0.52728\n",
      "Batch train [6] loss 0.46726, dsc 0.53274\n",
      "Batch train [7] loss 0.44313, dsc 0.55687\n",
      "Batch train [8] loss 0.42646, dsc 0.57354\n",
      "Batch train [9] loss 0.49218, dsc 0.50782\n",
      "Batch train [10] loss 0.44738, dsc 0.55262\n",
      "Batch train [11] loss 0.49785, dsc 0.50215\n",
      "Batch train [12] loss 0.50721, dsc 0.49279\n",
      "Batch train [13] loss 0.44614, dsc 0.55386\n",
      "Batch train [14] loss 0.44928, dsc 0.55072\n",
      "Batch train [15] loss 0.43007, dsc 0.56993\n",
      "Batch train [16] loss 0.42660, dsc 0.57340\n",
      "Batch train [17] loss 0.47074, dsc 0.52926\n",
      "Batch train [18] loss 0.42934, dsc 0.57066\n",
      "Batch train [19] loss 0.43398, dsc 0.56602\n",
      "Batch train [20] loss 0.45527, dsc 0.54473\n",
      "Batch train [21] loss 0.40302, dsc 0.59698\n",
      "Batch train [22] loss 0.51286, dsc 0.48714\n",
      "Batch train [23] loss 0.46918, dsc 0.53082\n",
      "Batch train [24] loss 0.42950, dsc 0.57050\n",
      "Batch train [25] loss 0.54040, dsc 0.45960\n",
      "Batch train [26] loss 0.42916, dsc 0.57084\n",
      "Batch train [27] loss 0.49431, dsc 0.50569\n",
      "Batch train [28] loss 0.43249, dsc 0.56751\n",
      "Batch train [29] loss 0.43329, dsc 0.56671\n",
      "Batch train [30] loss 0.48882, dsc 0.51118\n",
      "Batch train [31] loss 0.42818, dsc 0.57182\n",
      "Batch train [32] loss 0.46979, dsc 0.53021\n",
      "Batch train [33] loss 0.47269, dsc 0.52731\n",
      "Batch train [34] loss 0.37435, dsc 0.62565\n",
      "Batch train [35] loss 0.45205, dsc 0.54795\n",
      "Batch train [36] loss 0.44813, dsc 0.55187\n",
      "Batch train [37] loss 0.43293, dsc 0.56707\n",
      "Batch train [38] loss 0.48925, dsc 0.51075\n",
      "Batch train [39] loss 0.48685, dsc 0.51315\n",
      "Batch train [40] loss 0.46080, dsc 0.53920\n",
      "Epoch [4] train done\n",
      "DEBUG: Writing to tensorboard epoch 3, step 0\n",
      "Batch eval [1] loss 0.46578, dsc 0.53422\n",
      "Batch eval [2] loss 0.45415, dsc 0.54585\n",
      "Batch eval [3] loss 0.44904, dsc 0.55096\n",
      "Batch eval [4] loss 0.49738, dsc 0.50262\n",
      "Batch eval [5] loss 0.42643, dsc 0.57357\n",
      "Epoch [4] valid done\n",
      "Epoch [4] T 176.93s, deltaT 44.00s, loss: train 0.45696, valid 0.45856, dsc: train 0.54304, valid 0.54144\n",
      "DEBUG: Writing to tensorboard epoch 4, step 0\n",
      "Batch train [1] loss 0.41324, dsc 0.58676\n",
      "Batch train [2] loss 0.44212, dsc 0.55788\n",
      "Batch train [3] loss 0.42644, dsc 0.57356\n",
      "Batch train [4] loss 0.45167, dsc 0.54833\n",
      "Batch train [5] loss 0.40900, dsc 0.59100\n",
      "Batch train [6] loss 0.47626, dsc 0.52374\n",
      "Batch train [7] loss 0.41857, dsc 0.58143\n",
      "Batch train [8] loss 0.41576, dsc 0.58424\n",
      "Batch train [9] loss 0.44098, dsc 0.55902\n",
      "Batch train [10] loss 0.45470, dsc 0.54530\n",
      "Batch train [11] loss 0.42667, dsc 0.57333\n",
      "Batch train [12] loss 0.45456, dsc 0.54544\n",
      "Batch train [13] loss 0.45934, dsc 0.54066\n",
      "Batch train [14] loss 0.51827, dsc 0.48173\n",
      "Batch train [15] loss 0.43212, dsc 0.56788\n",
      "Batch train [16] loss 0.40331, dsc 0.59669\n",
      "Batch train [17] loss 0.43746, dsc 0.56254\n",
      "Batch train [18] loss 0.44031, dsc 0.55969\n",
      "Batch train [19] loss 0.43779, dsc 0.56221\n",
      "Batch train [20] loss 0.47706, dsc 0.52294\n",
      "Batch train [21] loss 0.42412, dsc 0.57588\n",
      "Batch train [22] loss 0.41078, dsc 0.58922\n",
      "Batch train [23] loss 0.44898, dsc 0.55102\n",
      "Batch train [24] loss 0.42466, dsc 0.57534\n",
      "Batch train [25] loss 0.40208, dsc 0.59792\n",
      "Batch train [26] loss 0.42043, dsc 0.57957\n",
      "Batch train [27] loss 0.44658, dsc 0.55342\n",
      "Batch train [28] loss 0.42854, dsc 0.57146\n",
      "Batch train [29] loss 0.47658, dsc 0.52342\n",
      "Batch train [30] loss 0.42504, dsc 0.57496\n",
      "Batch train [31] loss 0.51306, dsc 0.48694\n",
      "Batch train [32] loss 0.42492, dsc 0.57508\n",
      "Batch train [33] loss 0.44042, dsc 0.55958\n",
      "Batch train [34] loss 0.47183, dsc 0.52817\n",
      "Batch train [35] loss 0.41002, dsc 0.58998\n",
      "Batch train [36] loss 0.49480, dsc 0.50520\n",
      "Batch train [37] loss 0.43278, dsc 0.56722\n",
      "Batch train [38] loss 0.45176, dsc 0.54824\n",
      "Batch train [39] loss 0.44895, dsc 0.55105\n",
      "Batch train [40] loss 0.40980, dsc 0.59020\n",
      "Epoch [5] train done\n",
      "DEBUG: Writing to tensorboard epoch 4, step 0\n",
      "Batch eval [1] loss 0.72423, dsc 0.27577\n",
      "Batch eval [2] loss 0.63080, dsc 0.36920\n",
      "Batch eval [3] loss 0.78919, dsc 0.21081\n",
      "Batch eval [4] loss 0.53775, dsc 0.46225\n",
      "Batch eval [5] loss 0.58921, dsc 0.41079\n",
      "Epoch [5] valid done\n",
      "Epoch [5] T 220.70s, deltaT 43.77s, loss: train 0.44104, valid 0.65424, dsc: train 0.55896, valid 0.34576\n",
      "DEBUG: Writing to tensorboard epoch 5, step 0\n",
      "Batch train [1] loss 0.45212, dsc 0.54788\n",
      "Batch train [2] loss 0.40556, dsc 0.59444\n",
      "Batch train [3] loss 0.40798, dsc 0.59202\n",
      "Batch train [4] loss 0.43790, dsc 0.56210\n",
      "Batch train [5] loss 0.49970, dsc 0.50030\n",
      "Batch train [6] loss 0.47117, dsc 0.52883\n",
      "Batch train [7] loss 0.45352, dsc 0.54648\n",
      "Batch train [8] loss 0.49984, dsc 0.50016\n",
      "Batch train [9] loss 0.41600, dsc 0.58400\n",
      "Batch train [10] loss 0.41517, dsc 0.58483\n",
      "Batch train [11] loss 0.39559, dsc 0.60441\n",
      "Batch train [12] loss 0.51004, dsc 0.48996\n",
      "Batch train [13] loss 0.41900, dsc 0.58100\n",
      "Batch train [14] loss 0.42016, dsc 0.57984\n",
      "Batch train [15] loss 0.42768, dsc 0.57232\n",
      "Batch train [16] loss 0.41530, dsc 0.58470\n",
      "Batch train [17] loss 0.37916, dsc 0.62084\n",
      "Batch train [18] loss 0.41223, dsc 0.58777\n",
      "Batch train [19] loss 0.40870, dsc 0.59130\n",
      "Batch train [20] loss 0.42996, dsc 0.57004\n",
      "Batch train [21] loss 0.44068, dsc 0.55932\n",
      "Batch train [22] loss 0.41649, dsc 0.58351\n",
      "Batch train [23] loss 0.46800, dsc 0.53200\n",
      "Batch train [24] loss 0.43701, dsc 0.56299\n",
      "Batch train [25] loss 0.43032, dsc 0.56968\n",
      "Batch train [26] loss 0.48305, dsc 0.51695\n",
      "Batch train [27] loss 0.45316, dsc 0.54684\n",
      "Batch train [28] loss 0.42257, dsc 0.57743\n",
      "Batch train [29] loss 0.44601, dsc 0.55399\n",
      "Batch train [30] loss 0.39880, dsc 0.60120\n",
      "Batch train [31] loss 0.42843, dsc 0.57157\n",
      "Batch train [32] loss 0.47324, dsc 0.52676\n",
      "Batch train [33] loss 0.45287, dsc 0.54713\n",
      "Batch train [34] loss 0.50167, dsc 0.49833\n",
      "Batch train [35] loss 0.44695, dsc 0.55305\n",
      "Batch train [36] loss 0.50866, dsc 0.49134\n",
      "Batch train [37] loss 0.43718, dsc 0.56282\n",
      "Batch train [38] loss 0.42068, dsc 0.57932\n",
      "Batch train [39] loss 0.40599, dsc 0.59401\n",
      "Batch train [40] loss 0.42555, dsc 0.57445\n",
      "Epoch [6] train done\n",
      "DEBUG: Writing to tensorboard epoch 5, step 0\n",
      "Batch eval [1] loss 0.42687, dsc 0.57313\n",
      "Batch eval [2] loss 0.44206, dsc 0.55794\n",
      "Batch eval [3] loss 0.44321, dsc 0.55679\n",
      "Batch eval [4] loss 0.54178, dsc 0.45822\n",
      "Batch eval [5] loss 0.43095, dsc 0.56905\n",
      "Epoch [6] valid done\n",
      "Epoch [6] T 265.19s, deltaT 44.48s, loss: train 0.43935, valid 0.45698, dsc: train 0.56065, valid 0.54302\n",
      "DEBUG: Writing to tensorboard epoch 6, step 0\n",
      "Batch train [1] loss 0.42462, dsc 0.57538\n",
      "Batch train [2] loss 0.42956, dsc 0.57044\n",
      "Batch train [3] loss 0.41039, dsc 0.58961\n",
      "Batch train [4] loss 0.45134, dsc 0.54866\n",
      "Batch train [5] loss 0.42076, dsc 0.57924\n",
      "Batch train [6] loss 0.50590, dsc 0.49410\n",
      "Batch train [7] loss 0.41170, dsc 0.58830\n",
      "Batch train [8] loss 0.41776, dsc 0.58224\n",
      "Batch train [9] loss 0.38764, dsc 0.61236\n",
      "Batch train [10] loss 0.39752, dsc 0.60248\n",
      "Batch train [11] loss 0.45531, dsc 0.54469\n",
      "Batch train [12] loss 0.41387, dsc 0.58613\n",
      "Batch train [13] loss 0.41403, dsc 0.58597\n",
      "Batch train [14] loss 0.43490, dsc 0.56510\n",
      "Batch train [15] loss 0.44777, dsc 0.55223\n",
      "Batch train [16] loss 0.52903, dsc 0.47097\n",
      "Batch train [17] loss 0.47520, dsc 0.52480\n",
      "Batch train [18] loss 0.45530, dsc 0.54470\n",
      "Batch train [19] loss 0.43415, dsc 0.56585\n",
      "Batch train [20] loss 0.45425, dsc 0.54575\n",
      "Batch train [21] loss 0.44369, dsc 0.55631\n",
      "Batch train [22] loss 0.43412, dsc 0.56588\n",
      "Batch train [23] loss 0.43758, dsc 0.56242\n",
      "Batch train [24] loss 0.42544, dsc 0.57456\n",
      "Batch train [25] loss 0.40155, dsc 0.59845\n",
      "Batch train [26] loss 0.43079, dsc 0.56921\n",
      "Batch train [27] loss 0.48860, dsc 0.51140\n",
      "Batch train [28] loss 0.42818, dsc 0.57182\n",
      "Batch train [29] loss 0.42173, dsc 0.57827\n",
      "Batch train [30] loss 0.42295, dsc 0.57705\n",
      "Batch train [31] loss 0.42477, dsc 0.57523\n",
      "Batch train [32] loss 0.45284, dsc 0.54716\n",
      "Batch train [33] loss 0.41818, dsc 0.58182\n",
      "Batch train [34] loss 0.43318, dsc 0.56682\n",
      "Batch train [35] loss 0.40426, dsc 0.59574\n",
      "Batch train [36] loss 0.40090, dsc 0.59910\n",
      "Batch train [37] loss 0.40635, dsc 0.59365\n",
      "Batch train [38] loss 0.42410, dsc 0.57590\n",
      "Batch train [39] loss 0.47673, dsc 0.52327\n",
      "Batch train [40] loss 0.43804, dsc 0.56196\n",
      "Epoch [7] train done\n",
      "DEBUG: Writing to tensorboard epoch 6, step 0\n",
      "Batch eval [1] loss 0.49856, dsc 0.50144\n",
      "Batch eval [2] loss 0.45185, dsc 0.54815\n",
      "Batch eval [3] loss 0.49023, dsc 0.50977\n",
      "Batch eval [4] loss 0.53643, dsc 0.46357\n",
      "Batch eval [5] loss 0.40037, dsc 0.59963\n",
      "Epoch [7] valid done\n",
      "Epoch [7] T 309.32s, deltaT 44.13s, loss: train 0.43462, valid 0.47549, dsc: train 0.56538, valid 0.52451\n",
      "DEBUG: Writing to tensorboard epoch 7, step 0\n",
      "Batch train [1] loss 0.45160, dsc 0.54840\n",
      "Batch train [2] loss 0.39605, dsc 0.60395\n",
      "Batch train [3] loss 0.42064, dsc 0.57936\n",
      "Batch train [4] loss 0.40522, dsc 0.59478\n",
      "Batch train [5] loss 0.39447, dsc 0.60553\n",
      "Batch train [6] loss 0.39802, dsc 0.60198\n",
      "Batch train [7] loss 0.47403, dsc 0.52597\n",
      "Batch train [8] loss 0.43922, dsc 0.56078\n",
      "Batch train [9] loss 0.40388, dsc 0.59612\n",
      "Batch train [10] loss 0.46218, dsc 0.53782\n",
      "Batch train [11] loss 0.39572, dsc 0.60428\n",
      "Batch train [12] loss 0.41830, dsc 0.58170\n",
      "Batch train [13] loss 0.43492, dsc 0.56508\n",
      "Batch train [14] loss 0.43074, dsc 0.56926\n",
      "Batch train [15] loss 0.39533, dsc 0.60467\n",
      "Batch train [16] loss 0.42051, dsc 0.57949\n",
      "Batch train [17] loss 0.40754, dsc 0.59246\n",
      "Batch train [18] loss 0.39648, dsc 0.60352\n",
      "Batch train [19] loss 0.42142, dsc 0.57858\n",
      "Batch train [20] loss 0.44882, dsc 0.55118\n",
      "Batch train [21] loss 0.45615, dsc 0.54385\n",
      "Batch train [22] loss 0.38903, dsc 0.61097\n",
      "Batch train [23] loss 0.38416, dsc 0.61584\n",
      "Batch train [24] loss 0.42244, dsc 0.57756\n",
      "Batch train [25] loss 0.45833, dsc 0.54167\n",
      "Batch train [26] loss 0.42473, dsc 0.57527\n",
      "Batch train [27] loss 0.41732, dsc 0.58268\n",
      "Batch train [28] loss 0.46347, dsc 0.53653\n",
      "Batch train [29] loss 0.40502, dsc 0.59498\n",
      "Batch train [30] loss 0.41929, dsc 0.58071\n",
      "Batch train [31] loss 0.40529, dsc 0.59471\n",
      "Batch train [32] loss 0.42969, dsc 0.57031\n",
      "Batch train [33] loss 0.42319, dsc 0.57681\n",
      "Batch train [34] loss 0.42804, dsc 0.57196\n",
      "Batch train [35] loss 0.44533, dsc 0.55467\n",
      "Batch train [36] loss 0.44508, dsc 0.55492\n",
      "Batch train [37] loss 0.52114, dsc 0.47886\n",
      "Batch train [38] loss 0.45206, dsc 0.54794\n",
      "Batch train [39] loss 0.42955, dsc 0.57045\n",
      "Batch train [40] loss 0.40731, dsc 0.59269\n",
      "Epoch [8] train done\n",
      "DEBUG: Writing to tensorboard epoch 7, step 0\n",
      "Batch eval [1] loss 0.44357, dsc 0.55643\n",
      "Batch eval [2] loss 0.52700, dsc 0.47300\n",
      "Batch eval [3] loss 0.49838, dsc 0.50162\n",
      "Batch eval [4] loss 0.51855, dsc 0.48145\n",
      "Batch eval [5] loss 0.48608, dsc 0.51392\n",
      "Epoch [8] valid done\n",
      "Epoch [8] T 353.03s, deltaT 43.71s, loss: train 0.42604, valid 0.49472, dsc: train 0.57396, valid 0.50528\n",
      "DEBUG: Writing to tensorboard epoch 8, step 0\n",
      "Batch train [1] loss 0.42126, dsc 0.57874\n",
      "Batch train [2] loss 0.39890, dsc 0.60110\n",
      "Batch train [3] loss 0.38878, dsc 0.61122\n",
      "Batch train [4] loss 0.38514, dsc 0.61486\n",
      "Batch train [5] loss 0.40735, dsc 0.59265\n",
      "Batch train [6] loss 0.43383, dsc 0.56617\n",
      "Batch train [7] loss 0.46164, dsc 0.53836\n",
      "Batch train [8] loss 0.43308, dsc 0.56692\n",
      "Batch train [9] loss 0.44733, dsc 0.55267\n",
      "Batch train [10] loss 0.40231, dsc 0.59769\n",
      "Batch train [11] loss 0.44610, dsc 0.55390\n",
      "Batch train [12] loss 0.41108, dsc 0.58892\n",
      "Batch train [13] loss 0.43273, dsc 0.56727\n",
      "Batch train [14] loss 0.36721, dsc 0.63279\n",
      "Batch train [15] loss 0.39279, dsc 0.60721\n",
      "Batch train [16] loss 0.43080, dsc 0.56920\n",
      "Batch train [17] loss 0.42330, dsc 0.57670\n",
      "Batch train [18] loss 0.42992, dsc 0.57008\n",
      "Batch train [19] loss 0.47236, dsc 0.52764\n",
      "Batch train [20] loss 0.39243, dsc 0.60757\n",
      "Batch train [21] loss 0.43964, dsc 0.56036\n",
      "Batch train [22] loss 0.38621, dsc 0.61379\n",
      "Batch train [23] loss 0.41188, dsc 0.58812\n",
      "Batch train [24] loss 0.52568, dsc 0.47432\n",
      "Batch train [25] loss 0.40132, dsc 0.59868\n",
      "Batch train [26] loss 0.42956, dsc 0.57044\n",
      "Batch train [27] loss 0.41320, dsc 0.58680\n",
      "Batch train [28] loss 0.40769, dsc 0.59231\n",
      "Batch train [29] loss 0.41039, dsc 0.58961\n",
      "Batch train [30] loss 0.42263, dsc 0.57737\n",
      "Batch train [31] loss 0.41624, dsc 0.58376\n",
      "Batch train [32] loss 0.37486, dsc 0.62514\n",
      "Batch train [33] loss 0.39547, dsc 0.60453\n",
      "Batch train [34] loss 0.40963, dsc 0.59037\n",
      "Batch train [35] loss 0.42580, dsc 0.57420\n",
      "Batch train [36] loss 0.41347, dsc 0.58653\n",
      "Batch train [37] loss 0.40977, dsc 0.59023\n",
      "Batch train [38] loss 0.39441, dsc 0.60559\n",
      "Batch train [39] loss 0.42386, dsc 0.57614\n",
      "Batch train [40] loss 0.42776, dsc 0.57224\n",
      "Epoch [9] train done\n",
      "DEBUG: Writing to tensorboard epoch 8, step 0\n",
      "Batch eval [1] loss 0.42126, dsc 0.57874\n",
      "Batch eval [2] loss 0.43089, dsc 0.56911\n",
      "Batch eval [3] loss 0.58539, dsc 0.41461\n",
      "Batch eval [4] loss 0.51335, dsc 0.48665\n",
      "Batch eval [5] loss 0.41617, dsc 0.58383\n",
      "Epoch [9] valid done\n",
      "Epoch [9] T 396.79s, deltaT 43.75s, loss: train 0.41795, valid 0.47341, dsc: train 0.58205, valid 0.52659\n",
      "DEBUG: Writing to tensorboard epoch 9, step 0\n",
      "Batch train [1] loss 0.42182, dsc 0.57818\n",
      "Batch train [2] loss 0.43362, dsc 0.56638\n",
      "Batch train [3] loss 0.37944, dsc 0.62056\n",
      "Batch train [4] loss 0.39527, dsc 0.60473\n",
      "Batch train [5] loss 0.39752, dsc 0.60248\n",
      "Batch train [6] loss 0.53078, dsc 0.46922\n",
      "Batch train [7] loss 0.37993, dsc 0.62007\n",
      "Batch train [8] loss 0.42548, dsc 0.57452\n",
      "Batch train [9] loss 0.40446, dsc 0.59554\n",
      "Batch train [10] loss 0.45143, dsc 0.54857\n",
      "Batch train [11] loss 0.41640, dsc 0.58360\n",
      "Batch train [12] loss 0.38791, dsc 0.61209\n",
      "Batch train [13] loss 0.44998, dsc 0.55002\n",
      "Batch train [14] loss 0.44296, dsc 0.55704\n",
      "Batch train [15] loss 0.37771, dsc 0.62229\n",
      "Batch train [16] loss 0.38414, dsc 0.61586\n",
      "Batch train [17] loss 0.38536, dsc 0.61464\n",
      "Batch train [18] loss 0.42802, dsc 0.57198\n",
      "Batch train [19] loss 0.43004, dsc 0.56996\n",
      "Batch train [20] loss 0.41372, dsc 0.58628\n",
      "Batch train [21] loss 0.44863, dsc 0.55137\n",
      "Batch train [22] loss 0.40617, dsc 0.59383\n",
      "Batch train [23] loss 0.42413, dsc 0.57587\n",
      "Batch train [24] loss 0.41510, dsc 0.58490\n",
      "Batch train [25] loss 0.47674, dsc 0.52326\n",
      "Batch train [26] loss 0.41419, dsc 0.58581\n",
      "Batch train [27] loss 0.44912, dsc 0.55088\n",
      "Batch train [28] loss 0.38054, dsc 0.61946\n",
      "Batch train [29] loss 0.39210, dsc 0.60790\n",
      "Batch train [30] loss 0.40734, dsc 0.59266\n",
      "Batch train [31] loss 0.45397, dsc 0.54603\n",
      "Batch train [32] loss 0.41025, dsc 0.58975\n",
      "Batch train [33] loss 0.44408, dsc 0.55592\n",
      "Batch train [34] loss 0.38695, dsc 0.61305\n",
      "Batch train [35] loss 0.39057, dsc 0.60943\n",
      "Batch train [36] loss 0.44016, dsc 0.55984\n",
      "Batch train [37] loss 0.42934, dsc 0.57066\n",
      "Batch train [38] loss 0.46217, dsc 0.53783\n",
      "Batch train [39] loss 0.41819, dsc 0.58181\n",
      "Batch train [40] loss 0.40454, dsc 0.59546\n",
      "Epoch [10] train done\n",
      "DEBUG: Writing to tensorboard epoch 9, step 0\n",
      "Batch eval [1] loss 0.40934, dsc 0.59066\n",
      "Batch eval [2] loss 0.43085, dsc 0.56915\n",
      "Batch eval [3] loss 0.44285, dsc 0.55715\n",
      "Batch eval [4] loss 0.45025, dsc 0.54975\n",
      "Batch eval [5] loss 0.42881, dsc 0.57119\n",
      "Epoch [10] valid done\n",
      "Epoch [10] T 440.79s, deltaT 44.00s, loss: train 0.41976, valid 0.43242, dsc: train 0.58024, valid 0.56758\n",
      "DEBUG: Writing to tensorboard epoch 10, step 0\n",
      "Batch train [1] loss 0.41329, dsc 0.58671\n",
      "Batch train [2] loss 0.40442, dsc 0.59558\n",
      "Batch train [3] loss 0.44039, dsc 0.55961\n",
      "Batch train [4] loss 0.41933, dsc 0.58067\n",
      "Batch train [5] loss 0.39849, dsc 0.60151\n",
      "Batch train [6] loss 0.41787, dsc 0.58213\n",
      "Batch train [7] loss 0.42199, dsc 0.57801\n",
      "Batch train [8] loss 0.41997, dsc 0.58003\n",
      "Batch train [9] loss 0.44929, dsc 0.55071\n",
      "Batch train [10] loss 0.47861, dsc 0.52139\n",
      "Batch train [11] loss 0.38568, dsc 0.61432\n",
      "Batch train [12] loss 0.40933, dsc 0.59067\n",
      "Batch train [13] loss 0.41469, dsc 0.58531\n",
      "Batch train [14] loss 0.41498, dsc 0.58502\n",
      "Batch train [15] loss 0.39669, dsc 0.60331\n",
      "Batch train [16] loss 0.38647, dsc 0.61353\n",
      "Batch train [17] loss 0.50858, dsc 0.49142\n",
      "Batch train [18] loss 0.44131, dsc 0.55869\n",
      "Batch train [19] loss 0.47367, dsc 0.52633\n",
      "Batch train [20] loss 0.42219, dsc 0.57781\n",
      "Batch train [21] loss 0.40157, dsc 0.59843\n",
      "Batch train [22] loss 0.37108, dsc 0.62892\n",
      "Batch train [23] loss 0.38740, dsc 0.61260\n",
      "Batch train [24] loss 0.40314, dsc 0.59686\n",
      "Batch train [25] loss 0.40299, dsc 0.59701\n",
      "Batch train [26] loss 0.37914, dsc 0.62086\n",
      "Batch train [27] loss 0.39000, dsc 0.61000\n",
      "Batch train [28] loss 0.42183, dsc 0.57817\n",
      "Batch train [29] loss 0.42533, dsc 0.57467\n",
      "Batch train [30] loss 0.39282, dsc 0.60718\n",
      "Batch train [31] loss 0.43764, dsc 0.56236\n",
      "Batch train [32] loss 0.37483, dsc 0.62517\n",
      "Batch train [33] loss 0.41275, dsc 0.58725\n",
      "Batch train [34] loss 0.41845, dsc 0.58155\n",
      "Batch train [35] loss 0.40049, dsc 0.59951\n",
      "Batch train [36] loss 0.38088, dsc 0.61912\n",
      "Batch train [37] loss 0.37678, dsc 0.62322\n",
      "Batch train [38] loss 0.40772, dsc 0.59228\n",
      "Batch train [39] loss 0.41113, dsc 0.58887\n",
      "Batch train [40] loss 0.44186, dsc 0.55814\n",
      "Epoch [11] train done\n",
      "DEBUG: Writing to tensorboard epoch 10, step 0\n",
      "Batch eval [1] loss 0.48389, dsc 0.51611\n",
      "Batch eval [2] loss 0.49698, dsc 0.50302\n",
      "Batch eval [3] loss 0.46515, dsc 0.53485\n",
      "Batch eval [4] loss 0.49449, dsc 0.50551\n",
      "Batch eval [5] loss 0.43613, dsc 0.56387\n",
      "Epoch [11] valid done\n",
      "Epoch [11] T 485.20s, deltaT 44.40s, loss: train 0.41388, valid 0.47533, dsc: train 0.58612, valid 0.52467\n",
      "DEBUG: Writing to tensorboard epoch 11, step 0\n",
      "Batch train [1] loss 0.39572, dsc 0.60428\n",
      "Batch train [2] loss 0.45033, dsc 0.54967\n",
      "Batch train [3] loss 0.42576, dsc 0.57424\n",
      "Batch train [4] loss 0.41585, dsc 0.58415\n",
      "Batch train [5] loss 0.35229, dsc 0.64771\n",
      "Batch train [6] loss 0.40019, dsc 0.59981\n",
      "Batch train [7] loss 0.42627, dsc 0.57373\n",
      "Batch train [8] loss 0.40691, dsc 0.59309\n",
      "Batch train [9] loss 0.40810, dsc 0.59190\n",
      "Batch train [10] loss 0.39183, dsc 0.60817\n",
      "Batch train [11] loss 0.41853, dsc 0.58147\n",
      "Batch train [12] loss 0.38651, dsc 0.61349\n",
      "Batch train [13] loss 0.44671, dsc 0.55329\n",
      "Batch train [14] loss 0.41055, dsc 0.58945\n",
      "Batch train [15] loss 0.42182, dsc 0.57818\n",
      "Batch train [16] loss 0.41609, dsc 0.58391\n",
      "Batch train [17] loss 0.40693, dsc 0.59307\n",
      "Batch train [18] loss 0.43134, dsc 0.56866\n",
      "Batch train [19] loss 0.42029, dsc 0.57971\n",
      "Batch train [20] loss 0.40147, dsc 0.59853\n",
      "Batch train [21] loss 0.40954, dsc 0.59046\n",
      "Batch train [22] loss 0.39801, dsc 0.60199\n",
      "Batch train [23] loss 0.44067, dsc 0.55933\n",
      "Batch train [24] loss 0.38107, dsc 0.61893\n",
      "Batch train [25] loss 0.39259, dsc 0.60741\n",
      "Batch train [26] loss 0.39358, dsc 0.60642\n",
      "Batch train [27] loss 0.40457, dsc 0.59543\n",
      "Batch train [28] loss 0.40284, dsc 0.59716\n",
      "Batch train [29] loss 0.42142, dsc 0.57858\n",
      "Batch train [30] loss 0.39005, dsc 0.60995\n",
      "Batch train [31] loss 0.38661, dsc 0.61339\n",
      "Batch train [32] loss 0.39089, dsc 0.60911\n",
      "Batch train [33] loss 0.40777, dsc 0.59223\n",
      "Batch train [34] loss 0.37751, dsc 0.62249\n",
      "Batch train [35] loss 0.40599, dsc 0.59401\n",
      "Batch train [36] loss 0.50332, dsc 0.49668\n",
      "Batch train [37] loss 0.45025, dsc 0.54975\n",
      "Batch train [38] loss 0.37071, dsc 0.62929\n",
      "Batch train [39] loss 0.38122, dsc 0.61878\n",
      "Batch train [40] loss 0.44232, dsc 0.55768\n",
      "Epoch [12] train done\n",
      "DEBUG: Writing to tensorboard epoch 11, step 0\n",
      "Batch eval [1] loss 0.39997, dsc 0.60003\n",
      "Batch eval [2] loss 0.46797, dsc 0.53203\n",
      "Batch eval [3] loss 0.48264, dsc 0.51736\n",
      "Batch eval [4] loss 0.48626, dsc 0.51374\n",
      "Batch eval [5] loss 0.47069, dsc 0.52931\n",
      "Epoch [12] valid done\n",
      "Epoch [12] T 529.35s, deltaT 44.14s, loss: train 0.40961, valid 0.46151, dsc: train 0.59039, valid 0.53849\n",
      "DEBUG: Writing to tensorboard epoch 12, step 0\n",
      "Batch train [1] loss 0.42290, dsc 0.57710\n",
      "Batch train [2] loss 0.37280, dsc 0.62720\n",
      "Batch train [3] loss 0.38963, dsc 0.61037\n",
      "Batch train [4] loss 0.37635, dsc 0.62365\n",
      "Batch train [5] loss 0.40452, dsc 0.59548\n",
      "Batch train [6] loss 0.39586, dsc 0.60414\n",
      "Batch train [7] loss 0.35444, dsc 0.64556\n",
      "Batch train [8] loss 0.42882, dsc 0.57118\n",
      "Batch train [9] loss 0.42922, dsc 0.57078\n",
      "Batch train [10] loss 0.37129, dsc 0.62871\n",
      "Batch train [11] loss 0.40761, dsc 0.59239\n",
      "Batch train [12] loss 0.39211, dsc 0.60789\n",
      "Batch train [13] loss 0.43002, dsc 0.56998\n",
      "Batch train [14] loss 0.40610, dsc 0.59390\n",
      "Batch train [15] loss 0.39175, dsc 0.60825\n",
      "Batch train [16] loss 0.40267, dsc 0.59733\n",
      "Batch train [17] loss 0.41758, dsc 0.58242\n",
      "Batch train [18] loss 0.39486, dsc 0.60514\n",
      "Batch train [19] loss 0.39551, dsc 0.60449\n",
      "Batch train [20] loss 0.40330, dsc 0.59670\n",
      "Batch train [21] loss 0.37711, dsc 0.62289\n",
      "Batch train [22] loss 0.40166, dsc 0.59834\n",
      "Batch train [23] loss 0.38779, dsc 0.61221\n",
      "Batch train [24] loss 0.40151, dsc 0.59849\n",
      "Batch train [25] loss 0.41063, dsc 0.58937\n",
      "Batch train [26] loss 0.38534, dsc 0.61466\n",
      "Batch train [27] loss 0.50665, dsc 0.49335\n",
      "Batch train [28] loss 0.39715, dsc 0.60285\n",
      "Batch train [29] loss 0.38092, dsc 0.61908\n",
      "Batch train [30] loss 0.43846, dsc 0.56154\n",
      "Batch train [31] loss 0.40859, dsc 0.59141\n",
      "Batch train [32] loss 0.39670, dsc 0.60330\n",
      "Batch train [33] loss 0.43331, dsc 0.56669\n",
      "Batch train [34] loss 0.39180, dsc 0.60820\n",
      "Batch train [35] loss 0.39930, dsc 0.60070\n",
      "Batch train [36] loss 0.44409, dsc 0.55591\n",
      "Batch train [37] loss 0.36783, dsc 0.63217\n",
      "Batch train [38] loss 0.39844, dsc 0.60156\n",
      "Batch train [39] loss 0.41956, dsc 0.58044\n",
      "Batch train [40] loss 0.42365, dsc 0.57635\n",
      "Epoch [13] train done\n",
      "DEBUG: Writing to tensorboard epoch 12, step 0\n",
      "Batch eval [1] loss 0.45074, dsc 0.54926\n",
      "Batch eval [2] loss 0.41972, dsc 0.58028\n",
      "Batch eval [3] loss 0.40753, dsc 0.59247\n",
      "Batch eval [4] loss 0.47893, dsc 0.52107\n",
      "Batch eval [5] loss 0.38513, dsc 0.61487\n",
      "Epoch [13] valid done\n",
      "Epoch [13] T 573.62s, deltaT 44.27s, loss: train 0.40395, valid 0.42841, dsc: train 0.59605, valid 0.57159\n",
      "DEBUG: Writing to tensorboard epoch 13, step 0\n",
      "Batch train [1] loss 0.39011, dsc 0.60989\n",
      "Batch train [2] loss 0.42955, dsc 0.57045\n",
      "Batch train [3] loss 0.38891, dsc 0.61109\n",
      "Batch train [4] loss 0.34450, dsc 0.65550\n",
      "Batch train [5] loss 0.40167, dsc 0.59833\n",
      "Batch train [6] loss 0.40203, dsc 0.59797\n",
      "Batch train [7] loss 0.38295, dsc 0.61705\n",
      "Batch train [8] loss 0.42441, dsc 0.57559\n",
      "Batch train [9] loss 0.39830, dsc 0.60170\n",
      "Batch train [10] loss 0.42265, dsc 0.57735\n",
      "Batch train [11] loss 0.43415, dsc 0.56585\n",
      "Batch train [12] loss 0.37919, dsc 0.62081\n",
      "Batch train [13] loss 0.36550, dsc 0.63450\n",
      "Batch train [14] loss 0.40155, dsc 0.59845\n",
      "Batch train [15] loss 0.41027, dsc 0.58973\n",
      "Batch train [16] loss 0.47013, dsc 0.52987\n",
      "Batch train [17] loss 0.40267, dsc 0.59733\n",
      "Batch train [18] loss 0.46336, dsc 0.53664\n",
      "Batch train [19] loss 0.42156, dsc 0.57844\n",
      "Batch train [20] loss 0.44641, dsc 0.55359\n",
      "Batch train [21] loss 0.44651, dsc 0.55349\n",
      "Batch train [22] loss 0.41395, dsc 0.58605\n",
      "Batch train [23] loss 0.41821, dsc 0.58179\n",
      "Batch train [24] loss 0.39545, dsc 0.60455\n",
      "Batch train [25] loss 0.40330, dsc 0.59670\n",
      "Batch train [26] loss 0.44343, dsc 0.55657\n",
      "Batch train [27] loss 0.39326, dsc 0.60674\n",
      "Batch train [28] loss 0.47655, dsc 0.52345\n",
      "Batch train [29] loss 0.44423, dsc 0.55577\n",
      "Batch train [30] loss 0.40823, dsc 0.59177\n",
      "Batch train [31] loss 0.41645, dsc 0.58355\n",
      "Batch train [32] loss 0.39755, dsc 0.60245\n",
      "Batch train [33] loss 0.38625, dsc 0.61375\n",
      "Batch train [34] loss 0.39336, dsc 0.60664\n",
      "Batch train [35] loss 0.43214, dsc 0.56786\n",
      "Batch train [36] loss 0.39581, dsc 0.60419\n",
      "Batch train [37] loss 0.38683, dsc 0.61317\n",
      "Batch train [38] loss 0.42032, dsc 0.57968\n",
      "Batch train [39] loss 0.36634, dsc 0.63366\n",
      "Batch train [40] loss 0.39556, dsc 0.60444\n",
      "Epoch [14] train done\n",
      "DEBUG: Writing to tensorboard epoch 13, step 0\n",
      "Batch eval [1] loss 0.53927, dsc 0.46073\n",
      "Batch eval [2] loss 0.50822, dsc 0.49178\n",
      "Batch eval [3] loss 0.58597, dsc 0.41403\n",
      "Batch eval [4] loss 0.46218, dsc 0.53782\n",
      "Batch eval [5] loss 0.41418, dsc 0.58582\n",
      "Epoch [14] valid done\n",
      "Epoch [14] T 618.13s, deltaT 44.50s, loss: train 0.41034, valid 0.50196, dsc: train 0.58966, valid 0.49804\n",
      "DEBUG: Writing to tensorboard epoch 14, step 0\n",
      "Batch train [1] loss 0.38459, dsc 0.61541\n",
      "Batch train [2] loss 0.41758, dsc 0.58242\n",
      "Batch train [3] loss 0.39369, dsc 0.60631\n",
      "Batch train [4] loss 0.40826, dsc 0.59174\n",
      "Batch train [5] loss 0.40269, dsc 0.59731\n",
      "Batch train [6] loss 0.40065, dsc 0.59935\n",
      "Batch train [7] loss 0.36341, dsc 0.63659\n",
      "Batch train [8] loss 0.37426, dsc 0.62574\n",
      "Batch train [9] loss 0.41030, dsc 0.58970\n",
      "Batch train [10] loss 0.36643, dsc 0.63357\n",
      "Batch train [11] loss 0.42504, dsc 0.57496\n",
      "Batch train [12] loss 0.36771, dsc 0.63229\n",
      "Batch train [13] loss 0.38585, dsc 0.61415\n",
      "Batch train [14] loss 0.45167, dsc 0.54833\n",
      "Batch train [15] loss 0.44849, dsc 0.55151\n",
      "Batch train [16] loss 0.38469, dsc 0.61531\n",
      "Batch train [17] loss 0.38939, dsc 0.61061\n",
      "Batch train [18] loss 0.39846, dsc 0.60154\n",
      "Batch train [19] loss 0.45884, dsc 0.54116\n",
      "Batch train [20] loss 0.40592, dsc 0.59408\n",
      "Batch train [21] loss 0.41328, dsc 0.58672\n",
      "Batch train [22] loss 0.41850, dsc 0.58150\n",
      "Batch train [23] loss 0.38768, dsc 0.61232\n",
      "Batch train [24] loss 0.41693, dsc 0.58307\n",
      "Batch train [25] loss 0.39509, dsc 0.60491\n",
      "Batch train [26] loss 0.38002, dsc 0.61998\n",
      "Batch train [27] loss 0.41386, dsc 0.58614\n",
      "Batch train [28] loss 0.40233, dsc 0.59767\n",
      "Batch train [29] loss 0.45152, dsc 0.54848\n",
      "Batch train [30] loss 0.41601, dsc 0.58399\n",
      "Batch train [31] loss 0.38970, dsc 0.61030\n",
      "Batch train [32] loss 0.35409, dsc 0.64591\n",
      "Batch train [33] loss 0.38064, dsc 0.61936\n",
      "Batch train [34] loss 0.40066, dsc 0.59934\n",
      "Batch train [35] loss 0.39834, dsc 0.60166\n",
      "Batch train [36] loss 0.40072, dsc 0.59928\n",
      "Batch train [37] loss 0.35457, dsc 0.64543\n",
      "Batch train [38] loss 0.39401, dsc 0.60599\n",
      "Batch train [39] loss 0.38873, dsc 0.61127\n",
      "Batch train [40] loss 0.37029, dsc 0.62971\n",
      "Epoch [15] train done\n",
      "DEBUG: Writing to tensorboard epoch 14, step 0\n",
      "Batch eval [1] loss 0.39685, dsc 0.60315\n",
      "Batch eval [2] loss 0.40629, dsc 0.59371\n",
      "Batch eval [3] loss 0.41846, dsc 0.58154\n",
      "Batch eval [4] loss 0.46842, dsc 0.53158\n",
      "Batch eval [5] loss 0.38283, dsc 0.61717\n",
      "Epoch [15] valid done\n",
      "Epoch [15] T 662.51s, deltaT 44.37s, loss: train 0.39912, valid 0.41457, dsc: train 0.60088, valid 0.58543\n",
      "DEBUG: Writing to tensorboard epoch 15, step 0\n",
      "Batch train [1] loss 0.38407, dsc 0.61593\n",
      "Batch train [2] loss 0.37671, dsc 0.62329\n",
      "Batch train [3] loss 0.35506, dsc 0.64494\n",
      "Batch train [4] loss 0.40056, dsc 0.59944\n",
      "Batch train [5] loss 0.36817, dsc 0.63183\n",
      "Batch train [6] loss 0.38507, dsc 0.61493\n",
      "Batch train [7] loss 0.40941, dsc 0.59059\n",
      "Batch train [8] loss 0.42822, dsc 0.57178\n",
      "Batch train [9] loss 0.37272, dsc 0.62728\n",
      "Batch train [10] loss 0.39118, dsc 0.60882\n",
      "Batch train [11] loss 0.43831, dsc 0.56169\n",
      "Batch train [12] loss 0.41596, dsc 0.58404\n",
      "Batch train [13] loss 0.35518, dsc 0.64482\n",
      "Batch train [14] loss 0.36993, dsc 0.63007\n",
      "Batch train [15] loss 0.42080, dsc 0.57920\n",
      "Batch train [16] loss 0.44274, dsc 0.55726\n",
      "Batch train [17] loss 0.36458, dsc 0.63542\n",
      "Batch train [18] loss 0.37156, dsc 0.62844\n",
      "Batch train [19] loss 0.38576, dsc 0.61424\n",
      "Batch train [20] loss 0.37390, dsc 0.62610\n",
      "Batch train [21] loss 0.40430, dsc 0.59570\n",
      "Batch train [22] loss 0.39681, dsc 0.60319\n",
      "Batch train [23] loss 0.41551, dsc 0.58449\n",
      "Batch train [24] loss 0.37260, dsc 0.62740\n",
      "Batch train [25] loss 0.45788, dsc 0.54212\n",
      "Batch train [26] loss 0.39285, dsc 0.60715\n",
      "Batch train [27] loss 0.44090, dsc 0.55910\n",
      "Batch train [28] loss 0.38502, dsc 0.61498\n",
      "Batch train [29] loss 0.38001, dsc 0.61999\n",
      "Batch train [30] loss 0.40166, dsc 0.59834\n",
      "Batch train [31] loss 0.39920, dsc 0.60080\n",
      "Batch train [32] loss 0.37408, dsc 0.62592\n",
      "Batch train [33] loss 0.36767, dsc 0.63233\n",
      "Batch train [34] loss 0.42091, dsc 0.57909\n",
      "Batch train [35] loss 0.40004, dsc 0.59996\n",
      "Batch train [36] loss 0.36117, dsc 0.63883\n",
      "Batch train [37] loss 0.39284, dsc 0.60716\n",
      "Batch train [38] loss 0.38202, dsc 0.61798\n",
      "Batch train [39] loss 0.37331, dsc 0.62669\n",
      "Batch train [40] loss 0.41035, dsc 0.58965\n",
      "Epoch [16] train done\n",
      "DEBUG: Writing to tensorboard epoch 15, step 0\n",
      "Batch eval [1] loss 0.39524, dsc 0.60476\n",
      "Batch eval [2] loss 0.42827, dsc 0.57173\n",
      "Batch eval [3] loss 0.60204, dsc 0.39796\n",
      "Batch eval [4] loss 0.42261, dsc 0.57739\n",
      "Batch eval [5] loss 0.37039, dsc 0.62961\n",
      "Epoch [16] valid done\n",
      "Epoch [16] T 706.43s, deltaT 43.92s, loss: train 0.39348, valid 0.44371, dsc: train 0.60652, valid 0.55629\n",
      "DEBUG: Writing to tensorboard epoch 16, step 0\n",
      "Batch train [1] loss 0.39574, dsc 0.60426\n",
      "Batch train [2] loss 0.38208, dsc 0.61792\n",
      "Batch train [3] loss 0.42300, dsc 0.57700\n",
      "Batch train [4] loss 0.45338, dsc 0.54662\n",
      "Batch train [5] loss 0.38884, dsc 0.61116\n",
      "Batch train [6] loss 0.43637, dsc 0.56363\n",
      "Batch train [7] loss 0.38137, dsc 0.61863\n",
      "Batch train [8] loss 0.40302, dsc 0.59698\n",
      "Batch train [9] loss 0.41819, dsc 0.58181\n",
      "Batch train [10] loss 0.36275, dsc 0.63725\n",
      "Batch train [11] loss 0.41018, dsc 0.58982\n",
      "Batch train [12] loss 0.35267, dsc 0.64733\n",
      "Batch train [13] loss 0.39440, dsc 0.60560\n",
      "Batch train [14] loss 0.39770, dsc 0.60230\n",
      "Batch train [15] loss 0.38997, dsc 0.61003\n",
      "Batch train [16] loss 0.39810, dsc 0.60190\n",
      "Batch train [17] loss 0.44246, dsc 0.55754\n",
      "Batch train [18] loss 0.37903, dsc 0.62097\n",
      "Batch train [19] loss 0.40314, dsc 0.59686\n",
      "Batch train [20] loss 0.38581, dsc 0.61419\n",
      "Batch train [21] loss 0.35747, dsc 0.64253\n",
      "Batch train [22] loss 0.36737, dsc 0.63263\n",
      "Batch train [23] loss 0.39241, dsc 0.60759\n",
      "Batch train [24] loss 0.36606, dsc 0.63394\n",
      "Batch train [25] loss 0.40339, dsc 0.59661\n",
      "Batch train [26] loss 0.49558, dsc 0.50442\n",
      "Batch train [27] loss 0.43903, dsc 0.56097\n",
      "Batch train [28] loss 0.38333, dsc 0.61667\n",
      "Batch train [29] loss 0.39052, dsc 0.60948\n",
      "Batch train [30] loss 0.39252, dsc 0.60748\n",
      "Batch train [31] loss 0.39915, dsc 0.60085\n",
      "Batch train [32] loss 0.33260, dsc 0.66740\n",
      "Batch train [33] loss 0.38414, dsc 0.61586\n",
      "Batch train [34] loss 0.40468, dsc 0.59532\n",
      "Batch train [35] loss 0.39799, dsc 0.60201\n",
      "Batch train [36] loss 0.42172, dsc 0.57828\n",
      "Batch train [37] loss 0.40649, dsc 0.59351\n",
      "Batch train [38] loss 0.42589, dsc 0.57411\n",
      "Batch train [39] loss 0.39208, dsc 0.60792\n",
      "Batch train [40] loss 0.39851, dsc 0.60149\n",
      "Epoch [17] train done\n",
      "DEBUG: Writing to tensorboard epoch 16, step 0\n",
      "Batch eval [1] loss 0.38463, dsc 0.61537\n",
      "Batch eval [2] loss 0.48312, dsc 0.51688\n",
      "Batch eval [3] loss 0.51735, dsc 0.48265\n",
      "Batch eval [4] loss 0.41663, dsc 0.58337\n",
      "Batch eval [5] loss 0.36241, dsc 0.63759\n",
      "Epoch [17] valid done\n",
      "Epoch [17] T 750.09s, deltaT 43.66s, loss: train 0.39873, valid 0.43282, dsc: train 0.60127, valid 0.56718\n",
      "DEBUG: Writing to tensorboard epoch 17, step 0\n",
      "Batch train [1] loss 0.37545, dsc 0.62455\n",
      "Batch train [2] loss 0.38542, dsc 0.61458\n",
      "Batch train [3] loss 0.36468, dsc 0.63532\n",
      "Batch train [4] loss 0.33784, dsc 0.66216\n",
      "Batch train [5] loss 0.37698, dsc 0.62302\n",
      "Batch train [6] loss 0.37667, dsc 0.62333\n",
      "Batch train [7] loss 0.40377, dsc 0.59623\n",
      "Batch train [8] loss 0.37824, dsc 0.62176\n",
      "Batch train [9] loss 0.36695, dsc 0.63305\n",
      "Batch train [10] loss 0.40915, dsc 0.59085\n",
      "Batch train [11] loss 0.38298, dsc 0.61702\n",
      "Batch train [12] loss 0.36330, dsc 0.63670\n",
      "Batch train [13] loss 0.38354, dsc 0.61646\n",
      "Batch train [14] loss 0.38918, dsc 0.61082\n",
      "Batch train [15] loss 0.39584, dsc 0.60416\n",
      "Batch train [16] loss 0.40556, dsc 0.59444\n",
      "Batch train [17] loss 0.37258, dsc 0.62742\n",
      "Batch train [18] loss 0.36756, dsc 0.63244\n",
      "Batch train [19] loss 0.48534, dsc 0.51466\n",
      "Batch train [20] loss 0.40087, dsc 0.59913\n",
      "Batch train [21] loss 0.34053, dsc 0.65947\n",
      "Batch train [22] loss 0.35695, dsc 0.64305\n",
      "Batch train [23] loss 0.38436, dsc 0.61564\n",
      "Batch train [24] loss 0.36926, dsc 0.63074\n",
      "Batch train [25] loss 0.38129, dsc 0.61871\n",
      "Batch train [26] loss 0.37324, dsc 0.62676\n",
      "Batch train [27] loss 0.38203, dsc 0.61797\n",
      "Batch train [28] loss 0.36684, dsc 0.63316\n",
      "Batch train [29] loss 0.36326, dsc 0.63674\n",
      "Batch train [30] loss 0.45529, dsc 0.54471\n",
      "Batch train [31] loss 0.40747, dsc 0.59253\n",
      "Batch train [32] loss 0.37630, dsc 0.62370\n",
      "Batch train [33] loss 0.36512, dsc 0.63488\n",
      "Batch train [34] loss 0.40727, dsc 0.59273\n",
      "Batch train [35] loss 0.41096, dsc 0.58904\n",
      "Batch train [36] loss 0.41903, dsc 0.58097\n",
      "Batch train [37] loss 0.38266, dsc 0.61734\n",
      "Batch train [38] loss 0.34496, dsc 0.65504\n",
      "Batch train [39] loss 0.38792, dsc 0.61208\n",
      "Batch train [40] loss 0.38718, dsc 0.61282\n",
      "Epoch [18] train done\n",
      "DEBUG: Writing to tensorboard epoch 17, step 0\n",
      "Batch eval [1] loss 0.51127, dsc 0.48873\n",
      "Batch eval [2] loss 0.40884, dsc 0.59116\n",
      "Batch eval [3] loss 0.64016, dsc 0.35984\n",
      "Batch eval [4] loss 0.41284, dsc 0.58716\n",
      "Batch eval [5] loss 0.39185, dsc 0.60815\n",
      "Epoch [18] valid done\n",
      "Epoch [18] T 794.00s, deltaT 43.90s, loss: train 0.38460, valid 0.47299, dsc: train 0.61541, valid 0.52701\n",
      "DEBUG: Writing to tensorboard epoch 18, step 0\n",
      "Batch train [1] loss 0.36993, dsc 0.63007\n",
      "Batch train [2] loss 0.37444, dsc 0.62556\n",
      "Batch train [3] loss 0.38862, dsc 0.61138\n",
      "Batch train [4] loss 0.39348, dsc 0.60652\n",
      "Batch train [5] loss 0.36777, dsc 0.63223\n",
      "Batch train [6] loss 0.37751, dsc 0.62249\n",
      "Batch train [7] loss 0.36557, dsc 0.63443\n",
      "Batch train [8] loss 0.32294, dsc 0.67706\n",
      "Batch train [9] loss 0.38853, dsc 0.61147\n",
      "Batch train [10] loss 0.41638, dsc 0.58362\n",
      "Batch train [11] loss 0.36980, dsc 0.63020\n",
      "Batch train [12] loss 0.41490, dsc 0.58510\n",
      "Batch train [13] loss 0.37738, dsc 0.62262\n",
      "Batch train [14] loss 0.40403, dsc 0.59597\n",
      "Batch train [15] loss 0.37753, dsc 0.62247\n",
      "Batch train [16] loss 0.39947, dsc 0.60053\n",
      "Batch train [17] loss 0.32641, dsc 0.67359\n",
      "Batch train [18] loss 0.34692, dsc 0.65308\n",
      "Batch train [19] loss 0.38007, dsc 0.61993\n",
      "Batch train [20] loss 0.42086, dsc 0.57914\n",
      "Batch train [21] loss 0.36825, dsc 0.63175\n",
      "Batch train [22] loss 0.40348, dsc 0.59652\n",
      "Batch train [23] loss 0.34091, dsc 0.65909\n",
      "Batch train [24] loss 0.35641, dsc 0.64359\n",
      "Batch train [25] loss 0.36170, dsc 0.63830\n",
      "Batch train [26] loss 0.37146, dsc 0.62854\n",
      "Batch train [27] loss 0.42028, dsc 0.57972\n",
      "Batch train [28] loss 0.36863, dsc 0.63137\n",
      "Batch train [29] loss 0.39726, dsc 0.60274\n",
      "Batch train [30] loss 0.35978, dsc 0.64022\n",
      "Batch train [31] loss 0.36463, dsc 0.63537\n",
      "Batch train [32] loss 0.38571, dsc 0.61429\n",
      "Batch train [33] loss 0.38516, dsc 0.61484\n",
      "Batch train [34] loss 0.46589, dsc 0.53411\n",
      "Batch train [35] loss 0.41898, dsc 0.58102\n",
      "Batch train [36] loss 0.36827, dsc 0.63173\n",
      "Batch train [37] loss 0.37135, dsc 0.62865\n",
      "Batch train [38] loss 0.39389, dsc 0.60611\n",
      "Batch train [39] loss 0.37697, dsc 0.62303\n",
      "Batch train [40] loss 0.40035, dsc 0.59965\n",
      "Epoch [19] train done\n",
      "DEBUG: Writing to tensorboard epoch 18, step 0\n",
      "Batch eval [1] loss 0.37372, dsc 0.62628\n",
      "Batch eval [2] loss 0.39396, dsc 0.60604\n",
      "Batch eval [3] loss 0.38671, dsc 0.61329\n",
      "Batch eval [4] loss 0.43600, dsc 0.56400\n",
      "Batch eval [5] loss 0.41882, dsc 0.58118\n",
      "Epoch [19] valid done\n",
      "Epoch [19] T 838.37s, deltaT 44.36s, loss: train 0.38155, valid 0.40184, dsc: train 0.61845, valid 0.59816\n",
      "DEBUG: Writing to tensorboard epoch 19, step 0\n",
      "Batch train [1] loss 0.39245, dsc 0.60755\n",
      "Batch train [2] loss 0.35202, dsc 0.64798\n",
      "Batch train [3] loss 0.36606, dsc 0.63394\n",
      "Batch train [4] loss 0.38497, dsc 0.61503\n",
      "Batch train [5] loss 0.38997, dsc 0.61003\n",
      "Batch train [6] loss 0.38264, dsc 0.61736\n",
      "Batch train [7] loss 0.35650, dsc 0.64350\n",
      "Batch train [8] loss 0.33675, dsc 0.66325\n",
      "Batch train [9] loss 0.41132, dsc 0.58868\n",
      "Batch train [10] loss 0.37154, dsc 0.62846\n",
      "Batch train [11] loss 0.39866, dsc 0.60134\n",
      "Batch train [12] loss 0.36840, dsc 0.63160\n",
      "Batch train [13] loss 0.35196, dsc 0.64804\n",
      "Batch train [14] loss 0.41510, dsc 0.58490\n",
      "Batch train [15] loss 0.35577, dsc 0.64423\n",
      "Batch train [16] loss 0.43918, dsc 0.56082\n",
      "Batch train [17] loss 0.37550, dsc 0.62450\n",
      "Batch train [18] loss 0.35489, dsc 0.64511\n",
      "Batch train [19] loss 0.37118, dsc 0.62882\n",
      "Batch train [20] loss 0.35245, dsc 0.64755\n",
      "Batch train [21] loss 0.40530, dsc 0.59470\n",
      "Batch train [22] loss 0.36200, dsc 0.63800\n",
      "Batch train [23] loss 0.36285, dsc 0.63715\n",
      "Batch train [24] loss 0.39870, dsc 0.60130\n",
      "Batch train [25] loss 0.38089, dsc 0.61911\n",
      "Batch train [26] loss 0.42757, dsc 0.57243\n",
      "Batch train [27] loss 0.38529, dsc 0.61471\n",
      "Batch train [28] loss 0.34984, dsc 0.65016\n",
      "Batch train [29] loss 0.39252, dsc 0.60748\n",
      "Batch train [30] loss 0.39951, dsc 0.60049\n",
      "Batch train [31] loss 0.35456, dsc 0.64544\n",
      "Batch train [32] loss 0.37886, dsc 0.62114\n",
      "Batch train [33] loss 0.36400, dsc 0.63600\n",
      "Batch train [34] loss 0.34243, dsc 0.65757\n",
      "Batch train [35] loss 0.34531, dsc 0.65469\n",
      "Batch train [36] loss 0.36651, dsc 0.63349\n",
      "Batch train [37] loss 0.39337, dsc 0.60663\n",
      "Batch train [38] loss 0.45435, dsc 0.54565\n",
      "Batch train [39] loss 0.42289, dsc 0.57711\n",
      "Batch train [40] loss 0.36507, dsc 0.63493\n",
      "Epoch [20] train done\n",
      "DEBUG: Writing to tensorboard epoch 19, step 0\n",
      "Batch eval [1] loss 0.51979, dsc 0.48021\n",
      "Batch eval [2] loss 0.59810, dsc 0.40190\n",
      "Batch eval [3] loss 0.63710, dsc 0.36290\n",
      "Batch eval [4] loss 0.77427, dsc 0.22573\n",
      "Batch eval [5] loss 0.64050, dsc 0.35950\n",
      "Epoch [20] valid done\n",
      "Epoch [20] T 882.75s, deltaT 44.37s, loss: train 0.37948, valid 0.63395, dsc: train 0.62052, valid 0.36605\n",
      "DEBUG: Writing to tensorboard epoch 20, step 0\n",
      "Batch train [1] loss 0.41940, dsc 0.58060\n",
      "Batch train [2] loss 0.45239, dsc 0.54761\n",
      "Batch train [3] loss 0.37218, dsc 0.62782\n",
      "Batch train [4] loss 0.38314, dsc 0.61686\n",
      "Batch train [5] loss 0.43559, dsc 0.56441\n",
      "Batch train [6] loss 0.38848, dsc 0.61152\n",
      "Batch train [7] loss 0.42700, dsc 0.57300\n",
      "Batch train [8] loss 0.38044, dsc 0.61956\n",
      "Batch train [9] loss 0.43274, dsc 0.56726\n",
      "Batch train [10] loss 0.36710, dsc 0.63290\n",
      "Batch train [11] loss 0.39560, dsc 0.60440\n",
      "Batch train [12] loss 0.42221, dsc 0.57779\n",
      "Batch train [13] loss 0.44168, dsc 0.55832\n",
      "Batch train [14] loss 0.41097, dsc 0.58903\n",
      "Batch train [15] loss 0.42973, dsc 0.57027\n",
      "Batch train [16] loss 0.36343, dsc 0.63657\n",
      "Batch train [17] loss 0.41353, dsc 0.58647\n",
      "Batch train [18] loss 0.39506, dsc 0.60494\n",
      "Batch train [19] loss 0.40367, dsc 0.59633\n",
      "Batch train [20] loss 0.39903, dsc 0.60097\n",
      "Batch train [21] loss 0.39394, dsc 0.60606\n",
      "Batch train [22] loss 0.38055, dsc 0.61945\n",
      "Batch train [23] loss 0.40460, dsc 0.59540\n",
      "Batch train [24] loss 0.37636, dsc 0.62364\n",
      "Batch train [25] loss 0.46191, dsc 0.53809\n",
      "Batch train [26] loss 0.40849, dsc 0.59151\n",
      "Batch train [27] loss 0.37804, dsc 0.62196\n",
      "Batch train [28] loss 0.39001, dsc 0.60999\n",
      "Batch train [29] loss 0.36630, dsc 0.63370\n",
      "Batch train [30] loss 0.38041, dsc 0.61959\n",
      "Batch train [31] loss 0.36478, dsc 0.63522\n",
      "Batch train [32] loss 0.38999, dsc 0.61001\n",
      "Batch train [33] loss 0.39831, dsc 0.60169\n",
      "Batch train [34] loss 0.40507, dsc 0.59493\n",
      "Batch train [35] loss 0.40421, dsc 0.59579\n",
      "Batch train [36] loss 0.38831, dsc 0.61169\n",
      "Batch train [37] loss 0.38463, dsc 0.61537\n",
      "Batch train [38] loss 0.47016, dsc 0.52984\n",
      "Batch train [39] loss 0.36891, dsc 0.63109\n",
      "Batch train [40] loss 0.41368, dsc 0.58632\n",
      "Epoch [21] train done\n",
      "DEBUG: Writing to tensorboard epoch 20, step 0\n",
      "Batch eval [1] loss 0.40559, dsc 0.59441\n",
      "Batch eval [2] loss 0.44140, dsc 0.55860\n",
      "Batch eval [3] loss 0.50670, dsc 0.49330\n",
      "Batch eval [4] loss 0.45917, dsc 0.54083\n",
      "Batch eval [5] loss 0.49997, dsc 0.50003\n",
      "Epoch [21] valid done\n",
      "Epoch [21] T 926.70s, deltaT 43.95s, loss: train 0.40155, valid 0.46257, dsc: train 0.59845, valid 0.53743\n",
      "DEBUG: Writing to tensorboard epoch 21, step 0\n",
      "Batch train [1] loss 0.41705, dsc 0.58295\n",
      "Batch train [2] loss 0.37321, dsc 0.62679\n",
      "Batch train [3] loss 0.43519, dsc 0.56481\n",
      "Batch train [4] loss 0.39465, dsc 0.60535\n",
      "Batch train [5] loss 0.38992, dsc 0.61008\n",
      "Batch train [6] loss 0.39157, dsc 0.60843\n",
      "Batch train [7] loss 0.36040, dsc 0.63960\n",
      "Batch train [8] loss 0.40935, dsc 0.59065\n",
      "Batch train [9] loss 0.36278, dsc 0.63722\n",
      "Batch train [10] loss 0.43153, dsc 0.56847\n",
      "Batch train [11] loss 0.36754, dsc 0.63246\n",
      "Batch train [12] loss 0.37192, dsc 0.62808\n",
      "Batch train [13] loss 0.38131, dsc 0.61869\n",
      "Batch train [14] loss 0.35830, dsc 0.64170\n",
      "Batch train [15] loss 0.36320, dsc 0.63680\n",
      "Batch train [16] loss 0.38225, dsc 0.61775\n",
      "Batch train [17] loss 0.34561, dsc 0.65439\n",
      "Batch train [18] loss 0.35809, dsc 0.64191\n",
      "Batch train [19] loss 0.38015, dsc 0.61985\n",
      "Batch train [20] loss 0.38203, dsc 0.61797\n",
      "Batch train [21] loss 0.44251, dsc 0.55749\n",
      "Batch train [22] loss 0.39169, dsc 0.60831\n",
      "Batch train [23] loss 0.40868, dsc 0.59132\n",
      "Batch train [24] loss 0.36819, dsc 0.63181\n",
      "Batch train [25] loss 0.36623, dsc 0.63377\n",
      "Batch train [26] loss 0.36881, dsc 0.63119\n",
      "Batch train [27] loss 0.35500, dsc 0.64500\n",
      "Batch train [28] loss 0.39701, dsc 0.60299\n",
      "Batch train [29] loss 0.37183, dsc 0.62817\n",
      "Batch train [30] loss 0.41570, dsc 0.58430\n",
      "Batch train [31] loss 0.39155, dsc 0.60845\n",
      "Batch train [32] loss 0.50376, dsc 0.49624\n",
      "Batch train [33] loss 0.42040, dsc 0.57960\n",
      "Batch train [34] loss 0.36078, dsc 0.63922\n",
      "Batch train [35] loss 0.36232, dsc 0.63768\n",
      "Batch train [36] loss 0.42362, dsc 0.57638\n",
      "Batch train [37] loss 0.39860, dsc 0.60140\n",
      "Batch train [38] loss 0.36269, dsc 0.63731\n",
      "Batch train [39] loss 0.37846, dsc 0.62154\n",
      "Batch train [40] loss 0.39814, dsc 0.60186\n",
      "Epoch [22] train done\n",
      "DEBUG: Writing to tensorboard epoch 21, step 0\n",
      "Batch eval [1] loss 0.45947, dsc 0.54053\n",
      "Batch eval [2] loss 0.41258, dsc 0.58742\n",
      "Batch eval [3] loss 0.43252, dsc 0.56748\n",
      "Batch eval [4] loss 0.45044, dsc 0.54956\n",
      "Batch eval [5] loss 0.40753, dsc 0.59247\n",
      "Epoch [22] valid done\n",
      "Epoch [22] T 970.67s, deltaT 43.97s, loss: train 0.38855, valid 0.43251, dsc: train 0.61145, valid 0.56749\n",
      "DEBUG: Writing to tensorboard epoch 22, step 0\n",
      "Batch train [1] loss 0.36850, dsc 0.63150\n",
      "Batch train [2] loss 0.43407, dsc 0.56593\n",
      "Batch train [3] loss 0.41414, dsc 0.58586\n",
      "Batch train [4] loss 0.32312, dsc 0.67688\n",
      "Batch train [5] loss 0.35256, dsc 0.64744\n",
      "Batch train [6] loss 0.42224, dsc 0.57776\n",
      "Batch train [7] loss 0.43860, dsc 0.56140\n",
      "Batch train [8] loss 0.40298, dsc 0.59702\n",
      "Batch train [9] loss 0.35815, dsc 0.64185\n",
      "Batch train [10] loss 0.41509, dsc 0.58491\n",
      "Batch train [11] loss 0.36588, dsc 0.63412\n",
      "Batch train [12] loss 0.38630, dsc 0.61370\n",
      "Batch train [13] loss 0.34333, dsc 0.65667\n",
      "Batch train [14] loss 0.35901, dsc 0.64099\n",
      "Batch train [15] loss 0.40568, dsc 0.59432\n",
      "Batch train [16] loss 0.36752, dsc 0.63248\n",
      "Batch train [17] loss 0.38631, dsc 0.61369\n",
      "Batch train [18] loss 0.43759, dsc 0.56241\n",
      "Batch train [19] loss 0.37756, dsc 0.62244\n",
      "Batch train [20] loss 0.34207, dsc 0.65793\n",
      "Batch train [21] loss 0.40330, dsc 0.59670\n",
      "Batch train [22] loss 0.40948, dsc 0.59052\n",
      "Batch train [23] loss 0.40167, dsc 0.59833\n",
      "Batch train [24] loss 0.38200, dsc 0.61800\n",
      "Batch train [25] loss 0.39804, dsc 0.60196\n",
      "Batch train [26] loss 0.35715, dsc 0.64285\n",
      "Batch train [27] loss 0.36874, dsc 0.63126\n",
      "Batch train [28] loss 0.40791, dsc 0.59209\n",
      "Batch train [29] loss 0.45433, dsc 0.54567\n",
      "Batch train [30] loss 0.38635, dsc 0.61365\n",
      "Batch train [31] loss 0.37666, dsc 0.62334\n",
      "Batch train [32] loss 0.37032, dsc 0.62968\n",
      "Batch train [33] loss 0.37663, dsc 0.62337\n",
      "Batch train [34] loss 0.36779, dsc 0.63221\n",
      "Batch train [35] loss 0.39368, dsc 0.60632\n",
      "Batch train [36] loss 0.34759, dsc 0.65241\n",
      "Batch train [37] loss 0.36492, dsc 0.63508\n",
      "Batch train [38] loss 0.34355, dsc 0.65645\n",
      "Batch train [39] loss 0.36947, dsc 0.63053\n",
      "Batch train [40] loss 0.39393, dsc 0.60607\n",
      "Epoch [23] train done\n",
      "DEBUG: Writing to tensorboard epoch 22, step 0\n",
      "Batch eval [1] loss 0.36038, dsc 0.63962\n",
      "Batch eval [2] loss 0.39201, dsc 0.60799\n",
      "Batch eval [3] loss 0.37008, dsc 0.62992\n",
      "Batch eval [4] loss 0.47173, dsc 0.52827\n",
      "Batch eval [5] loss 0.40559, dsc 0.59441\n",
      "Epoch [23] valid done\n",
      "Epoch [23] T 1014.49s, deltaT 43.81s, loss: train 0.38436, valid 0.39996, dsc: train 0.61564, valid 0.60004\n",
      "DEBUG: Writing to tensorboard epoch 23, step 0\n",
      "Batch train [1] loss 0.36221, dsc 0.63779\n",
      "Batch train [2] loss 0.35943, dsc 0.64057\n",
      "Batch train [3] loss 0.40113, dsc 0.59887\n",
      "Batch train [4] loss 0.33817, dsc 0.66183\n",
      "Batch train [5] loss 0.47552, dsc 0.52448\n",
      "Batch train [6] loss 0.39002, dsc 0.60998\n",
      "Batch train [7] loss 0.35234, dsc 0.64766\n",
      "Batch train [8] loss 0.34972, dsc 0.65028\n",
      "Batch train [9] loss 0.34723, dsc 0.65277\n",
      "Batch train [10] loss 0.35652, dsc 0.64348\n",
      "Batch train [11] loss 0.36351, dsc 0.63649\n",
      "Batch train [12] loss 0.38675, dsc 0.61325\n",
      "Batch train [13] loss 0.36345, dsc 0.63655\n",
      "Batch train [14] loss 0.38332, dsc 0.61668\n",
      "Batch train [15] loss 0.37756, dsc 0.62244\n",
      "Batch train [16] loss 0.36842, dsc 0.63158\n",
      "Batch train [17] loss 0.38778, dsc 0.61222\n",
      "Batch train [18] loss 0.35513, dsc 0.64487\n",
      "Batch train [19] loss 0.33884, dsc 0.66116\n",
      "Batch train [20] loss 0.32107, dsc 0.67893\n",
      "Batch train [21] loss 0.36948, dsc 0.63052\n",
      "Batch train [22] loss 0.35625, dsc 0.64375\n",
      "Batch train [23] loss 0.43664, dsc 0.56336\n",
      "Batch train [24] loss 0.40979, dsc 0.59021\n",
      "Batch train [25] loss 0.40596, dsc 0.59404\n",
      "Batch train [26] loss 0.35988, dsc 0.64012\n",
      "Batch train [27] loss 0.36571, dsc 0.63429\n",
      "Batch train [28] loss 0.39142, dsc 0.60858\n",
      "Batch train [29] loss 0.34306, dsc 0.65694\n",
      "Batch train [30] loss 0.37544, dsc 0.62456\n",
      "Batch train [31] loss 0.38747, dsc 0.61253\n",
      "Batch train [32] loss 0.40249, dsc 0.59751\n",
      "Batch train [33] loss 0.35573, dsc 0.64427\n",
      "Batch train [34] loss 0.37389, dsc 0.62611\n",
      "Batch train [35] loss 0.44170, dsc 0.55830\n",
      "Batch train [36] loss 0.37337, dsc 0.62663\n",
      "Batch train [37] loss 0.41347, dsc 0.58653\n",
      "Batch train [38] loss 0.33919, dsc 0.66081\n",
      "Batch train [39] loss 0.41205, dsc 0.58795\n",
      "Batch train [40] loss 0.38030, dsc 0.61970\n",
      "Epoch [24] train done\n",
      "DEBUG: Writing to tensorboard epoch 23, step 0\n",
      "Batch eval [1] loss 0.37777, dsc 0.62223\n",
      "Batch eval [2] loss 0.37573, dsc 0.62427\n",
      "Batch eval [3] loss 0.39023, dsc 0.60977\n",
      "Batch eval [4] loss 0.40635, dsc 0.59365\n",
      "Batch eval [5] loss 0.39181, dsc 0.60819\n",
      "Epoch [24] valid done\n",
      "Epoch [24] T 1066.67s, deltaT 52.17s, loss: train 0.37679, valid 0.38838, dsc: train 0.62321, valid 0.61162\n",
      "DEBUG: Writing to tensorboard epoch 24, step 0\n",
      "Batch train [1] loss 0.37576, dsc 0.62424\n",
      "Batch train [2] loss 0.34389, dsc 0.65611\n",
      "Batch train [3] loss 0.36535, dsc 0.63465\n",
      "Batch train [4] loss 0.35454, dsc 0.64546\n",
      "Batch train [5] loss 0.38094, dsc 0.61906\n",
      "Batch train [6] loss 0.36304, dsc 0.63696\n",
      "Batch train [7] loss 0.43637, dsc 0.56363\n",
      "Batch train [8] loss 0.36827, dsc 0.63173\n",
      "Batch train [9] loss 0.37549, dsc 0.62451\n",
      "Batch train [10] loss 0.37897, dsc 0.62103\n",
      "Batch train [11] loss 0.36704, dsc 0.63296\n",
      "Batch train [12] loss 0.38395, dsc 0.61605\n",
      "Batch train [13] loss 0.45194, dsc 0.54806\n",
      "Batch train [14] loss 0.36449, dsc 0.63551\n",
      "Batch train [15] loss 0.40973, dsc 0.59027\n",
      "Batch train [16] loss 0.35973, dsc 0.64027\n",
      "Batch train [17] loss 0.37570, dsc 0.62430\n",
      "Batch train [18] loss 0.34939, dsc 0.65061\n",
      "Batch train [19] loss 0.39830, dsc 0.60170\n",
      "Batch train [20] loss 0.36168, dsc 0.63832\n",
      "Batch train [21] loss 0.39747, dsc 0.60253\n",
      "Batch train [22] loss 0.35080, dsc 0.64920\n",
      "Batch train [23] loss 0.32609, dsc 0.67391\n",
      "Batch train [24] loss 0.35952, dsc 0.64048\n",
      "Batch train [25] loss 0.35531, dsc 0.64469\n",
      "Batch train [26] loss 0.35434, dsc 0.64566\n",
      "Batch train [27] loss 0.35942, dsc 0.64058\n",
      "Batch train [28] loss 0.35099, dsc 0.64901\n",
      "Batch train [29] loss 0.38454, dsc 0.61546\n",
      "Batch train [30] loss 0.44359, dsc 0.55641\n",
      "Batch train [31] loss 0.33452, dsc 0.66548\n",
      "Batch train [32] loss 0.39712, dsc 0.60288\n",
      "Batch train [33] loss 0.37797, dsc 0.62203\n",
      "Batch train [34] loss 0.37341, dsc 0.62659\n",
      "Batch train [35] loss 0.33584, dsc 0.66416\n",
      "Batch train [36] loss 0.37766, dsc 0.62234\n",
      "Batch train [37] loss 0.38610, dsc 0.61390\n",
      "Batch train [38] loss 0.35571, dsc 0.64429\n",
      "Batch train [39] loss 0.36109, dsc 0.63891\n",
      "Batch train [40] loss 0.38976, dsc 0.61024\n",
      "Epoch [25] train done\n",
      "DEBUG: Writing to tensorboard epoch 24, step 0\n",
      "Batch eval [1] loss 0.37522, dsc 0.62478\n",
      "Batch eval [2] loss 0.39924, dsc 0.60076\n",
      "Batch eval [3] loss 0.36732, dsc 0.63268\n",
      "Batch eval [4] loss 0.44638, dsc 0.55362\n",
      "Batch eval [5] loss 0.37912, dsc 0.62088\n",
      "Epoch [25] valid done\n",
      "Epoch [25] T 1110.72s, deltaT 44.05s, loss: train 0.37340, valid 0.39346, dsc: train 0.62660, valid 0.60654\n",
      "DEBUG: Writing to tensorboard epoch 25, step 0\n",
      "Batch train [1] loss 0.37479, dsc 0.62521\n",
      "Batch train [2] loss 0.35710, dsc 0.64290\n",
      "Batch train [3] loss 0.33451, dsc 0.66549\n",
      "Batch train [4] loss 0.34646, dsc 0.65354\n",
      "Batch train [5] loss 0.35920, dsc 0.64080\n",
      "Batch train [6] loss 0.38644, dsc 0.61356\n",
      "Batch train [7] loss 0.36884, dsc 0.63116\n",
      "Batch train [8] loss 0.37325, dsc 0.62675\n",
      "Batch train [9] loss 0.35620, dsc 0.64380\n",
      "Batch train [10] loss 0.33279, dsc 0.66721\n",
      "Batch train [11] loss 0.31948, dsc 0.68052\n",
      "Batch train [12] loss 0.33868, dsc 0.66132\n",
      "Batch train [13] loss 0.37542, dsc 0.62458\n",
      "Batch train [14] loss 0.38514, dsc 0.61486\n",
      "Batch train [15] loss 0.42126, dsc 0.57874\n",
      "Batch train [16] loss 0.31334, dsc 0.68666\n",
      "Batch train [17] loss 0.37538, dsc 0.62462\n",
      "Batch train [18] loss 0.33800, dsc 0.66200\n",
      "Batch train [19] loss 0.33057, dsc 0.66943\n",
      "Batch train [20] loss 0.36081, dsc 0.63919\n",
      "Batch train [21] loss 0.39925, dsc 0.60075\n",
      "Batch train [22] loss 0.38447, dsc 0.61553\n",
      "Batch train [23] loss 0.36844, dsc 0.63156\n",
      "Batch train [24] loss 0.34383, dsc 0.65617\n",
      "Batch train [25] loss 0.38704, dsc 0.61296\n",
      "Batch train [26] loss 0.36949, dsc 0.63051\n",
      "Batch train [27] loss 0.37202, dsc 0.62798\n",
      "Batch train [28] loss 0.39821, dsc 0.60179\n",
      "Batch train [29] loss 0.39392, dsc 0.60608\n",
      "Batch train [30] loss 0.35038, dsc 0.64962\n",
      "Batch train [31] loss 0.32527, dsc 0.67473\n",
      "Batch train [32] loss 0.38022, dsc 0.61978\n",
      "Batch train [33] loss 0.36666, dsc 0.63334\n",
      "Batch train [34] loss 0.37500, dsc 0.62500\n",
      "Batch train [35] loss 0.37920, dsc 0.62080\n",
      "Batch train [36] loss 0.34884, dsc 0.65116\n",
      "Batch train [37] loss 0.35626, dsc 0.64374\n",
      "Batch train [38] loss 0.39090, dsc 0.60910\n",
      "Batch train [39] loss 0.34412, dsc 0.65588\n",
      "Batch train [40] loss 0.41818, dsc 0.58182\n",
      "Epoch [26] train done\n",
      "DEBUG: Writing to tensorboard epoch 25, step 0\n",
      "Batch eval [1] loss 0.36211, dsc 0.63789\n",
      "Batch eval [2] loss 0.40464, dsc 0.59536\n",
      "Batch eval [3] loss 0.36348, dsc 0.63652\n",
      "Batch eval [4] loss 0.41292, dsc 0.58708\n",
      "Batch eval [5] loss 0.40518, dsc 0.59482\n",
      "Epoch [26] valid done\n",
      "Epoch [26] T 1154.51s, deltaT 43.78s, loss: train 0.36498, valid 0.38967, dsc: train 0.63502, valid 0.61033\n",
      "DEBUG: Writing to tensorboard epoch 26, step 0\n",
      "Batch train [1] loss 0.34893, dsc 0.65107\n",
      "Batch train [2] loss 0.37163, dsc 0.62837\n",
      "Batch train [3] loss 0.41708, dsc 0.58292\n",
      "Batch train [4] loss 0.40751, dsc 0.59249\n",
      "Batch train [5] loss 0.35794, dsc 0.64206\n",
      "Batch train [6] loss 0.35367, dsc 0.64633\n",
      "Batch train [7] loss 0.36294, dsc 0.63706\n",
      "Batch train [8] loss 0.36617, dsc 0.63383\n",
      "Batch train [9] loss 0.34041, dsc 0.65959\n",
      "Batch train [10] loss 0.36587, dsc 0.63413\n",
      "Batch train [11] loss 0.35991, dsc 0.64009\n",
      "Batch train [12] loss 0.36230, dsc 0.63770\n",
      "Batch train [13] loss 0.39552, dsc 0.60448\n",
      "Batch train [14] loss 0.34762, dsc 0.65238\n",
      "Batch train [15] loss 0.37729, dsc 0.62271\n",
      "Batch train [16] loss 0.37708, dsc 0.62292\n",
      "Batch train [17] loss 0.38078, dsc 0.61922\n",
      "Batch train [18] loss 0.33954, dsc 0.66046\n",
      "Batch train [19] loss 0.35838, dsc 0.64162\n",
      "Batch train [20] loss 0.37044, dsc 0.62956\n",
      "Batch train [21] loss 0.32354, dsc 0.67646\n",
      "Batch train [22] loss 0.36760, dsc 0.63240\n",
      "Batch train [23] loss 0.38212, dsc 0.61788\n",
      "Batch train [24] loss 0.36000, dsc 0.64000\n",
      "Batch train [25] loss 0.34325, dsc 0.65675\n",
      "Batch train [26] loss 0.33417, dsc 0.66583\n",
      "Batch train [27] loss 0.38690, dsc 0.61310\n",
      "Batch train [28] loss 0.31059, dsc 0.68941\n",
      "Batch train [29] loss 0.35698, dsc 0.64302\n",
      "Batch train [30] loss 0.35008, dsc 0.64992\n",
      "Batch train [31] loss 0.35763, dsc 0.64237\n",
      "Batch train [32] loss 0.36013, dsc 0.63987\n",
      "Batch train [33] loss 0.42992, dsc 0.57008\n",
      "Batch train [34] loss 0.32473, dsc 0.67527\n",
      "Batch train [35] loss 0.33546, dsc 0.66454\n",
      "Batch train [36] loss 0.39608, dsc 0.60392\n",
      "Batch train [37] loss 0.36429, dsc 0.63571\n",
      "Batch train [38] loss 0.35964, dsc 0.64036\n",
      "Batch train [39] loss 0.38245, dsc 0.61755\n",
      "Batch train [40] loss 0.33019, dsc 0.66981\n",
      "Epoch [27] train done\n",
      "DEBUG: Writing to tensorboard epoch 26, step 0\n",
      "Batch eval [1] loss 0.34768, dsc 0.65232\n",
      "Batch eval [2] loss 0.39045, dsc 0.60955\n",
      "Batch eval [3] loss 0.36329, dsc 0.63671\n",
      "Batch eval [4] loss 0.40784, dsc 0.59216\n",
      "Batch eval [5] loss 0.38113, dsc 0.61887\n",
      "Epoch [27] valid done\n",
      "Epoch [27] T 1198.40s, deltaT 43.88s, loss: train 0.36292, valid 0.37808, dsc: train 0.63708, valid 0.62192\n",
      "DEBUG: Writing to tensorboard epoch 27, step 0\n",
      "Batch train [1] loss 0.37212, dsc 0.62788\n",
      "Batch train [2] loss 0.33337, dsc 0.66663\n",
      "Batch train [3] loss 0.34149, dsc 0.65851\n",
      "Batch train [4] loss 0.34905, dsc 0.65095\n",
      "Batch train [5] loss 0.34587, dsc 0.65413\n",
      "Batch train [6] loss 0.36007, dsc 0.63993\n",
      "Batch train [7] loss 0.36488, dsc 0.63512\n",
      "Batch train [8] loss 0.41470, dsc 0.58530\n",
      "Batch train [9] loss 0.35319, dsc 0.64681\n",
      "Batch train [10] loss 0.36347, dsc 0.63653\n",
      "Batch train [11] loss 0.37035, dsc 0.62965\n",
      "Batch train [12] loss 0.37421, dsc 0.62579\n",
      "Batch train [13] loss 0.39472, dsc 0.60528\n",
      "Batch train [14] loss 0.36573, dsc 0.63427\n",
      "Batch train [15] loss 0.33243, dsc 0.66757\n",
      "Batch train [16] loss 0.35395, dsc 0.64605\n",
      "Batch train [17] loss 0.31668, dsc 0.68332\n",
      "Batch train [18] loss 0.38119, dsc 0.61881\n",
      "Batch train [19] loss 0.34078, dsc 0.65922\n",
      "Batch train [20] loss 0.36760, dsc 0.63240\n",
      "Batch train [21] loss 0.37655, dsc 0.62345\n",
      "Batch train [22] loss 0.36786, dsc 0.63214\n",
      "Batch train [23] loss 0.36880, dsc 0.63120\n",
      "Batch train [24] loss 0.36773, dsc 0.63227\n",
      "Batch train [25] loss 0.36483, dsc 0.63517\n",
      "Batch train [26] loss 0.35876, dsc 0.64124\n",
      "Batch train [27] loss 0.33254, dsc 0.66746\n",
      "Batch train [28] loss 0.37356, dsc 0.62644\n",
      "Batch train [29] loss 0.34752, dsc 0.65248\n",
      "Batch train [30] loss 0.33093, dsc 0.66907\n",
      "Batch train [31] loss 0.33122, dsc 0.66878\n",
      "Batch train [32] loss 0.39447, dsc 0.60553\n",
      "Batch train [33] loss 0.37550, dsc 0.62450\n",
      "Batch train [34] loss 0.45156, dsc 0.54844\n",
      "Batch train [35] loss 0.36114, dsc 0.63886\n",
      "Batch train [36] loss 0.35535, dsc 0.64465\n",
      "Batch train [37] loss 0.38324, dsc 0.61676\n",
      "Batch train [38] loss 0.36330, dsc 0.63670\n",
      "Batch train [39] loss 0.38692, dsc 0.61308\n",
      "Batch train [40] loss 0.35044, dsc 0.64956\n",
      "Epoch [28] train done\n",
      "DEBUG: Writing to tensorboard epoch 27, step 0\n",
      "Batch eval [1] loss 0.52504, dsc 0.47496\n",
      "Batch eval [2] loss 0.56703, dsc 0.43297\n",
      "Batch eval [3] loss 0.58175, dsc 0.41825\n",
      "Batch eval [4] loss 0.44960, dsc 0.55040\n",
      "Batch eval [5] loss 0.40000, dsc 0.60000\n",
      "Epoch [28] valid done\n",
      "Epoch [28] T 1241.78s, deltaT 43.38s, loss: train 0.36345, valid 0.50468, dsc: train 0.63655, valid 0.49532\n",
      "DEBUG: Writing to tensorboard epoch 28, step 0\n",
      "Batch train [1] loss 0.35689, dsc 0.64311\n",
      "Batch train [2] loss 0.37435, dsc 0.62565\n",
      "Batch train [3] loss 0.36041, dsc 0.63959\n",
      "Batch train [4] loss 0.33626, dsc 0.66374\n",
      "Batch train [5] loss 0.39294, dsc 0.60706\n",
      "Batch train [6] loss 0.39945, dsc 0.60055\n",
      "Batch train [7] loss 0.32595, dsc 0.67405\n",
      "Batch train [8] loss 0.35097, dsc 0.64903\n",
      "Batch train [9] loss 0.38099, dsc 0.61901\n",
      "Batch train [10] loss 0.38156, dsc 0.61844\n",
      "Batch train [11] loss 0.38011, dsc 0.61989\n",
      "Batch train [12] loss 0.36032, dsc 0.63968\n",
      "Batch train [13] loss 0.33550, dsc 0.66450\n",
      "Batch train [14] loss 0.34677, dsc 0.65323\n",
      "Batch train [15] loss 0.37161, dsc 0.62839\n",
      "Batch train [16] loss 0.35418, dsc 0.64582\n",
      "Batch train [17] loss 0.36550, dsc 0.63450\n",
      "Batch train [18] loss 0.33473, dsc 0.66527\n",
      "Batch train [19] loss 0.36292, dsc 0.63708\n",
      "Batch train [20] loss 0.31863, dsc 0.68137\n",
      "Batch train [21] loss 0.32353, dsc 0.67647\n",
      "Batch train [22] loss 0.32777, dsc 0.67223\n",
      "Batch train [23] loss 0.37280, dsc 0.62720\n",
      "Batch train [24] loss 0.37328, dsc 0.62672\n",
      "Batch train [25] loss 0.41838, dsc 0.58162\n",
      "Batch train [26] loss 0.34322, dsc 0.65678\n",
      "Batch train [27] loss 0.36713, dsc 0.63287\n",
      "Batch train [28] loss 0.36426, dsc 0.63574\n",
      "Batch train [29] loss 0.33901, dsc 0.66099\n",
      "Batch train [30] loss 0.31634, dsc 0.68366\n",
      "Batch train [31] loss 0.33744, dsc 0.66256\n",
      "Batch train [32] loss 0.37793, dsc 0.62207\n",
      "Batch train [33] loss 0.34236, dsc 0.65764\n",
      "Batch train [34] loss 0.42246, dsc 0.57754\n",
      "Batch train [35] loss 0.39222, dsc 0.60778\n",
      "Batch train [36] loss 0.33614, dsc 0.66386\n",
      "Batch train [37] loss 0.36536, dsc 0.63464\n",
      "Batch train [38] loss 0.36575, dsc 0.63425\n",
      "Batch train [39] loss 0.32719, dsc 0.67281\n",
      "Batch train [40] loss 0.33116, dsc 0.66884\n",
      "Epoch [29] train done\n",
      "DEBUG: Writing to tensorboard epoch 28, step 0\n",
      "Batch eval [1] loss 0.35003, dsc 0.64997\n",
      "Batch eval [2] loss 0.36169, dsc 0.63831\n",
      "Batch eval [3] loss 0.34893, dsc 0.65107\n",
      "Batch eval [4] loss 0.41017, dsc 0.58983\n",
      "Batch eval [5] loss 0.37135, dsc 0.62865\n",
      "Epoch [29] valid done\n",
      "Epoch [29] T 1285.19s, deltaT 43.41s, loss: train 0.35834, valid 0.36843, dsc: train 0.64166, valid 0.63157\n",
      "DEBUG: Writing to tensorboard epoch 29, step 0\n",
      "Batch train [1] loss 0.36431, dsc 0.63569\n",
      "Batch train [2] loss 0.33599, dsc 0.66401\n",
      "Batch train [3] loss 0.35715, dsc 0.64285\n",
      "Batch train [4] loss 0.31226, dsc 0.68774\n",
      "Batch train [5] loss 0.34818, dsc 0.65182\n",
      "Batch train [6] loss 0.36504, dsc 0.63496\n",
      "Batch train [7] loss 0.32915, dsc 0.67085\n",
      "Batch train [8] loss 0.32632, dsc 0.67368\n",
      "Batch train [9] loss 0.39803, dsc 0.60197\n",
      "Batch train [10] loss 0.41244, dsc 0.58756\n",
      "Batch train [11] loss 0.39339, dsc 0.60661\n",
      "Batch train [12] loss 0.37875, dsc 0.62125\n",
      "Batch train [13] loss 0.36140, dsc 0.63860\n",
      "Batch train [14] loss 0.35512, dsc 0.64488\n",
      "Batch train [15] loss 0.35422, dsc 0.64578\n",
      "Batch train [16] loss 0.34293, dsc 0.65707\n",
      "Batch train [17] loss 0.36588, dsc 0.63412\n",
      "Batch train [18] loss 0.37368, dsc 0.62632\n",
      "Batch train [19] loss 0.36094, dsc 0.63906\n",
      "Batch train [20] loss 0.35056, dsc 0.64944\n",
      "Batch train [21] loss 0.34774, dsc 0.65226\n",
      "Batch train [22] loss 0.35424, dsc 0.64576\n",
      "Batch train [23] loss 0.38536, dsc 0.61464\n",
      "Batch train [24] loss 0.35276, dsc 0.64724\n",
      "Batch train [25] loss 0.37572, dsc 0.62428\n",
      "Batch train [26] loss 0.33621, dsc 0.66379\n",
      "Batch train [27] loss 0.36778, dsc 0.63222\n",
      "Batch train [28] loss 0.32856, dsc 0.67144\n",
      "Batch train [29] loss 0.33049, dsc 0.66951\n",
      "Batch train [30] loss 0.30649, dsc 0.69351\n",
      "Batch train [31] loss 0.34390, dsc 0.65610\n",
      "Batch train [32] loss 0.37959, dsc 0.62041\n",
      "Batch train [33] loss 0.34042, dsc 0.65958\n",
      "Batch train [34] loss 0.36071, dsc 0.63929\n",
      "Batch train [35] loss 0.35833, dsc 0.64167\n",
      "Batch train [36] loss 0.30421, dsc 0.69579\n",
      "Batch train [37] loss 0.39740, dsc 0.60260\n",
      "Batch train [38] loss 0.34233, dsc 0.65767\n",
      "Batch train [39] loss 0.36159, dsc 0.63841\n",
      "Batch train [40] loss 0.42097, dsc 0.57903\n",
      "Epoch [30] train done\n",
      "DEBUG: Writing to tensorboard epoch 29, step 0\n",
      "Batch eval [1] loss 0.35688, dsc 0.64312\n",
      "Batch eval [2] loss 0.36984, dsc 0.63016\n",
      "Batch eval [3] loss 0.38455, dsc 0.61545\n",
      "Batch eval [4] loss 0.42077, dsc 0.57923\n",
      "Batch eval [5] loss 0.38458, dsc 0.61542\n",
      "Epoch [30] valid done\n",
      "Epoch [30] T 1329.59s, deltaT 44.39s, loss: train 0.35701, valid 0.38332, dsc: train 0.64299, valid 0.61668\n",
      "DEBUG: Writing to tensorboard epoch 30, step 0\n",
      "Batch train [1] loss 0.34523, dsc 0.65477\n",
      "Batch train [2] loss 0.33506, dsc 0.66494\n",
      "Batch train [3] loss 0.37057, dsc 0.62943\n",
      "Batch train [4] loss 0.33694, dsc 0.66306\n",
      "Batch train [5] loss 0.33176, dsc 0.66824\n",
      "Batch train [6] loss 0.33819, dsc 0.66181\n",
      "Batch train [7] loss 0.36733, dsc 0.63267\n",
      "Batch train [8] loss 0.34575, dsc 0.65425\n",
      "Batch train [9] loss 0.33045, dsc 0.66955\n",
      "Batch train [10] loss 0.37571, dsc 0.62429\n",
      "Batch train [11] loss 0.34799, dsc 0.65201\n",
      "Batch train [12] loss 0.33191, dsc 0.66809\n",
      "Batch train [13] loss 0.34558, dsc 0.65442\n",
      "Batch train [14] loss 0.30647, dsc 0.69353\n",
      "Batch train [15] loss 0.38055, dsc 0.61945\n",
      "Batch train [16] loss 0.34142, dsc 0.65858\n",
      "Batch train [17] loss 0.29495, dsc 0.70505\n",
      "Batch train [18] loss 0.35250, dsc 0.64750\n",
      "Batch train [19] loss 0.33748, dsc 0.66252\n",
      "Batch train [20] loss 0.33644, dsc 0.66356\n",
      "Batch train [21] loss 0.35419, dsc 0.64581\n",
      "Batch train [22] loss 0.32404, dsc 0.67596\n",
      "Batch train [23] loss 0.38702, dsc 0.61298\n",
      "Batch train [24] loss 0.35286, dsc 0.64714\n",
      "Batch train [25] loss 0.35787, dsc 0.64213\n",
      "Batch train [26] loss 0.38188, dsc 0.61812\n",
      "Batch train [27] loss 0.35237, dsc 0.64763\n",
      "Batch train [28] loss 0.36377, dsc 0.63623\n",
      "Batch train [29] loss 0.37215, dsc 0.62785\n",
      "Batch train [30] loss 0.36797, dsc 0.63203\n",
      "Batch train [31] loss 0.33720, dsc 0.66280\n",
      "Batch train [32] loss 0.31086, dsc 0.68914\n",
      "Batch train [33] loss 0.36041, dsc 0.63959\n",
      "Batch train [34] loss 0.36915, dsc 0.63085\n",
      "Batch train [35] loss 0.33033, dsc 0.66967\n",
      "Batch train [36] loss 0.32541, dsc 0.67459\n",
      "Batch train [37] loss 0.33258, dsc 0.66742\n",
      "Batch train [38] loss 0.31853, dsc 0.68147\n",
      "Batch train [39] loss 0.34894, dsc 0.65106\n",
      "Batch train [40] loss 0.39960, dsc 0.60040\n",
      "Epoch [31] train done\n",
      "DEBUG: Writing to tensorboard epoch 30, step 0\n",
      "Batch eval [1] loss 0.37106, dsc 0.62894\n",
      "Batch eval [2] loss 0.37839, dsc 0.62161\n",
      "Batch eval [3] loss 0.43941, dsc 0.56059\n",
      "Batch eval [4] loss 0.42174, dsc 0.57826\n",
      "Batch eval [5] loss 0.40519, dsc 0.59481\n",
      "Epoch [31] valid done\n",
      "Epoch [31] T 1373.28s, deltaT 43.69s, loss: train 0.34749, valid 0.40316, dsc: train 0.65251, valid 0.59684\n",
      "DEBUG: Writing to tensorboard epoch 31, step 0\n",
      "Batch train [1] loss 0.32490, dsc 0.67510\n",
      "Batch train [2] loss 0.33905, dsc 0.66095\n",
      "Batch train [3] loss 0.32979, dsc 0.67021\n",
      "Batch train [4] loss 0.37122, dsc 0.62878\n",
      "Batch train [5] loss 0.33111, dsc 0.66889\n",
      "Batch train [6] loss 0.33373, dsc 0.66627\n",
      "Batch train [7] loss 0.30095, dsc 0.69905\n",
      "Batch train [8] loss 0.36336, dsc 0.63664\n",
      "Batch train [9] loss 0.34613, dsc 0.65387\n",
      "Batch train [10] loss 0.41753, dsc 0.58247\n",
      "Batch train [11] loss 0.37436, dsc 0.62564\n",
      "Batch train [12] loss 0.32999, dsc 0.67001\n",
      "Batch train [13] loss 0.33168, dsc 0.66832\n",
      "Batch train [14] loss 0.36429, dsc 0.63571\n",
      "Batch train [15] loss 0.33730, dsc 0.66270\n",
      "Batch train [16] loss 0.31896, dsc 0.68104\n",
      "Batch train [17] loss 0.34371, dsc 0.65629\n",
      "Batch train [18] loss 0.32202, dsc 0.67798\n",
      "Batch train [19] loss 0.34245, dsc 0.65755\n",
      "Batch train [20] loss 0.32362, dsc 0.67638\n",
      "Batch train [21] loss 0.33001, dsc 0.66999\n",
      "Batch train [22] loss 0.34679, dsc 0.65321\n",
      "Batch train [23] loss 0.33578, dsc 0.66422\n",
      "Batch train [24] loss 0.32853, dsc 0.67147\n",
      "Batch train [25] loss 0.34563, dsc 0.65437\n",
      "Batch train [26] loss 0.36234, dsc 0.63766\n",
      "Batch train [27] loss 0.33713, dsc 0.66287\n",
      "Batch train [28] loss 0.34715, dsc 0.65285\n",
      "Batch train [29] loss 0.38590, dsc 0.61410\n",
      "Batch train [30] loss 0.32494, dsc 0.67506\n",
      "Batch train [31] loss 0.39514, dsc 0.60486\n",
      "Batch train [32] loss 0.35624, dsc 0.64376\n",
      "Batch train [33] loss 0.34728, dsc 0.65272\n",
      "Batch train [34] loss 0.41149, dsc 0.58851\n",
      "Batch train [35] loss 0.37521, dsc 0.62479\n",
      "Batch train [36] loss 0.37006, dsc 0.62994\n",
      "Batch train [37] loss 0.35899, dsc 0.64101\n",
      "Batch train [38] loss 0.32337, dsc 0.67663\n",
      "Batch train [39] loss 0.36536, dsc 0.63464\n",
      "Batch train [40] loss 0.37374, dsc 0.62626\n",
      "Epoch [32] train done\n",
      "DEBUG: Writing to tensorboard epoch 31, step 0\n",
      "Batch eval [1] loss 0.34821, dsc 0.65179\n",
      "Batch eval [2] loss 0.37069, dsc 0.62931\n",
      "Batch eval [3] loss 0.37830, dsc 0.62170\n",
      "Batch eval [4] loss 0.40434, dsc 0.59566\n",
      "Batch eval [5] loss 0.35788, dsc 0.64212\n",
      "Epoch [32] valid done\n",
      "Epoch [32] T 1416.94s, deltaT 43.66s, loss: train 0.34918, valid 0.37189, dsc: train 0.65082, valid 0.62811\n",
      "DEBUG: Writing to tensorboard epoch 32, step 0\n",
      "Batch train [1] loss 0.30910, dsc 0.69090\n",
      "Batch train [2] loss 0.30106, dsc 0.69894\n",
      "Batch train [3] loss 0.32229, dsc 0.67771\n",
      "Batch train [4] loss 0.34704, dsc 0.65296\n",
      "Batch train [5] loss 0.34134, dsc 0.65866\n",
      "Batch train [6] loss 0.30221, dsc 0.69779\n",
      "Batch train [7] loss 0.36535, dsc 0.63465\n",
      "Batch train [8] loss 0.37862, dsc 0.62138\n",
      "Batch train [9] loss 0.39864, dsc 0.60136\n",
      "Batch train [10] loss 0.34088, dsc 0.65912\n",
      "Batch train [11] loss 0.33030, dsc 0.66970\n",
      "Batch train [12] loss 0.41449, dsc 0.58551\n",
      "Batch train [13] loss 0.36295, dsc 0.63705\n",
      "Batch train [14] loss 0.33527, dsc 0.66473\n",
      "Batch train [15] loss 0.36850, dsc 0.63150\n",
      "Batch train [16] loss 0.35302, dsc 0.64698\n",
      "Batch train [17] loss 0.30700, dsc 0.69300\n",
      "Batch train [18] loss 0.34944, dsc 0.65056\n",
      "Batch train [19] loss 0.35978, dsc 0.64022\n",
      "Batch train [20] loss 0.34051, dsc 0.65949\n",
      "Batch train [21] loss 0.32210, dsc 0.67790\n",
      "Batch train [22] loss 0.33620, dsc 0.66380\n",
      "Batch train [23] loss 0.35374, dsc 0.64626\n",
      "Batch train [24] loss 0.31933, dsc 0.68067\n",
      "Batch train [25] loss 0.33888, dsc 0.66112\n",
      "Batch train [26] loss 0.36593, dsc 0.63407\n",
      "Batch train [27] loss 0.34955, dsc 0.65045\n",
      "Batch train [28] loss 0.35911, dsc 0.64089\n",
      "Batch train [29] loss 0.33994, dsc 0.66006\n",
      "Batch train [30] loss 0.33965, dsc 0.66035\n",
      "Batch train [31] loss 0.31142, dsc 0.68858\n",
      "Batch train [32] loss 0.35870, dsc 0.64130\n",
      "Batch train [33] loss 0.33583, dsc 0.66417\n",
      "Batch train [34] loss 0.33014, dsc 0.66986\n",
      "Batch train [35] loss 0.32580, dsc 0.67420\n",
      "Batch train [36] loss 0.34146, dsc 0.65854\n",
      "Batch train [37] loss 0.32626, dsc 0.67374\n",
      "Batch train [38] loss 0.34044, dsc 0.65956\n",
      "Batch train [39] loss 0.39872, dsc 0.60128\n",
      "Batch train [40] loss 0.43830, dsc 0.56170\n",
      "Epoch [33] train done\n",
      "DEBUG: Writing to tensorboard epoch 32, step 0\n",
      "Batch eval [1] loss 0.34397, dsc 0.65603\n",
      "Batch eval [2] loss 0.39599, dsc 0.60401\n",
      "Batch eval [3] loss 0.32418, dsc 0.67582\n",
      "Batch eval [4] loss 0.44088, dsc 0.55912\n",
      "Batch eval [5] loss 0.35745, dsc 0.64255\n",
      "Epoch [33] valid done\n",
      "Epoch [33] T 1460.62s, deltaT 43.67s, loss: train 0.34648, valid 0.37249, dsc: train 0.65352, valid 0.62751\n",
      "DEBUG: Writing to tensorboard epoch 33, step 0\n",
      "Batch train [1] loss 0.40391, dsc 0.59609\n",
      "Batch train [2] loss 0.29706, dsc 0.70294\n",
      "Batch train [3] loss 0.36883, dsc 0.63117\n",
      "Batch train [4] loss 0.35338, dsc 0.64662\n",
      "Batch train [5] loss 0.40273, dsc 0.59727\n",
      "Batch train [6] loss 0.32964, dsc 0.67036\n",
      "Batch train [7] loss 0.34554, dsc 0.65446\n",
      "Batch train [8] loss 0.33074, dsc 0.66926\n",
      "Batch train [9] loss 0.35351, dsc 0.64649\n",
      "Batch train [10] loss 0.35336, dsc 0.64664\n",
      "Batch train [11] loss 0.34648, dsc 0.65352\n",
      "Batch train [12] loss 0.30525, dsc 0.69475\n",
      "Batch train [13] loss 0.34790, dsc 0.65210\n",
      "Batch train [14] loss 0.36298, dsc 0.63702\n",
      "Batch train [15] loss 0.32183, dsc 0.67817\n",
      "Batch train [16] loss 0.38976, dsc 0.61024\n",
      "Batch train [17] loss 0.34079, dsc 0.65921\n",
      "Batch train [18] loss 0.29248, dsc 0.70752\n",
      "Batch train [19] loss 0.35669, dsc 0.64331\n",
      "Batch train [20] loss 0.35390, dsc 0.64610\n",
      "Batch train [21] loss 0.36552, dsc 0.63448\n",
      "Batch train [22] loss 0.31126, dsc 0.68874\n",
      "Batch train [23] loss 0.31051, dsc 0.68949\n",
      "Batch train [24] loss 0.36817, dsc 0.63183\n",
      "Batch train [25] loss 0.32931, dsc 0.67069\n",
      "Batch train [26] loss 0.36663, dsc 0.63337\n",
      "Batch train [27] loss 0.38034, dsc 0.61966\n",
      "Batch train [28] loss 0.34271, dsc 0.65729\n",
      "Batch train [29] loss 0.34098, dsc 0.65902\n",
      "Batch train [30] loss 0.35589, dsc 0.64411\n",
      "Batch train [31] loss 0.33530, dsc 0.66470\n",
      "Batch train [32] loss 0.36484, dsc 0.63516\n",
      "Batch train [33] loss 0.36334, dsc 0.63666\n",
      "Batch train [34] loss 0.35984, dsc 0.64016\n",
      "Batch train [35] loss 0.38641, dsc 0.61359\n",
      "Batch train [36] loss 0.33538, dsc 0.66462\n",
      "Batch train [37] loss 0.34619, dsc 0.65381\n",
      "Batch train [38] loss 0.35099, dsc 0.64901\n",
      "Batch train [39] loss 0.32242, dsc 0.67758\n",
      "Batch train [40] loss 0.33414, dsc 0.66586\n",
      "Epoch [34] train done\n",
      "DEBUG: Writing to tensorboard epoch 33, step 0\n",
      "Batch eval [1] loss 0.34248, dsc 0.65752\n",
      "Batch eval [2] loss 0.36927, dsc 0.63073\n",
      "Batch eval [3] loss 0.36576, dsc 0.63424\n",
      "Batch eval [4] loss 0.40660, dsc 0.59340\n",
      "Batch eval [5] loss 0.37319, dsc 0.62681\n",
      "Epoch [34] valid done\n",
      "Epoch [34] T 1504.24s, deltaT 43.61s, loss: train 0.34817, valid 0.37146, dsc: train 0.65183, valid 0.62854\n",
      "DEBUG: Writing to tensorboard epoch 34, step 0\n",
      "Batch train [1] loss 0.31646, dsc 0.68354\n",
      "Batch train [2] loss 0.36295, dsc 0.63705\n",
      "Batch train [3] loss 0.35739, dsc 0.64261\n",
      "Batch train [4] loss 0.35307, dsc 0.64693\n",
      "Batch train [5] loss 0.30245, dsc 0.69755\n",
      "Batch train [6] loss 0.32328, dsc 0.67672\n",
      "Batch train [7] loss 0.34469, dsc 0.65531\n",
      "Batch train [8] loss 0.35051, dsc 0.64949\n",
      "Batch train [9] loss 0.36327, dsc 0.63673\n",
      "Batch train [10] loss 0.32412, dsc 0.67588\n",
      "Batch train [11] loss 0.33603, dsc 0.66397\n",
      "Batch train [12] loss 0.31961, dsc 0.68039\n",
      "Batch train [13] loss 0.33666, dsc 0.66334\n",
      "Batch train [14] loss 0.35031, dsc 0.64969\n",
      "Batch train [15] loss 0.35728, dsc 0.64272\n",
      "Batch train [16] loss 0.35280, dsc 0.64720\n",
      "Batch train [17] loss 0.34387, dsc 0.65613\n",
      "Batch train [18] loss 0.35742, dsc 0.64258\n",
      "Batch train [19] loss 0.32968, dsc 0.67032\n",
      "Batch train [20] loss 0.34377, dsc 0.65623\n",
      "Batch train [21] loss 0.32035, dsc 0.67965\n",
      "Batch train [22] loss 0.37703, dsc 0.62297\n",
      "Batch train [23] loss 0.36338, dsc 0.63662\n",
      "Batch train [24] loss 0.43470, dsc 0.56530\n",
      "Batch train [25] loss 0.30120, dsc 0.69880\n",
      "Batch train [26] loss 0.31342, dsc 0.68658\n",
      "Batch train [27] loss 0.32508, dsc 0.67492\n",
      "Batch train [28] loss 0.37215, dsc 0.62785\n",
      "Batch train [29] loss 0.35878, dsc 0.64122\n",
      "Batch train [30] loss 0.36434, dsc 0.63566\n",
      "Batch train [31] loss 0.34832, dsc 0.65168\n",
      "Batch train [32] loss 0.35344, dsc 0.64656\n",
      "Batch train [33] loss 0.38554, dsc 0.61446\n",
      "Batch train [34] loss 0.29985, dsc 0.70015\n",
      "Batch train [35] loss 0.33510, dsc 0.66490\n",
      "Batch train [36] loss 0.31334, dsc 0.68666\n",
      "Batch train [37] loss 0.38585, dsc 0.61415\n",
      "Batch train [38] loss 0.37308, dsc 0.62692\n",
      "Batch train [39] loss 0.34010, dsc 0.65990\n",
      "Batch train [40] loss 0.32993, dsc 0.67007\n",
      "Epoch [35] train done\n",
      "DEBUG: Writing to tensorboard epoch 34, step 0\n",
      "Batch eval [1] loss 0.33825, dsc 0.66175\n",
      "Batch eval [2] loss 0.35729, dsc 0.64271\n",
      "Batch eval [3] loss 0.41514, dsc 0.58486\n",
      "Batch eval [4] loss 0.39574, dsc 0.60426\n",
      "Batch eval [5] loss 0.36416, dsc 0.63584\n",
      "Epoch [35] valid done\n",
      "Epoch [35] T 1548.06s, deltaT 43.82s, loss: train 0.34551, valid 0.37412, dsc: train 0.65449, valid 0.62588\n",
      "DEBUG: Writing to tensorboard epoch 35, step 0\n",
      "Batch train [1] loss 0.36473, dsc 0.63527\n",
      "Batch train [2] loss 0.37456, dsc 0.62544\n",
      "Batch train [3] loss 0.31042, dsc 0.68958\n",
      "Batch train [4] loss 0.35027, dsc 0.64973\n",
      "Batch train [5] loss 0.36164, dsc 0.63836\n",
      "Batch train [6] loss 0.36903, dsc 0.63097\n",
      "Batch train [7] loss 0.33494, dsc 0.66506\n",
      "Batch train [8] loss 0.39826, dsc 0.60174\n",
      "Batch train [9] loss 0.30867, dsc 0.69133\n",
      "Batch train [10] loss 0.30089, dsc 0.69911\n",
      "Batch train [11] loss 0.33363, dsc 0.66637\n",
      "Batch train [12] loss 0.34554, dsc 0.65446\n",
      "Batch train [13] loss 0.33871, dsc 0.66129\n",
      "Batch train [14] loss 0.33517, dsc 0.66483\n",
      "Batch train [15] loss 0.34532, dsc 0.65468\n",
      "Batch train [16] loss 0.33060, dsc 0.66940\n",
      "Batch train [17] loss 0.36147, dsc 0.63853\n",
      "Batch train [18] loss 0.35104, dsc 0.64896\n",
      "Batch train [19] loss 0.34176, dsc 0.65824\n",
      "Batch train [20] loss 0.35902, dsc 0.64098\n",
      "Batch train [21] loss 0.36421, dsc 0.63579\n",
      "Batch train [22] loss 0.31093, dsc 0.68907\n",
      "Batch train [23] loss 0.36511, dsc 0.63489\n",
      "Batch train [24] loss 0.40581, dsc 0.59419\n",
      "Batch train [25] loss 0.35290, dsc 0.64710\n",
      "Batch train [26] loss 0.36548, dsc 0.63452\n",
      "Batch train [27] loss 0.32821, dsc 0.67179\n",
      "Batch train [28] loss 0.30441, dsc 0.69559\n",
      "Batch train [29] loss 0.37131, dsc 0.62869\n",
      "Batch train [30] loss 0.40396, dsc 0.59604\n",
      "Batch train [31] loss 0.31147, dsc 0.68853\n",
      "Batch train [32] loss 0.32616, dsc 0.67384\n",
      "Batch train [33] loss 0.40972, dsc 0.59028\n",
      "Batch train [34] loss 0.36250, dsc 0.63750\n",
      "Batch train [35] loss 0.32032, dsc 0.67968\n",
      "Batch train [36] loss 0.35831, dsc 0.64169\n",
      "Batch train [37] loss 0.31407, dsc 0.68593\n",
      "Batch train [38] loss 0.34172, dsc 0.65828\n",
      "Batch train [39] loss 0.32713, dsc 0.67287\n",
      "Batch train [40] loss 0.31558, dsc 0.68442\n",
      "Epoch [36] train done\n",
      "DEBUG: Writing to tensorboard epoch 35, step 0\n",
      "Batch eval [1] loss 0.32158, dsc 0.67842\n",
      "Batch eval [2] loss 0.37356, dsc 0.62644\n",
      "Batch eval [3] loss 0.37239, dsc 0.62761\n",
      "Batch eval [4] loss 0.39991, dsc 0.60009\n",
      "Batch eval [5] loss 0.35423, dsc 0.64577\n",
      "Epoch [36] valid done\n",
      "Epoch [36] T 1592.03s, deltaT 43.97s, loss: train 0.34687, valid 0.36434, dsc: train 0.65313, valid 0.63566\n",
      "DEBUG: Writing to tensorboard epoch 36, step 0\n",
      "Batch train [1] loss 0.31214, dsc 0.68786\n",
      "Batch train [2] loss 0.29621, dsc 0.70379\n",
      "Batch train [3] loss 0.35223, dsc 0.64777\n",
      "Batch train [4] loss 0.37101, dsc 0.62899\n",
      "Batch train [5] loss 0.32475, dsc 0.67525\n",
      "Batch train [6] loss 0.37893, dsc 0.62107\n",
      "Batch train [7] loss 0.36513, dsc 0.63487\n",
      "Batch train [8] loss 0.37041, dsc 0.62959\n",
      "Batch train [9] loss 0.35052, dsc 0.64948\n",
      "Batch train [10] loss 0.34268, dsc 0.65732\n",
      "Batch train [11] loss 0.33808, dsc 0.66192\n",
      "Batch train [12] loss 0.32648, dsc 0.67352\n",
      "Batch train [13] loss 0.32272, dsc 0.67728\n",
      "Batch train [14] loss 0.34290, dsc 0.65710\n",
      "Batch train [15] loss 0.32206, dsc 0.67794\n",
      "Batch train [16] loss 0.33386, dsc 0.66614\n",
      "Batch train [17] loss 0.31698, dsc 0.68302\n",
      "Batch train [18] loss 0.31259, dsc 0.68741\n",
      "Batch train [19] loss 0.36333, dsc 0.63667\n",
      "Batch train [20] loss 0.33870, dsc 0.66130\n",
      "Batch train [21] loss 0.38936, dsc 0.61064\n",
      "Batch train [22] loss 0.30727, dsc 0.69273\n",
      "Batch train [23] loss 0.33488, dsc 0.66512\n",
      "Batch train [24] loss 0.32382, dsc 0.67618\n",
      "Batch train [25] loss 0.32980, dsc 0.67020\n",
      "Batch train [26] loss 0.35245, dsc 0.64755\n",
      "Batch train [27] loss 0.34837, dsc 0.65163\n",
      "Batch train [28] loss 0.38207, dsc 0.61793\n",
      "Batch train [29] loss 0.34459, dsc 0.65541\n",
      "Batch train [30] loss 0.35127, dsc 0.64873\n",
      "Batch train [31] loss 0.32917, dsc 0.67083\n",
      "Batch train [32] loss 0.39522, dsc 0.60478\n",
      "Batch train [33] loss 0.31596, dsc 0.68404\n",
      "Batch train [34] loss 0.32107, dsc 0.67893\n",
      "Batch train [35] loss 0.39494, dsc 0.60506\n",
      "Batch train [36] loss 0.34475, dsc 0.65525\n",
      "Batch train [37] loss 0.40775, dsc 0.59225\n",
      "Batch train [38] loss 0.30616, dsc 0.69384\n",
      "Batch train [39] loss 0.31043, dsc 0.68957\n",
      "Batch train [40] loss 0.35634, dsc 0.64366\n",
      "Epoch [37] train done\n",
      "DEBUG: Writing to tensorboard epoch 36, step 0\n",
      "Batch eval [1] loss 0.40158, dsc 0.59842\n",
      "Batch eval [2] loss 0.39201, dsc 0.60799\n",
      "Batch eval [3] loss 0.50942, dsc 0.49058\n",
      "Batch eval [4] loss 0.38636, dsc 0.61364\n",
      "Batch eval [5] loss 0.41589, dsc 0.58411\n",
      "Epoch [37] valid done\n",
      "Epoch [37] T 1635.68s, deltaT 43.65s, loss: train 0.34318, valid 0.42105, dsc: train 0.65682, valid 0.57895\n",
      "DEBUG: Writing to tensorboard epoch 37, step 0\n",
      "Batch train [1] loss 0.32500, dsc 0.67500\n",
      "Batch train [2] loss 0.33225, dsc 0.66775\n",
      "Batch train [3] loss 0.29682, dsc 0.70318\n",
      "Batch train [4] loss 0.33394, dsc 0.66606\n",
      "Batch train [5] loss 0.33064, dsc 0.66936\n",
      "Batch train [6] loss 0.35850, dsc 0.64150\n",
      "Batch train [7] loss 0.32813, dsc 0.67187\n",
      "Batch train [8] loss 0.29419, dsc 0.70581\n",
      "Batch train [9] loss 0.33439, dsc 0.66561\n",
      "Batch train [10] loss 0.35553, dsc 0.64447\n",
      "Batch train [11] loss 0.33291, dsc 0.66709\n",
      "Batch train [12] loss 0.33853, dsc 0.66147\n",
      "Batch train [13] loss 0.46109, dsc 0.53891\n",
      "Batch train [14] loss 0.37595, dsc 0.62405\n",
      "Batch train [15] loss 0.35730, dsc 0.64270\n",
      "Batch train [16] loss 0.32217, dsc 0.67783\n",
      "Batch train [17] loss 0.31754, dsc 0.68246\n",
      "Batch train [18] loss 0.33370, dsc 0.66630\n",
      "Batch train [19] loss 0.38776, dsc 0.61224\n",
      "Batch train [20] loss 0.36870, dsc 0.63130\n",
      "Batch train [21] loss 0.36678, dsc 0.63322\n",
      "Batch train [22] loss 0.35394, dsc 0.64606\n",
      "Batch train [23] loss 0.31052, dsc 0.68948\n",
      "Batch train [24] loss 0.36014, dsc 0.63986\n",
      "Batch train [25] loss 0.34844, dsc 0.65156\n",
      "Batch train [26] loss 0.36995, dsc 0.63005\n",
      "Batch train [27] loss 0.40818, dsc 0.59182\n",
      "Batch train [28] loss 0.35940, dsc 0.64060\n",
      "Batch train [29] loss 0.34213, dsc 0.65787\n",
      "Batch train [30] loss 0.35687, dsc 0.64313\n",
      "Batch train [31] loss 0.32976, dsc 0.67024\n",
      "Batch train [32] loss 0.33485, dsc 0.66515\n",
      "Batch train [33] loss 0.30173, dsc 0.69827\n",
      "Batch train [34] loss 0.36157, dsc 0.63843\n",
      "Batch train [35] loss 0.32321, dsc 0.67679\n",
      "Batch train [36] loss 0.39111, dsc 0.60889\n",
      "Batch train [37] loss 0.37422, dsc 0.62578\n",
      "Batch train [38] loss 0.33214, dsc 0.66786\n",
      "Batch train [39] loss 0.36031, dsc 0.63969\n",
      "Batch train [40] loss 0.33273, dsc 0.66727\n",
      "Epoch [38] train done\n",
      "DEBUG: Writing to tensorboard epoch 37, step 0\n",
      "Batch eval [1] loss 0.34143, dsc 0.65857\n",
      "Batch eval [2] loss 0.36841, dsc 0.63159\n",
      "Batch eval [3] loss 0.41731, dsc 0.58269\n",
      "Batch eval [4] loss 0.39791, dsc 0.60209\n",
      "Batch eval [5] loss 0.38858, dsc 0.61142\n",
      "Epoch [38] valid done\n",
      "Epoch [38] T 1679.46s, deltaT 43.77s, loss: train 0.34758, valid 0.38273, dsc: train 0.65242, valid 0.61727\n",
      "DEBUG: Writing to tensorboard epoch 38, step 0\n",
      "Batch train [1] loss 0.36116, dsc 0.63884\n",
      "Batch train [2] loss 0.32957, dsc 0.67043\n",
      "Batch train [3] loss 0.34786, dsc 0.65214\n",
      "Batch train [4] loss 0.38123, dsc 0.61877\n",
      "Batch train [5] loss 0.33081, dsc 0.66919\n",
      "Batch train [6] loss 0.34054, dsc 0.65946\n",
      "Batch train [7] loss 0.31480, dsc 0.68520\n",
      "Batch train [8] loss 0.35961, dsc 0.64039\n",
      "Batch train [9] loss 0.35056, dsc 0.64944\n",
      "Batch train [10] loss 0.34027, dsc 0.65973\n",
      "Batch train [11] loss 0.30773, dsc 0.69227\n",
      "Batch train [12] loss 0.30937, dsc 0.69063\n",
      "Batch train [13] loss 0.32221, dsc 0.67779\n",
      "Batch train [14] loss 0.27901, dsc 0.72099\n",
      "Batch train [15] loss 0.38821, dsc 0.61179\n",
      "Batch train [16] loss 0.30554, dsc 0.69446\n",
      "Batch train [17] loss 0.35465, dsc 0.64535\n",
      "Batch train [18] loss 0.30035, dsc 0.69965\n",
      "Batch train [19] loss 0.29045, dsc 0.70955\n",
      "Batch train [20] loss 0.34325, dsc 0.65675\n",
      "Batch train [21] loss 0.33476, dsc 0.66524\n",
      "Batch train [22] loss 0.34765, dsc 0.65235\n",
      "Batch train [23] loss 0.36283, dsc 0.63717\n",
      "Batch train [24] loss 0.33073, dsc 0.66927\n",
      "Batch train [25] loss 0.34847, dsc 0.65153\n",
      "Batch train [26] loss 0.39356, dsc 0.60644\n",
      "Batch train [27] loss 0.31484, dsc 0.68516\n",
      "Batch train [28] loss 0.35029, dsc 0.64971\n",
      "Batch train [29] loss 0.30008, dsc 0.69992\n",
      "Batch train [30] loss 0.33568, dsc 0.66432\n",
      "Batch train [31] loss 0.32280, dsc 0.67720\n",
      "Batch train [32] loss 0.36400, dsc 0.63600\n",
      "Batch train [33] loss 0.34146, dsc 0.65854\n",
      "Batch train [34] loss 0.33463, dsc 0.66537\n",
      "Batch train [35] loss 0.36499, dsc 0.63501\n",
      "Batch train [36] loss 0.32626, dsc 0.67374\n",
      "Batch train [37] loss 0.36787, dsc 0.63213\n",
      "Batch train [38] loss 0.31226, dsc 0.68774\n",
      "Batch train [39] loss 0.34176, dsc 0.65824\n",
      "Batch train [40] loss 0.35521, dsc 0.64479\n",
      "Epoch [39] train done\n",
      "DEBUG: Writing to tensorboard epoch 38, step 0\n",
      "Batch eval [1] loss 0.32110, dsc 0.67890\n",
      "Batch eval [2] loss 0.38550, dsc 0.61450\n",
      "Batch eval [3] loss 0.35921, dsc 0.64079\n",
      "Batch eval [4] loss 0.39738, dsc 0.60262\n",
      "Batch eval [5] loss 0.36694, dsc 0.63306\n",
      "Epoch [39] valid done\n",
      "Epoch [39] T 1723.15s, deltaT 43.68s, loss: train 0.33768, valid 0.36602, dsc: train 0.66232, valid 0.63398\n",
      "DEBUG: Writing to tensorboard epoch 39, step 0\n",
      "Batch train [1] loss 0.34885, dsc 0.65115\n",
      "Batch train [2] loss 0.31398, dsc 0.68602\n",
      "Batch train [3] loss 0.36730, dsc 0.63270\n",
      "Batch train [4] loss 0.34221, dsc 0.65779\n",
      "Batch train [5] loss 0.33162, dsc 0.66838\n",
      "Batch train [6] loss 0.35058, dsc 0.64942\n",
      "Batch train [7] loss 0.40044, dsc 0.59956\n",
      "Batch train [8] loss 0.34606, dsc 0.65394\n",
      "Batch train [9] loss 0.32041, dsc 0.67959\n",
      "Batch train [10] loss 0.32074, dsc 0.67926\n",
      "Batch train [11] loss 0.31826, dsc 0.68174\n",
      "Batch train [12] loss 0.36507, dsc 0.63493\n",
      "Batch train [13] loss 0.36818, dsc 0.63182\n",
      "Batch train [14] loss 0.36206, dsc 0.63794\n",
      "Batch train [15] loss 0.35517, dsc 0.64483\n",
      "Batch train [16] loss 0.37093, dsc 0.62907\n",
      "Batch train [17] loss 0.39115, dsc 0.60885\n",
      "Batch train [18] loss 0.32773, dsc 0.67227\n",
      "Batch train [19] loss 0.33913, dsc 0.66087\n",
      "Batch train [20] loss 0.39283, dsc 0.60717\n",
      "Batch train [21] loss 0.32963, dsc 0.67037\n",
      "Batch train [22] loss 0.35995, dsc 0.64005\n",
      "Batch train [23] loss 0.33130, dsc 0.66870\n",
      "Batch train [24] loss 0.35964, dsc 0.64036\n",
      "Batch train [25] loss 0.32790, dsc 0.67210\n",
      "Batch train [26] loss 0.35836, dsc 0.64164\n",
      "Batch train [27] loss 0.35092, dsc 0.64908\n",
      "Batch train [28] loss 0.29446, dsc 0.70554\n",
      "Batch train [29] loss 0.32194, dsc 0.67806\n",
      "Batch train [30] loss 0.31275, dsc 0.68725\n",
      "Batch train [31] loss 0.35226, dsc 0.64774\n",
      "Batch train [32] loss 0.32116, dsc 0.67884\n",
      "Batch train [33] loss 0.37942, dsc 0.62058\n",
      "Batch train [34] loss 0.35746, dsc 0.64254\n",
      "Batch train [35] loss 0.30905, dsc 0.69095\n",
      "Batch train [36] loss 0.37626, dsc 0.62374\n",
      "Batch train [37] loss 0.33566, dsc 0.66434\n",
      "Batch train [38] loss 0.31757, dsc 0.68243\n",
      "Batch train [39] loss 0.31440, dsc 0.68560\n",
      "Batch train [40] loss 0.31909, dsc 0.68091\n",
      "Epoch [40] train done\n",
      "DEBUG: Writing to tensorboard epoch 39, step 0\n",
      "Batch eval [1] loss 0.30959, dsc 0.69041\n",
      "Batch eval [2] loss 0.35161, dsc 0.64839\n",
      "Batch eval [3] loss 0.41221, dsc 0.58779\n",
      "Batch eval [4] loss 0.38570, dsc 0.61430\n",
      "Batch eval [5] loss 0.37805, dsc 0.62195\n",
      "Epoch [40] valid done\n",
      "Epoch [40] T 1766.71s, deltaT 43.56s, loss: train 0.34405, valid 0.36743, dsc: train 0.65595, valid 0.63257\n",
      "DEBUG: Writing to tensorboard epoch 40, step 0\n",
      "Batch train [1] loss 0.36510, dsc 0.63490\n",
      "Batch train [2] loss 0.33023, dsc 0.66977\n",
      "Batch train [3] loss 0.34641, dsc 0.65359\n",
      "Batch train [4] loss 0.34972, dsc 0.65028\n",
      "Batch train [5] loss 0.34266, dsc 0.65734\n",
      "Batch train [6] loss 0.30754, dsc 0.69246\n",
      "Batch train [7] loss 0.31804, dsc 0.68196\n",
      "Batch train [8] loss 0.35604, dsc 0.64396\n",
      "Batch train [9] loss 0.29716, dsc 0.70284\n",
      "Batch train [10] loss 0.34526, dsc 0.65474\n",
      "Batch train [11] loss 0.30426, dsc 0.69574\n",
      "Batch train [12] loss 0.33393, dsc 0.66607\n",
      "Batch train [13] loss 0.35134, dsc 0.64866\n",
      "Batch train [14] loss 0.36885, dsc 0.63115\n",
      "Batch train [15] loss 0.32023, dsc 0.67977\n",
      "Batch train [16] loss 0.36329, dsc 0.63671\n",
      "Batch train [17] loss 0.32978, dsc 0.67022\n",
      "Batch train [18] loss 0.41734, dsc 0.58266\n",
      "Batch train [19] loss 0.34619, dsc 0.65381\n",
      "Batch train [20] loss 0.33974, dsc 0.66026\n",
      "Batch train [21] loss 0.42008, dsc 0.57992\n",
      "Batch train [22] loss 0.32311, dsc 0.67689\n",
      "Batch train [23] loss 0.34697, dsc 0.65303\n",
      "Batch train [24] loss 0.33878, dsc 0.66122\n",
      "Batch train [25] loss 0.34973, dsc 0.65027\n",
      "Batch train [26] loss 0.35164, dsc 0.64836\n",
      "Batch train [27] loss 0.30872, dsc 0.69128\n",
      "Batch train [28] loss 0.36038, dsc 0.63962\n",
      "Batch train [29] loss 0.33210, dsc 0.66790\n",
      "Batch train [30] loss 0.34721, dsc 0.65279\n",
      "Batch train [31] loss 0.33447, dsc 0.66553\n",
      "Batch train [32] loss 0.32127, dsc 0.67873\n",
      "Batch train [33] loss 0.36677, dsc 0.63323\n",
      "Batch train [34] loss 0.34156, dsc 0.65844\n",
      "Batch train [35] loss 0.30390, dsc 0.69610\n",
      "Batch train [36] loss 0.33286, dsc 0.66714\n",
      "Batch train [37] loss 0.35085, dsc 0.64915\n",
      "Batch train [38] loss 0.32165, dsc 0.67835\n",
      "Batch train [39] loss 0.35142, dsc 0.64858\n",
      "Batch train [40] loss 0.38817, dsc 0.61183\n",
      "Epoch [41] train done\n",
      "DEBUG: Writing to tensorboard epoch 40, step 0\n",
      "Batch eval [1] loss 0.38151, dsc 0.61849\n",
      "Batch eval [2] loss 0.39180, dsc 0.60820\n",
      "Batch eval [3] loss 0.37877, dsc 0.62123\n",
      "Batch eval [4] loss 0.42849, dsc 0.57151\n",
      "Batch eval [5] loss 0.38030, dsc 0.61970\n",
      "Epoch [41] valid done\n",
      "Epoch [41] T 1810.60s, deltaT 43.89s, loss: train 0.34312, valid 0.39217, dsc: train 0.65688, valid 0.60783\n",
      "DEBUG: Writing to tensorboard epoch 41, step 0\n",
      "Batch train [1] loss 0.30750, dsc 0.69250\n",
      "Batch train [2] loss 0.35644, dsc 0.64356\n",
      "Batch train [3] loss 0.36702, dsc 0.63298\n",
      "Batch train [4] loss 0.36065, dsc 0.63935\n",
      "Batch train [5] loss 0.33246, dsc 0.66754\n",
      "Batch train [6] loss 0.36007, dsc 0.63993\n",
      "Batch train [7] loss 0.33830, dsc 0.66170\n",
      "Batch train [8] loss 0.35760, dsc 0.64240\n",
      "Batch train [9] loss 0.30565, dsc 0.69435\n",
      "Batch train [10] loss 0.33818, dsc 0.66182\n",
      "Batch train [11] loss 0.35236, dsc 0.64764\n",
      "Batch train [12] loss 0.34960, dsc 0.65040\n",
      "Batch train [13] loss 0.33571, dsc 0.66429\n",
      "Batch train [14] loss 0.35611, dsc 0.64389\n",
      "Batch train [15] loss 0.33038, dsc 0.66962\n",
      "Batch train [16] loss 0.40200, dsc 0.59800\n",
      "Batch train [17] loss 0.31331, dsc 0.68669\n",
      "Batch train [18] loss 0.34410, dsc 0.65590\n",
      "Batch train [19] loss 0.30903, dsc 0.69097\n",
      "Batch train [20] loss 0.34297, dsc 0.65703\n",
      "Batch train [21] loss 0.32940, dsc 0.67060\n",
      "Batch train [22] loss 0.32160, dsc 0.67840\n",
      "Batch train [23] loss 0.33852, dsc 0.66148\n",
      "Batch train [24] loss 0.33792, dsc 0.66208\n",
      "Batch train [25] loss 0.35852, dsc 0.64148\n",
      "Batch train [26] loss 0.33584, dsc 0.66416\n",
      "Batch train [27] loss 0.38035, dsc 0.61965\n",
      "Batch train [28] loss 0.38563, dsc 0.61437\n",
      "Batch train [29] loss 0.34414, dsc 0.65586\n",
      "Batch train [30] loss 0.32681, dsc 0.67319\n",
      "Batch train [31] loss 0.45408, dsc 0.54592\n",
      "Batch train [32] loss 0.31079, dsc 0.68921\n",
      "Batch train [33] loss 0.31957, dsc 0.68043\n",
      "Batch train [34] loss 0.31462, dsc 0.68538\n",
      "Batch train [35] loss 0.34332, dsc 0.65668\n",
      "Batch train [36] loss 0.31704, dsc 0.68296\n",
      "Batch train [37] loss 0.35445, dsc 0.64555\n",
      "Batch train [38] loss 0.33551, dsc 0.66449\n",
      "Batch train [39] loss 0.31682, dsc 0.68318\n",
      "Batch train [40] loss 0.32079, dsc 0.67921\n",
      "Epoch [42] train done\n",
      "DEBUG: Writing to tensorboard epoch 41, step 0\n",
      "Batch eval [1] loss 0.33614, dsc 0.66386\n",
      "Batch eval [2] loss 0.39836, dsc 0.60164\n",
      "Batch eval [3] loss 0.40446, dsc 0.59554\n",
      "Batch eval [4] loss 0.42812, dsc 0.57188\n",
      "Batch eval [5] loss 0.38695, dsc 0.61305\n",
      "Epoch [42] valid done\n",
      "Epoch [42] T 1853.98s, deltaT 43.37s, loss: train 0.34263, valid 0.39080, dsc: train 0.65737, valid 0.60920\n",
      "DEBUG: Writing to tensorboard epoch 42, step 0\n",
      "Batch train [1] loss 0.33272, dsc 0.66728\n",
      "Batch train [2] loss 0.31254, dsc 0.68746\n",
      "Batch train [3] loss 0.33591, dsc 0.66409\n",
      "Batch train [4] loss 0.32278, dsc 0.67722\n",
      "Batch train [5] loss 0.33051, dsc 0.66949\n",
      "Batch train [6] loss 0.32681, dsc 0.67319\n",
      "Batch train [7] loss 0.33349, dsc 0.66651\n",
      "Batch train [8] loss 0.29285, dsc 0.70715\n",
      "Batch train [9] loss 0.31063, dsc 0.68937\n",
      "Batch train [10] loss 0.33325, dsc 0.66675\n",
      "Batch train [11] loss 0.28723, dsc 0.71277\n",
      "Batch train [12] loss 0.31090, dsc 0.68910\n",
      "Batch train [13] loss 0.34874, dsc 0.65126\n",
      "Batch train [14] loss 0.32656, dsc 0.67344\n",
      "Batch train [15] loss 0.34975, dsc 0.65025\n",
      "Batch train [16] loss 0.37162, dsc 0.62838\n",
      "Batch train [17] loss 0.38731, dsc 0.61269\n",
      "Batch train [18] loss 0.35615, dsc 0.64385\n",
      "Batch train [19] loss 0.32473, dsc 0.67527\n",
      "Batch train [20] loss 0.32943, dsc 0.67057\n",
      "Batch train [21] loss 0.35130, dsc 0.64870\n",
      "Batch train [22] loss 0.33826, dsc 0.66174\n",
      "Batch train [23] loss 0.38392, dsc 0.61608\n",
      "Batch train [24] loss 0.32772, dsc 0.67228\n",
      "Batch train [25] loss 0.33042, dsc 0.66958\n",
      "Batch train [26] loss 0.34673, dsc 0.65327\n",
      "Batch train [27] loss 0.37982, dsc 0.62018\n",
      "Batch train [28] loss 0.36050, dsc 0.63950\n",
      "Batch train [29] loss 0.33309, dsc 0.66691\n",
      "Batch train [30] loss 0.34010, dsc 0.65990\n",
      "Batch train [31] loss 0.37395, dsc 0.62605\n",
      "Batch train [32] loss 0.34405, dsc 0.65595\n",
      "Batch train [33] loss 0.35487, dsc 0.64513\n",
      "Batch train [34] loss 0.37804, dsc 0.62196\n",
      "Batch train [35] loss 0.33984, dsc 0.66016\n",
      "Batch train [36] loss 0.34220, dsc 0.65780\n",
      "Batch train [37] loss 0.37195, dsc 0.62805\n",
      "Batch train [38] loss 0.31631, dsc 0.68369\n",
      "Batch train [39] loss 0.32578, dsc 0.67422\n",
      "Batch train [40] loss 0.35230, dsc 0.64770\n",
      "Epoch [43] train done\n",
      "DEBUG: Writing to tensorboard epoch 42, step 0\n",
      "Batch eval [1] loss 0.37964, dsc 0.62036\n",
      "Batch eval [2] loss 0.38597, dsc 0.61403\n",
      "Batch eval [3] loss 0.37599, dsc 0.62401\n",
      "Batch eval [4] loss 0.42557, dsc 0.57443\n",
      "Batch eval [5] loss 0.36650, dsc 0.63350\n",
      "Epoch [43] valid done\n",
      "Epoch [43] T 1897.30s, deltaT 43.31s, loss: train 0.34038, valid 0.38673, dsc: train 0.65962, valid 0.61327\n",
      "DEBUG: Writing to tensorboard epoch 43, step 0\n",
      "Batch train [1] loss 0.36144, dsc 0.63856\n",
      "Batch train [2] loss 0.34246, dsc 0.65754\n",
      "Batch train [3] loss 0.34960, dsc 0.65040\n",
      "Batch train [4] loss 0.35737, dsc 0.64263\n",
      "Batch train [5] loss 0.33696, dsc 0.66304\n",
      "Batch train [6] loss 0.28261, dsc 0.71739\n",
      "Batch train [7] loss 0.35567, dsc 0.64433\n",
      "Batch train [8] loss 0.31341, dsc 0.68659\n",
      "Batch train [9] loss 0.34843, dsc 0.65157\n",
      "Batch train [10] loss 0.34231, dsc 0.65769\n",
      "Batch train [11] loss 0.32733, dsc 0.67267\n",
      "Batch train [12] loss 0.33360, dsc 0.66640\n",
      "Batch train [13] loss 0.33379, dsc 0.66621\n",
      "Batch train [14] loss 0.33439, dsc 0.66561\n",
      "Batch train [15] loss 0.40790, dsc 0.59210\n",
      "Batch train [16] loss 0.32512, dsc 0.67488\n",
      "Batch train [17] loss 0.35066, dsc 0.64934\n",
      "Batch train [18] loss 0.34238, dsc 0.65762\n",
      "Batch train [19] loss 0.32869, dsc 0.67131\n",
      "Batch train [20] loss 0.30380, dsc 0.69620\n",
      "Batch train [21] loss 0.29021, dsc 0.70979\n",
      "Batch train [22] loss 0.32845, dsc 0.67155\n",
      "Batch train [23] loss 0.34723, dsc 0.65277\n",
      "Batch train [24] loss 0.31369, dsc 0.68631\n",
      "Batch train [25] loss 0.30768, dsc 0.69232\n",
      "Batch train [26] loss 0.35473, dsc 0.64527\n",
      "Batch train [27] loss 0.28827, dsc 0.71173\n",
      "Batch train [28] loss 0.31771, dsc 0.68229\n",
      "Batch train [29] loss 0.32947, dsc 0.67053\n",
      "Batch train [30] loss 0.33952, dsc 0.66048\n",
      "Batch train [31] loss 0.34223, dsc 0.65777\n",
      "Batch train [32] loss 0.35044, dsc 0.64956\n",
      "Batch train [33] loss 0.40263, dsc 0.59737\n",
      "Batch train [34] loss 0.40235, dsc 0.59765\n",
      "Batch train [35] loss 0.32044, dsc 0.67956\n",
      "Batch train [36] loss 0.37415, dsc 0.62585\n",
      "Batch train [37] loss 0.34469, dsc 0.65531\n",
      "Batch train [38] loss 0.32171, dsc 0.67829\n",
      "Batch train [39] loss 0.33713, dsc 0.66287\n",
      "Batch train [40] loss 0.33377, dsc 0.66623\n",
      "Epoch [44] train done\n",
      "DEBUG: Writing to tensorboard epoch 43, step 0\n",
      "Batch eval [1] loss 0.33315, dsc 0.66685\n",
      "Batch eval [2] loss 0.37321, dsc 0.62679\n",
      "Batch eval [3] loss 0.35201, dsc 0.64799\n",
      "Batch eval [4] loss 0.39296, dsc 0.60704\n",
      "Batch eval [5] loss 0.37492, dsc 0.62508\n",
      "Epoch [44] valid done\n",
      "Epoch [44] T 1940.81s, deltaT 43.50s, loss: train 0.33811, valid 0.36525, dsc: train 0.66189, valid 0.63475\n",
      "DEBUG: Writing to tensorboard epoch 44, step 0\n",
      "Batch train [1] loss 0.35389, dsc 0.64611\n",
      "Batch train [2] loss 0.31085, dsc 0.68915\n",
      "Batch train [3] loss 0.27856, dsc 0.72144\n",
      "Batch train [4] loss 0.33012, dsc 0.66988\n",
      "Batch train [5] loss 0.33668, dsc 0.66332\n",
      "Batch train [6] loss 0.30162, dsc 0.69838\n",
      "Batch train [7] loss 0.30749, dsc 0.69251\n",
      "Batch train [8] loss 0.32717, dsc 0.67283\n",
      "Batch train [9] loss 0.35488, dsc 0.64512\n",
      "Batch train [10] loss 0.30622, dsc 0.69378\n",
      "Batch train [11] loss 0.30618, dsc 0.69382\n",
      "Batch train [12] loss 0.30412, dsc 0.69588\n",
      "Batch train [13] loss 0.28882, dsc 0.71118\n",
      "Batch train [14] loss 0.31817, dsc 0.68183\n",
      "Batch train [15] loss 0.32673, dsc 0.67327\n",
      "Batch train [16] loss 0.34770, dsc 0.65230\n",
      "Batch train [17] loss 0.31770, dsc 0.68230\n",
      "Batch train [18] loss 0.35195, dsc 0.64805\n",
      "Batch train [19] loss 0.33235, dsc 0.66765\n",
      "Batch train [20] loss 0.30375, dsc 0.69625\n",
      "Batch train [21] loss 0.35392, dsc 0.64608\n",
      "Batch train [22] loss 0.33627, dsc 0.66373\n",
      "Batch train [23] loss 0.29559, dsc 0.70441\n",
      "Batch train [24] loss 0.36449, dsc 0.63551\n",
      "Batch train [25] loss 0.34916, dsc 0.65084\n",
      "Batch train [26] loss 0.34449, dsc 0.65551\n",
      "Batch train [27] loss 0.36923, dsc 0.63077\n",
      "Batch train [28] loss 0.33617, dsc 0.66383\n",
      "Batch train [29] loss 0.34356, dsc 0.65644\n",
      "Batch train [30] loss 0.34131, dsc 0.65869\n",
      "Batch train [31] loss 0.28567, dsc 0.71433\n",
      "Batch train [32] loss 0.33475, dsc 0.66525\n",
      "Batch train [33] loss 0.29569, dsc 0.70431\n",
      "Batch train [34] loss 0.30841, dsc 0.69159\n",
      "Batch train [35] loss 0.32631, dsc 0.67369\n",
      "Batch train [36] loss 0.31727, dsc 0.68273\n",
      "Batch train [37] loss 0.31541, dsc 0.68459\n",
      "Batch train [38] loss 0.37089, dsc 0.62911\n",
      "Batch train [39] loss 0.35937, dsc 0.64063\n",
      "Batch train [40] loss 0.36329, dsc 0.63671\n",
      "Epoch [45] train done\n",
      "DEBUG: Writing to tensorboard epoch 44, step 0\n",
      "Batch eval [1] loss 0.33669, dsc 0.66331\n",
      "Batch eval [2] loss 0.35879, dsc 0.64121\n",
      "Batch eval [3] loss 0.38668, dsc 0.61332\n",
      "Batch eval [4] loss 0.37340, dsc 0.62660\n",
      "Batch eval [5] loss 0.35230, dsc 0.64770\n",
      "Epoch [45] valid done\n",
      "Epoch [45] T 1984.99s, deltaT 44.17s, loss: train 0.32790, valid 0.36157, dsc: train 0.67210, valid 0.63843\n",
      "DEBUG: Writing to tensorboard epoch 45, step 0\n",
      "Batch train [1] loss 0.29878, dsc 0.70122\n",
      "Batch train [2] loss 0.30286, dsc 0.69714\n",
      "Batch train [3] loss 0.33164, dsc 0.66836\n",
      "Batch train [4] loss 0.28307, dsc 0.71693\n",
      "Batch train [5] loss 0.35276, dsc 0.64724\n",
      "Batch train [6] loss 0.33663, dsc 0.66337\n",
      "Batch train [7] loss 0.32696, dsc 0.67304\n",
      "Batch train [8] loss 0.32282, dsc 0.67718\n",
      "Batch train [9] loss 0.38128, dsc 0.61872\n",
      "Batch train [10] loss 0.36294, dsc 0.63706\n",
      "Batch train [11] loss 0.34651, dsc 0.65349\n",
      "Batch train [12] loss 0.30251, dsc 0.69749\n",
      "Batch train [13] loss 0.32116, dsc 0.67884\n",
      "Batch train [14] loss 0.32694, dsc 0.67306\n",
      "Batch train [15] loss 0.33012, dsc 0.66988\n",
      "Batch train [16] loss 0.37645, dsc 0.62355\n",
      "Batch train [17] loss 0.35812, dsc 0.64188\n",
      "Batch train [18] loss 0.33320, dsc 0.66680\n",
      "Batch train [19] loss 0.31970, dsc 0.68030\n",
      "Batch train [20] loss 0.31798, dsc 0.68202\n",
      "Batch train [21] loss 0.36843, dsc 0.63157\n",
      "Batch train [22] loss 0.36345, dsc 0.63655\n",
      "Batch train [23] loss 0.31629, dsc 0.68371\n",
      "Batch train [24] loss 0.37077, dsc 0.62923\n",
      "Batch train [25] loss 0.29341, dsc 0.70659\n",
      "Batch train [26] loss 0.35849, dsc 0.64151\n",
      "Batch train [27] loss 0.34004, dsc 0.65996\n",
      "Batch train [28] loss 0.35549, dsc 0.64451\n",
      "Batch train [29] loss 0.31608, dsc 0.68392\n",
      "Batch train [30] loss 0.30591, dsc 0.69409\n",
      "Batch train [31] loss 0.28115, dsc 0.71885\n",
      "Batch train [32] loss 0.30605, dsc 0.69395\n",
      "Batch train [33] loss 0.33989, dsc 0.66011\n",
      "Batch train [34] loss 0.32906, dsc 0.67094\n",
      "Batch train [35] loss 0.33218, dsc 0.66782\n",
      "Batch train [36] loss 0.38105, dsc 0.61895\n",
      "Batch train [37] loss 0.31405, dsc 0.68595\n",
      "Batch train [38] loss 0.28472, dsc 0.71528\n",
      "Batch train [39] loss 0.31992, dsc 0.68008\n",
      "Batch train [40] loss 0.34207, dsc 0.65793\n",
      "Epoch [46] train done\n",
      "DEBUG: Writing to tensorboard epoch 45, step 0\n",
      "Batch eval [1] loss 0.36018, dsc 0.63982\n",
      "Batch eval [2] loss 0.39569, dsc 0.60431\n",
      "Batch eval [3] loss 0.39447, dsc 0.60553\n",
      "Batch eval [4] loss 0.36318, dsc 0.63682\n",
      "Batch eval [5] loss 0.38817, dsc 0.61183\n",
      "Epoch [46] valid done\n",
      "Epoch [46] T 2029.46s, deltaT 44.46s, loss: train 0.33127, valid 0.38034, dsc: train 0.66873, valid 0.61966\n",
      "DEBUG: Writing to tensorboard epoch 46, step 0\n",
      "Batch train [1] loss 0.38797, dsc 0.61203\n",
      "Batch train [2] loss 0.33208, dsc 0.66792\n",
      "Batch train [3] loss 0.35332, dsc 0.64668\n",
      "Batch train [4] loss 0.32980, dsc 0.67020\n",
      "Batch train [5] loss 0.36703, dsc 0.63297\n",
      "Batch train [6] loss 0.33474, dsc 0.66526\n",
      "Batch train [7] loss 0.34110, dsc 0.65890\n",
      "Batch train [8] loss 0.32268, dsc 0.67732\n",
      "Batch train [9] loss 0.30697, dsc 0.69303\n",
      "Batch train [10] loss 0.29346, dsc 0.70654\n",
      "Batch train [11] loss 0.29942, dsc 0.70058\n",
      "Batch train [12] loss 0.32291, dsc 0.67709\n",
      "Batch train [13] loss 0.28913, dsc 0.71087\n",
      "Batch train [14] loss 0.31782, dsc 0.68218\n",
      "Batch train [15] loss 0.31786, dsc 0.68214\n",
      "Batch train [16] loss 0.39530, dsc 0.60470\n",
      "Batch train [17] loss 0.33006, dsc 0.66994\n",
      "Batch train [18] loss 0.37140, dsc 0.62860\n",
      "Batch train [19] loss 0.31525, dsc 0.68475\n",
      "Batch train [20] loss 0.31517, dsc 0.68483\n",
      "Batch train [21] loss 0.34235, dsc 0.65765\n",
      "Batch train [22] loss 0.33891, dsc 0.66109\n",
      "Batch train [23] loss 0.36502, dsc 0.63498\n",
      "Batch train [24] loss 0.30806, dsc 0.69194\n",
      "Batch train [25] loss 0.30517, dsc 0.69483\n",
      "Batch train [26] loss 0.31936, dsc 0.68064\n",
      "Batch train [27] loss 0.34415, dsc 0.65585\n",
      "Batch train [28] loss 0.37304, dsc 0.62696\n",
      "Batch train [29] loss 0.33931, dsc 0.66069\n",
      "Batch train [30] loss 0.32528, dsc 0.67472\n",
      "Batch train [31] loss 0.38176, dsc 0.61824\n",
      "Batch train [32] loss 0.34261, dsc 0.65739\n",
      "Batch train [33] loss 0.35769, dsc 0.64231\n",
      "Batch train [34] loss 0.31340, dsc 0.68660\n",
      "Batch train [35] loss 0.29544, dsc 0.70456\n",
      "Batch train [36] loss 0.31648, dsc 0.68352\n",
      "Batch train [37] loss 0.33622, dsc 0.66378\n",
      "Batch train [38] loss 0.34378, dsc 0.65622\n",
      "Batch train [39] loss 0.36884, dsc 0.63116\n",
      "Batch train [40] loss 0.38721, dsc 0.61279\n",
      "Epoch [47] train done\n",
      "DEBUG: Writing to tensorboard epoch 46, step 0\n",
      "Batch eval [1] loss 0.35970, dsc 0.64030\n",
      "Batch eval [2] loss 0.35875, dsc 0.64125\n",
      "Batch eval [3] loss 0.38910, dsc 0.61090\n",
      "Batch eval [4] loss 0.39692, dsc 0.60308\n",
      "Batch eval [5] loss 0.37394, dsc 0.62606\n",
      "Epoch [47] valid done\n",
      "Epoch [47] T 2073.19s, deltaT 43.73s, loss: train 0.33619, valid 0.37568, dsc: train 0.66381, valid 0.62432\n",
      "DEBUG: Writing to tensorboard epoch 47, step 0\n",
      "Batch train [1] loss 0.31293, dsc 0.68707\n",
      "Batch train [2] loss 0.30690, dsc 0.69310\n",
      "Batch train [3] loss 0.32734, dsc 0.67266\n",
      "Batch train [4] loss 0.30750, dsc 0.69250\n",
      "Batch train [5] loss 0.32178, dsc 0.67822\n",
      "Batch train [6] loss 0.33470, dsc 0.66530\n",
      "Batch train [7] loss 0.32848, dsc 0.67152\n",
      "Batch train [8] loss 0.38701, dsc 0.61299\n",
      "Batch train [9] loss 0.35460, dsc 0.64540\n",
      "Batch train [10] loss 0.33584, dsc 0.66416\n",
      "Batch train [11] loss 0.28171, dsc 0.71829\n",
      "Batch train [12] loss 0.36955, dsc 0.63045\n",
      "Batch train [13] loss 0.33014, dsc 0.66986\n",
      "Batch train [14] loss 0.37362, dsc 0.62638\n",
      "Batch train [15] loss 0.35754, dsc 0.64246\n",
      "Batch train [16] loss 0.33682, dsc 0.66318\n",
      "Batch train [17] loss 0.32823, dsc 0.67177\n",
      "Batch train [18] loss 0.32982, dsc 0.67018\n",
      "Batch train [19] loss 0.31337, dsc 0.68663\n",
      "Batch train [20] loss 0.32351, dsc 0.67649\n",
      "Batch train [21] loss 0.33404, dsc 0.66596\n",
      "Batch train [22] loss 0.35470, dsc 0.64530\n",
      "Batch train [23] loss 0.30698, dsc 0.69302\n",
      "Batch train [24] loss 0.35779, dsc 0.64221\n",
      "Batch train [25] loss 0.31395, dsc 0.68605\n",
      "Batch train [26] loss 0.31603, dsc 0.68397\n",
      "Batch train [27] loss 0.42662, dsc 0.57338\n",
      "Batch train [28] loss 0.35850, dsc 0.64150\n",
      "Batch train [29] loss 0.35986, dsc 0.64014\n",
      "Batch train [30] loss 0.33239, dsc 0.66761\n",
      "Batch train [31] loss 0.32530, dsc 0.67470\n",
      "Batch train [32] loss 0.40954, dsc 0.59046\n",
      "Batch train [33] loss 0.32711, dsc 0.67289\n",
      "Batch train [34] loss 0.34562, dsc 0.65438\n",
      "Batch train [35] loss 0.35069, dsc 0.64931\n",
      "Batch train [36] loss 0.34707, dsc 0.65293\n",
      "Batch train [37] loss 0.34912, dsc 0.65088\n",
      "Batch train [38] loss 0.32301, dsc 0.67699\n",
      "Batch train [39] loss 0.30857, dsc 0.69143\n",
      "Batch train [40] loss 0.32493, dsc 0.67507\n",
      "Epoch [48] train done\n",
      "DEBUG: Writing to tensorboard epoch 47, step 0\n",
      "Batch eval [1] loss 0.37567, dsc 0.62433\n",
      "Batch eval [2] loss 0.36373, dsc 0.63627\n",
      "Batch eval [3] loss 0.40518, dsc 0.59482\n",
      "Batch eval [4] loss 0.37690, dsc 0.62310\n",
      "Batch eval [5] loss 0.37376, dsc 0.62624\n",
      "Epoch [48] valid done\n",
      "Epoch [48] T 2117.92s, deltaT 44.73s, loss: train 0.33833, valid 0.37905, dsc: train 0.66167, valid 0.62095\n",
      "DEBUG: Writing to tensorboard epoch 48, step 0\n",
      "Batch train [1] loss 0.34143, dsc 0.65857\n",
      "Batch train [2] loss 0.32062, dsc 0.67938\n",
      "Batch train [3] loss 0.31939, dsc 0.68061\n",
      "Batch train [4] loss 0.30575, dsc 0.69425\n",
      "Batch train [5] loss 0.31342, dsc 0.68658\n",
      "Batch train [6] loss 0.38372, dsc 0.61628\n",
      "Batch train [7] loss 0.29459, dsc 0.70541\n",
      "Batch train [8] loss 0.32536, dsc 0.67464\n",
      "Batch train [9] loss 0.28544, dsc 0.71456\n",
      "Batch train [10] loss 0.32552, dsc 0.67448\n",
      "Batch train [11] loss 0.30821, dsc 0.69179\n",
      "Batch train [12] loss 0.30032, dsc 0.69968\n",
      "Batch train [13] loss 0.29440, dsc 0.70560\n",
      "Batch train [14] loss 0.37709, dsc 0.62291\n",
      "Batch train [15] loss 0.31903, dsc 0.68097\n",
      "Batch train [16] loss 0.30098, dsc 0.69902\n",
      "Batch train [17] loss 0.33670, dsc 0.66330\n",
      "Batch train [18] loss 0.28331, dsc 0.71669\n",
      "Batch train [19] loss 0.33750, dsc 0.66250\n",
      "Batch train [20] loss 0.34199, dsc 0.65801\n",
      "Batch train [21] loss 0.33615, dsc 0.66385\n",
      "Batch train [22] loss 0.33773, dsc 0.66227\n",
      "Batch train [23] loss 0.36648, dsc 0.63352\n",
      "Batch train [24] loss 0.35860, dsc 0.64140\n",
      "Batch train [25] loss 0.33925, dsc 0.66075\n",
      "Batch train [26] loss 0.31378, dsc 0.68622\n",
      "Batch train [27] loss 0.31678, dsc 0.68322\n",
      "Batch train [28] loss 0.33816, dsc 0.66184\n",
      "Batch train [29] loss 0.37343, dsc 0.62657\n",
      "Batch train [30] loss 0.32268, dsc 0.67732\n",
      "Batch train [31] loss 0.35108, dsc 0.64892\n",
      "Batch train [32] loss 0.36477, dsc 0.63523\n",
      "Batch train [33] loss 0.33622, dsc 0.66378\n",
      "Batch train [34] loss 0.38613, dsc 0.61387\n",
      "Batch train [35] loss 0.31557, dsc 0.68443\n",
      "Batch train [36] loss 0.32026, dsc 0.67974\n",
      "Batch train [37] loss 0.32682, dsc 0.67318\n",
      "Batch train [38] loss 0.33653, dsc 0.66347\n",
      "Batch train [39] loss 0.38129, dsc 0.61871\n",
      "Batch train [40] loss 0.31883, dsc 0.68117\n",
      "Epoch [49] train done\n",
      "DEBUG: Writing to tensorboard epoch 48, step 0\n",
      "Batch eval [1] loss 0.33200, dsc 0.66800\n",
      "Batch eval [2] loss 0.36830, dsc 0.63170\n",
      "Batch eval [3] loss 0.36608, dsc 0.63392\n",
      "Batch eval [4] loss 0.39865, dsc 0.60135\n",
      "Batch eval [5] loss 0.37195, dsc 0.62805\n",
      "Epoch [49] valid done\n",
      "Epoch [49] T 2161.74s, deltaT 43.81s, loss: train 0.33138, valid 0.36740, dsc: train 0.66862, valid 0.63260\n",
      "DEBUG: Writing to tensorboard epoch 49, step 0\n",
      "Batch train [1] loss 0.31303, dsc 0.68697\n",
      "Batch train [2] loss 0.37952, dsc 0.62048\n",
      "Batch train [3] loss 0.38426, dsc 0.61574\n",
      "Batch train [4] loss 0.33949, dsc 0.66051\n",
      "Batch train [5] loss 0.33419, dsc 0.66581\n",
      "Batch train [6] loss 0.36616, dsc 0.63384\n",
      "Batch train [7] loss 0.30857, dsc 0.69143\n",
      "Batch train [8] loss 0.30999, dsc 0.69001\n",
      "Batch train [9] loss 0.30272, dsc 0.69728\n",
      "Batch train [10] loss 0.29802, dsc 0.70198\n",
      "Batch train [11] loss 0.36345, dsc 0.63655\n",
      "Batch train [12] loss 0.35293, dsc 0.64707\n",
      "Batch train [13] loss 0.34373, dsc 0.65627\n",
      "Batch train [14] loss 0.32568, dsc 0.67432\n",
      "Batch train [15] loss 0.32652, dsc 0.67348\n",
      "Batch train [16] loss 0.31661, dsc 0.68339\n",
      "Batch train [17] loss 0.32087, dsc 0.67913\n",
      "Batch train [18] loss 0.33655, dsc 0.66345\n",
      "Batch train [19] loss 0.29996, dsc 0.70004\n",
      "Batch train [20] loss 0.34260, dsc 0.65740\n",
      "Batch train [21] loss 0.30006, dsc 0.69994\n",
      "Batch train [22] loss 0.31782, dsc 0.68218\n",
      "Batch train [23] loss 0.30958, dsc 0.69042\n",
      "Batch train [24] loss 0.29366, dsc 0.70634\n",
      "Batch train [25] loss 0.37160, dsc 0.62840\n",
      "Batch train [26] loss 0.33835, dsc 0.66165\n",
      "Batch train [27] loss 0.34241, dsc 0.65759\n",
      "Batch train [28] loss 0.27333, dsc 0.72667\n",
      "Batch train [29] loss 0.28729, dsc 0.71271\n",
      "Batch train [30] loss 0.33104, dsc 0.66896\n",
      "Batch train [31] loss 0.37108, dsc 0.62892\n",
      "Batch train [32] loss 0.32412, dsc 0.67588\n",
      "Batch train [33] loss 0.36290, dsc 0.63710\n",
      "Batch train [34] loss 0.34966, dsc 0.65034\n",
      "Batch train [35] loss 0.31581, dsc 0.68419\n",
      "Batch train [36] loss 0.30468, dsc 0.69532\n",
      "Batch train [37] loss 0.35573, dsc 0.64427\n",
      "Batch train [38] loss 0.33840, dsc 0.66160\n",
      "Batch train [39] loss 0.36159, dsc 0.63841\n",
      "Batch train [40] loss 0.33006, dsc 0.66994\n",
      "Epoch [50] train done\n",
      "DEBUG: Writing to tensorboard epoch 49, step 0\n",
      "Batch eval [1] loss 0.32499, dsc 0.67501\n",
      "Batch eval [2] loss 0.39033, dsc 0.60967\n",
      "Batch eval [3] loss 0.34680, dsc 0.65320\n",
      "Batch eval [4] loss 0.37356, dsc 0.62644\n",
      "Batch eval [5] loss 0.35402, dsc 0.64598\n",
      "Epoch [50] valid done\n",
      "Epoch [50] T 2205.89s, deltaT 44.15s, loss: train 0.33110, valid 0.35794, dsc: train 0.66890, valid 0.64206\n",
      "Elapsed time 0:36:45\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.model_and_training.unet_architecture_v3 import UNetV3\n",
    "from src.model_and_training.unet_architecture_v3v1 import UNetV3v1\n",
    "from src.model_and_training.unet_architecture_v2 import UNetV2\n",
    "\n",
    "log_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f'{log_date}_3d_unet_model3'\n",
    "\n",
    "print(f'Training model with dataset MODEL3')\n",
    "print(f'folder \\'{model_name}\\'')\n",
    "model_info = prepare_model(epochs=50,\n",
    "                           learning_rate=3e-4,\n",
    "                           in_channels=8,\n",
    "                           input_data_channels=1,\n",
    "                           output_label_channels=1,\n",
    "                           dropout_rate=0.2,\n",
    "                           train_batch_size=1,\n",
    "                           model_name=model_name,\n",
    "                           train_dataset=train_dataset, \n",
    "                           valid_dataset=valid_dataset, \n",
    "                           test_dataset=test_dataset,\n",
    "                           model_class=UNetV3v1) # UNetV3, UNetV3v1\n",
    "show_model_info(model_info)\n",
    "print('\\n\\n')\n",
    "train_loop(model_info)\n",
    "print('\\n\\n')\n",
    "\n",
    "# UNetV3\n",
    "# Epoch [50] train done\n",
    "# Batch eval [1] loss 0.37472, dsc 0.62528\n",
    "# Batch eval [2] loss 0.39908, dsc 0.60092\n",
    "# Batch eval [3] loss 0.39951, dsc 0.60049\n",
    "# Batch eval [4] loss 0.38327, dsc 0.61673\n",
    "# Batch eval [5] loss 0.36132, dsc 0.63868\n",
    "# Epoch [50] valid done\n",
    "# Epoch [50] T 2145.86s, deltaT 43.58s, loss: train 0.33236, valid 0.38358, dsc: train 0.66764, valid 0.61642\n",
    "\n",
    "# UNetV2\n",
    "# Epoch [50] train done\n",
    "# Batch eval [1] loss 0.23973, dsc 0.76027\n",
    "# Batch eval [2] loss 0.24580, dsc 0.75420\n",
    "# Batch eval [3] loss 0.25798, dsc 0.74202\n",
    "# Batch eval [4] loss 0.24973, dsc 0.75027\n",
    "# Batch eval [5] loss 0.26769, dsc 0.73231\n",
    "# Epoch [50] valid done\n",
    "# Epoch [50] T 1410.82s, deltaT 28.02s, loss: train 0.20945, valid 0.25219, dsc: train 0.79055, valid 0.74781\n",
    "\n",
    "\n",
    "# UNetV3 - 20210301-115221_3d_unet_model3\n",
    "# Without channel expansion\n",
    "# Epoch [50] train done\n",
    "# Batch eval [1] loss 0.38633, dsc 0.61367\n",
    "# Batch eval [2] loss 0.37051, dsc 0.62949\n",
    "# Batch eval [3] loss 0.39343, dsc 0.60657\n",
    "# Batch eval [4] loss 0.37833, dsc 0.62167\n",
    "# Batch eval [5] loss 0.36725, dsc 0.63275\n",
    "# Epoch [50] valid done\n",
    "# Epoch [50] T 2363.05s, deltaT 42.47s, loss: train 0.33946, valid 0.37917, dsc: train 0.66054, valid 0.62083\n",
    "# Elapsed time 0:39:23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': UNetV3v1(\n",
       "   (dconv_down1): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "       (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU(inplace=True)\n",
       "       (3): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "       (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (5): ReLU(inplace=True)\n",
       "     )\n",
       "     (1): Dropout(p=0.2, inplace=False)\n",
       "   )\n",
       "   (cse_down1): Sequential(\n",
       "     (0): Linear(in_features=16, out_features=8, bias=True)\n",
       "     (1): Linear(in_features=8, out_features=16, bias=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (dconv_atten1): Sequential(\n",
       "     (0): Conv3d(16, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (pool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "   (atten_pool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "   (dconv_down2): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "       (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU(inplace=True)\n",
       "       (3): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "       (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (5): ReLU(inplace=True)\n",
       "     )\n",
       "     (1): Dropout(p=0.2, inplace=False)\n",
       "   )\n",
       "   (cse_down2): Sequential(\n",
       "     (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "     (1): Linear(in_features=16, out_features=32, bias=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (dconv_atten2): Sequential(\n",
       "     (0): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (dconv_merge_atten2): Sequential(\n",
       "     (0): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (pool2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "   (atten_pool2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "   (dconv_down3): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "       (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU(inplace=True)\n",
       "       (3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "       (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (5): ReLU(inplace=True)\n",
       "     )\n",
       "     (1): Dropout(p=0.2, inplace=False)\n",
       "   )\n",
       "   (cse_down3): Sequential(\n",
       "     (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (dconv_atten3): Sequential(\n",
       "     (0): Conv3d(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (dconv_merge_atten3): Sequential(\n",
       "     (0): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (pool3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "   (atten_pool3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "   (dconv_middle): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "       (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU(inplace=True)\n",
       "       (3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "       (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (5): ReLU(inplace=True)\n",
       "     )\n",
       "     (1): Dropout(p=0.2, inplace=False)\n",
       "   )\n",
       "   (cse_mid): Sequential(\n",
       "     (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "     (1): Linear(in_features=64, out_features=128, bias=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (middle_atten): Sequential(\n",
       "     (0): Conv3d(128, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (middle_merge_atten): Sequential(\n",
       "     (0): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (up1): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "   (dconv_up1): Sequential(\n",
       "     (0): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "     (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "     (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "     (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (5): ReLU(inplace=True)\n",
       "   )\n",
       "   (cse_up1): Sequential(\n",
       "     (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (atten_up1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "   (atten_dconv_up1): Sequential(\n",
       "     (0): Conv3d(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (atten_dconv_merge_up1): Sequential(\n",
       "     (0): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (up2): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "   (dconv_up2): Sequential(\n",
       "     (0): Conv3d(96, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "     (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "     (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "     (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (5): ReLU(inplace=True)\n",
       "   )\n",
       "   (cse_up2): Sequential(\n",
       "     (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "     (1): Linear(in_features=16, out_features=32, bias=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (atten_up2): Upsample(scale_factor=2.0, mode=nearest)\n",
       "   (atten_dconv_up2): Sequential(\n",
       "     (0): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (atten_dconv_merge_up2): Sequential(\n",
       "     (0): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (up3): ConvTranspose3d(32, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "   (dconv_up3): Sequential(\n",
       "     (0): Conv3d(48, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "     (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "     (3): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "     (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (5): ReLU(inplace=True)\n",
       "   )\n",
       "   (cse_up3): Sequential(\n",
       "     (0): Linear(in_features=16, out_features=8, bias=True)\n",
       "     (1): Linear(in_features=8, out_features=16, bias=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (atten_up3): Upsample(scale_factor=2.0, mode=nearest)\n",
       "   (atten_dconv_up3): Sequential(\n",
       "     (0): Conv3d(16, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (atten_dconv_merge_up3): Sequential(\n",
       "     (0): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "     (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): Sigmoid()\n",
       "   )\n",
       "   (final): Conv3d(16, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " 'model_params': {'in_channels': 8,\n",
       "  'dropout_rate': 0.2,\n",
       "  'input_data_channels': 1,\n",
       "  'output_label_channels': 1},\n",
       " 'model_name': '20210301-162727_3d_unet_model3',\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.0003\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'criterion': DiceLoss(),\n",
       " 'epochs': 50,\n",
       " 'learning_rate': 0.0003,\n",
       " 'train_batch_size': 1,\n",
       " 'device': device(type='cuda'),\n",
       " 'tensorboard_writer': <torch.utils.tensorboard.writer.SummaryWriter at 0x7fc5cc43bb50>,\n",
       " 'train_dataloader': <torch.utils.data.dataloader.DataLoader at 0x7fc5cc43bbb0>,\n",
       " 'valid_dataloader': <torch.utils.data.dataloader.DataLoader at 0x7fc5cc43bcd0>,\n",
       " 'test_dataloader': <torch.utils.data.dataloader.DataLoader at 0x7fc5cc43bdc0>,\n",
       " 'model_total_params': 1221604,\n",
       " 'model_total_trainable_params': 1221604}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<IPython.core.display.Markdown object>,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<IPython.core.display.Markdown object>,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'MAX_PADDING_SLICES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-34c71c044a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"### Train Eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshow_model_dataset_pred_preview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_slices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_PADDING_SLICES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_slice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"### Valid Eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_PADDING_SLICES' is not defined"
     ]
    }
   ],
   "source": [
    "display((Markdown(\"## Model evaluation\"),))\n",
    "\n",
    "display((Markdown(\"### Train Eval\"),))\n",
    "show_model_dataset_pred_preview(model_info, train_dataset, max_slices=MAX_PADDING_SLICES, default_slice=90)\n",
    "\n",
    "display((Markdown(\"### Valid Eval\"),))\n",
    "show_model_dataset_pred_preview(model_info, valid_dataset, max_slices=MAX_PADDING_SLICES, default_slice=90)\n",
    "\n",
    "# display(Markdown(\"### Test Eval\"))\n",
    "# eval_image_dataset(test_dataset, 78, 'test_plot.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
