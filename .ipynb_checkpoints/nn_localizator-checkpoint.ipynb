{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset biggest bounding box wihtout spinal cord [56, 177, 156]\n",
      "Cut target size [72, 192, 168]\n",
      "Done Init\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from src.consts import IN_COLAB, MAX_PADDING_SLICES, DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Found Google Colab')\n",
    "    !pip3 install torch torchvision torchsummary\n",
    "    !pip3 install simpleitk\n",
    "\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import src.helpers.oars_labels_consts as OARS_LABELS\n",
    "\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "from importlib import reload\n",
    "\n",
    "from src.training_helpers import loss_batch, show_model_info\n",
    "from src.helpers.prepare_model import prepare_model\n",
    "from src.helpers.train_loop import train_loop\n",
    "from src.helpers.get_dataset import get_dataset, get_dataloaders, get_copy_dataloaders\n",
    "from src.helpers.get_dataset_info import get_dataset_info\n",
    "from src.helpers.preview_dataset import preview_dataset\n",
    "from src.helpers.get_bounding_box import get_bounding_box, get_bounding_box_3D, get_bounding_box_3D_size, get_dividable_bounding_box, get_final_bounding_box_slice\n",
    "from src.helpers.dataset_cut_helpers import get_full_res_cut\n",
    "from src.helpers.show_model_dataset_pred_preview import show_model_dataset_pred_preview\n",
    "\n",
    "torch.manual_seed(20)\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading low res dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 16x dataset\n",
      "normalizing dataset\n",
      "filtering labels\n",
      "dilatating 1x dataset\n",
      "parsing dataset to numpy\n",
      "data type: float64 int8\n",
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_LIST\n",
    "if OARS_LABELS.SPINAL_CORD in filter_labels:\n",
    "    filter_labels.remove(OARS_LABELS.SPINAL_CORD)\n",
    "\n",
    "low_res_dataset = get_dataset(dataset_size=50, shrink_factor=16, filter_labels=filter_labels, unify_labels=True)\n",
    "low_res_dataset.dilatate_labels(repeat=1)\n",
    "low_res_dataset.to_numpy()\n",
    "low_res_dataset.show_data_type()\n",
    "low_res_dataloaders_obj = get_dataloaders(low_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "\n",
    "get_dataset_info(low_res_dataset, low_res_dataloaders_obj)\n",
    "train_low_res_dataset, valid_low_res_dataset, test_low_res_dataset = itemgetter('train_dataset', 'valid_dataset', 'test_dataset')(low_res_dataloaders_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 12.505709639268096, min -0.40698009878688973\n",
      "label max 1, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b566dc1aff124ff4916f10edf60c42fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33aef34448c64d9286ea8154d463cb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_dataset(low_res_dataset, preview_index=0, show_hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training low res model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device running \"cuda\"\n",
      "max output channels 128\n",
      "Model number of params: 1193537, trainable 1193537\n",
      "Running training loop\n",
      "Batch train [1] loss 0.97630, dsc 0.02370\n",
      "Batch train [2] loss 0.97167, dsc 0.02833\n",
      "Batch train [3] loss 0.97257, dsc 0.02743\n",
      "Batch train [4] loss 0.96968, dsc 0.03032\n",
      "Batch train [5] loss 0.97364, dsc 0.02636\n",
      "Batch train [6] loss 0.96659, dsc 0.03341\n",
      "Batch train [7] loss 0.95242, dsc 0.04758\n",
      "Batch train [8] loss 0.96548, dsc 0.03452\n",
      "Batch train [9] loss 0.96903, dsc 0.03097\n",
      "Batch train [10] loss 0.96693, dsc 0.03307\n",
      "Batch train [11] loss 0.96507, dsc 0.03493\n",
      "Batch train [12] loss 0.97499, dsc 0.02501\n",
      "Batch train [13] loss 0.95384, dsc 0.04616\n",
      "Batch train [14] loss 0.95651, dsc 0.04349\n",
      "Batch train [15] loss 0.95468, dsc 0.04532\n",
      "Batch train [16] loss 0.96143, dsc 0.03857\n",
      "Batch train [17] loss 0.96530, dsc 0.03470\n",
      "Batch train [18] loss 0.96914, dsc 0.03086\n",
      "Batch train [19] loss 0.96070, dsc 0.03930\n",
      "Batch train [20] loss 0.96140, dsc 0.03860\n",
      "Batch train [21] loss 0.95840, dsc 0.04160\n",
      "Batch train [22] loss 0.97223, dsc 0.02777\n",
      "Batch train [23] loss 0.95842, dsc 0.04158\n",
      "Batch train [24] loss 0.96060, dsc 0.03940\n",
      "Batch train [25] loss 0.95559, dsc 0.04441\n",
      "Batch train [26] loss 0.95190, dsc 0.04810\n",
      "Batch train [27] loss 0.96540, dsc 0.03460\n",
      "Batch train [28] loss 0.95479, dsc 0.04521\n",
      "Batch train [29] loss 0.96611, dsc 0.03389\n",
      "Batch train [30] loss 0.95614, dsc 0.04386\n",
      "Batch train [31] loss 0.94883, dsc 0.05117\n",
      "Batch train [32] loss 0.95543, dsc 0.04457\n",
      "Batch train [33] loss 0.94562, dsc 0.05438\n",
      "Batch train [34] loss 0.95600, dsc 0.04400\n",
      "Batch train [35] loss 0.94252, dsc 0.05748\n",
      "Batch train [36] loss 0.95475, dsc 0.04525\n",
      "Batch train [37] loss 0.95572, dsc 0.04428\n",
      "Batch train [38] loss 0.96641, dsc 0.03359\n",
      "Batch train [39] loss 0.95874, dsc 0.04126\n",
      "Batch train [40] loss 0.95793, dsc 0.04207\n",
      "Epoch [1] train done\n",
      "Batch eval [1] loss 0.95312, dsc 0.04688\n",
      "Batch eval [2] loss 0.96419, dsc 0.03581\n",
      "Batch eval [3] loss 0.96522, dsc 0.03478\n",
      "Batch eval [4] loss 0.96275, dsc 0.03725\n",
      "Batch eval [5] loss 0.96087, dsc 0.03913\n",
      "Epoch [1] valid done\n",
      "Epoch [1] T 7.56s, deltaT 7.56s, loss: train 0.96122, valid 0.96123, dsc: train 0.03878, valid 0.03877\n",
      "Batch train [1] loss 0.94802, dsc 0.05198\n",
      "Batch train [2] loss 0.95494, dsc 0.04506\n",
      "Batch train [3] loss 0.96264, dsc 0.03736\n",
      "Batch train [4] loss 0.97084, dsc 0.02916\n",
      "Batch train [5] loss 0.95813, dsc 0.04187\n",
      "Batch train [6] loss 0.95416, dsc 0.04584\n",
      "Batch train [7] loss 0.96329, dsc 0.03671\n",
      "Batch train [8] loss 0.96406, dsc 0.03594\n",
      "Batch train [9] loss 0.94341, dsc 0.05659\n",
      "Batch train [10] loss 0.94784, dsc 0.05216\n",
      "Batch train [11] loss 0.95955, dsc 0.04045\n",
      "Batch train [12] loss 0.96515, dsc 0.03485\n",
      "Batch train [13] loss 0.95134, dsc 0.04866\n",
      "Batch train [14] loss 0.95741, dsc 0.04259\n",
      "Batch train [15] loss 0.95430, dsc 0.04570\n",
      "Batch train [16] loss 0.95657, dsc 0.04343\n",
      "Batch train [17] loss 0.95195, dsc 0.04805\n",
      "Batch train [18] loss 0.94575, dsc 0.05425\n",
      "Batch train [19] loss 0.95248, dsc 0.04752\n",
      "Batch train [20] loss 0.96146, dsc 0.03854\n",
      "Batch train [21] loss 0.96869, dsc 0.03131\n",
      "Batch train [22] loss 0.96459, dsc 0.03541\n",
      "Batch train [23] loss 0.95875, dsc 0.04125\n",
      "Batch train [24] loss 0.96372, dsc 0.03628\n",
      "Batch train [25] loss 0.95259, dsc 0.04741\n",
      "Batch train [26] loss 0.94707, dsc 0.05293\n",
      "Batch train [27] loss 0.95019, dsc 0.04981\n",
      "Batch train [28] loss 0.94770, dsc 0.05230\n",
      "Batch train [29] loss 0.93752, dsc 0.06248\n",
      "Batch train [30] loss 0.94442, dsc 0.05558\n",
      "Batch train [31] loss 0.95487, dsc 0.04513\n",
      "Batch train [32] loss 0.95213, dsc 0.04787\n",
      "Batch train [33] loss 0.95443, dsc 0.04557\n",
      "Batch train [34] loss 0.95419, dsc 0.04581\n",
      "Batch train [35] loss 0.95482, dsc 0.04518\n",
      "Batch train [36] loss 0.95285, dsc 0.04715\n",
      "Batch train [37] loss 0.95233, dsc 0.04767\n",
      "Batch train [38] loss 0.93662, dsc 0.06338\n",
      "Batch train [39] loss 0.95698, dsc 0.04302\n",
      "Batch train [40] loss 0.95479, dsc 0.04521\n",
      "Epoch [2] train done\n",
      "Batch eval [1] loss 0.94425, dsc 0.05575\n",
      "Batch eval [2] loss 0.95305, dsc 0.04695\n",
      "Batch eval [3] loss 0.94691, dsc 0.05309\n",
      "Batch eval [4] loss 0.95668, dsc 0.04332\n",
      "Batch eval [5] loss 0.94467, dsc 0.05533\n",
      "Epoch [2] valid done\n",
      "Epoch [2] T 15.04s, deltaT 7.49s, loss: train 0.95456, valid 0.94911, dsc: train 0.04544, valid 0.05089\n",
      "Batch train [1] loss 0.96267, dsc 0.03733\n",
      "Batch train [2] loss 0.96083, dsc 0.03917\n",
      "Batch train [3] loss 0.95169, dsc 0.04831\n",
      "Batch train [4] loss 0.94566, dsc 0.05434\n",
      "Batch train [5] loss 0.96776, dsc 0.03224\n",
      "Batch train [6] loss 0.94314, dsc 0.05686\n",
      "Batch train [7] loss 0.93536, dsc 0.06464\n",
      "Batch train [8] loss 0.96168, dsc 0.03832\n",
      "Batch train [9] loss 0.93897, dsc 0.06103\n",
      "Batch train [10] loss 0.95316, dsc 0.04684\n",
      "Batch train [11] loss 0.95859, dsc 0.04141\n",
      "Batch train [12] loss 0.96028, dsc 0.03972\n",
      "Batch train [13] loss 0.94815, dsc 0.05185\n",
      "Batch train [14] loss 0.94977, dsc 0.05023\n",
      "Batch train [15] loss 0.94940, dsc 0.05060\n",
      "Batch train [16] loss 0.95461, dsc 0.04539\n",
      "Batch train [17] loss 0.94833, dsc 0.05167\n",
      "Batch train [18] loss 0.95318, dsc 0.04682\n",
      "Batch train [19] loss 0.95239, dsc 0.04761\n",
      "Batch train [20] loss 0.94763, dsc 0.05237\n",
      "Batch train [21] loss 0.95510, dsc 0.04490\n",
      "Batch train [22] loss 0.95261, dsc 0.04739\n",
      "Batch train [23] loss 0.95159, dsc 0.04841\n",
      "Batch train [24] loss 0.94759, dsc 0.05241\n",
      "Batch train [25] loss 0.95164, dsc 0.04836\n",
      "Batch train [26] loss 0.96091, dsc 0.03909\n",
      "Batch train [27] loss 0.94939, dsc 0.05061\n",
      "Batch train [28] loss 0.95473, dsc 0.04527\n",
      "Batch train [29] loss 0.94816, dsc 0.05184\n",
      "Batch train [30] loss 0.93765, dsc 0.06235\n",
      "Batch train [31] loss 0.94074, dsc 0.05926\n",
      "Batch train [32] loss 0.94183, dsc 0.05817\n",
      "Batch train [33] loss 0.95204, dsc 0.04796\n",
      "Batch train [34] loss 0.95618, dsc 0.04382\n",
      "Batch train [35] loss 0.93833, dsc 0.06167\n",
      "Batch train [36] loss 0.94885, dsc 0.05115\n",
      "Batch train [37] loss 0.96453, dsc 0.03547\n",
      "Batch train [38] loss 0.93119, dsc 0.06881\n",
      "Batch train [39] loss 0.94438, dsc 0.05562\n",
      "Batch train [40] loss 0.94977, dsc 0.05023\n",
      "Epoch [3] train done\n",
      "Batch eval [1] loss 0.93692, dsc 0.06308\n",
      "Batch eval [2] loss 0.94726, dsc 0.05274\n",
      "Batch eval [3] loss 0.93961, dsc 0.06039\n",
      "Batch eval [4] loss 0.95149, dsc 0.04851\n",
      "Batch eval [5] loss 0.93762, dsc 0.06238\n",
      "Epoch [3] valid done\n",
      "Epoch [3] T 22.61s, deltaT 7.56s, loss: train 0.95051, valid 0.94258, dsc: train 0.04949, valid 0.05742\n",
      "Batch train [1] loss 0.94464, dsc 0.05536\n",
      "Batch train [2] loss 0.95953, dsc 0.04047\n",
      "Batch train [3] loss 0.93804, dsc 0.06196\n",
      "Batch train [4] loss 0.95301, dsc 0.04699\n",
      "Batch train [5] loss 0.94975, dsc 0.05025\n",
      "Batch train [6] loss 0.94980, dsc 0.05020\n",
      "Batch train [7] loss 0.95153, dsc 0.04847\n",
      "Batch train [8] loss 0.94601, dsc 0.05399\n",
      "Batch train [9] loss 0.94859, dsc 0.05141\n",
      "Batch train [10] loss 0.95214, dsc 0.04786\n",
      "Batch train [11] loss 0.92907, dsc 0.07093\n",
      "Batch train [12] loss 0.94216, dsc 0.05784\n",
      "Batch train [13] loss 0.94395, dsc 0.05605\n",
      "Batch train [14] loss 0.94270, dsc 0.05730\n",
      "Batch train [15] loss 0.94413, dsc 0.05587\n",
      "Batch train [16] loss 0.93535, dsc 0.06465\n",
      "Batch train [17] loss 0.95612, dsc 0.04388\n",
      "Batch train [18] loss 0.96276, dsc 0.03724\n",
      "Batch train [19] loss 0.94455, dsc 0.05545\n",
      "Batch train [20] loss 0.93752, dsc 0.06248\n",
      "Batch train [21] loss 0.96357, dsc 0.03643\n",
      "Batch train [22] loss 0.95679, dsc 0.04321\n",
      "Batch train [23] loss 0.94532, dsc 0.05468\n",
      "Batch train [24] loss 0.95738, dsc 0.04262\n",
      "Batch train [25] loss 0.95486, dsc 0.04514\n",
      "Batch train [26] loss 0.94446, dsc 0.05554\n",
      "Batch train [27] loss 0.94780, dsc 0.05220\n",
      "Batch train [28] loss 0.94343, dsc 0.05657\n",
      "Batch train [29] loss 0.95261, dsc 0.04739\n",
      "Batch train [30] loss 0.94654, dsc 0.05346\n",
      "Batch train [31] loss 0.92620, dsc 0.07380\n",
      "Batch train [32] loss 0.92930, dsc 0.07070\n",
      "Batch train [33] loss 0.95233, dsc 0.04767\n",
      "Batch train [34] loss 0.93342, dsc 0.06658\n",
      "Batch train [35] loss 0.94562, dsc 0.05438\n",
      "Batch train [36] loss 0.94706, dsc 0.05294\n",
      "Batch train [37] loss 0.94408, dsc 0.05592\n",
      "Batch train [38] loss 0.93565, dsc 0.06435\n",
      "Batch train [39] loss 0.93644, dsc 0.06356\n",
      "Batch train [40] loss 0.94518, dsc 0.05482\n",
      "Epoch [4] train done\n",
      "Batch eval [1] loss 0.93002, dsc 0.06998\n",
      "Batch eval [2] loss 0.94156, dsc 0.05844\n",
      "Batch eval [3] loss 0.93306, dsc 0.06694\n",
      "Batch eval [4] loss 0.94613, dsc 0.05387\n",
      "Batch eval [5] loss 0.93166, dsc 0.06834\n",
      "Epoch [4] valid done\n",
      "Epoch [4] T 29.99s, deltaT 7.38s, loss: train 0.94598, valid 0.93649, dsc: train 0.05402, valid 0.06351\n",
      "Batch train [1] loss 0.94583, dsc 0.05417\n",
      "Batch train [2] loss 0.94195, dsc 0.05805\n",
      "Batch train [3] loss 0.96189, dsc 0.03811\n",
      "Batch train [4] loss 0.93224, dsc 0.06776\n",
      "Batch train [5] loss 0.93304, dsc 0.06696\n",
      "Batch train [6] loss 0.94585, dsc 0.05415\n",
      "Batch train [7] loss 0.94395, dsc 0.05605\n",
      "Batch train [8] loss 0.94516, dsc 0.05484\n",
      "Batch train [9] loss 0.93726, dsc 0.06274\n",
      "Batch train [10] loss 0.95201, dsc 0.04799\n",
      "Batch train [11] loss 0.94629, dsc 0.05371\n",
      "Batch train [12] loss 0.94754, dsc 0.05246\n",
      "Batch train [13] loss 0.94102, dsc 0.05898\n",
      "Batch train [14] loss 0.94956, dsc 0.05044\n",
      "Batch train [15] loss 0.94912, dsc 0.05088\n",
      "Batch train [16] loss 0.93320, dsc 0.06680\n",
      "Batch train [17] loss 0.93800, dsc 0.06200\n",
      "Batch train [18] loss 0.94214, dsc 0.05786\n",
      "Batch train [19] loss 0.92911, dsc 0.07089\n",
      "Batch train [20] loss 0.94582, dsc 0.05418\n",
      "Batch train [21] loss 0.95345, dsc 0.04655\n",
      "Batch train [22] loss 0.92633, dsc 0.07367\n",
      "Batch train [23] loss 0.92350, dsc 0.07650\n",
      "Batch train [24] loss 0.93064, dsc 0.06936\n",
      "Batch train [25] loss 0.95069, dsc 0.04931\n",
      "Batch train [26] loss 0.93530, dsc 0.06470\n",
      "Batch train [27] loss 0.91877, dsc 0.08123\n",
      "Batch train [28] loss 0.94128, dsc 0.05872\n",
      "Batch train [29] loss 0.95239, dsc 0.04761\n",
      "Batch train [30] loss 0.95155, dsc 0.04845\n",
      "Batch train [31] loss 0.95785, dsc 0.04215\n",
      "Batch train [32] loss 0.93929, dsc 0.06071\n",
      "Batch train [33] loss 0.93614, dsc 0.06386\n",
      "Batch train [34] loss 0.93726, dsc 0.06274\n",
      "Batch train [35] loss 0.93660, dsc 0.06340\n",
      "Batch train [36] loss 0.93716, dsc 0.06284\n",
      "Batch train [37] loss 0.93970, dsc 0.06030\n",
      "Batch train [38] loss 0.93301, dsc 0.06699\n",
      "Batch train [39] loss 0.94030, dsc 0.05970\n",
      "Batch train [40] loss 0.91626, dsc 0.08374\n",
      "Epoch [5] train done\n",
      "Batch eval [1] loss 0.92336, dsc 0.07664\n",
      "Batch eval [2] loss 0.93626, dsc 0.06374\n",
      "Batch eval [3] loss 0.93241, dsc 0.06759\n",
      "Batch eval [4] loss 0.94025, dsc 0.05975\n",
      "Batch eval [5] loss 0.92512, dsc 0.07488\n",
      "Epoch [5] valid done\n",
      "Epoch [5] T 37.31s, deltaT 7.32s, loss: train 0.94046, valid 0.93148, dsc: train 0.05954, valid 0.06852\n",
      "Batch train [1] loss 0.91966, dsc 0.08034\n",
      "Batch train [2] loss 0.91536, dsc 0.08464\n",
      "Batch train [3] loss 0.91533, dsc 0.08467\n",
      "Batch train [4] loss 0.94533, dsc 0.05467\n",
      "Batch train [5] loss 0.94004, dsc 0.05996\n",
      "Batch train [6] loss 0.92258, dsc 0.07742\n",
      "Batch train [7] loss 0.93080, dsc 0.06920\n",
      "Batch train [8] loss 0.93103, dsc 0.06897\n",
      "Batch train [9] loss 0.94214, dsc 0.05786\n",
      "Batch train [10] loss 0.93712, dsc 0.06288\n",
      "Batch train [11] loss 0.93200, dsc 0.06800\n",
      "Batch train [12] loss 0.92450, dsc 0.07550\n",
      "Batch train [13] loss 0.95624, dsc 0.04376\n",
      "Batch train [14] loss 0.93456, dsc 0.06544\n",
      "Batch train [15] loss 0.92239, dsc 0.07761\n",
      "Batch train [16] loss 0.92642, dsc 0.07358\n",
      "Batch train [17] loss 0.93158, dsc 0.06842\n",
      "Batch train [18] loss 0.94362, dsc 0.05638\n",
      "Batch train [19] loss 0.93574, dsc 0.06426\n",
      "Batch train [20] loss 0.93239, dsc 0.06761\n",
      "Batch train [21] loss 0.92334, dsc 0.07666\n",
      "Batch train [22] loss 0.94687, dsc 0.05313\n",
      "Batch train [23] loss 0.93325, dsc 0.06675\n",
      "Batch train [24] loss 0.93124, dsc 0.06876\n",
      "Batch train [25] loss 0.94749, dsc 0.05251\n",
      "Batch train [26] loss 0.93224, dsc 0.06776\n",
      "Batch train [27] loss 0.93543, dsc 0.06457\n",
      "Batch train [28] loss 0.95274, dsc 0.04726\n",
      "Batch train [29] loss 0.93401, dsc 0.06599\n",
      "Batch train [30] loss 0.94634, dsc 0.05366\n",
      "Batch train [31] loss 0.92932, dsc 0.07068\n",
      "Batch train [32] loss 0.93392, dsc 0.06608\n",
      "Batch train [33] loss 0.93307, dsc 0.06693\n",
      "Batch train [34] loss 0.91715, dsc 0.08285\n",
      "Batch train [35] loss 0.93709, dsc 0.06291\n",
      "Batch train [36] loss 0.94308, dsc 0.05692\n",
      "Batch train [37] loss 0.93304, dsc 0.06696\n",
      "Batch train [38] loss 0.94239, dsc 0.05761\n",
      "Batch train [39] loss 0.92553, dsc 0.07447\n",
      "Batch train [40] loss 0.93533, dsc 0.06467\n",
      "Epoch [6] train done\n",
      "Batch eval [1] loss 0.92206, dsc 0.07794\n",
      "Batch eval [2] loss 0.93347, dsc 0.06653\n",
      "Batch eval [3] loss 0.92463, dsc 0.07537\n",
      "Batch eval [4] loss 0.94047, dsc 0.05953\n",
      "Batch eval [5] loss 0.92375, dsc 0.07625\n",
      "Epoch [6] valid done\n",
      "Epoch [6] T 44.64s, deltaT 7.33s, loss: train 0.93379, valid 0.92887, dsc: train 0.06621, valid 0.07113\n",
      "Batch train [1] loss 0.91096, dsc 0.08904\n",
      "Batch train [2] loss 0.92739, dsc 0.07261\n",
      "Batch train [3] loss 0.94480, dsc 0.05520\n",
      "Batch train [4] loss 0.91765, dsc 0.08235\n",
      "Batch train [5] loss 0.93184, dsc 0.06816\n",
      "Batch train [6] loss 0.94448, dsc 0.05552\n",
      "Batch train [7] loss 0.93232, dsc 0.06768\n",
      "Batch train [8] loss 0.95017, dsc 0.04983\n",
      "Batch train [9] loss 0.93101, dsc 0.06899\n",
      "Batch train [10] loss 0.91243, dsc 0.08757\n",
      "Batch train [11] loss 0.92164, dsc 0.07836\n",
      "Batch train [12] loss 0.94250, dsc 0.05750\n",
      "Batch train [13] loss 0.91613, dsc 0.08387\n",
      "Batch train [14] loss 0.92296, dsc 0.07704\n",
      "Batch train [15] loss 0.90244, dsc 0.09756\n",
      "Batch train [16] loss 0.92283, dsc 0.07717\n",
      "Batch train [17] loss 0.93650, dsc 0.06350\n",
      "Batch train [18] loss 0.91045, dsc 0.08955\n",
      "Batch train [19] loss 0.92767, dsc 0.07233\n",
      "Batch train [20] loss 0.92384, dsc 0.07616\n",
      "Batch train [21] loss 0.92710, dsc 0.07290\n",
      "Batch train [22] loss 0.91534, dsc 0.08466\n",
      "Batch train [23] loss 0.92219, dsc 0.07781\n",
      "Batch train [24] loss 0.93846, dsc 0.06154\n",
      "Batch train [25] loss 0.91836, dsc 0.08164\n",
      "Batch train [26] loss 0.93092, dsc 0.06908\n",
      "Batch train [27] loss 0.92527, dsc 0.07473\n",
      "Batch train [28] loss 0.89951, dsc 0.10049\n",
      "Batch train [29] loss 0.93380, dsc 0.06620\n",
      "Batch train [30] loss 0.92704, dsc 0.07296\n",
      "Batch train [31] loss 0.90931, dsc 0.09069\n",
      "Batch train [32] loss 0.92234, dsc 0.07766\n",
      "Batch train [33] loss 0.92263, dsc 0.07737\n",
      "Batch train [34] loss 0.92966, dsc 0.07034\n",
      "Batch train [35] loss 0.92813, dsc 0.07187\n",
      "Batch train [36] loss 0.92352, dsc 0.07648\n",
      "Batch train [37] loss 0.94680, dsc 0.05320\n",
      "Batch train [38] loss 0.91488, dsc 0.08512\n",
      "Batch train [39] loss 0.93383, dsc 0.06617\n",
      "Batch train [40] loss 0.91839, dsc 0.08161\n",
      "Epoch [7] train done\n",
      "Batch eval [1] loss 0.92475, dsc 0.07525\n",
      "Batch eval [2] loss 0.93852, dsc 0.06148\n",
      "Batch eval [3] loss 0.93365, dsc 0.06635\n",
      "Batch eval [4] loss 0.93390, dsc 0.06610\n",
      "Batch eval [5] loss 0.92739, dsc 0.07261\n",
      "Epoch [7] valid done\n",
      "Epoch [7] T 52.00s, deltaT 7.36s, loss: train 0.92544, valid 0.93164, dsc: train 0.07456, valid 0.06836\n",
      "Batch train [1] loss 0.91940, dsc 0.08060\n",
      "Batch train [2] loss 0.91296, dsc 0.08704\n",
      "Batch train [3] loss 0.93625, dsc 0.06375\n",
      "Batch train [4] loss 0.92022, dsc 0.07978\n",
      "Batch train [5] loss 0.91411, dsc 0.08589\n",
      "Batch train [6] loss 0.92911, dsc 0.07089\n",
      "Batch train [7] loss 0.92091, dsc 0.07909\n",
      "Batch train [8] loss 0.90138, dsc 0.09862\n",
      "Batch train [9] loss 0.91429, dsc 0.08571\n",
      "Batch train [10] loss 0.91957, dsc 0.08043\n",
      "Batch train [11] loss 0.92451, dsc 0.07549\n",
      "Batch train [12] loss 0.92052, dsc 0.07948\n",
      "Batch train [13] loss 0.91572, dsc 0.08428\n",
      "Batch train [14] loss 0.90351, dsc 0.09649\n",
      "Batch train [15] loss 0.92747, dsc 0.07253\n",
      "Batch train [16] loss 0.90455, dsc 0.09545\n",
      "Batch train [17] loss 0.90193, dsc 0.09807\n",
      "Batch train [18] loss 0.92973, dsc 0.07027\n",
      "Batch train [19] loss 0.90835, dsc 0.09165\n",
      "Batch train [20] loss 0.91583, dsc 0.08417\n",
      "Batch train [21] loss 0.94118, dsc 0.05882\n",
      "Batch train [22] loss 0.93927, dsc 0.06073\n",
      "Batch train [23] loss 0.90989, dsc 0.09011\n",
      "Batch train [24] loss 0.90674, dsc 0.09326\n",
      "Batch train [25] loss 0.90744, dsc 0.09256\n",
      "Batch train [26] loss 0.92632, dsc 0.07368\n",
      "Batch train [27] loss 0.92979, dsc 0.07021\n",
      "Batch train [28] loss 0.88391, dsc 0.11609\n",
      "Batch train [29] loss 0.91328, dsc 0.08672\n",
      "Batch train [30] loss 0.92826, dsc 0.07174\n",
      "Batch train [31] loss 0.91376, dsc 0.08624\n",
      "Batch train [32] loss 0.89097, dsc 0.10903\n",
      "Batch train [33] loss 0.90710, dsc 0.09290\n",
      "Batch train [34] loss 0.90616, dsc 0.09384\n",
      "Batch train [35] loss 0.91508, dsc 0.08492\n",
      "Batch train [36] loss 0.91632, dsc 0.08368\n",
      "Batch train [37] loss 0.88348, dsc 0.11652\n",
      "Batch train [38] loss 0.90932, dsc 0.09068\n",
      "Batch train [39] loss 0.88793, dsc 0.11207\n",
      "Batch train [40] loss 0.87530, dsc 0.12470\n",
      "Epoch [8] train done\n",
      "Batch eval [1] loss 0.96635, dsc 0.03365\n",
      "Batch eval [2] loss 0.97039, dsc 0.02961\n",
      "Batch eval [3] loss 0.95938, dsc 0.04062\n",
      "Batch eval [4] loss 0.94859, dsc 0.05141\n",
      "Batch eval [5] loss 0.94114, dsc 0.05886\n",
      "Epoch [8] valid done\n",
      "Epoch [8] T 59.34s, deltaT 7.34s, loss: train 0.91330, valid 0.95717, dsc: train 0.08670, valid 0.04283\n",
      "Batch train [1] loss 0.90820, dsc 0.09180\n",
      "Batch train [2] loss 0.90017, dsc 0.09983\n",
      "Batch train [3] loss 0.89650, dsc 0.10350\n",
      "Batch train [4] loss 0.91264, dsc 0.08736\n",
      "Batch train [5] loss 0.90741, dsc 0.09259\n",
      "Batch train [6] loss 0.90591, dsc 0.09409\n",
      "Batch train [7] loss 0.89044, dsc 0.10956\n",
      "Batch train [8] loss 0.87061, dsc 0.12939\n",
      "Batch train [9] loss 0.91109, dsc 0.08891\n",
      "Batch train [10] loss 0.93202, dsc 0.06798\n",
      "Batch train [11] loss 0.90308, dsc 0.09692\n",
      "Batch train [12] loss 0.89744, dsc 0.10256\n",
      "Batch train [13] loss 0.91373, dsc 0.08627\n",
      "Batch train [14] loss 0.91624, dsc 0.08376\n",
      "Batch train [15] loss 0.87134, dsc 0.12866\n",
      "Batch train [16] loss 0.89441, dsc 0.10559\n",
      "Batch train [17] loss 0.91546, dsc 0.08454\n",
      "Batch train [18] loss 0.89506, dsc 0.10494\n",
      "Batch train [19] loss 0.91824, dsc 0.08176\n",
      "Batch train [20] loss 0.88101, dsc 0.11899\n",
      "Batch train [21] loss 0.88887, dsc 0.11113\n",
      "Batch train [22] loss 0.89288, dsc 0.10712\n",
      "Batch train [23] loss 0.91658, dsc 0.08342\n",
      "Batch train [24] loss 0.86144, dsc 0.13856\n",
      "Batch train [25] loss 0.88883, dsc 0.11117\n",
      "Batch train [26] loss 0.89723, dsc 0.10277\n",
      "Batch train [27] loss 0.88375, dsc 0.11625\n",
      "Batch train [28] loss 0.89381, dsc 0.10619\n",
      "Batch train [29] loss 0.92642, dsc 0.07358\n",
      "Batch train [30] loss 0.89077, dsc 0.10923\n",
      "Batch train [31] loss 0.86798, dsc 0.13202\n",
      "Batch train [32] loss 0.87462, dsc 0.12538\n",
      "Batch train [33] loss 0.89708, dsc 0.10292\n",
      "Batch train [34] loss 0.90287, dsc 0.09713\n",
      "Batch train [35] loss 0.86727, dsc 0.13273\n",
      "Batch train [36] loss 0.88778, dsc 0.11222\n",
      "Batch train [37] loss 0.90979, dsc 0.09021\n",
      "Batch train [38] loss 0.89022, dsc 0.10978\n",
      "Batch train [39] loss 0.86149, dsc 0.13851\n",
      "Batch train [40] loss 0.88173, dsc 0.11827\n",
      "Epoch [9] train done\n",
      "Batch eval [1] loss 0.89045, dsc 0.10955\n",
      "Batch eval [2] loss 0.90352, dsc 0.09648\n",
      "Batch eval [3] loss 0.89663, dsc 0.10337\n",
      "Batch eval [4] loss 0.91090, dsc 0.08910\n",
      "Batch eval [5] loss 0.88806, dsc 0.11194\n",
      "Epoch [9] valid done\n",
      "Epoch [9] T 66.67s, deltaT 7.32s, loss: train 0.89556, valid 0.89791, dsc: train 0.10444, valid 0.10209\n",
      "Batch train [1] loss 0.90544, dsc 0.09456\n",
      "Batch train [2] loss 0.87614, dsc 0.12386\n",
      "Batch train [3] loss 0.91570, dsc 0.08430\n",
      "Batch train [4] loss 0.86529, dsc 0.13471\n",
      "Batch train [5] loss 0.91683, dsc 0.08317\n",
      "Batch train [6] loss 0.85969, dsc 0.14031\n",
      "Batch train [7] loss 0.86262, dsc 0.13738\n",
      "Batch train [8] loss 0.85228, dsc 0.14772\n",
      "Batch train [9] loss 0.86992, dsc 0.13008\n",
      "Batch train [10] loss 0.87614, dsc 0.12386\n",
      "Batch train [11] loss 0.87974, dsc 0.12026\n",
      "Batch train [12] loss 0.86804, dsc 0.13196\n",
      "Batch train [13] loss 0.87224, dsc 0.12776\n",
      "Batch train [14] loss 0.85934, dsc 0.14066\n",
      "Batch train [15] loss 0.87892, dsc 0.12108\n",
      "Batch train [16] loss 0.88425, dsc 0.11575\n",
      "Batch train [17] loss 0.86507, dsc 0.13493\n",
      "Batch train [18] loss 0.87302, dsc 0.12698\n",
      "Batch train [19] loss 0.84134, dsc 0.15866\n",
      "Batch train [20] loss 0.85793, dsc 0.14207\n",
      "Batch train [21] loss 0.88222, dsc 0.11778\n",
      "Batch train [22] loss 0.86139, dsc 0.13861\n",
      "Batch train [23] loss 0.85430, dsc 0.14570\n",
      "Batch train [24] loss 0.82509, dsc 0.17491\n",
      "Batch train [25] loss 0.84657, dsc 0.15343\n",
      "Batch train [26] loss 0.87456, dsc 0.12544\n",
      "Batch train [27] loss 0.80845, dsc 0.19155\n",
      "Batch train [28] loss 0.83688, dsc 0.16312\n",
      "Batch train [29] loss 0.85332, dsc 0.14668\n",
      "Batch train [30] loss 0.85008, dsc 0.14992\n",
      "Batch train [31] loss 0.79915, dsc 0.20085\n",
      "Batch train [32] loss 0.87332, dsc 0.12668\n",
      "Batch train [33] loss 0.85710, dsc 0.14290\n",
      "Batch train [34] loss 0.86084, dsc 0.13916\n",
      "Batch train [35] loss 0.84412, dsc 0.15588\n",
      "Batch train [36] loss 0.80841, dsc 0.19159\n",
      "Batch train [37] loss 0.83488, dsc 0.16512\n",
      "Batch train [38] loss 0.82793, dsc 0.17207\n",
      "Batch train [39] loss 0.82707, dsc 0.17293\n",
      "Batch train [40] loss 0.86861, dsc 0.13139\n",
      "Epoch [10] train done\n",
      "Batch eval [1] loss 0.80743, dsc 0.19257\n",
      "Batch eval [2] loss 0.82565, dsc 0.17435\n",
      "Batch eval [3] loss 0.80683, dsc 0.19317\n",
      "Batch eval [4] loss 0.84761, dsc 0.15239\n",
      "Batch eval [5] loss 0.80507, dsc 0.19493\n",
      "Epoch [10] valid done\n",
      "Epoch [10] T 74.00s, deltaT 7.33s, loss: train 0.85936, valid 0.81852, dsc: train 0.14064, valid 0.18148\n",
      "Batch train [1] loss 0.81073, dsc 0.18927\n",
      "Batch train [2] loss 0.84568, dsc 0.15432\n",
      "Batch train [3] loss 0.80096, dsc 0.19904\n",
      "Batch train [4] loss 0.83720, dsc 0.16280\n",
      "Batch train [5] loss 0.88054, dsc 0.11946\n",
      "Batch train [6] loss 0.83053, dsc 0.16947\n",
      "Batch train [7] loss 0.82744, dsc 0.17256\n",
      "Batch train [8] loss 0.80261, dsc 0.19739\n",
      "Batch train [9] loss 0.84806, dsc 0.15194\n",
      "Batch train [10] loss 0.81185, dsc 0.18815\n",
      "Batch train [11] loss 0.77914, dsc 0.22086\n",
      "Batch train [12] loss 0.84460, dsc 0.15540\n",
      "Batch train [13] loss 0.80685, dsc 0.19315\n",
      "Batch train [14] loss 0.83864, dsc 0.16136\n",
      "Batch train [15] loss 0.76909, dsc 0.23091\n",
      "Batch train [16] loss 0.81673, dsc 0.18327\n",
      "Batch train [17] loss 0.80411, dsc 0.19589\n",
      "Batch train [18] loss 0.82399, dsc 0.17601\n",
      "Batch train [19] loss 0.80406, dsc 0.19594\n",
      "Batch train [20] loss 0.81018, dsc 0.18982\n",
      "Batch train [21] loss 0.80803, dsc 0.19197\n",
      "Batch train [22] loss 0.81650, dsc 0.18350\n",
      "Batch train [23] loss 0.78904, dsc 0.21096\n",
      "Batch train [24] loss 0.81922, dsc 0.18078\n",
      "Batch train [25] loss 0.80723, dsc 0.19277\n",
      "Batch train [26] loss 0.84997, dsc 0.15003\n",
      "Batch train [27] loss 0.82824, dsc 0.17176\n",
      "Batch train [28] loss 0.82841, dsc 0.17159\n",
      "Batch train [29] loss 0.79035, dsc 0.20965\n",
      "Batch train [30] loss 0.76175, dsc 0.23825\n",
      "Batch train [31] loss 0.75789, dsc 0.24211\n",
      "Batch train [32] loss 0.78096, dsc 0.21904\n",
      "Batch train [33] loss 0.77819, dsc 0.22181\n",
      "Batch train [34] loss 0.72340, dsc 0.27660\n",
      "Batch train [35] loss 0.74494, dsc 0.25506\n",
      "Batch train [36] loss 0.76362, dsc 0.23638\n",
      "Batch train [37] loss 0.78051, dsc 0.21949\n",
      "Batch train [38] loss 0.77300, dsc 0.22700\n",
      "Batch train [39] loss 0.81028, dsc 0.18972\n",
      "Batch train [40] loss 0.75772, dsc 0.24228\n",
      "Epoch [11] train done\n",
      "Batch eval [1] loss 0.73228, dsc 0.26772\n",
      "Batch eval [2] loss 0.76757, dsc 0.23243\n",
      "Batch eval [3] loss 0.75572, dsc 0.24428\n",
      "Batch eval [4] loss 0.78789, dsc 0.21211\n",
      "Batch eval [5] loss 0.73820, dsc 0.26180\n",
      "Epoch [11] valid done\n",
      "Epoch [11] T 81.37s, deltaT 7.36s, loss: train 0.80406, valid 0.75633, dsc: train 0.19594, valid 0.24367\n",
      "Batch train [1] loss 0.70831, dsc 0.29169\n",
      "Batch train [2] loss 0.74464, dsc 0.25536\n",
      "Batch train [3] loss 0.73711, dsc 0.26289\n",
      "Batch train [4] loss 0.82451, dsc 0.17549\n",
      "Batch train [5] loss 0.74185, dsc 0.25815\n",
      "Batch train [6] loss 0.75647, dsc 0.24353\n",
      "Batch train [7] loss 0.75932, dsc 0.24068\n",
      "Batch train [8] loss 0.73695, dsc 0.26305\n",
      "Batch train [9] loss 0.75531, dsc 0.24469\n",
      "Batch train [10] loss 0.71869, dsc 0.28131\n",
      "Batch train [11] loss 0.73286, dsc 0.26714\n",
      "Batch train [12] loss 0.75811, dsc 0.24189\n",
      "Batch train [13] loss 0.72421, dsc 0.27579\n",
      "Batch train [14] loss 0.73664, dsc 0.26336\n",
      "Batch train [15] loss 0.69567, dsc 0.30433\n",
      "Batch train [16] loss 0.77487, dsc 0.22513\n",
      "Batch train [17] loss 0.75168, dsc 0.24832\n",
      "Batch train [18] loss 0.69834, dsc 0.30166\n",
      "Batch train [19] loss 0.66313, dsc 0.33687\n",
      "Batch train [20] loss 0.72982, dsc 0.27018\n",
      "Batch train [21] loss 0.72333, dsc 0.27667\n",
      "Batch train [22] loss 0.75788, dsc 0.24212\n",
      "Batch train [23] loss 0.74470, dsc 0.25530\n",
      "Batch train [24] loss 0.74489, dsc 0.25511\n",
      "Batch train [25] loss 0.75942, dsc 0.24058\n",
      "Batch train [26] loss 0.74869, dsc 0.25131\n",
      "Batch train [27] loss 0.69941, dsc 0.30059\n",
      "Batch train [28] loss 0.66021, dsc 0.33979\n",
      "Batch train [29] loss 0.73148, dsc 0.26852\n",
      "Batch train [30] loss 0.65563, dsc 0.34437\n",
      "Batch train [31] loss 0.70737, dsc 0.29263\n",
      "Batch train [32] loss 0.70682, dsc 0.29318\n",
      "Batch train [33] loss 0.68825, dsc 0.31175\n",
      "Batch train [34] loss 0.68090, dsc 0.31910\n",
      "Batch train [35] loss 0.68936, dsc 0.31064\n",
      "Batch train [36] loss 0.73681, dsc 0.26319\n",
      "Batch train [37] loss 0.68893, dsc 0.31107\n",
      "Batch train [38] loss 0.63096, dsc 0.36904\n",
      "Batch train [39] loss 0.76935, dsc 0.23065\n",
      "Batch train [40] loss 0.69280, dsc 0.30720\n",
      "Epoch [12] train done\n",
      "Batch eval [1] loss 0.65590, dsc 0.34410\n",
      "Batch eval [2] loss 0.68454, dsc 0.31546\n",
      "Batch eval [3] loss 0.66468, dsc 0.33532\n",
      "Batch eval [4] loss 0.71103, dsc 0.28897\n",
      "Batch eval [5] loss 0.67073, dsc 0.32927\n",
      "Epoch [12] valid done\n",
      "Epoch [12] T 88.76s, deltaT 7.39s, loss: train 0.72414, valid 0.67738, dsc: train 0.27586, valid 0.32262\n",
      "Batch train [1] loss 0.70959, dsc 0.29041\n",
      "Batch train [2] loss 0.63338, dsc 0.36662\n",
      "Batch train [3] loss 0.70429, dsc 0.29571\n",
      "Batch train [4] loss 0.62599, dsc 0.37401\n",
      "Batch train [5] loss 0.64680, dsc 0.35320\n",
      "Batch train [6] loss 0.71186, dsc 0.28814\n",
      "Batch train [7] loss 0.66477, dsc 0.33523\n",
      "Batch train [8] loss 0.66316, dsc 0.33684\n",
      "Batch train [9] loss 0.64897, dsc 0.35103\n",
      "Batch train [10] loss 0.65058, dsc 0.34942\n",
      "Batch train [11] loss 0.63744, dsc 0.36256\n",
      "Batch train [12] loss 0.60296, dsc 0.39704\n",
      "Batch train [13] loss 0.67284, dsc 0.32716\n",
      "Batch train [14] loss 0.64396, dsc 0.35604\n",
      "Batch train [15] loss 0.66825, dsc 0.33175\n",
      "Batch train [16] loss 0.72253, dsc 0.27747\n",
      "Batch train [17] loss 0.63921, dsc 0.36078\n",
      "Batch train [18] loss 0.61716, dsc 0.38284\n",
      "Batch train [19] loss 0.63231, dsc 0.36769\n",
      "Batch train [20] loss 0.63989, dsc 0.36011\n",
      "Batch train [21] loss 0.64890, dsc 0.35110\n",
      "Batch train [22] loss 0.67378, dsc 0.32622\n",
      "Batch train [23] loss 0.64668, dsc 0.35332\n",
      "Batch train [24] loss 0.60142, dsc 0.39858\n",
      "Batch train [25] loss 0.61717, dsc 0.38283\n",
      "Batch train [26] loss 0.60899, dsc 0.39101\n",
      "Batch train [27] loss 0.58647, dsc 0.41353\n",
      "Batch train [28] loss 0.65376, dsc 0.34624\n",
      "Batch train [29] loss 0.70661, dsc 0.29339\n",
      "Batch train [30] loss 0.59749, dsc 0.40251\n",
      "Batch train [31] loss 0.65397, dsc 0.34603\n",
      "Batch train [32] loss 0.60358, dsc 0.39642\n",
      "Batch train [33] loss 0.58196, dsc 0.41804\n",
      "Batch train [34] loss 0.57084, dsc 0.42916\n",
      "Batch train [35] loss 0.60140, dsc 0.39860\n",
      "Batch train [36] loss 0.53677, dsc 0.46323\n",
      "Batch train [37] loss 0.53809, dsc 0.46191\n",
      "Batch train [38] loss 0.50428, dsc 0.49572\n",
      "Batch train [39] loss 0.54447, dsc 0.45553\n",
      "Batch train [40] loss 0.58866, dsc 0.41134\n",
      "Epoch [13] train done\n",
      "Batch eval [1] loss 0.54005, dsc 0.45995\n",
      "Batch eval [2] loss 0.58685, dsc 0.41315\n",
      "Batch eval [3] loss 0.54799, dsc 0.45201\n",
      "Batch eval [4] loss 0.61922, dsc 0.38078\n",
      "Batch eval [5] loss 0.55692, dsc 0.44308\n",
      "Epoch [13] valid done\n",
      "Epoch [13] T 96.31s, deltaT 7.54s, loss: train 0.63003, valid 0.57021, dsc: train 0.36997, valid 0.42979\n",
      "Batch train [1] loss 0.56695, dsc 0.43305\n",
      "Batch train [2] loss 0.58397, dsc 0.41603\n",
      "Batch train [3] loss 0.49300, dsc 0.50700\n",
      "Batch train [4] loss 0.60598, dsc 0.39402\n",
      "Batch train [5] loss 0.51324, dsc 0.48676\n",
      "Batch train [6] loss 0.52891, dsc 0.47109\n",
      "Batch train [7] loss 0.55212, dsc 0.44788\n",
      "Batch train [8] loss 0.54720, dsc 0.45280\n",
      "Batch train [9] loss 0.54027, dsc 0.45973\n",
      "Batch train [10] loss 0.60568, dsc 0.39432\n",
      "Batch train [11] loss 0.53705, dsc 0.46295\n",
      "Batch train [12] loss 0.54828, dsc 0.45172\n",
      "Batch train [13] loss 0.60095, dsc 0.39905\n",
      "Batch train [14] loss 0.48993, dsc 0.51007\n",
      "Batch train [15] loss 0.59520, dsc 0.40480\n",
      "Batch train [16] loss 0.50691, dsc 0.49309\n",
      "Batch train [17] loss 0.58159, dsc 0.41841\n",
      "Batch train [18] loss 0.53894, dsc 0.46106\n",
      "Batch train [19] loss 0.46508, dsc 0.53492\n",
      "Batch train [20] loss 0.52335, dsc 0.47665\n",
      "Batch train [21] loss 0.52551, dsc 0.47449\n",
      "Batch train [22] loss 0.51712, dsc 0.48288\n",
      "Batch train [23] loss 0.52054, dsc 0.47946\n",
      "Batch train [24] loss 0.51847, dsc 0.48153\n",
      "Batch train [25] loss 0.54183, dsc 0.45817\n",
      "Batch train [26] loss 0.61231, dsc 0.38769\n",
      "Batch train [27] loss 0.51998, dsc 0.48002\n",
      "Batch train [28] loss 0.54198, dsc 0.45802\n",
      "Batch train [29] loss 0.51673, dsc 0.48327\n",
      "Batch train [30] loss 0.48577, dsc 0.51423\n",
      "Batch train [31] loss 0.53031, dsc 0.46969\n",
      "Batch train [32] loss 0.52619, dsc 0.47381\n",
      "Batch train [33] loss 0.50386, dsc 0.49614\n",
      "Batch train [34] loss 0.59570, dsc 0.40430\n",
      "Batch train [35] loss 0.51634, dsc 0.48366\n",
      "Batch train [36] loss 0.54250, dsc 0.45750\n",
      "Batch train [37] loss 0.46565, dsc 0.53435\n",
      "Batch train [38] loss 0.43068, dsc 0.56932\n",
      "Batch train [39] loss 0.48627, dsc 0.51373\n",
      "Batch train [40] loss 0.48586, dsc 0.51414\n",
      "Epoch [14] train done\n",
      "Batch eval [1] loss 0.46383, dsc 0.53617\n",
      "Batch eval [2] loss 0.50814, dsc 0.49186\n",
      "Batch eval [3] loss 0.46863, dsc 0.53137\n",
      "Batch eval [4] loss 0.55407, dsc 0.44593\n",
      "Batch eval [5] loss 0.48046, dsc 0.51954\n",
      "Epoch [14] valid done\n",
      "Epoch [14] T 103.80s, deltaT 7.50s, loss: train 0.53270, valid 0.49502, dsc: train 0.46730, valid 0.50498\n",
      "Batch train [1] loss 0.60223, dsc 0.39777\n",
      "Batch train [2] loss 0.44648, dsc 0.55352\n",
      "Batch train [3] loss 0.43094, dsc 0.56906\n",
      "Batch train [4] loss 0.52467, dsc 0.47533\n",
      "Batch train [5] loss 0.49970, dsc 0.50030\n",
      "Batch train [6] loss 0.45891, dsc 0.54109\n",
      "Batch train [7] loss 0.48371, dsc 0.51629\n",
      "Batch train [8] loss 0.47906, dsc 0.52094\n",
      "Batch train [9] loss 0.43393, dsc 0.56607\n",
      "Batch train [10] loss 0.49283, dsc 0.50717\n",
      "Batch train [11] loss 0.48419, dsc 0.51581\n",
      "Batch train [12] loss 0.44762, dsc 0.55238\n",
      "Batch train [13] loss 0.45943, dsc 0.54057\n",
      "Batch train [14] loss 0.45284, dsc 0.54716\n",
      "Batch train [15] loss 0.42793, dsc 0.57207\n",
      "Batch train [16] loss 0.46070, dsc 0.53930\n",
      "Batch train [17] loss 0.41751, dsc 0.58249\n",
      "Batch train [18] loss 0.45700, dsc 0.54300\n",
      "Batch train [19] loss 0.48084, dsc 0.51916\n",
      "Batch train [20] loss 0.44574, dsc 0.55426\n",
      "Batch train [21] loss 0.45123, dsc 0.54877\n",
      "Batch train [22] loss 0.47815, dsc 0.52185\n",
      "Batch train [23] loss 0.43951, dsc 0.56049\n",
      "Batch train [24] loss 0.36428, dsc 0.63572\n",
      "Batch train [25] loss 0.41817, dsc 0.58183\n",
      "Batch train [26] loss 0.47609, dsc 0.52391\n",
      "Batch train [27] loss 0.44420, dsc 0.55580\n",
      "Batch train [28] loss 0.39744, dsc 0.60256\n",
      "Batch train [29] loss 0.41801, dsc 0.58199\n",
      "Batch train [30] loss 0.36487, dsc 0.63513\n",
      "Batch train [31] loss 0.37815, dsc 0.62185\n",
      "Batch train [32] loss 0.41877, dsc 0.58123\n",
      "Batch train [33] loss 0.52177, dsc 0.47823\n",
      "Batch train [34] loss 0.41457, dsc 0.58543\n",
      "Batch train [35] loss 0.45516, dsc 0.54484\n",
      "Batch train [36] loss 0.39689, dsc 0.60311\n",
      "Batch train [37] loss 0.39667, dsc 0.60333\n",
      "Batch train [38] loss 0.45181, dsc 0.54819\n",
      "Batch train [39] loss 0.39639, dsc 0.60361\n",
      "Batch train [40] loss 0.36994, dsc 0.63006\n",
      "Epoch [15] train done\n",
      "Batch eval [1] loss 0.37492, dsc 0.62508\n",
      "Batch eval [2] loss 0.40061, dsc 0.59939\n",
      "Batch eval [3] loss 0.40519, dsc 0.59481\n",
      "Batch eval [4] loss 0.40506, dsc 0.59494\n",
      "Batch eval [5] loss 0.38576, dsc 0.61424\n",
      "Epoch [15] valid done\n",
      "Epoch [15] T 111.36s, deltaT 7.56s, loss: train 0.44596, valid 0.39431, dsc: train 0.55404, valid 0.60569\n",
      "Batch train [1] loss 0.38368, dsc 0.61632\n",
      "Batch train [2] loss 0.39786, dsc 0.60214\n",
      "Batch train [3] loss 0.40388, dsc 0.59612\n",
      "Batch train [4] loss 0.37029, dsc 0.62971\n",
      "Batch train [5] loss 0.32085, dsc 0.67915\n",
      "Batch train [6] loss 0.44539, dsc 0.55461\n",
      "Batch train [7] loss 0.39009, dsc 0.60991\n",
      "Batch train [8] loss 0.42894, dsc 0.57106\n",
      "Batch train [9] loss 0.40988, dsc 0.59012\n",
      "Batch train [10] loss 0.40745, dsc 0.59255\n",
      "Batch train [11] loss 0.38471, dsc 0.61529\n",
      "Batch train [12] loss 0.39180, dsc 0.60820\n",
      "Batch train [13] loss 0.37305, dsc 0.62695\n",
      "Batch train [14] loss 0.49366, dsc 0.50634\n",
      "Batch train [15] loss 0.39027, dsc 0.60973\n",
      "Batch train [16] loss 0.33218, dsc 0.66782\n",
      "Batch train [17] loss 0.39654, dsc 0.60346\n",
      "Batch train [18] loss 0.35029, dsc 0.64971\n",
      "Batch train [19] loss 0.35467, dsc 0.64533\n",
      "Batch train [20] loss 0.35996, dsc 0.64004\n",
      "Batch train [21] loss 0.34041, dsc 0.65959\n",
      "Batch train [22] loss 0.37336, dsc 0.62664\n",
      "Batch train [23] loss 0.31192, dsc 0.68808\n",
      "Batch train [24] loss 0.35265, dsc 0.64735\n",
      "Batch train [25] loss 0.37472, dsc 0.62528\n",
      "Batch train [26] loss 0.35332, dsc 0.64668\n",
      "Batch train [27] loss 0.31572, dsc 0.68428\n",
      "Batch train [28] loss 0.34384, dsc 0.65616\n",
      "Batch train [29] loss 0.34969, dsc 0.65031\n",
      "Batch train [30] loss 0.38134, dsc 0.61866\n",
      "Batch train [31] loss 0.31674, dsc 0.68326\n",
      "Batch train [32] loss 0.36151, dsc 0.63849\n",
      "Batch train [33] loss 0.32296, dsc 0.67704\n",
      "Batch train [34] loss 0.36725, dsc 0.63275\n",
      "Batch train [35] loss 0.37882, dsc 0.62118\n",
      "Batch train [36] loss 0.34360, dsc 0.65640\n",
      "Batch train [37] loss 0.37429, dsc 0.62571\n",
      "Batch train [38] loss 0.31449, dsc 0.68551\n",
      "Batch train [39] loss 0.33531, dsc 0.66469\n",
      "Batch train [40] loss 0.43615, dsc 0.56385\n",
      "Epoch [16] train done\n",
      "Batch eval [1] loss 0.33520, dsc 0.66480\n",
      "Batch eval [2] loss 0.36413, dsc 0.63587\n",
      "Batch eval [3] loss 0.33968, dsc 0.66032\n",
      "Batch eval [4] loss 0.37559, dsc 0.62441\n",
      "Batch eval [5] loss 0.33351, dsc 0.66649\n",
      "Epoch [16] valid done\n",
      "Epoch [16] T 118.82s, deltaT 7.45s, loss: train 0.37084, valid 0.34962, dsc: train 0.62916, valid 0.65038\n",
      "Batch train [1] loss 0.31970, dsc 0.68030\n",
      "Batch train [2] loss 0.33433, dsc 0.66567\n",
      "Batch train [3] loss 0.33279, dsc 0.66721\n",
      "Batch train [4] loss 0.32003, dsc 0.67997\n",
      "Batch train [5] loss 0.42595, dsc 0.57405\n",
      "Batch train [6] loss 0.32886, dsc 0.67114\n",
      "Batch train [7] loss 0.36440, dsc 0.63560\n",
      "Batch train [8] loss 0.32117, dsc 0.67883\n",
      "Batch train [9] loss 0.30595, dsc 0.69405\n",
      "Batch train [10] loss 0.32122, dsc 0.67878\n",
      "Batch train [11] loss 0.31602, dsc 0.68398\n",
      "Batch train [12] loss 0.31150, dsc 0.68850\n",
      "Batch train [13] loss 0.29588, dsc 0.70412\n",
      "Batch train [14] loss 0.30046, dsc 0.69954\n",
      "Batch train [15] loss 0.33705, dsc 0.66295\n",
      "Batch train [16] loss 0.28215, dsc 0.71785\n",
      "Batch train [17] loss 0.33728, dsc 0.66272\n",
      "Batch train [18] loss 0.34043, dsc 0.65957\n",
      "Batch train [19] loss 0.31590, dsc 0.68410\n",
      "Batch train [20] loss 0.29760, dsc 0.70240\n",
      "Batch train [21] loss 0.30131, dsc 0.69869\n",
      "Batch train [22] loss 0.32544, dsc 0.67456\n",
      "Batch train [23] loss 0.26953, dsc 0.73047\n",
      "Batch train [24] loss 0.31154, dsc 0.68846\n",
      "Batch train [25] loss 0.30091, dsc 0.69909\n",
      "Batch train [26] loss 0.29015, dsc 0.70985\n",
      "Batch train [27] loss 0.32945, dsc 0.67055\n",
      "Batch train [28] loss 0.29165, dsc 0.70835\n",
      "Batch train [29] loss 0.28842, dsc 0.71158\n",
      "Batch train [30] loss 0.31210, dsc 0.68790\n",
      "Batch train [31] loss 0.30406, dsc 0.69594\n",
      "Batch train [32] loss 0.28111, dsc 0.71889\n",
      "Batch train [33] loss 0.29308, dsc 0.70692\n",
      "Batch train [34] loss 0.30600, dsc 0.69400\n",
      "Batch train [35] loss 0.32952, dsc 0.67048\n",
      "Batch train [36] loss 0.34126, dsc 0.65874\n",
      "Batch train [37] loss 0.28572, dsc 0.71428\n",
      "Batch train [38] loss 0.31712, dsc 0.68288\n",
      "Batch train [39] loss 0.41165, dsc 0.58835\n",
      "Batch train [40] loss 0.33405, dsc 0.66595\n",
      "Epoch [17] train done\n",
      "Batch eval [1] loss 0.34828, dsc 0.65172\n",
      "Batch eval [2] loss 0.34343, dsc 0.65657\n",
      "Batch eval [3] loss 0.37282, dsc 0.62718\n",
      "Batch eval [4] loss 0.33766, dsc 0.66234\n",
      "Batch eval [5] loss 0.32411, dsc 0.67589\n",
      "Epoch [17] valid done\n",
      "Epoch [17] T 126.37s, deltaT 7.55s, loss: train 0.31832, valid 0.34526, dsc: train 0.68168, valid 0.65474\n",
      "Batch train [1] loss 0.29333, dsc 0.70667\n",
      "Batch train [2] loss 0.30876, dsc 0.69124\n",
      "Batch train [3] loss 0.25194, dsc 0.74806\n",
      "Batch train [4] loss 0.37408, dsc 0.62592\n",
      "Batch train [5] loss 0.24232, dsc 0.75768\n",
      "Batch train [6] loss 0.26909, dsc 0.73091\n",
      "Batch train [7] loss 0.26000, dsc 0.74000\n",
      "Batch train [8] loss 0.28885, dsc 0.71115\n",
      "Batch train [9] loss 0.31239, dsc 0.68761\n",
      "Batch train [10] loss 0.41382, dsc 0.58618\n",
      "Batch train [11] loss 0.27154, dsc 0.72846\n",
      "Batch train [12] loss 0.29749, dsc 0.70251\n",
      "Batch train [13] loss 0.31759, dsc 0.68241\n",
      "Batch train [14] loss 0.29377, dsc 0.70623\n",
      "Batch train [15] loss 0.32052, dsc 0.67948\n",
      "Batch train [16] loss 0.26271, dsc 0.73729\n",
      "Batch train [17] loss 0.28220, dsc 0.71780\n",
      "Batch train [18] loss 0.31018, dsc 0.68982\n",
      "Batch train [19] loss 0.27183, dsc 0.72817\n",
      "Batch train [20] loss 0.28622, dsc 0.71378\n",
      "Batch train [21] loss 0.31067, dsc 0.68933\n",
      "Batch train [22] loss 0.29822, dsc 0.70178\n",
      "Batch train [23] loss 0.25588, dsc 0.74412\n",
      "Batch train [24] loss 0.26763, dsc 0.73237\n",
      "Batch train [25] loss 0.26291, dsc 0.73709\n",
      "Batch train [26] loss 0.25706, dsc 0.74294\n",
      "Batch train [27] loss 0.26424, dsc 0.73576\n",
      "Batch train [28] loss 0.24823, dsc 0.75177\n",
      "Batch train [29] loss 0.25446, dsc 0.74554\n",
      "Batch train [30] loss 0.24699, dsc 0.75301\n",
      "Batch train [31] loss 0.24367, dsc 0.75633\n",
      "Batch train [32] loss 0.30077, dsc 0.69923\n",
      "Batch train [33] loss 0.27689, dsc 0.72311\n",
      "Batch train [34] loss 0.27356, dsc 0.72644\n",
      "Batch train [35] loss 0.25678, dsc 0.74322\n",
      "Batch train [36] loss 0.26054, dsc 0.73946\n",
      "Batch train [37] loss 0.21535, dsc 0.78465\n",
      "Batch train [38] loss 0.26547, dsc 0.73453\n",
      "Batch train [39] loss 0.25439, dsc 0.74561\n",
      "Batch train [40] loss 0.26886, dsc 0.73114\n",
      "Epoch [18] train done\n",
      "Batch eval [1] loss 0.24027, dsc 0.75973\n",
      "Batch eval [2] loss 0.27310, dsc 0.72690\n",
      "Batch eval [3] loss 0.27200, dsc 0.72800\n",
      "Batch eval [4] loss 0.30095, dsc 0.69905\n",
      "Batch eval [5] loss 0.24824, dsc 0.75176\n",
      "Epoch [18] valid done\n",
      "Epoch [18] T 133.69s, deltaT 7.32s, loss: train 0.28028, valid 0.26691, dsc: train 0.71972, valid 0.73309\n",
      "Batch train [1] loss 0.28401, dsc 0.71599\n",
      "Batch train [2] loss 0.26773, dsc 0.73227\n",
      "Batch train [3] loss 0.27062, dsc 0.72938\n",
      "Batch train [4] loss 0.26224, dsc 0.73776\n",
      "Batch train [5] loss 0.28357, dsc 0.71643\n",
      "Batch train [6] loss 0.28242, dsc 0.71758\n",
      "Batch train [7] loss 0.27854, dsc 0.72146\n",
      "Batch train [8] loss 0.24986, dsc 0.75014\n",
      "Batch train [9] loss 0.24915, dsc 0.75085\n",
      "Batch train [10] loss 0.23815, dsc 0.76185\n",
      "Batch train [11] loss 0.23401, dsc 0.76599\n",
      "Batch train [12] loss 0.32844, dsc 0.67156\n",
      "Batch train [13] loss 0.25395, dsc 0.74605\n",
      "Batch train [14] loss 0.23488, dsc 0.76512\n",
      "Batch train [15] loss 0.25004, dsc 0.74996\n",
      "Batch train [16] loss 0.26433, dsc 0.73567\n",
      "Batch train [17] loss 0.34727, dsc 0.65273\n",
      "Batch train [18] loss 0.25739, dsc 0.74261\n",
      "Batch train [19] loss 0.23219, dsc 0.76781\n",
      "Batch train [20] loss 0.23894, dsc 0.76106\n",
      "Batch train [21] loss 0.27183, dsc 0.72817\n",
      "Batch train [22] loss 0.25166, dsc 0.74834\n",
      "Batch train [23] loss 0.19264, dsc 0.80736\n",
      "Batch train [24] loss 0.24704, dsc 0.75296\n",
      "Batch train [25] loss 0.23800, dsc 0.76200\n",
      "Batch train [26] loss 0.23432, dsc 0.76568\n",
      "Batch train [27] loss 0.26862, dsc 0.73138\n",
      "Batch train [28] loss 0.23092, dsc 0.76908\n",
      "Batch train [29] loss 0.23111, dsc 0.76889\n",
      "Batch train [30] loss 0.26311, dsc 0.73689\n",
      "Batch train [31] loss 0.22603, dsc 0.77397\n",
      "Batch train [32] loss 0.22777, dsc 0.77223\n",
      "Batch train [33] loss 0.25846, dsc 0.74154\n",
      "Batch train [34] loss 0.19770, dsc 0.80230\n",
      "Batch train [35] loss 0.23928, dsc 0.76072\n",
      "Batch train [36] loss 0.23300, dsc 0.76700\n",
      "Batch train [37] loss 0.24064, dsc 0.75936\n",
      "Batch train [38] loss 0.24235, dsc 0.75765\n",
      "Batch train [39] loss 0.21968, dsc 0.78032\n",
      "Batch train [40] loss 0.22820, dsc 0.77180\n",
      "Epoch [19] train done\n",
      "Batch eval [1] loss 0.26538, dsc 0.73462\n",
      "Batch eval [2] loss 0.26416, dsc 0.73584\n",
      "Batch eval [3] loss 0.31888, dsc 0.68112\n",
      "Batch eval [4] loss 0.25319, dsc 0.74681\n",
      "Batch eval [5] loss 0.24952, dsc 0.75048\n",
      "Epoch [19] valid done\n",
      "Epoch [19] T 141.04s, deltaT 7.35s, loss: train 0.25125, valid 0.27023, dsc: train 0.74875, valid 0.72977\n",
      "Batch train [1] loss 0.21488, dsc 0.78512\n",
      "Batch train [2] loss 0.25775, dsc 0.74225\n",
      "Batch train [3] loss 0.22660, dsc 0.77340\n",
      "Batch train [4] loss 0.34103, dsc 0.65897\n",
      "Batch train [5] loss 0.22913, dsc 0.77087\n",
      "Batch train [6] loss 0.25147, dsc 0.74853\n",
      "Batch train [7] loss 0.25072, dsc 0.74928\n",
      "Batch train [8] loss 0.20358, dsc 0.79642\n",
      "Batch train [9] loss 0.22249, dsc 0.77751\n",
      "Batch train [10] loss 0.26425, dsc 0.73575\n",
      "Batch train [11] loss 0.30117, dsc 0.69883\n",
      "Batch train [12] loss 0.25083, dsc 0.74917\n",
      "Batch train [13] loss 0.21814, dsc 0.78186\n",
      "Batch train [14] loss 0.22716, dsc 0.77284\n",
      "Batch train [15] loss 0.21752, dsc 0.78248\n",
      "Batch train [16] loss 0.23658, dsc 0.76342\n",
      "Batch train [17] loss 0.20119, dsc 0.79881\n",
      "Batch train [18] loss 0.18691, dsc 0.81309\n",
      "Batch train [19] loss 0.22970, dsc 0.77030\n",
      "Batch train [20] loss 0.24125, dsc 0.75875\n",
      "Batch train [21] loss 0.18431, dsc 0.81569\n",
      "Batch train [22] loss 0.19899, dsc 0.80101\n",
      "Batch train [23] loss 0.23939, dsc 0.76061\n",
      "Batch train [24] loss 0.18940, dsc 0.81060\n",
      "Batch train [25] loss 0.22344, dsc 0.77656\n",
      "Batch train [26] loss 0.22854, dsc 0.77146\n",
      "Batch train [27] loss 0.22185, dsc 0.77815\n",
      "Batch train [28] loss 0.24807, dsc 0.75193\n",
      "Batch train [29] loss 0.22229, dsc 0.77771\n",
      "Batch train [30] loss 0.20858, dsc 0.79142\n",
      "Batch train [31] loss 0.20449, dsc 0.79551\n",
      "Batch train [32] loss 0.23698, dsc 0.76302\n",
      "Batch train [33] loss 0.19901, dsc 0.80099\n",
      "Batch train [34] loss 0.24627, dsc 0.75373\n",
      "Batch train [35] loss 0.21339, dsc 0.78661\n",
      "Batch train [36] loss 0.23280, dsc 0.76720\n",
      "Batch train [37] loss 0.21155, dsc 0.78845\n",
      "Batch train [38] loss 0.20343, dsc 0.79657\n",
      "Batch train [39] loss 0.23024, dsc 0.76976\n",
      "Batch train [40] loss 0.22326, dsc 0.77674\n",
      "Epoch [20] train done\n",
      "Batch eval [1] loss 0.22165, dsc 0.77835\n",
      "Batch eval [2] loss 0.25695, dsc 0.74305\n",
      "Batch eval [3] loss 0.23165, dsc 0.76835\n",
      "Batch eval [4] loss 0.25135, dsc 0.74865\n",
      "Batch eval [5] loss 0.23073, dsc 0.76927\n",
      "Epoch [20] valid done\n",
      "Epoch [20] T 148.38s, deltaT 7.34s, loss: train 0.22847, valid 0.23846, dsc: train 0.77153, valid 0.76154\n",
      "Batch train [1] loss 0.19337, dsc 0.80663\n",
      "Batch train [2] loss 0.19272, dsc 0.80728\n",
      "Batch train [3] loss 0.17805, dsc 0.82195\n",
      "Batch train [4] loss 0.21481, dsc 0.78519\n",
      "Batch train [5] loss 0.20409, dsc 0.79591\n",
      "Batch train [6] loss 0.19507, dsc 0.80493\n",
      "Batch train [7] loss 0.21022, dsc 0.78978\n",
      "Batch train [8] loss 0.23567, dsc 0.76433\n",
      "Batch train [9] loss 0.21963, dsc 0.78037\n",
      "Batch train [10] loss 0.23295, dsc 0.76705\n",
      "Batch train [11] loss 0.31114, dsc 0.68886\n",
      "Batch train [12] loss 0.19508, dsc 0.80492\n",
      "Batch train [13] loss 0.26806, dsc 0.73194\n",
      "Batch train [14] loss 0.19198, dsc 0.80802\n",
      "Batch train [15] loss 0.21437, dsc 0.78563\n",
      "Batch train [16] loss 0.22553, dsc 0.77447\n",
      "Batch train [17] loss 0.19197, dsc 0.80803\n",
      "Batch train [18] loss 0.19894, dsc 0.80106\n",
      "Batch train [19] loss 0.19568, dsc 0.80432\n",
      "Batch train [20] loss 0.21546, dsc 0.78454\n",
      "Batch train [21] loss 0.21387, dsc 0.78613\n",
      "Batch train [22] loss 0.21021, dsc 0.78979\n",
      "Batch train [23] loss 0.24260, dsc 0.75740\n",
      "Batch train [24] loss 0.18133, dsc 0.81867\n",
      "Batch train [25] loss 0.22939, dsc 0.77061\n",
      "Batch train [26] loss 0.19920, dsc 0.80080\n",
      "Batch train [27] loss 0.17939, dsc 0.82061\n",
      "Batch train [28] loss 0.19620, dsc 0.80380\n",
      "Batch train [29] loss 0.22597, dsc 0.77403\n",
      "Batch train [30] loss 0.18073, dsc 0.81927\n",
      "Batch train [31] loss 0.20324, dsc 0.79676\n",
      "Batch train [32] loss 0.21648, dsc 0.78352\n",
      "Batch train [33] loss 0.20572, dsc 0.79428\n",
      "Batch train [34] loss 0.22714, dsc 0.77286\n",
      "Batch train [35] loss 0.17404, dsc 0.82596\n",
      "Batch train [36] loss 0.21726, dsc 0.78274\n",
      "Batch train [37] loss 0.18779, dsc 0.81221\n",
      "Batch train [38] loss 0.19332, dsc 0.80668\n",
      "Batch train [39] loss 0.20955, dsc 0.79045\n",
      "Batch train [40] loss 0.17423, dsc 0.82577\n",
      "Epoch [21] train done\n",
      "Batch eval [1] loss 0.22634, dsc 0.77366\n",
      "Batch eval [2] loss 0.23685, dsc 0.76315\n",
      "Batch eval [3] loss 0.27434, dsc 0.72566\n",
      "Batch eval [4] loss 0.21466, dsc 0.78534\n",
      "Batch eval [5] loss 0.23469, dsc 0.76531\n",
      "Epoch [21] valid done\n",
      "Epoch [21] T 155.73s, deltaT 7.35s, loss: train 0.20881, valid 0.23738, dsc: train 0.79119, valid 0.76262\n",
      "Batch train [1] loss 0.19026, dsc 0.80974\n",
      "Batch train [2] loss 0.17439, dsc 0.82561\n",
      "Batch train [3] loss 0.19630, dsc 0.80370\n",
      "Batch train [4] loss 0.18459, dsc 0.81541\n",
      "Batch train [5] loss 0.16635, dsc 0.83365\n",
      "Batch train [6] loss 0.18668, dsc 0.81332\n",
      "Batch train [7] loss 0.25569, dsc 0.74431\n",
      "Batch train [8] loss 0.18019, dsc 0.81981\n",
      "Batch train [9] loss 0.17492, dsc 0.82508\n",
      "Batch train [10] loss 0.19108, dsc 0.80892\n",
      "Batch train [11] loss 0.18042, dsc 0.81958\n",
      "Batch train [12] loss 0.19005, dsc 0.80995\n",
      "Batch train [13] loss 0.18050, dsc 0.81950\n",
      "Batch train [14] loss 0.20456, dsc 0.79544\n",
      "Batch train [15] loss 0.18316, dsc 0.81684\n",
      "Batch train [16] loss 0.21859, dsc 0.78141\n",
      "Batch train [17] loss 0.21681, dsc 0.78319\n",
      "Batch train [18] loss 0.19569, dsc 0.80431\n",
      "Batch train [19] loss 0.21193, dsc 0.78807\n",
      "Batch train [20] loss 0.18888, dsc 0.81112\n",
      "Batch train [21] loss 0.18386, dsc 0.81614\n",
      "Batch train [22] loss 0.30125, dsc 0.69875\n",
      "Batch train [23] loss 0.20048, dsc 0.79952\n",
      "Batch train [24] loss 0.16602, dsc 0.83398\n",
      "Batch train [25] loss 0.18544, dsc 0.81456\n",
      "Batch train [26] loss 0.19153, dsc 0.80847\n",
      "Batch train [27] loss 0.17802, dsc 0.82198\n",
      "Batch train [28] loss 0.17053, dsc 0.82947\n",
      "Batch train [29] loss 0.17629, dsc 0.82371\n",
      "Batch train [30] loss 0.17482, dsc 0.82518\n",
      "Batch train [31] loss 0.20618, dsc 0.79382\n",
      "Batch train [32] loss 0.16163, dsc 0.83837\n",
      "Batch train [33] loss 0.18943, dsc 0.81057\n",
      "Batch train [34] loss 0.19054, dsc 0.80946\n",
      "Batch train [35] loss 0.18739, dsc 0.81261\n",
      "Batch train [36] loss 0.21773, dsc 0.78227\n",
      "Batch train [37] loss 0.16242, dsc 0.83758\n",
      "Batch train [38] loss 0.18846, dsc 0.81154\n",
      "Batch train [39] loss 0.18992, dsc 0.81008\n",
      "Batch train [40] loss 0.20293, dsc 0.79707\n",
      "Epoch [22] train done\n",
      "Batch eval [1] loss 0.23443, dsc 0.76557\n",
      "Batch eval [2] loss 0.25425, dsc 0.74575\n",
      "Batch eval [3] loss 0.27600, dsc 0.72400\n",
      "Batch eval [4] loss 0.20969, dsc 0.79031\n",
      "Batch eval [5] loss 0.22143, dsc 0.77857\n",
      "Epoch [22] valid done\n",
      "Epoch [22] T 163.08s, deltaT 7.34s, loss: train 0.19240, valid 0.23916, dsc: train 0.80760, valid 0.76084\n",
      "Batch train [1] loss 0.16626, dsc 0.83374\n",
      "Batch train [2] loss 0.18429, dsc 0.81571\n",
      "Batch train [3] loss 0.19692, dsc 0.80308\n",
      "Batch train [4] loss 0.17400, dsc 0.82600\n",
      "Batch train [5] loss 0.15967, dsc 0.84033\n",
      "Batch train [6] loss 0.22042, dsc 0.77958\n",
      "Batch train [7] loss 0.19749, dsc 0.80251\n",
      "Batch train [8] loss 0.17438, dsc 0.82562\n",
      "Batch train [9] loss 0.18331, dsc 0.81669\n",
      "Batch train [10] loss 0.18088, dsc 0.81912\n",
      "Batch train [11] loss 0.17041, dsc 0.82959\n",
      "Batch train [12] loss 0.16972, dsc 0.83028\n",
      "Batch train [13] loss 0.18033, dsc 0.81967\n",
      "Batch train [14] loss 0.18609, dsc 0.81391\n",
      "Batch train [15] loss 0.16810, dsc 0.83190\n",
      "Batch train [16] loss 0.18376, dsc 0.81624\n",
      "Batch train [17] loss 0.20750, dsc 0.79250\n",
      "Batch train [18] loss 0.18632, dsc 0.81368\n",
      "Batch train [19] loss 0.14212, dsc 0.85788\n",
      "Batch train [20] loss 0.17693, dsc 0.82307\n",
      "Batch train [21] loss 0.19249, dsc 0.80751\n",
      "Batch train [22] loss 0.18032, dsc 0.81968\n",
      "Batch train [23] loss 0.15032, dsc 0.84968\n",
      "Batch train [24] loss 0.19078, dsc 0.80922\n",
      "Batch train [25] loss 0.19428, dsc 0.80572\n",
      "Batch train [26] loss 0.18515, dsc 0.81485\n",
      "Batch train [27] loss 0.17288, dsc 0.82712\n",
      "Batch train [28] loss 0.15208, dsc 0.84792\n",
      "Batch train [29] loss 0.26370, dsc 0.73630\n",
      "Batch train [30] loss 0.18214, dsc 0.81786\n",
      "Batch train [31] loss 0.17699, dsc 0.82301\n",
      "Batch train [32] loss 0.17423, dsc 0.82577\n",
      "Batch train [33] loss 0.18756, dsc 0.81244\n",
      "Batch train [34] loss 0.16116, dsc 0.83884\n",
      "Batch train [35] loss 0.21287, dsc 0.78713\n",
      "Batch train [36] loss 0.16965, dsc 0.83035\n",
      "Batch train [37] loss 0.15206, dsc 0.84794\n",
      "Batch train [38] loss 0.16395, dsc 0.83605\n",
      "Batch train [39] loss 0.23814, dsc 0.76186\n",
      "Batch train [40] loss 0.18547, dsc 0.81453\n",
      "Epoch [23] train done\n",
      "Batch eval [1] loss 0.18831, dsc 0.81169\n",
      "Batch eval [2] loss 0.23148, dsc 0.76852\n",
      "Batch eval [3] loss 0.21921, dsc 0.78079\n",
      "Batch eval [4] loss 0.20712, dsc 0.79288\n",
      "Batch eval [5] loss 0.21715, dsc 0.78285\n",
      "Epoch [23] valid done\n",
      "Epoch [23] T 170.46s, deltaT 7.38s, loss: train 0.18238, valid 0.21265, dsc: train 0.81762, valid 0.78735\n",
      "Batch train [1] loss 0.17774, dsc 0.82226\n",
      "Batch train [2] loss 0.17550, dsc 0.82450\n",
      "Batch train [3] loss 0.15921, dsc 0.84079\n",
      "Batch train [4] loss 0.17418, dsc 0.82582\n",
      "Batch train [5] loss 0.14961, dsc 0.85039\n",
      "Batch train [6] loss 0.15003, dsc 0.84997\n",
      "Batch train [7] loss 0.13597, dsc 0.86403\n",
      "Batch train [8] loss 0.21210, dsc 0.78790\n",
      "Batch train [9] loss 0.18227, dsc 0.81773\n",
      "Batch train [10] loss 0.17703, dsc 0.82297\n",
      "Batch train [11] loss 0.16991, dsc 0.83009\n",
      "Batch train [12] loss 0.15031, dsc 0.84969\n",
      "Batch train [13] loss 0.17087, dsc 0.82913\n",
      "Batch train [14] loss 0.17970, dsc 0.82030\n",
      "Batch train [15] loss 0.16249, dsc 0.83751\n",
      "Batch train [16] loss 0.14842, dsc 0.85158\n",
      "Batch train [17] loss 0.17053, dsc 0.82947\n",
      "Batch train [18] loss 0.17792, dsc 0.82208\n",
      "Batch train [19] loss 0.21919, dsc 0.78081\n",
      "Batch train [20] loss 0.18924, dsc 0.81076\n",
      "Batch train [21] loss 0.17910, dsc 0.82090\n",
      "Batch train [22] loss 0.17278, dsc 0.82722\n",
      "Batch train [23] loss 0.19104, dsc 0.80896\n",
      "Batch train [24] loss 0.15022, dsc 0.84978\n",
      "Batch train [25] loss 0.16145, dsc 0.83855\n",
      "Batch train [26] loss 0.17317, dsc 0.82683\n",
      "Batch train [27] loss 0.14634, dsc 0.85366\n",
      "Batch train [28] loss 0.14276, dsc 0.85724\n",
      "Batch train [29] loss 0.16422, dsc 0.83578\n",
      "Batch train [30] loss 0.15797, dsc 0.84203\n",
      "Batch train [31] loss 0.16415, dsc 0.83585\n",
      "Batch train [32] loss 0.18318, dsc 0.81682\n",
      "Batch train [33] loss 0.16892, dsc 0.83108\n",
      "Batch train [34] loss 0.16276, dsc 0.83724\n",
      "Batch train [35] loss 0.18458, dsc 0.81542\n",
      "Batch train [36] loss 0.15662, dsc 0.84338\n",
      "Batch train [37] loss 0.16411, dsc 0.83589\n",
      "Batch train [38] loss 0.25656, dsc 0.74344\n",
      "Batch train [39] loss 0.17992, dsc 0.82008\n",
      "Batch train [40] loss 0.17115, dsc 0.82885\n",
      "Epoch [24] train done\n",
      "Batch eval [1] loss 0.19760, dsc 0.80240\n",
      "Batch eval [2] loss 0.22832, dsc 0.77168\n",
      "Batch eval [3] loss 0.26012, dsc 0.73988\n",
      "Batch eval [4] loss 0.20864, dsc 0.79136\n",
      "Batch eval [5] loss 0.22239, dsc 0.77761\n",
      "Epoch [24] valid done\n",
      "Epoch [24] T 177.81s, deltaT 7.35s, loss: train 0.17158, valid 0.22341, dsc: train 0.82842, valid 0.77659\n",
      "Batch train [1] loss 0.16431, dsc 0.83569\n",
      "Batch train [2] loss 0.13761, dsc 0.86239\n",
      "Batch train [3] loss 0.23540, dsc 0.76460\n",
      "Batch train [4] loss 0.16112, dsc 0.83888\n",
      "Batch train [5] loss 0.15619, dsc 0.84381\n",
      "Batch train [6] loss 0.16260, dsc 0.83740\n",
      "Batch train [7] loss 0.18140, dsc 0.81860\n",
      "Batch train [8] loss 0.15615, dsc 0.84385\n",
      "Batch train [9] loss 0.15350, dsc 0.84650\n",
      "Batch train [10] loss 0.13679, dsc 0.86321\n",
      "Batch train [11] loss 0.15389, dsc 0.84611\n",
      "Batch train [12] loss 0.13833, dsc 0.86167\n",
      "Batch train [13] loss 0.16474, dsc 0.83526\n",
      "Batch train [14] loss 0.16459, dsc 0.83541\n",
      "Batch train [15] loss 0.17761, dsc 0.82239\n",
      "Batch train [16] loss 0.14647, dsc 0.85353\n",
      "Batch train [17] loss 0.16643, dsc 0.83357\n",
      "Batch train [18] loss 0.16344, dsc 0.83656\n",
      "Batch train [19] loss 0.17389, dsc 0.82611\n",
      "Batch train [20] loss 0.15533, dsc 0.84467\n",
      "Batch train [21] loss 0.15242, dsc 0.84758\n",
      "Batch train [22] loss 0.18231, dsc 0.81769\n",
      "Batch train [23] loss 0.16402, dsc 0.83598\n",
      "Batch train [24] loss 0.17366, dsc 0.82634\n",
      "Batch train [25] loss 0.16588, dsc 0.83412\n",
      "Batch train [26] loss 0.15817, dsc 0.84183\n",
      "Batch train [27] loss 0.15437, dsc 0.84563\n",
      "Batch train [28] loss 0.16232, dsc 0.83768\n",
      "Batch train [29] loss 0.17558, dsc 0.82442\n",
      "Batch train [30] loss 0.15764, dsc 0.84236\n",
      "Batch train [31] loss 0.20075, dsc 0.79925\n",
      "Batch train [32] loss 0.14751, dsc 0.85249\n",
      "Batch train [33] loss 0.16019, dsc 0.83981\n",
      "Batch train [34] loss 0.15839, dsc 0.84161\n",
      "Batch train [35] loss 0.15323, dsc 0.84677\n",
      "Batch train [36] loss 0.15573, dsc 0.84427\n",
      "Batch train [37] loss 0.14799, dsc 0.85201\n",
      "Batch train [38] loss 0.13436, dsc 0.86564\n",
      "Batch train [39] loss 0.15751, dsc 0.84249\n",
      "Batch train [40] loss 0.20883, dsc 0.79117\n",
      "Epoch [25] train done\n",
      "Batch eval [1] loss 0.19612, dsc 0.80388\n",
      "Batch eval [2] loss 0.22374, dsc 0.77626\n",
      "Batch eval [3] loss 0.21202, dsc 0.78798\n",
      "Batch eval [4] loss 0.20330, dsc 0.79670\n",
      "Batch eval [5] loss 0.19616, dsc 0.80384\n",
      "Epoch [25] valid done\n",
      "Epoch [25] T 185.16s, deltaT 7.35s, loss: train 0.16302, valid 0.20627, dsc: train 0.83698, valid 0.79373\n",
      "Batch train [1] loss 0.14730, dsc 0.85270\n",
      "Batch train [2] loss 0.13832, dsc 0.86168\n",
      "Batch train [3] loss 0.15881, dsc 0.84119\n",
      "Batch train [4] loss 0.16750, dsc 0.83250\n",
      "Batch train [5] loss 0.14494, dsc 0.85506\n",
      "Batch train [6] loss 0.14290, dsc 0.85710\n",
      "Batch train [7] loss 0.15069, dsc 0.84931\n",
      "Batch train [8] loss 0.15241, dsc 0.84759\n",
      "Batch train [9] loss 0.14970, dsc 0.85030\n",
      "Batch train [10] loss 0.13967, dsc 0.86033\n",
      "Batch train [11] loss 0.13453, dsc 0.86547\n",
      "Batch train [12] loss 0.14315, dsc 0.85685\n",
      "Batch train [13] loss 0.15454, dsc 0.84546\n",
      "Batch train [14] loss 0.15824, dsc 0.84176\n",
      "Batch train [15] loss 0.14836, dsc 0.85164\n",
      "Batch train [16] loss 0.16766, dsc 0.83234\n",
      "Batch train [17] loss 0.16293, dsc 0.83707\n",
      "Batch train [18] loss 0.15170, dsc 0.84830\n",
      "Batch train [19] loss 0.13953, dsc 0.86047\n",
      "Batch train [20] loss 0.17086, dsc 0.82914\n",
      "Batch train [21] loss 0.17023, dsc 0.82977\n",
      "Batch train [22] loss 0.17356, dsc 0.82644\n",
      "Batch train [23] loss 0.14465, dsc 0.85535\n",
      "Batch train [24] loss 0.14244, dsc 0.85756\n",
      "Batch train [25] loss 0.15428, dsc 0.84572\n",
      "Batch train [26] loss 0.13380, dsc 0.86620\n",
      "Batch train [27] loss 0.15482, dsc 0.84518\n",
      "Batch train [28] loss 0.19679, dsc 0.80321\n",
      "Batch train [29] loss 0.17742, dsc 0.82258\n",
      "Batch train [30] loss 0.15782, dsc 0.84218\n",
      "Batch train [31] loss 0.13737, dsc 0.86263\n",
      "Batch train [32] loss 0.16214, dsc 0.83786\n",
      "Batch train [33] loss 0.16195, dsc 0.83805\n",
      "Batch train [34] loss 0.18837, dsc 0.81163\n",
      "Batch train [35] loss 0.15043, dsc 0.84957\n",
      "Batch train [36] loss 0.15566, dsc 0.84434\n",
      "Batch train [37] loss 0.16006, dsc 0.83994\n",
      "Batch train [38] loss 0.16226, dsc 0.83774\n",
      "Batch train [39] loss 0.23012, dsc 0.76988\n",
      "Batch train [40] loss 0.18147, dsc 0.81853\n",
      "Epoch [26] train done\n",
      "Batch eval [1] loss 0.20409, dsc 0.79591\n",
      "Batch eval [2] loss 0.21680, dsc 0.78320\n",
      "Batch eval [3] loss 0.21356, dsc 0.78644\n",
      "Batch eval [4] loss 0.18638, dsc 0.81362\n",
      "Batch eval [5] loss 0.20016, dsc 0.79984\n",
      "Epoch [26] valid done\n",
      "Epoch [26] T 192.52s, deltaT 7.36s, loss: train 0.15798, valid 0.20420, dsc: train 0.84202, valid 0.79580\n",
      "Batch train [1] loss 0.13528, dsc 0.86472\n",
      "Batch train [2] loss 0.14967, dsc 0.85033\n",
      "Batch train [3] loss 0.15769, dsc 0.84231\n",
      "Batch train [4] loss 0.14725, dsc 0.85275\n",
      "Batch train [5] loss 0.12734, dsc 0.87266\n",
      "Batch train [6] loss 0.15697, dsc 0.84303\n",
      "Batch train [7] loss 0.16287, dsc 0.83713\n",
      "Batch train [8] loss 0.13590, dsc 0.86410\n",
      "Batch train [9] loss 0.13872, dsc 0.86128\n",
      "Batch train [10] loss 0.15758, dsc 0.84242\n",
      "Batch train [11] loss 0.13704, dsc 0.86296\n",
      "Batch train [12] loss 0.15379, dsc 0.84621\n",
      "Batch train [13] loss 0.14166, dsc 0.85834\n",
      "Batch train [14] loss 0.15006, dsc 0.84994\n",
      "Batch train [15] loss 0.16914, dsc 0.83086\n",
      "Batch train [16] loss 0.17047, dsc 0.82953\n",
      "Batch train [17] loss 0.15188, dsc 0.84812\n",
      "Batch train [18] loss 0.14124, dsc 0.85876\n",
      "Batch train [19] loss 0.15563, dsc 0.84437\n",
      "Batch train [20] loss 0.13865, dsc 0.86135\n",
      "Batch train [21] loss 0.13273, dsc 0.86727\n",
      "Batch train [22] loss 0.14572, dsc 0.85428\n",
      "Batch train [23] loss 0.12124, dsc 0.87876\n",
      "Batch train [24] loss 0.20134, dsc 0.79866\n",
      "Batch train [25] loss 0.14827, dsc 0.85173\n",
      "Batch train [26] loss 0.14373, dsc 0.85627\n",
      "Batch train [27] loss 0.15212, dsc 0.84788\n",
      "Batch train [28] loss 0.22816, dsc 0.77184\n",
      "Batch train [29] loss 0.15763, dsc 0.84237\n",
      "Batch train [30] loss 0.17394, dsc 0.82606\n",
      "Batch train [31] loss 0.14625, dsc 0.85375\n",
      "Batch train [32] loss 0.18180, dsc 0.81820\n",
      "Batch train [33] loss 0.15752, dsc 0.84248\n",
      "Batch train [34] loss 0.15291, dsc 0.84709\n",
      "Batch train [35] loss 0.14129, dsc 0.85871\n",
      "Batch train [36] loss 0.15152, dsc 0.84848\n",
      "Batch train [37] loss 0.15289, dsc 0.84711\n",
      "Batch train [38] loss 0.18451, dsc 0.81549\n",
      "Batch train [39] loss 0.15639, dsc 0.84361\n",
      "Batch train [40] loss 0.14597, dsc 0.85403\n",
      "Epoch [27] train done\n",
      "Batch eval [1] loss 0.17784, dsc 0.82216\n",
      "Batch eval [2] loss 0.22687, dsc 0.77313\n",
      "Batch eval [3] loss 0.21546, dsc 0.78454\n",
      "Batch eval [4] loss 0.22663, dsc 0.77337\n",
      "Batch eval [5] loss 0.18610, dsc 0.81390\n",
      "Epoch [27] valid done\n",
      "Epoch [27] T 199.88s, deltaT 7.36s, loss: train 0.15387, valid 0.20658, dsc: train 0.84613, valid 0.79342\n",
      "Batch train [1] loss 0.16457, dsc 0.83543\n",
      "Batch train [2] loss 0.16185, dsc 0.83815\n",
      "Batch train [3] loss 0.14862, dsc 0.85138\n",
      "Batch train [4] loss 0.15220, dsc 0.84780\n",
      "Batch train [5] loss 0.12147, dsc 0.87853\n",
      "Batch train [6] loss 0.13416, dsc 0.86584\n",
      "Batch train [7] loss 0.13982, dsc 0.86018\n",
      "Batch train [8] loss 0.14778, dsc 0.85222\n",
      "Batch train [9] loss 0.17301, dsc 0.82699\n",
      "Batch train [10] loss 0.16501, dsc 0.83499\n",
      "Batch train [11] loss 0.13455, dsc 0.86545\n",
      "Batch train [12] loss 0.15495, dsc 0.84505\n",
      "Batch train [13] loss 0.13074, dsc 0.86926\n",
      "Batch train [14] loss 0.12869, dsc 0.87131\n",
      "Batch train [15] loss 0.12442, dsc 0.87558\n",
      "Batch train [16] loss 0.13163, dsc 0.86837\n",
      "Batch train [17] loss 0.14684, dsc 0.85316\n",
      "Batch train [18] loss 0.13402, dsc 0.86598\n",
      "Batch train [19] loss 0.13503, dsc 0.86497\n",
      "Batch train [20] loss 0.17825, dsc 0.82175\n",
      "Batch train [21] loss 0.13219, dsc 0.86781\n",
      "Batch train [22] loss 0.14730, dsc 0.85270\n",
      "Batch train [23] loss 0.12510, dsc 0.87490\n",
      "Batch train [24] loss 0.13370, dsc 0.86630\n",
      "Batch train [25] loss 0.15002, dsc 0.84998\n",
      "Batch train [26] loss 0.13453, dsc 0.86547\n",
      "Batch train [27] loss 0.13848, dsc 0.86152\n",
      "Batch train [28] loss 0.14681, dsc 0.85319\n",
      "Batch train [29] loss 0.14434, dsc 0.85566\n",
      "Batch train [30] loss 0.16081, dsc 0.83919\n",
      "Batch train [31] loss 0.16060, dsc 0.83940\n",
      "Batch train [32] loss 0.13709, dsc 0.86291\n",
      "Batch train [33] loss 0.17155, dsc 0.82845\n",
      "Batch train [34] loss 0.21697, dsc 0.78303\n",
      "Batch train [35] loss 0.14901, dsc 0.85099\n",
      "Batch train [36] loss 0.14438, dsc 0.85562\n",
      "Batch train [37] loss 0.15263, dsc 0.84737\n",
      "Batch train [38] loss 0.15865, dsc 0.84135\n",
      "Batch train [39] loss 0.13834, dsc 0.86166\n",
      "Batch train [40] loss 0.14338, dsc 0.85662\n",
      "Epoch [28] train done\n",
      "Batch eval [1] loss 0.18832, dsc 0.81168\n",
      "Batch eval [2] loss 0.19397, dsc 0.80603\n",
      "Batch eval [3] loss 0.18570, dsc 0.81430\n",
      "Batch eval [4] loss 0.19399, dsc 0.80601\n",
      "Batch eval [5] loss 0.18767, dsc 0.81233\n",
      "Epoch [28] valid done\n",
      "Epoch [28] T 207.23s, deltaT 7.35s, loss: train 0.14734, valid 0.18993, dsc: train 0.85266, valid 0.81007\n",
      "Batch train [1] loss 0.14243, dsc 0.85757\n",
      "Batch train [2] loss 0.12616, dsc 0.87384\n",
      "Batch train [3] loss 0.12811, dsc 0.87189\n",
      "Batch train [4] loss 0.11589, dsc 0.88411\n",
      "Batch train [5] loss 0.13592, dsc 0.86408\n",
      "Batch train [6] loss 0.13993, dsc 0.86007\n",
      "Batch train [7] loss 0.16833, dsc 0.83167\n",
      "Batch train [8] loss 0.13200, dsc 0.86800\n",
      "Batch train [9] loss 0.13576, dsc 0.86424\n",
      "Batch train [10] loss 0.21937, dsc 0.78063\n",
      "Batch train [11] loss 0.14784, dsc 0.85216\n",
      "Batch train [12] loss 0.14269, dsc 0.85731\n",
      "Batch train [13] loss 0.14598, dsc 0.85402\n",
      "Batch train [14] loss 0.15941, dsc 0.84059\n",
      "Batch train [15] loss 0.16086, dsc 0.83914\n",
      "Batch train [16] loss 0.14310, dsc 0.85690\n",
      "Batch train [17] loss 0.12896, dsc 0.87104\n",
      "Batch train [18] loss 0.14643, dsc 0.85357\n",
      "Batch train [19] loss 0.13881, dsc 0.86119\n",
      "Batch train [20] loss 0.13678, dsc 0.86322\n",
      "Batch train [21] loss 0.13439, dsc 0.86561\n",
      "Batch train [22] loss 0.15205, dsc 0.84795\n",
      "Batch train [23] loss 0.15084, dsc 0.84916\n",
      "Batch train [24] loss 0.14357, dsc 0.85643\n",
      "Batch train [25] loss 0.12172, dsc 0.87828\n",
      "Batch train [26] loss 0.13153, dsc 0.86847\n",
      "Batch train [27] loss 0.12776, dsc 0.87224\n",
      "Batch train [28] loss 0.14482, dsc 0.85518\n",
      "Batch train [29] loss 0.13274, dsc 0.86726\n",
      "Batch train [30] loss 0.13102, dsc 0.86898\n",
      "Batch train [31] loss 0.12048, dsc 0.87952\n",
      "Batch train [32] loss 0.14628, dsc 0.85372\n",
      "Batch train [33] loss 0.14511, dsc 0.85489\n",
      "Batch train [34] loss 0.13471, dsc 0.86529\n",
      "Batch train [35] loss 0.13204, dsc 0.86796\n",
      "Batch train [36] loss 0.13429, dsc 0.86571\n",
      "Batch train [37] loss 0.13889, dsc 0.86111\n",
      "Batch train [38] loss 0.16242, dsc 0.83758\n",
      "Batch train [39] loss 0.15081, dsc 0.84919\n",
      "Batch train [40] loss 0.12832, dsc 0.87168\n",
      "Epoch [29] train done\n",
      "Batch eval [1] loss 0.18704, dsc 0.81296\n",
      "Batch eval [2] loss 0.19541, dsc 0.80459\n",
      "Batch eval [3] loss 0.18630, dsc 0.81370\n",
      "Batch eval [4] loss 0.18601, dsc 0.81399\n",
      "Batch eval [5] loss 0.17122, dsc 0.82878\n",
      "Epoch [29] valid done\n",
      "Epoch [29] T 214.59s, deltaT 7.36s, loss: train 0.14146, valid 0.18520, dsc: train 0.85854, valid 0.81480\n",
      "Batch train [1] loss 0.12859, dsc 0.87141\n",
      "Batch train [2] loss 0.12398, dsc 0.87602\n",
      "Batch train [3] loss 0.13666, dsc 0.86334\n",
      "Batch train [4] loss 0.12839, dsc 0.87161\n",
      "Batch train [5] loss 0.13837, dsc 0.86163\n",
      "Batch train [6] loss 0.12877, dsc 0.87123\n",
      "Batch train [7] loss 0.12459, dsc 0.87541\n",
      "Batch train [8] loss 0.13662, dsc 0.86338\n",
      "Batch train [9] loss 0.14537, dsc 0.85463\n",
      "Batch train [10] loss 0.12569, dsc 0.87431\n",
      "Batch train [11] loss 0.17576, dsc 0.82424\n",
      "Batch train [12] loss 0.12374, dsc 0.87626\n",
      "Batch train [13] loss 0.13249, dsc 0.86751\n",
      "Batch train [14] loss 0.11544, dsc 0.88456\n",
      "Batch train [15] loss 0.12711, dsc 0.87289\n",
      "Batch train [16] loss 0.12030, dsc 0.87970\n",
      "Batch train [17] loss 0.14600, dsc 0.85400\n",
      "Batch train [18] loss 0.12708, dsc 0.87292\n",
      "Batch train [19] loss 0.10394, dsc 0.89606\n",
      "Batch train [20] loss 0.14253, dsc 0.85747\n",
      "Batch train [21] loss 0.13587, dsc 0.86413\n",
      "Batch train [22] loss 0.13163, dsc 0.86837\n",
      "Batch train [23] loss 0.13938, dsc 0.86062\n",
      "Batch train [24] loss 0.21221, dsc 0.78779\n",
      "Batch train [25] loss 0.15746, dsc 0.84254\n",
      "Batch train [26] loss 0.11610, dsc 0.88390\n",
      "Batch train [27] loss 0.14111, dsc 0.85889\n",
      "Batch train [28] loss 0.12259, dsc 0.87741\n",
      "Batch train [29] loss 0.13085, dsc 0.86915\n",
      "Batch train [30] loss 0.14990, dsc 0.85010\n",
      "Batch train [31] loss 0.13487, dsc 0.86513\n",
      "Batch train [32] loss 0.14026, dsc 0.85974\n",
      "Batch train [33] loss 0.11912, dsc 0.88088\n",
      "Batch train [34] loss 0.12327, dsc 0.87673\n",
      "Batch train [35] loss 0.11761, dsc 0.88239\n",
      "Batch train [36] loss 0.13181, dsc 0.86819\n",
      "Batch train [37] loss 0.15094, dsc 0.84906\n",
      "Batch train [38] loss 0.13726, dsc 0.86274\n",
      "Batch train [39] loss 0.13727, dsc 0.86273\n",
      "Batch train [40] loss 0.15557, dsc 0.84443\n",
      "Epoch [30] train done\n",
      "Batch eval [1] loss 0.18945, dsc 0.81055\n",
      "Batch eval [2] loss 0.20917, dsc 0.79083\n",
      "Batch eval [3] loss 0.21213, dsc 0.78787\n",
      "Batch eval [4] loss 0.17399, dsc 0.82601\n",
      "Batch eval [5] loss 0.18580, dsc 0.81420\n",
      "Epoch [30] valid done\n",
      "Epoch [30] T 221.95s, deltaT 7.36s, loss: train 0.13541, valid 0.19411, dsc: train 0.86459, valid 0.80589\n",
      "Elapsed time 0:03:41\n"
     ]
    }
   ],
   "source": [
    "# preparing model loop params\n",
    "low_res_model_info = prepare_model(epochs=30, in_channels=8, train_dataset=train_low_res_dataset, valid_dataset=valid_low_res_dataset, test_dataset=test_low_res_dataset)\n",
    "show_model_info(low_res_model_info)\n",
    "\n",
    "# getting everything necessary for model training\n",
    "low_res_train_loop_params = {k:v for k,v in low_res_model_info.items() if k not in ['model_total_params', 'model_total_trainable_params']}\n",
    "# running training loop\n",
    "train_loop(**low_res_train_loop_params)\n",
    "\n",
    "low_res_model = itemgetter('model')(low_res_model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading high/full res dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 1x dataset\n",
      "normalizing dataset\n",
      "filtering labels\n",
      "parsing dataset to numpy\n",
      "data type: float64 int8\n",
      "full dataset RAM sizes in GB 18874368000\n",
      "data max 12.81577046544424, min -0.40489707167932215\n",
      "label max 22, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0978b06ed14b1eb0fadb3fb731b9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7234720fef84cb68da8673bc77e678a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset data and label shapes (1, 160, 32, 32) (1, 160, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "full_res_dataset = get_dataset(dataset_size=50, shrink_factor=1, filter_labels=filter_labels, unify_labels=False)\n",
    "full_res_dataset.to_numpy()\n",
    "full_res_dataset.show_data_type()\n",
    "print('full dataset RAM sizes in GB', full_res_dataset.get_data_size())\n",
    "\n",
    "preview_dataset(full_res_dataset, preview_index=0, show_hist=False)\n",
    "\n",
    "print('dataset data and label shapes', low_res_dataset.data_list[0].shape, full_res_dataset.data_list[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing precoarse network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved model to cpu\n"
     ]
    }
   ],
   "source": [
    "# moving model to cpu and setting to eval mode, preventing model params changes/training\n",
    "low_res_model.to('cpu')\n",
    "low_res_model.eval()\n",
    "print('moved model to cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting bounding box cut in full res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cut_lists(low_res_model, low_res_dataset, full_res_dataset, cut_full_res_dataset, low_res_mask_threshold=0.5):    \n",
    "    for i in range(len(full_res_dataset)):\n",
    "        print(f'getting cut index {i}')\n",
    "        low_res_data_img = low_res_dataset.data_list[i]\n",
    "        full_res_data_img = full_res_dataset.data_list[i]\n",
    "        full_res_label_img = full_res_dataset.label_list[i]\n",
    "\n",
    "        data_cut, label_cut, new_bounding_box = get_full_res_cut(low_res_model, \n",
    "                                                                 low_res_data_img, \n",
    "                                                                 full_res_data_img, \n",
    "                                                                 full_res_label_img, \n",
    "                                                                 low_res_mask_threshold,\n",
    "                                                                 DESIRE_BOUNDING_BOX_SIZE, \n",
    "                                                                 show_debug=False)\n",
    "        cut_full_res_dataset.data_list[i] = data_cut\n",
    "        cut_full_res_dataset.label_list[i] = label_cut\n",
    "        \n",
    "    return cut_full_res_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "full_res_dataloaders_obj = get_copy_dataloaders(full_res_dataset, low_res_dataloaders_obj)\n",
    "get_dataset_info(full_res_dataset, full_res_dataloaders_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debuging cut algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug box delta [28 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 951804 951804\n",
      "debug bounding box sizes (44, 160, 144) (72, 192, 168)\n",
      "debug bounding boxes (65, 108, 160, 319, 176, 319) (51, 122, 144, 335, 164, 331)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca77472fc52e4572b72e19e9e97e81f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadbe08b29d248fc8dd41bc6d69864ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99057a4d18fa4e4093072e06fb0d9a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d17a633153e4dd7a3f78ad158a8222a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "low_res_mask_threshold = 0.5\n",
    "dataset_index = 42\n",
    "low_res_data_img = low_res_dataset.data_list[dataset_index]\n",
    "full_res_data_img = full_res_dataset.data_list[dataset_index]\n",
    "full_res_label_img = full_res_dataset.label_list[dataset_index]\n",
    "\n",
    "data_cut, label_cut, new_bounding_box = get_full_res_cut(low_res_model, \n",
    "                                                         low_res_data_img,\n",
    "                                                         full_res_data_img, \n",
    "                                                         full_res_label_img, \n",
    "                                                         low_res_mask_threshold,\n",
    "                                                         DESIRE_BOUNDING_BOX_SIZE, \n",
    "                                                         show_debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running cut algorithm, creating cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting cut index 0\n",
      "debug box delta [22 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1223526 1223526\n",
      "getting cut index 1\n",
      "debug box delta [23 16 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1326052 1326052\n",
      "getting cut index 2\n",
      "debug box delta [20  0 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1890464 1890464\n",
      "getting cut index 3\n",
      "debug box delta [16 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1560217 1560217\n",
      "getting cut index 4\n",
      "debug box delta [21 32 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1451227 1451227\n",
      "getting cut index 5\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1262651 1262651\n",
      "getting cut index 6\n",
      "debug box delta [21 48 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1566938 1566938\n",
      "getting cut index 7\n",
      "debug box delta [27 48  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 869847 869847\n",
      "getting cut index 8\n",
      "debug box delta [20 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1397249 1397249\n",
      "getting cut index 9\n",
      "debug box delta [20 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1350330 1350330\n",
      "getting cut index 10\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1635868 1635868\n",
      "getting cut index 11\n",
      "debug box delta [23 32 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1283062 1283062\n",
      "getting cut index 12\n",
      "debug box delta [23 32 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1518406 1518406\n",
      "getting cut index 13\n",
      "debug box delta [23 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1504194 1504194\n",
      "getting cut index 14\n",
      "debug box delta [25 48  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1084254 1084254\n",
      "getting cut index 15\n",
      "debug box delta [25 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1221257 1221257\n",
      "getting cut index 16\n",
      "debug box delta [21 64 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 945639 945639\n",
      "getting cut index 17\n",
      "debug box delta [22 32 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1469035 1469035\n",
      "getting cut index 18\n",
      "debug box delta [25 48  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1322571 1322571\n",
      "getting cut index 19\n",
      "debug box delta [17 48  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1593516 1593516\n",
      "getting cut index 20\n",
      "debug box delta [23 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1390348 1390348\n",
      "getting cut index 21\n",
      "debug box delta [23 16 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1463017 1463017\n",
      "getting cut index 22\n",
      "debug box delta [24 48 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1162215 1162215\n",
      "getting cut index 23\n",
      "debug box delta [22 48 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1029805 1029805\n",
      "getting cut index 24\n",
      "debug box delta [ 28 -16   8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1689537 1689537\n",
      "getting cut index 25\n",
      "debug box delta [22 48  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1141739 1141739\n",
      "getting cut index 26\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1167835 1167835\n",
      "getting cut index 27\n",
      "debug box delta [24  0 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1783264 1783264\n",
      "getting cut index 28\n",
      "debug box delta [22  0 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1944758 1944758\n",
      "getting cut index 29\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1583396 1583396\n",
      "getting cut index 30\n",
      "debug box delta [19 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1248609 1248609\n",
      "getting cut index 31\n",
      "debug box delta [26 48 40]\n",
      "debug, does cut and original label contain the same amount of pixels? True 947124 947124\n",
      "getting cut index 32\n",
      "debug box delta [17 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1648187 1648187\n",
      "getting cut index 33\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1214697 1214697\n",
      "getting cut index 34\n",
      "debug box delta [24 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1445951 1445951\n",
      "getting cut index 35\n",
      "debug box delta [ 14  32 -24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1963068 1963068\n",
      "getting cut index 36\n",
      "debug box delta [24 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1241941 1241941\n",
      "getting cut index 37\n",
      "debug box delta [19 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1298886 1298886\n",
      "getting cut index 38\n",
      "debug box delta [24 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1731533 1731533\n",
      "getting cut index 39\n",
      "debug box delta [20 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1067335 1067335\n",
      "getting cut index 40\n",
      "debug box delta [23 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1311715 1311715\n",
      "getting cut index 41\n",
      "debug box delta [20 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1411792 1411792\n",
      "getting cut index 42\n",
      "debug box delta [28 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 951804 951804\n",
      "getting cut index 43\n",
      "debug box delta [30 48 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1024831 1024831\n",
      "getting cut index 44\n",
      "debug box delta [14 48  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1763923 1763923\n",
      "getting cut index 45\n",
      "debug box delta [28 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1115633 1115633\n",
      "getting cut index 46\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1670156 1670156\n",
      "getting cut index 47\n",
      "debug box delta [20 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1413179 1413179\n",
      "getting cut index 48\n",
      "debug box delta [29 64 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 781269 781269\n",
      "getting cut index 49\n",
      "debug box delta [19 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1756965 1756965\n"
     ]
    }
   ],
   "source": [
    "cut_full_res_dataset = full_res_dataset.copy(copy_lists=False)\n",
    "cut_full_res_dataset = get_cut_lists(low_res_model, low_res_dataset, full_res_dataset, cut_full_res_dataset, low_res_mask_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reviewing full res and cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type: float64 int8\n",
      "\n",
      "full res shape (1, 160, 512, 512) (160, 512, 512)\n",
      "cut full res shape (1, 72, 192, 168) (72, 192, 168)\n",
      "\n",
      "dataset RAM sizes in GB 17.578125 0.9733200073242188\n",
      "single item RAM in GB 0.0390625 0.3125\n",
      "\n",
      "data max 11.780218856954171, min -0.42423961281850314\n",
      "label max 22, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0469b04a50f943499553a8a6afbaa9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fb73af7afa4c70b46e3fface1b9439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cut_full_res_dataset.show_data_type()\n",
    "print()\n",
    "print('full res shape', full_res_dataset.data_list[0].shape, full_res_dataset.label_list[0].shape)\n",
    "print('cut full res shape', cut_full_res_dataset.data_list[0].shape, cut_full_res_dataset.label_list[0].shape)\n",
    "print()\n",
    "print('dataset RAM sizes in GB', full_res_dataset.get_data_size() / 1024**3, cut_full_res_dataset.get_data_size() / 1024**3)\n",
    "print('single item RAM in GB', full_res_dataset.label_list[0].nbytes / 1024**3, full_res_dataset.data_list[0].nbytes / 1024**3)\n",
    "print()\n",
    "preview_dataset(cut_full_res_dataset, max_slices=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing cut model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "cut_full_res_dataset.set_output_label([OARS_LABELS.EYE_L, OARS_LABELS.EYE_R, OARS_LABELS.LENS_L, OARS_LABELS.LENS_R])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cut_full_res_dataset.__getitem__(0)[1][47])\n",
    "plt.show()\n",
    "\n",
    "cut_dataloaders_obj = get_copy_dataloaders(cut_full_res_dataset, low_res_dataloaders_obj)\n",
    "get_dataset_info(cut_full_res_dataset, cut_dataloaders_obj)\n",
    "\n",
    "cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter('train_dataset', 'valid_dataset', 'test_dataset')(cut_dataloaders_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device running \"cuda\"\n",
      "max output channels 128\n",
      "Model number of params: 1193537, trainable 1193537\n",
      "Running training loop\n",
      "Batch train [1] loss 0.99714, dsc 0.00286\n",
      "Batch train [2] loss 0.99630, dsc 0.00370\n",
      "Batch train [3] loss 0.99535, dsc 0.00465\n",
      "Batch train [4] loss 0.99437, dsc 0.00563\n",
      "Batch train [5] loss 0.99417, dsc 0.00583\n",
      "Batch train [6] loss 0.99105, dsc 0.00895\n",
      "Batch train [7] loss 0.99187, dsc 0.00813\n",
      "Batch train [8] loss 0.98926, dsc 0.01074\n",
      "Batch train [9] loss 0.99563, dsc 0.00437\n",
      "Batch train [10] loss 0.99328, dsc 0.00672\n",
      "Batch train [11] loss 0.99012, dsc 0.00988\n",
      "Batch train [12] loss 0.98951, dsc 0.01049\n",
      "Batch train [13] loss 0.99407, dsc 0.00593\n",
      "Batch train [14] loss 0.98940, dsc 0.01060\n",
      "Batch train [15] loss 0.99217, dsc 0.00783\n",
      "Batch train [16] loss 0.99089, dsc 0.00911\n",
      "Batch train [17] loss 0.99018, dsc 0.00982\n",
      "Batch train [18] loss 0.99227, dsc 0.00773\n",
      "Batch train [19] loss 0.99314, dsc 0.00686\n",
      "Batch train [20] loss 0.99174, dsc 0.00826\n",
      "Batch train [21] loss 0.98874, dsc 0.01126\n",
      "Batch train [22] loss 0.99264, dsc 0.00736\n",
      "Batch train [23] loss 0.99259, dsc 0.00741\n",
      "Batch train [24] loss 0.99092, dsc 0.00908\n",
      "Batch train [25] loss 0.98926, dsc 0.01074\n",
      "Batch train [26] loss 0.99236, dsc 0.00764\n",
      "Batch train [27] loss 0.98972, dsc 0.01028\n",
      "Batch train [28] loss 0.98847, dsc 0.01153\n",
      "Batch train [29] loss 0.98997, dsc 0.01003\n",
      "Batch train [30] loss 0.99314, dsc 0.00686\n",
      "Batch train [31] loss 0.98908, dsc 0.01092\n",
      "Batch train [32] loss 0.99007, dsc 0.00993\n",
      "Batch train [33] loss 0.98875, dsc 0.01125\n",
      "Batch train [34] loss 0.99315, dsc 0.00685\n",
      "Batch train [35] loss 0.99321, dsc 0.00679\n",
      "Batch train [36] loss 0.98817, dsc 0.01183\n",
      "Batch train [37] loss 0.99006, dsc 0.00994\n",
      "Batch train [38] loss 0.98999, dsc 0.01001\n",
      "Batch train [39] loss 0.99132, dsc 0.00868\n",
      "Batch train [40] loss 0.99260, dsc 0.00740\n",
      "Epoch [1] train done\n",
      "Batch eval [1] loss 0.99195, dsc 0.00805\n",
      "Batch eval [2] loss 0.99355, dsc 0.00645\n",
      "Batch eval [3] loss 0.99155, dsc 0.00845\n",
      "Batch eval [4] loss 0.99063, dsc 0.00937\n",
      "Batch eval [5] loss 0.98606, dsc 0.01394\n",
      "Epoch [1] valid done\n",
      "Epoch [1] T 95.33s, deltaT 95.33s, loss: train 0.99165, valid 0.99075, dsc: train 0.00835, valid 0.00925\n",
      "Batch train [1] loss 0.98845, dsc 0.01155\n",
      "Batch train [2] loss 0.98791, dsc 0.01209\n",
      "Batch train [3] loss 0.98950, dsc 0.01050\n",
      "Batch train [4] loss 0.99276, dsc 0.00724\n",
      "Batch train [5] loss 0.99178, dsc 0.00822\n",
      "Batch train [6] loss 0.99018, dsc 0.00982\n",
      "Batch train [7] loss 0.98788, dsc 0.01212\n",
      "Batch train [8] loss 0.98982, dsc 0.01018\n",
      "Batch train [9] loss 0.98873, dsc 0.01127\n",
      "Batch train [10] loss 0.99204, dsc 0.00796\n",
      "Batch train [11] loss 0.99188, dsc 0.00812\n",
      "Batch train [12] loss 0.99245, dsc 0.00755\n",
      "Batch train [13] loss 0.99044, dsc 0.00956\n",
      "Batch train [14] loss 0.98954, dsc 0.01046\n",
      "Batch train [15] loss 0.98838, dsc 0.01162\n",
      "Batch train [16] loss 0.98831, dsc 0.01169\n",
      "Batch train [17] loss 0.98792, dsc 0.01208\n",
      "Batch train [18] loss 0.99288, dsc 0.00712\n",
      "Batch train [19] loss 0.98957, dsc 0.01043\n",
      "Batch train [20] loss 0.98973, dsc 0.01027\n",
      "Batch train [21] loss 0.98663, dsc 0.01337\n",
      "Batch train [22] loss 0.99099, dsc 0.00901\n",
      "Batch train [23] loss 0.99131, dsc 0.00869\n",
      "Batch train [24] loss 0.98913, dsc 0.01087\n",
      "Batch train [25] loss 0.99197, dsc 0.00803\n",
      "Batch train [26] loss 0.99068, dsc 0.00932\n",
      "Batch train [27] loss 0.98950, dsc 0.01050\n",
      "Batch train [28] loss 0.99265, dsc 0.00735\n",
      "Batch train [29] loss 0.99461, dsc 0.00539\n",
      "Batch train [30] loss 0.98765, dsc 0.01235\n",
      "Batch train [31] loss 0.99204, dsc 0.00796\n",
      "Batch train [32] loss 0.99154, dsc 0.00846\n",
      "Batch train [33] loss 0.99100, dsc 0.00900\n",
      "Batch train [34] loss 0.99188, dsc 0.00812\n",
      "Batch train [35] loss 0.98776, dsc 0.01224\n",
      "Batch train [36] loss 0.99280, dsc 0.00720\n",
      "Batch train [37] loss 0.99339, dsc 0.00661\n",
      "Batch train [38] loss 0.99244, dsc 0.00756\n",
      "Batch train [39] loss 0.98840, dsc 0.01160\n",
      "Batch train [40] loss 0.99312, dsc 0.00688\n",
      "Epoch [2] train done\n",
      "Batch eval [1] loss 0.99159, dsc 0.00841\n",
      "Batch eval [2] loss 0.99310, dsc 0.00690\n",
      "Batch eval [3] loss 0.99113, dsc 0.00887\n",
      "Batch eval [4] loss 0.99023, dsc 0.00977\n",
      "Batch eval [5] loss 0.98523, dsc 0.01477\n",
      "Epoch [2] valid done\n",
      "Epoch [2] T 186.97s, deltaT 91.64s, loss: train 0.99049, valid 0.99026, dsc: train 0.00951, valid 0.00974\n",
      "Batch train [1] loss 0.99094, dsc 0.00906\n",
      "Batch train [2] loss 0.98931, dsc 0.01069\n",
      "Batch train [3] loss 0.99142, dsc 0.00858\n",
      "Batch train [4] loss 0.99174, dsc 0.00826\n",
      "Batch train [5] loss 0.99138, dsc 0.00862\n",
      "Batch train [6] loss 0.98895, dsc 0.01105\n",
      "Batch train [7] loss 0.99236, dsc 0.00764\n",
      "Batch train [8] loss 0.99187, dsc 0.00813\n",
      "Batch train [9] loss 0.99210, dsc 0.00790\n",
      "Batch train [10] loss 0.98724, dsc 0.01276\n",
      "Batch train [11] loss 0.99172, dsc 0.00828\n",
      "Batch train [12] loss 0.98999, dsc 0.01001\n",
      "Batch train [13] loss 0.98960, dsc 0.01040\n",
      "Batch train [14] loss 0.98771, dsc 0.01229\n",
      "Batch train [15] loss 0.99143, dsc 0.00857\n",
      "Batch train [16] loss 0.98750, dsc 0.01250\n",
      "Batch train [17] loss 0.98929, dsc 0.01071\n",
      "Batch train [18] loss 0.98899, dsc 0.01101\n",
      "Batch train [19] loss 0.98813, dsc 0.01187\n",
      "Batch train [20] loss 0.99441, dsc 0.00559\n",
      "Batch train [21] loss 0.99239, dsc 0.00761\n",
      "Batch train [22] loss 0.98717, dsc 0.01283\n",
      "Batch train [23] loss 0.98608, dsc 0.01392\n",
      "Batch train [24] loss 0.98905, dsc 0.01095\n",
      "Batch train [25] loss 0.99254, dsc 0.00746\n",
      "Batch train [26] loss 0.99315, dsc 0.00685\n",
      "Batch train [27] loss 0.98762, dsc 0.01238\n",
      "Batch train [28] loss 0.98802, dsc 0.01198\n",
      "Batch train [29] loss 0.99050, dsc 0.00950\n",
      "Batch train [30] loss 0.99019, dsc 0.00981\n",
      "Batch train [31] loss 0.98718, dsc 0.01282\n",
      "Batch train [32] loss 0.99285, dsc 0.00715\n",
      "Batch train [33] loss 0.99141, dsc 0.00859\n",
      "Batch train [34] loss 0.99076, dsc 0.00924\n",
      "Batch train [35] loss 0.98848, dsc 0.01152\n",
      "Batch train [36] loss 0.98899, dsc 0.01101\n",
      "Batch train [37] loss 0.98758, dsc 0.01242\n",
      "Batch train [38] loss 0.99207, dsc 0.00793\n",
      "Batch train [39] loss 0.99232, dsc 0.00768\n",
      "Batch train [40] loss 0.98665, dsc 0.01335\n",
      "Epoch [3] train done\n",
      "Batch eval [1] loss 0.99122, dsc 0.00878\n",
      "Batch eval [2] loss 0.99277, dsc 0.00723\n",
      "Batch eval [3] loss 0.99075, dsc 0.00925\n",
      "Batch eval [4] loss 0.98970, dsc 0.01030\n",
      "Batch eval [5] loss 0.98444, dsc 0.01556\n",
      "Epoch [3] valid done\n",
      "Epoch [3] T 278.58s, deltaT 91.61s, loss: train 0.99003, valid 0.98978, dsc: train 0.00997, valid 0.01022\n",
      "Batch train [1] loss 0.99065, dsc 0.00935\n",
      "Batch train [2] loss 0.98568, dsc 0.01432\n",
      "Batch train [3] loss 0.99424, dsc 0.00576\n",
      "Batch train [4] loss 0.98769, dsc 0.01231\n",
      "Batch train [5] loss 0.98869, dsc 0.01131\n",
      "Batch train [6] loss 0.99197, dsc 0.00803\n",
      "Batch train [7] loss 0.99268, dsc 0.00732\n",
      "Batch train [8] loss 0.98687, dsc 0.01313\n",
      "Batch train [9] loss 0.99118, dsc 0.00882\n",
      "Batch train [10] loss 0.98819, dsc 0.01181\n",
      "Batch train [11] loss 0.99289, dsc 0.00711\n",
      "Batch train [12] loss 0.98876, dsc 0.01124\n",
      "Batch train [13] loss 0.99187, dsc 0.00813\n",
      "Batch train [14] loss 0.99115, dsc 0.00885\n",
      "Batch train [15] loss 0.98653, dsc 0.01347\n",
      "Batch train [16] loss 0.99130, dsc 0.00870\n",
      "Batch train [17] loss 0.98716, dsc 0.01284\n",
      "Batch train [18] loss 0.98674, dsc 0.01326\n",
      "Batch train [19] loss 0.98848, dsc 0.01152\n",
      "Batch train [20] loss 0.99151, dsc 0.00849\n",
      "Batch train [21] loss 0.99110, dsc 0.00890\n",
      "Batch train [22] loss 0.99210, dsc 0.00790\n",
      "Batch train [23] loss 0.98999, dsc 0.01001\n",
      "Batch train [24] loss 0.98609, dsc 0.01391\n",
      "Batch train [25] loss 0.98614, dsc 0.01386\n",
      "Batch train [26] loss 0.98916, dsc 0.01084\n",
      "Batch train [27] loss 0.99059, dsc 0.00941\n",
      "Batch train [28] loss 0.98870, dsc 0.01130\n",
      "Batch train [29] loss 0.98676, dsc 0.01324\n",
      "Batch train [30] loss 0.99173, dsc 0.00827\n",
      "Batch train [31] loss 0.99189, dsc 0.00811\n",
      "Batch train [32] loss 0.99050, dsc 0.00950\n",
      "Batch train [33] loss 0.98949, dsc 0.01051\n",
      "Batch train [34] loss 0.99062, dsc 0.00938\n",
      "Batch train [35] loss 0.98664, dsc 0.01336\n",
      "Batch train [36] loss 0.98819, dsc 0.01181\n",
      "Batch train [37] loss 0.98705, dsc 0.01295\n",
      "Batch train [38] loss 0.98773, dsc 0.01227\n",
      "Batch train [39] loss 0.98979, dsc 0.01021\n",
      "Batch train [40] loss 0.98787, dsc 0.01213\n",
      "Epoch [4] train done\n",
      "Batch eval [1] loss 0.99073, dsc 0.00927\n",
      "Batch eval [2] loss 0.99236, dsc 0.00764\n",
      "Batch eval [3] loss 0.99022, dsc 0.00978\n",
      "Batch eval [4] loss 0.98913, dsc 0.01087\n",
      "Batch eval [5] loss 0.98372, dsc 0.01628\n",
      "Epoch [4] valid done\n",
      "Epoch [4] T 369.94s, deltaT 91.36s, loss: train 0.98941, valid 0.98923, dsc: train 0.01059, valid 0.01077\n",
      "Batch train [1] loss 0.98783, dsc 0.01217\n",
      "Batch train [2] loss 0.98573, dsc 0.01427\n",
      "Batch train [3] loss 0.99046, dsc 0.00954\n",
      "Batch train [4] loss 0.98836, dsc 0.01164\n",
      "Batch train [5] loss 0.98960, dsc 0.01040\n",
      "Batch train [6] loss 0.98684, dsc 0.01316\n",
      "Batch train [7] loss 0.99376, dsc 0.00624\n",
      "Batch train [8] loss 0.98462, dsc 0.01538\n",
      "Batch train [9] loss 0.98626, dsc 0.01374\n",
      "Batch train [10] loss 0.99168, dsc 0.00832\n",
      "Batch train [11] loss 0.99073, dsc 0.00927\n",
      "Batch train [12] loss 0.98793, dsc 0.01207\n",
      "Batch train [13] loss 0.98590, dsc 0.01410\n",
      "Batch train [14] loss 0.98859, dsc 0.01141\n",
      "Batch train [15] loss 0.99123, dsc 0.00877\n",
      "Batch train [16] loss 0.98556, dsc 0.01444\n",
      "Batch train [17] loss 0.99132, dsc 0.00868\n",
      "Batch train [18] loss 0.98718, dsc 0.01282\n",
      "Batch train [19] loss 0.98567, dsc 0.01433\n",
      "Batch train [20] loss 0.98767, dsc 0.01233\n",
      "Batch train [21] loss 0.99197, dsc 0.00803\n",
      "Batch train [22] loss 0.98755, dsc 0.01245\n",
      "Batch train [23] loss 0.99035, dsc 0.00965\n",
      "Batch train [24] loss 0.98891, dsc 0.01109\n",
      "Batch train [25] loss 0.99216, dsc 0.00784\n",
      "Batch train [26] loss 0.98637, dsc 0.01363\n",
      "Batch train [27] loss 0.99134, dsc 0.00866\n",
      "Batch train [28] loss 0.99105, dsc 0.00895\n",
      "Batch train [29] loss 0.99032, dsc 0.00968\n",
      "Batch train [30] loss 0.98918, dsc 0.01082\n",
      "Batch train [31] loss 0.98978, dsc 0.01022\n",
      "Batch train [32] loss 0.98484, dsc 0.01516\n",
      "Batch train [33] loss 0.98978, dsc 0.01022\n",
      "Batch train [34] loss 0.99063, dsc 0.00937\n",
      "Batch train [35] loss 0.98579, dsc 0.01421\n",
      "Batch train [36] loss 0.98686, dsc 0.01314\n",
      "Batch train [37] loss 0.98555, dsc 0.01445\n",
      "Batch train [38] loss 0.98719, dsc 0.01281\n",
      "Batch train [39] loss 0.98925, dsc 0.01075\n",
      "Batch train [40] loss 0.98995, dsc 0.01005\n",
      "Epoch [5] train done\n",
      "Batch eval [1] loss 0.98984, dsc 0.01016\n",
      "Batch eval [2] loss 0.99155, dsc 0.00845\n",
      "Batch eval [3] loss 0.98920, dsc 0.01080\n",
      "Batch eval [4] loss 0.98800, dsc 0.01200\n",
      "Batch eval [5] loss 0.98179, dsc 0.01821\n",
      "Epoch [5] valid done\n",
      "Epoch [5] T 461.32s, deltaT 91.38s, loss: train 0.98864, valid 0.98808, dsc: train 0.01136, valid 0.01192\n",
      "Batch train [1] loss 0.98463, dsc 0.01537\n",
      "Batch train [2] loss 0.98711, dsc 0.01289\n",
      "Batch train [3] loss 0.98705, dsc 0.01295\n",
      "Batch train [4] loss 0.99104, dsc 0.00896\n",
      "Batch train [5] loss 0.98683, dsc 0.01317\n",
      "Batch train [6] loss 0.99008, dsc 0.00992\n",
      "Batch train [7] loss 0.98687, dsc 0.01313\n",
      "Batch train [8] loss 0.98979, dsc 0.01021\n",
      "Batch train [9] loss 0.98942, dsc 0.01058\n",
      "Batch train [10] loss 0.99322, dsc 0.00678\n",
      "Batch train [11] loss 0.98512, dsc 0.01488\n",
      "Batch train [12] loss 0.98823, dsc 0.01177\n",
      "Batch train [13] loss 0.98892, dsc 0.01108\n",
      "Batch train [14] loss 0.99141, dsc 0.00859\n",
      "Batch train [15] loss 0.98500, dsc 0.01500\n",
      "Batch train [16] loss 0.98513, dsc 0.01487\n",
      "Batch train [17] loss 0.98964, dsc 0.01036\n",
      "Batch train [18] loss 0.99160, dsc 0.00840\n",
      "Batch train [19] loss 0.98606, dsc 0.01394\n",
      "Batch train [20] loss 0.98962, dsc 0.01038\n",
      "Batch train [21] loss 0.98421, dsc 0.01579\n",
      "Batch train [22] loss 0.98528, dsc 0.01472\n",
      "Batch train [23] loss 0.98832, dsc 0.01168\n",
      "Batch train [24] loss 0.98518, dsc 0.01482\n",
      "Batch train [25] loss 0.99031, dsc 0.00969\n",
      "Batch train [26] loss 0.98266, dsc 0.01734\n",
      "Batch train [27] loss 0.98607, dsc 0.01393\n",
      "Batch train [28] loss 0.98898, dsc 0.01102\n",
      "Batch train [29] loss 0.98915, dsc 0.01085\n",
      "Batch train [30] loss 0.98827, dsc 0.01173\n",
      "Batch train [31] loss 0.99025, dsc 0.00975\n",
      "Batch train [32] loss 0.99068, dsc 0.00932\n",
      "Batch train [33] loss 0.98722, dsc 0.01278\n",
      "Batch train [34] loss 0.99029, dsc 0.00971\n",
      "Batch train [35] loss 0.98664, dsc 0.01336\n",
      "Batch train [36] loss 0.98382, dsc 0.01618\n",
      "Batch train [37] loss 0.98381, dsc 0.01619\n",
      "Batch train [38] loss 0.98973, dsc 0.01027\n",
      "Batch train [39] loss 0.98324, dsc 0.01676\n",
      "Batch train [40] loss 0.98611, dsc 0.01389\n",
      "Epoch [6] train done\n",
      "Batch eval [1] loss 0.98939, dsc 0.01061\n",
      "Batch eval [2] loss 0.99104, dsc 0.00896\n",
      "Batch eval [3] loss 0.98856, dsc 0.01144\n",
      "Batch eval [4] loss 0.98723, dsc 0.01277\n",
      "Batch eval [5] loss 0.98232, dsc 0.01768\n",
      "Epoch [6] valid done\n",
      "Epoch [6] T 553.89s, deltaT 92.57s, loss: train 0.98767, valid 0.98771, dsc: train 0.01233, valid 0.01229\n",
      "Batch train [1] loss 0.98395, dsc 0.01605\n",
      "Batch train [2] loss 0.98310, dsc 0.01690\n",
      "Batch train [3] loss 0.98861, dsc 0.01139\n",
      "Batch train [4] loss 0.99079, dsc 0.00921\n",
      "Batch train [5] loss 0.98583, dsc 0.01417\n",
      "Batch train [6] loss 0.98951, dsc 0.01049\n",
      "Batch train [7] loss 0.98586, dsc 0.01414\n",
      "Batch train [8] loss 0.99012, dsc 0.00988\n",
      "Batch train [9] loss 0.98976, dsc 0.01024\n",
      "Batch train [10] loss 0.98609, dsc 0.01391\n",
      "Batch train [11] loss 0.98497, dsc 0.01503\n",
      "Batch train [12] loss 0.98966, dsc 0.01034\n",
      "Batch train [13] loss 0.98880, dsc 0.01120\n",
      "Batch train [14] loss 0.98755, dsc 0.01245\n",
      "Batch train [15] loss 0.98980, dsc 0.01020\n",
      "Batch train [16] loss 0.99082, dsc 0.00918\n",
      "Batch train [17] loss 0.98305, dsc 0.01695\n",
      "Batch train [18] loss 0.98524, dsc 0.01476\n",
      "Batch train [19] loss 0.98634, dsc 0.01366\n",
      "Batch train [20] loss 0.98387, dsc 0.01613\n",
      "Batch train [21] loss 0.98386, dsc 0.01614\n",
      "Batch train [22] loss 0.98852, dsc 0.01148\n",
      "Batch train [23] loss 0.98492, dsc 0.01508\n",
      "Batch train [24] loss 0.98796, dsc 0.01204\n",
      "Batch train [25] loss 0.98705, dsc 0.01295\n",
      "Batch train [26] loss 0.98210, dsc 0.01790\n",
      "Batch train [27] loss 0.98243, dsc 0.01757\n",
      "Batch train [28] loss 0.98082, dsc 0.01918\n",
      "Batch train [29] loss 0.98289, dsc 0.01711\n",
      "Batch train [30] loss 0.98967, dsc 0.01033\n",
      "Batch train [31] loss 0.98478, dsc 0.01522\n",
      "Batch train [32] loss 0.99214, dsc 0.00786\n",
      "Batch train [33] loss 0.98653, dsc 0.01347\n",
      "Batch train [34] loss 0.98720, dsc 0.01280\n",
      "Batch train [35] loss 0.98783, dsc 0.01217\n",
      "Batch train [36] loss 0.98807, dsc 0.01193\n",
      "Batch train [37] loss 0.98201, dsc 0.01799\n",
      "Batch train [38] loss 0.98416, dsc 0.01584\n",
      "Batch train [39] loss 0.98825, dsc 0.01175\n",
      "Batch train [40] loss 0.98257, dsc 0.01743\n",
      "Epoch [7] train done\n",
      "Batch eval [1] loss 0.98749, dsc 0.01251\n",
      "Batch eval [2] loss 0.98937, dsc 0.01063\n",
      "Batch eval [3] loss 0.98646, dsc 0.01354\n",
      "Batch eval [4] loss 0.98496, dsc 0.01504\n",
      "Batch eval [5] loss 0.97729, dsc 0.02271\n",
      "Epoch [7] valid done\n",
      "Epoch [7] T 646.03s, deltaT 92.14s, loss: train 0.98644, valid 0.98511, dsc: train 0.01356, valid 0.01489\n",
      "Batch train [1] loss 0.98801, dsc 0.01199\n",
      "Batch train [2] loss 0.98444, dsc 0.01556\n",
      "Batch train [3] loss 0.98181, dsc 0.01819\n",
      "Batch train [4] loss 0.97981, dsc 0.02019\n",
      "Batch train [5] loss 0.98873, dsc 0.01127\n",
      "Batch train [6] loss 0.98586, dsc 0.01414\n",
      "Batch train [7] loss 0.98758, dsc 0.01242\n",
      "Batch train [8] loss 0.98875, dsc 0.01125\n",
      "Batch train [9] loss 0.98759, dsc 0.01241\n",
      "Batch train [10] loss 0.98891, dsc 0.01109\n",
      "Batch train [11] loss 0.98619, dsc 0.01381\n",
      "Batch train [12] loss 0.98698, dsc 0.01302\n",
      "Batch train [13] loss 0.98806, dsc 0.01194\n",
      "Batch train [14] loss 0.98837, dsc 0.01163\n",
      "Batch train [15] loss 0.98384, dsc 0.01616\n",
      "Batch train [16] loss 0.98935, dsc 0.01065\n",
      "Batch train [17] loss 0.98178, dsc 0.01822\n",
      "Batch train [18] loss 0.98967, dsc 0.01033\n",
      "Batch train [19] loss 0.99150, dsc 0.00850\n",
      "Batch train [20] loss 0.98304, dsc 0.01696\n",
      "Batch train [21] loss 0.98673, dsc 0.01327\n",
      "Batch train [22] loss 0.98686, dsc 0.01314\n",
      "Batch train [23] loss 0.98465, dsc 0.01535\n",
      "Batch train [24] loss 0.98318, dsc 0.01682\n",
      "Batch train [25] loss 0.98567, dsc 0.01433\n",
      "Batch train [26] loss 0.98321, dsc 0.01679\n",
      "Batch train [27] loss 0.98083, dsc 0.01917\n",
      "Batch train [28] loss 0.98250, dsc 0.01750\n",
      "Batch train [29] loss 0.98369, dsc 0.01631\n",
      "Batch train [30] loss 0.98153, dsc 0.01847\n",
      "Batch train [31] loss 0.98000, dsc 0.02000\n",
      "Batch train [32] loss 0.98575, dsc 0.01425\n",
      "Batch train [33] loss 0.98070, dsc 0.01930\n",
      "Batch train [34] loss 0.98701, dsc 0.01299\n",
      "Batch train [35] loss 0.98124, dsc 0.01876\n",
      "Batch train [36] loss 0.97926, dsc 0.02074\n",
      "Batch train [37] loss 0.98057, dsc 0.01943\n",
      "Batch train [38] loss 0.98813, dsc 0.01187\n",
      "Batch train [39] loss 0.98237, dsc 0.01763\n",
      "Batch train [40] loss 0.97990, dsc 0.02010\n",
      "Epoch [8] train done\n",
      "Batch eval [1] loss 0.98661, dsc 0.01339\n",
      "Batch eval [2] loss 0.99305, dsc 0.00695\n",
      "Batch eval [3] loss 0.98770, dsc 0.01230\n",
      "Batch eval [4] loss 0.98681, dsc 0.01319\n",
      "Batch eval [5] loss 0.98369, dsc 0.01631\n",
      "Epoch [8] valid done\n",
      "Epoch [8] T 738.29s, deltaT 92.27s, loss: train 0.98485, valid 0.98757, dsc: train 0.01515, valid 0.01243\n",
      "Batch train [1] loss 0.98712, dsc 0.01288\n",
      "Batch train [2] loss 0.98365, dsc 0.01635\n",
      "Batch train [3] loss 0.97995, dsc 0.02005\n",
      "Batch train [4] loss 0.97991, dsc 0.02009\n",
      "Batch train [5] loss 0.98038, dsc 0.01962\n",
      "Batch train [6] loss 0.97995, dsc 0.02005\n",
      "Batch train [7] loss 0.98832, dsc 0.01168\n",
      "Batch train [8] loss 0.98208, dsc 0.01792\n",
      "Batch train [9] loss 0.98615, dsc 0.01385\n",
      "Batch train [10] loss 0.97882, dsc 0.02118\n",
      "Batch train [11] loss 0.98527, dsc 0.01473\n",
      "Batch train [12] loss 0.98741, dsc 0.01259\n",
      "Batch train [13] loss 0.97640, dsc 0.02360\n",
      "Batch train [14] loss 0.98148, dsc 0.01852\n",
      "Batch train [15] loss 0.97882, dsc 0.02118\n",
      "Batch train [16] loss 0.98583, dsc 0.01417\n",
      "Batch train [17] loss 0.97799, dsc 0.02201\n",
      "Batch train [18] loss 0.97807, dsc 0.02193\n",
      "Batch train [19] loss 0.98546, dsc 0.01454\n",
      "Batch train [20] loss 0.98488, dsc 0.01512\n",
      "Batch train [21] loss 0.98323, dsc 0.01677\n",
      "Batch train [22] loss 0.98715, dsc 0.01285\n",
      "Batch train [23] loss 0.98109, dsc 0.01891\n",
      "Batch train [24] loss 0.98635, dsc 0.01365\n",
      "Batch train [25] loss 0.99009, dsc 0.00991\n",
      "Batch train [26] loss 0.98134, dsc 0.01866\n",
      "Batch train [27] loss 0.97982, dsc 0.02018\n",
      "Batch train [28] loss 0.97873, dsc 0.02127\n",
      "Batch train [29] loss 0.98041, dsc 0.01959\n",
      "Batch train [30] loss 0.97964, dsc 0.02036\n",
      "Batch train [31] loss 0.98505, dsc 0.01495\n",
      "Batch train [32] loss 0.98759, dsc 0.01241\n",
      "Batch train [33] loss 0.98298, dsc 0.01702\n",
      "Batch train [34] loss 0.98268, dsc 0.01732\n",
      "Batch train [35] loss 0.97633, dsc 0.02367\n",
      "Batch train [36] loss 0.98561, dsc 0.01439\n",
      "Batch train [37] loss 0.98579, dsc 0.01421\n",
      "Batch train [38] loss 0.98392, dsc 0.01608\n",
      "Batch train [39] loss 0.97961, dsc 0.02039\n",
      "Batch train [40] loss 0.98293, dsc 0.01707\n",
      "Epoch [9] train done\n",
      "Batch eval [1] loss 0.98351, dsc 0.01649\n",
      "Batch eval [2] loss 0.98621, dsc 0.01379\n",
      "Batch eval [3] loss 0.98244, dsc 0.01756\n",
      "Batch eval [4] loss 0.98087, dsc 0.01913\n",
      "Batch eval [5] loss 0.97089, dsc 0.02911\n",
      "Epoch [9] valid done\n",
      "Epoch [9] T 829.49s, deltaT 91.19s, loss: train 0.98271, valid 0.98078, dsc: train 0.01729, valid 0.01922\n",
      "Batch train [1] loss 0.98491, dsc 0.01509\n",
      "Batch train [2] loss 0.97607, dsc 0.02393\n",
      "Batch train [3] loss 0.98538, dsc 0.01462\n",
      "Batch train [4] loss 0.97568, dsc 0.02432\n",
      "Batch train [5] loss 0.97629, dsc 0.02371\n",
      "Batch train [6] loss 0.98377, dsc 0.01623\n",
      "Batch train [7] loss 0.98328, dsc 0.01672\n",
      "Batch train [8] loss 0.97606, dsc 0.02394\n",
      "Batch train [9] loss 0.97962, dsc 0.02038\n",
      "Batch train [10] loss 0.98674, dsc 0.01326\n",
      "Batch train [11] loss 0.98030, dsc 0.01970\n",
      "Batch train [12] loss 0.98357, dsc 0.01643\n",
      "Batch train [13] loss 0.97764, dsc 0.02236\n",
      "Batch train [14] loss 0.98136, dsc 0.01864\n",
      "Batch train [15] loss 0.97429, dsc 0.02571\n",
      "Batch train [16] loss 0.98879, dsc 0.01121\n",
      "Batch train [17] loss 0.97840, dsc 0.02160\n",
      "Batch train [18] loss 0.97808, dsc 0.02192\n",
      "Batch train [19] loss 0.98218, dsc 0.01782\n",
      "Batch train [20] loss 0.98098, dsc 0.01902\n",
      "Batch train [21] loss 0.98460, dsc 0.01540\n",
      "Batch train [22] loss 0.97546, dsc 0.02454\n",
      "Batch train [23] loss 0.97108, dsc 0.02892\n",
      "Batch train [24] loss 0.98381, dsc 0.01619\n",
      "Batch train [25] loss 0.97705, dsc 0.02295\n",
      "Batch train [26] loss 0.97494, dsc 0.02506\n",
      "Batch train [27] loss 0.98359, dsc 0.01641\n",
      "Batch train [28] loss 0.97252, dsc 0.02748\n",
      "Batch train [29] loss 0.98427, dsc 0.01573\n",
      "Batch train [30] loss 0.98240, dsc 0.01760\n",
      "Batch train [31] loss 0.97681, dsc 0.02319\n",
      "Batch train [32] loss 0.98179, dsc 0.01821\n",
      "Batch train [33] loss 0.97617, dsc 0.02383\n",
      "Batch train [34] loss 0.98095, dsc 0.01905\n",
      "Batch train [35] loss 0.97315, dsc 0.02685\n",
      "Batch train [36] loss 0.97881, dsc 0.02119\n",
      "Batch train [37] loss 0.97990, dsc 0.02010\n",
      "Batch train [38] loss 0.98427, dsc 0.01573\n",
      "Batch train [39] loss 0.97495, dsc 0.02505\n",
      "Batch train [40] loss 0.97131, dsc 0.02869\n",
      "Epoch [10] train done\n",
      "Batch eval [1] loss 0.98060, dsc 0.01940\n",
      "Batch eval [2] loss 0.98369, dsc 0.01631\n",
      "Batch eval [3] loss 0.97923, dsc 0.02077\n",
      "Batch eval [4] loss 0.97774, dsc 0.02226\n",
      "Batch eval [5] loss 0.96628, dsc 0.03372\n",
      "Epoch [10] valid done\n",
      "Epoch [10] T 920.85s, deltaT 91.36s, loss: train 0.97953, valid 0.97751, dsc: train 0.02047, valid 0.02249\n",
      "Batch train [1] loss 0.98120, dsc 0.01880\n",
      "Batch train [2] loss 0.97306, dsc 0.02694\n",
      "Batch train [3] loss 0.97934, dsc 0.02066\n",
      "Batch train [4] loss 0.97550, dsc 0.02450\n",
      "Batch train [5] loss 0.98240, dsc 0.01760\n",
      "Batch train [6] loss 0.97751, dsc 0.02249\n",
      "Batch train [7] loss 0.97450, dsc 0.02550\n",
      "Batch train [8] loss 0.97038, dsc 0.02962\n",
      "Batch train [9] loss 0.97116, dsc 0.02884\n",
      "Batch train [10] loss 0.97966, dsc 0.02034\n",
      "Batch train [11] loss 0.97418, dsc 0.02582\n",
      "Batch train [12] loss 0.96914, dsc 0.03086\n",
      "Batch train [13] loss 0.96648, dsc 0.03352\n",
      "Batch train [14] loss 0.97396, dsc 0.02604\n",
      "Batch train [15] loss 0.97337, dsc 0.02663\n",
      "Batch train [16] loss 0.97070, dsc 0.02930\n",
      "Batch train [17] loss 0.98206, dsc 0.01794\n",
      "Batch train [18] loss 0.98253, dsc 0.01747\n",
      "Batch train [19] loss 0.97206, dsc 0.02794\n",
      "Batch train [20] loss 0.96752, dsc 0.03248\n",
      "Batch train [21] loss 0.97894, dsc 0.02106\n",
      "Batch train [22] loss 0.96880, dsc 0.03120\n",
      "Batch train [23] loss 0.97783, dsc 0.02217\n",
      "Batch train [24] loss 0.97900, dsc 0.02100\n",
      "Batch train [25] loss 0.96721, dsc 0.03279\n",
      "Batch train [26] loss 0.97816, dsc 0.02184\n",
      "Batch train [27] loss 0.97717, dsc 0.02283\n",
      "Batch train [28] loss 0.97573, dsc 0.02427\n",
      "Batch train [29] loss 0.98034, dsc 0.01966\n",
      "Batch train [30] loss 0.97888, dsc 0.02112\n",
      "Batch train [31] loss 0.97004, dsc 0.02996\n",
      "Batch train [32] loss 0.96785, dsc 0.03215\n",
      "Batch train [33] loss 0.97947, dsc 0.02053\n",
      "Batch train [34] loss 0.97496, dsc 0.02504\n",
      "Batch train [35] loss 0.96637, dsc 0.03363\n",
      "Batch train [36] loss 0.98171, dsc 0.01829\n",
      "Batch train [37] loss 0.97303, dsc 0.02697\n",
      "Batch train [38] loss 0.97154, dsc 0.02846\n",
      "Batch train [39] loss 0.97885, dsc 0.02115\n",
      "Batch train [40] loss 0.98493, dsc 0.01507\n",
      "Epoch [11] train done\n",
      "Batch eval [1] loss 0.97664, dsc 0.02336\n",
      "Batch eval [2] loss 0.98150, dsc 0.01850\n",
      "Batch eval [3] loss 0.97550, dsc 0.02450\n",
      "Batch eval [4] loss 0.97417, dsc 0.02583\n",
      "Batch eval [5] loss 0.96122, dsc 0.03878\n",
      "Epoch [11] valid done\n",
      "Epoch [11] T 1012.70s, deltaT 91.85s, loss: train 0.97519, valid 0.97381, dsc: train 0.02481, valid 0.02619\n",
      "Batch train [1] loss 0.97598, dsc 0.02402\n",
      "Batch train [2] loss 0.96979, dsc 0.03021\n",
      "Batch train [3] loss 0.98096, dsc 0.01904\n",
      "Batch train [4] loss 0.97167, dsc 0.02833\n",
      "Batch train [5] loss 0.97636, dsc 0.02364\n",
      "Batch train [6] loss 0.97894, dsc 0.02106\n",
      "Batch train [7] loss 0.96814, dsc 0.03186\n",
      "Batch train [8] loss 0.97379, dsc 0.02621\n",
      "Batch train [9] loss 0.96352, dsc 0.03648\n",
      "Batch train [10] loss 0.97295, dsc 0.02705\n",
      "Batch train [11] loss 0.96318, dsc 0.03682\n",
      "Batch train [12] loss 0.97227, dsc 0.02773\n",
      "Batch train [13] loss 0.97531, dsc 0.02469\n",
      "Batch train [14] loss 0.96821, dsc 0.03179\n",
      "Batch train [15] loss 0.96633, dsc 0.03367\n",
      "Batch train [16] loss 0.96144, dsc 0.03856\n",
      "Batch train [17] loss 0.97683, dsc 0.02317\n",
      "Batch train [18] loss 0.96278, dsc 0.03722\n",
      "Batch train [19] loss 0.96334, dsc 0.03666\n",
      "Batch train [20] loss 0.97522, dsc 0.02478\n",
      "Batch train [21] loss 0.97250, dsc 0.02750\n",
      "Batch train [22] loss 0.96114, dsc 0.03886\n",
      "Batch train [23] loss 0.97415, dsc 0.02585\n",
      "Batch train [24] loss 0.96505, dsc 0.03495\n",
      "Batch train [25] loss 0.97305, dsc 0.02695\n",
      "Batch train [26] loss 0.98215, dsc 0.01785\n",
      "Batch train [27] loss 0.95603, dsc 0.04397\n",
      "Batch train [28] loss 0.95995, dsc 0.04005\n",
      "Batch train [29] loss 0.96015, dsc 0.03985\n",
      "Batch train [30] loss 0.97217, dsc 0.02783\n",
      "Batch train [31] loss 0.97492, dsc 0.02508\n",
      "Batch train [32] loss 0.97724, dsc 0.02276\n",
      "Batch train [33] loss 0.96681, dsc 0.03319\n",
      "Batch train [34] loss 0.96410, dsc 0.03590\n",
      "Batch train [35] loss 0.96359, dsc 0.03641\n",
      "Batch train [36] loss 0.97445, dsc 0.02555\n",
      "Batch train [37] loss 0.95718, dsc 0.04282\n",
      "Batch train [38] loss 0.96635, dsc 0.03365\n",
      "Batch train [39] loss 0.97290, dsc 0.02710\n",
      "Batch train [40] loss 0.95813, dsc 0.04187\n",
      "Epoch [12] train done\n",
      "Batch eval [1] loss 0.96527, dsc 0.03473\n",
      "Batch eval [2] loss 0.97950, dsc 0.02050\n",
      "Batch eval [3] loss 0.97212, dsc 0.02788\n",
      "Batch eval [4] loss 0.96453, dsc 0.03547\n",
      "Batch eval [5] loss 0.96982, dsc 0.03018\n",
      "Epoch [12] valid done\n",
      "Epoch [12] T 1104.36s, deltaT 91.66s, loss: train 0.96923, valid 0.97025, dsc: train 0.03077, valid 0.02975\n",
      "Batch train [1] loss 0.97230, dsc 0.02770\n",
      "Batch train [2] loss 0.96686, dsc 0.03314\n",
      "Batch train [3] loss 0.95825, dsc 0.04175\n",
      "Batch train [4] loss 0.96097, dsc 0.03903\n",
      "Batch train [5] loss 0.96056, dsc 0.03944\n",
      "Batch train [6] loss 0.95380, dsc 0.04620\n",
      "Batch train [7] loss 0.96139, dsc 0.03861\n",
      "Batch train [8] loss 0.95686, dsc 0.04314\n",
      "Batch train [9] loss 0.95867, dsc 0.04133\n",
      "Batch train [10] loss 0.96724, dsc 0.03276\n",
      "Batch train [11] loss 0.97003, dsc 0.02997\n",
      "Batch train [12] loss 0.96735, dsc 0.03265\n",
      "Batch train [13] loss 0.96015, dsc 0.03985\n",
      "Batch train [14] loss 0.97386, dsc 0.02614\n",
      "Batch train [15] loss 0.95133, dsc 0.04867\n",
      "Batch train [16] loss 0.96301, dsc 0.03699\n",
      "Batch train [17] loss 0.94483, dsc 0.05517\n",
      "Batch train [18] loss 0.97155, dsc 0.02845\n",
      "Batch train [19] loss 0.97015, dsc 0.02985\n",
      "Batch train [20] loss 0.96813, dsc 0.03187\n",
      "Batch train [21] loss 0.97704, dsc 0.02296\n",
      "Batch train [22] loss 0.96000, dsc 0.04000\n",
      "Batch train [23] loss 0.96533, dsc 0.03467\n",
      "Batch train [24] loss 0.95762, dsc 0.04238\n",
      "Batch train [25] loss 0.95025, dsc 0.04975\n",
      "Batch train [26] loss 0.96805, dsc 0.03195\n",
      "Batch train [27] loss 0.96731, dsc 0.03269\n",
      "Batch train [28] loss 0.94730, dsc 0.05270\n",
      "Batch train [29] loss 0.94392, dsc 0.05608\n",
      "Batch train [30] loss 0.96071, dsc 0.03929\n",
      "Batch train [31] loss 0.94606, dsc 0.05394\n",
      "Batch train [32] loss 0.95256, dsc 0.04744\n",
      "Batch train [33] loss 0.96129, dsc 0.03871\n",
      "Batch train [34] loss 0.96258, dsc 0.03742\n",
      "Batch train [35] loss 0.96128, dsc 0.03872\n",
      "Batch train [36] loss 0.96172, dsc 0.03828\n",
      "Batch train [37] loss 0.94932, dsc 0.05068\n",
      "Batch train [38] loss 0.94070, dsc 0.05930\n",
      "Batch train [39] loss 0.94060, dsc 0.05940\n",
      "Batch train [40] loss 0.94632, dsc 0.05368\n",
      "Epoch [13] train done\n",
      "Batch eval [1] loss 0.99669, dsc 0.00331\n",
      "Batch eval [2] loss 0.99750, dsc 0.00250\n",
      "Batch eval [3] loss 0.99638, dsc 0.00362\n",
      "Batch eval [4] loss 0.98433, dsc 0.01567\n",
      "Batch eval [5] loss 0.99452, dsc 0.00548\n",
      "Epoch [13] valid done\n",
      "Epoch [13] T 1196.77s, deltaT 92.41s, loss: train 0.95943, valid 0.99389, dsc: train 0.04057, valid 0.00611\n",
      "Batch train [1] loss 0.95998, dsc 0.04002\n",
      "Batch train [2] loss 0.96365, dsc 0.03635\n",
      "Batch train [3] loss 0.96531, dsc 0.03469\n",
      "Batch train [4] loss 0.93805, dsc 0.06195\n",
      "Batch train [5] loss 0.96330, dsc 0.03670\n",
      "Batch train [6] loss 0.93915, dsc 0.06085\n",
      "Batch train [7] loss 0.94020, dsc 0.05980\n",
      "Batch train [8] loss 0.93549, dsc 0.06451\n",
      "Batch train [9] loss 0.95233, dsc 0.04767\n",
      "Batch train [10] loss 0.93592, dsc 0.06408\n",
      "Batch train [11] loss 0.95858, dsc 0.04142\n",
      "Batch train [12] loss 0.94219, dsc 0.05781\n",
      "Batch train [13] loss 0.93443, dsc 0.06557\n",
      "Batch train [14] loss 0.94145, dsc 0.05855\n",
      "Batch train [15] loss 0.96319, dsc 0.03681\n",
      "Batch train [16] loss 0.95136, dsc 0.04864\n",
      "Batch train [17] loss 0.95447, dsc 0.04553\n",
      "Batch train [18] loss 0.93859, dsc 0.06141\n",
      "Batch train [19] loss 0.95151, dsc 0.04849\n",
      "Batch train [20] loss 0.94371, dsc 0.05629\n",
      "Batch train [21] loss 0.95591, dsc 0.04409\n",
      "Batch train [22] loss 0.95543, dsc 0.04457\n",
      "Batch train [23] loss 0.95033, dsc 0.04967\n",
      "Batch train [24] loss 0.93094, dsc 0.06906\n",
      "Batch train [25] loss 0.95110, dsc 0.04890\n",
      "Batch train [26] loss 0.93912, dsc 0.06088\n",
      "Batch train [27] loss 0.96576, dsc 0.03424\n",
      "Batch train [28] loss 0.93565, dsc 0.06435\n",
      "Batch train [29] loss 0.93249, dsc 0.06751\n",
      "Batch train [30] loss 0.93412, dsc 0.06588\n",
      "Batch train [31] loss 0.95266, dsc 0.04734\n",
      "Batch train [32] loss 0.91423, dsc 0.08577\n",
      "Batch train [33] loss 0.92088, dsc 0.07912\n",
      "Batch train [34] loss 0.93832, dsc 0.06168\n",
      "Batch train [35] loss 0.92142, dsc 0.07858\n",
      "Batch train [36] loss 0.94486, dsc 0.05514\n",
      "Batch train [37] loss 0.93798, dsc 0.06202\n",
      "Batch train [38] loss 0.91520, dsc 0.08480\n",
      "Batch train [39] loss 0.94169, dsc 0.05831\n",
      "Batch train [40] loss 0.92687, dsc 0.07313\n",
      "Epoch [14] train done\n",
      "Batch eval [1] loss 0.92335, dsc 0.07665\n",
      "Batch eval [2] loss 0.97404, dsc 0.02596\n",
      "Batch eval [3] loss 0.93565, dsc 0.06435\n",
      "Batch eval [4] loss 0.90491, dsc 0.09509\n",
      "Batch eval [5] loss 0.92953, dsc 0.07047\n",
      "Epoch [14] valid done\n",
      "Epoch [14] T 1289.34s, deltaT 92.57s, loss: train 0.94345, valid 0.93350, dsc: train 0.05655, valid 0.06650\n",
      "Batch train [1] loss 0.91607, dsc 0.08393\n",
      "Batch train [2] loss 0.94762, dsc 0.05238\n",
      "Batch train [3] loss 0.91776, dsc 0.08224\n",
      "Batch train [4] loss 0.93443, dsc 0.06557\n",
      "Batch train [5] loss 0.94013, dsc 0.05987\n",
      "Batch train [6] loss 0.93834, dsc 0.06166\n",
      "Batch train [7] loss 0.92185, dsc 0.07815\n",
      "Batch train [8] loss 0.93127, dsc 0.06873\n",
      "Batch train [9] loss 0.91102, dsc 0.08898\n",
      "Batch train [10] loss 0.94648, dsc 0.05352\n",
      "Batch train [11] loss 0.93412, dsc 0.06588\n",
      "Batch train [12] loss 0.94192, dsc 0.05808\n",
      "Batch train [13] loss 0.90267, dsc 0.09733\n",
      "Batch train [14] loss 0.89384, dsc 0.10616\n",
      "Batch train [15] loss 0.93516, dsc 0.06484\n",
      "Batch train [16] loss 0.94553, dsc 0.05447\n",
      "Batch train [17] loss 0.91048, dsc 0.08952\n",
      "Batch train [18] loss 0.92639, dsc 0.07361\n",
      "Batch train [19] loss 0.89477, dsc 0.10523\n",
      "Batch train [20] loss 0.90719, dsc 0.09281\n",
      "Batch train [21] loss 0.89410, dsc 0.10590\n",
      "Batch train [22] loss 0.93296, dsc 0.06704\n",
      "Batch train [23] loss 0.89470, dsc 0.10530\n",
      "Batch train [24] loss 0.92530, dsc 0.07470\n",
      "Batch train [25] loss 0.90354, dsc 0.09646\n",
      "Batch train [26] loss 0.90766, dsc 0.09234\n",
      "Batch train [27] loss 0.91230, dsc 0.08770\n",
      "Batch train [28] loss 0.90262, dsc 0.09738\n",
      "Batch train [29] loss 0.88524, dsc 0.11476\n",
      "Batch train [30] loss 0.92679, dsc 0.07321\n",
      "Batch train [31] loss 0.91843, dsc 0.08157\n",
      "Batch train [32] loss 0.92349, dsc 0.07651\n",
      "Batch train [33] loss 0.90445, dsc 0.09555\n",
      "Batch train [34] loss 0.89413, dsc 0.10587\n",
      "Batch train [35] loss 0.91762, dsc 0.08238\n",
      "Batch train [36] loss 0.90075, dsc 0.09925\n",
      "Batch train [37] loss 0.88457, dsc 0.11543\n",
      "Batch train [38] loss 0.87642, dsc 0.12358\n",
      "Batch train [39] loss 0.92460, dsc 0.07540\n",
      "Batch train [40] loss 0.94160, dsc 0.05840\n",
      "Epoch [15] train done\n",
      "Batch eval [1] loss 0.97443, dsc 0.02557\n",
      "Batch eval [2] loss 0.99432, dsc 0.00568\n",
      "Batch eval [3] loss 0.97910, dsc 0.02090\n",
      "Batch eval [4] loss 0.95189, dsc 0.04811\n",
      "Batch eval [5] loss 0.98991, dsc 0.01009\n",
      "Epoch [15] valid done\n",
      "Epoch [15] T 1381.27s, deltaT 91.93s, loss: train 0.91671, valid 0.97793, dsc: train 0.08329, valid 0.02207\n",
      "Batch train [1] loss 0.87714, dsc 0.12286\n",
      "Batch train [2] loss 0.91866, dsc 0.08134\n",
      "Batch train [3] loss 0.87071, dsc 0.12929\n",
      "Batch train [4] loss 0.92447, dsc 0.07553\n",
      "Batch train [5] loss 0.90864, dsc 0.09136\n",
      "Batch train [6] loss 0.91392, dsc 0.08608\n",
      "Batch train [7] loss 0.90259, dsc 0.09741\n",
      "Batch train [8] loss 0.91654, dsc 0.08346\n",
      "Batch train [9] loss 0.86860, dsc 0.13140\n",
      "Batch train [10] loss 0.87500, dsc 0.12500\n",
      "Batch train [11] loss 0.88710, dsc 0.11290\n",
      "Batch train [12] loss 0.85976, dsc 0.14024\n",
      "Batch train [13] loss 0.88874, dsc 0.11126\n",
      "Batch train [14] loss 0.85406, dsc 0.14594\n",
      "Batch train [15] loss 0.84921, dsc 0.15079\n",
      "Batch train [16] loss 0.89477, dsc 0.10523\n",
      "Batch train [17] loss 0.85941, dsc 0.14059\n",
      "Batch train [18] loss 0.87509, dsc 0.12491\n",
      "Batch train [19] loss 0.89843, dsc 0.10157\n",
      "Batch train [20] loss 0.86297, dsc 0.13703\n",
      "Batch train [21] loss 0.85495, dsc 0.14505\n",
      "Batch train [22] loss 0.85585, dsc 0.14415\n",
      "Batch train [23] loss 0.87313, dsc 0.12687\n",
      "Batch train [24] loss 0.88074, dsc 0.11926\n",
      "Batch train [25] loss 0.88416, dsc 0.11584\n",
      "Batch train [26] loss 0.83046, dsc 0.16954\n",
      "Batch train [27] loss 0.87927, dsc 0.12073\n",
      "Batch train [28] loss 0.88196, dsc 0.11804\n",
      "Batch train [29] loss 0.81537, dsc 0.18463\n",
      "Batch train [30] loss 0.84318, dsc 0.15682\n",
      "Batch train [31] loss 0.89380, dsc 0.10620\n",
      "Batch train [32] loss 0.91975, dsc 0.08025\n",
      "Batch train [33] loss 0.83526, dsc 0.16474\n",
      "Batch train [34] loss 0.88502, dsc 0.11498\n",
      "Batch train [35] loss 0.80918, dsc 0.19082\n",
      "Batch train [36] loss 0.86733, dsc 0.13267\n",
      "Batch train [37] loss 0.83326, dsc 0.16674\n",
      "Batch train [38] loss 0.87544, dsc 0.12456\n",
      "Batch train [39] loss 0.82570, dsc 0.17430\n",
      "Batch train [40] loss 0.81499, dsc 0.18501\n",
      "Epoch [16] train done\n",
      "Batch eval [1] loss 0.88980, dsc 0.11020\n",
      "Batch eval [2] loss 0.90520, dsc 0.09480\n",
      "Batch eval [3] loss 0.88323, dsc 0.11677\n",
      "Batch eval [4] loss 0.87031, dsc 0.12969\n",
      "Batch eval [5] loss 0.81859, dsc 0.18141\n",
      "Epoch [16] valid done\n",
      "Epoch [16] T 1473.41s, deltaT 92.13s, loss: train 0.87162, valid 0.87343, dsc: train 0.12838, valid 0.12657\n",
      "Batch train [1] loss 0.87818, dsc 0.12182\n",
      "Batch train [2] loss 0.79489, dsc 0.20511\n",
      "Batch train [3] loss 0.79981, dsc 0.20019\n",
      "Batch train [4] loss 0.78841, dsc 0.21159\n",
      "Batch train [5] loss 0.80475, dsc 0.19525\n",
      "Batch train [6] loss 0.85475, dsc 0.14525\n",
      "Batch train [7] loss 0.84388, dsc 0.15612\n",
      "Batch train [8] loss 0.81929, dsc 0.18071\n",
      "Batch train [9] loss 0.83749, dsc 0.16251\n",
      "Batch train [10] loss 0.79138, dsc 0.20862\n",
      "Batch train [11] loss 0.76464, dsc 0.23536\n",
      "Batch train [12] loss 0.84458, dsc 0.15542\n",
      "Batch train [13] loss 0.77215, dsc 0.22785\n",
      "Batch train [14] loss 0.79853, dsc 0.20147\n",
      "Batch train [15] loss 0.76735, dsc 0.23265\n",
      "Batch train [16] loss 0.83104, dsc 0.16896\n",
      "Batch train [17] loss 0.74160, dsc 0.25840\n",
      "Batch train [18] loss 0.76866, dsc 0.23134\n",
      "Batch train [19] loss 0.72087, dsc 0.27913\n",
      "Batch train [20] loss 0.82606, dsc 0.17394\n",
      "Batch train [21] loss 0.74340, dsc 0.25660\n",
      "Batch train [22] loss 0.83347, dsc 0.16653\n",
      "Batch train [23] loss 0.83112, dsc 0.16888\n",
      "Batch train [24] loss 0.75992, dsc 0.24008\n",
      "Batch train [25] loss 0.80523, dsc 0.19477\n",
      "Batch train [26] loss 0.78937, dsc 0.21063\n",
      "Batch train [27] loss 0.78102, dsc 0.21898\n",
      "Batch train [28] loss 0.74680, dsc 0.25320\n",
      "Batch train [29] loss 0.76104, dsc 0.23896\n",
      "Batch train [30] loss 0.84673, dsc 0.15327\n",
      "Batch train [31] loss 0.76633, dsc 0.23367\n",
      "Batch train [32] loss 0.71894, dsc 0.28106\n",
      "Batch train [33] loss 0.74780, dsc 0.25220\n",
      "Batch train [34] loss 0.69889, dsc 0.30111\n",
      "Batch train [35] loss 0.75102, dsc 0.24898\n",
      "Batch train [36] loss 0.75739, dsc 0.24261\n",
      "Batch train [37] loss 0.72248, dsc 0.27752\n",
      "Batch train [38] loss 0.67552, dsc 0.32448\n",
      "Batch train [39] loss 0.64442, dsc 0.35558\n",
      "Batch train [40] loss 0.69582, dsc 0.30418\n",
      "Epoch [17] train done\n",
      "Batch eval [1] loss 0.78454, dsc 0.21546\n",
      "Batch eval [2] loss 0.80837, dsc 0.19163\n",
      "Batch eval [3] loss 0.77044, dsc 0.22956\n",
      "Batch eval [4] loss 0.75035, dsc 0.24965\n",
      "Batch eval [5] loss 0.65571, dsc 0.34429\n",
      "Epoch [17] valid done\n",
      "Epoch [17] T 1564.83s, deltaT 91.42s, loss: train 0.77813, valid 0.75388, dsc: train 0.22187, valid 0.24612\n",
      "Batch train [1] loss 0.65385, dsc 0.34615\n",
      "Batch train [2] loss 0.63285, dsc 0.36715\n",
      "Batch train [3] loss 0.68719, dsc 0.31281\n",
      "Batch train [4] loss 0.66242, dsc 0.33758\n",
      "Batch train [5] loss 0.62231, dsc 0.37769\n",
      "Batch train [6] loss 0.63388, dsc 0.36612\n",
      "Batch train [7] loss 0.63158, dsc 0.36842\n",
      "Batch train [8] loss 0.70627, dsc 0.29373\n",
      "Batch train [9] loss 0.72195, dsc 0.27805\n",
      "Batch train [10] loss 0.74618, dsc 0.25382\n",
      "Batch train [11] loss 0.70249, dsc 0.29751\n",
      "Batch train [12] loss 0.71958, dsc 0.28042\n",
      "Batch train [13] loss 0.69373, dsc 0.30627\n",
      "Batch train [14] loss 0.70881, dsc 0.29119\n",
      "Batch train [15] loss 0.68506, dsc 0.31494\n",
      "Batch train [16] loss 0.61794, dsc 0.38206\n",
      "Batch train [17] loss 0.58196, dsc 0.41804\n",
      "Batch train [18] loss 0.76524, dsc 0.23476\n",
      "Batch train [19] loss 0.60027, dsc 0.39973\n",
      "Batch train [20] loss 0.66413, dsc 0.33587\n",
      "Batch train [21] loss 0.64558, dsc 0.35442\n",
      "Batch train [22] loss 0.58585, dsc 0.41415\n",
      "Batch train [23] loss 0.58038, dsc 0.41962\n",
      "Batch train [24] loss 0.67960, dsc 0.32040\n",
      "Batch train [25] loss 0.59519, dsc 0.40481\n",
      "Batch train [26] loss 0.53461, dsc 0.46539\n",
      "Batch train [27] loss 0.65814, dsc 0.34186\n",
      "Batch train [28] loss 0.54558, dsc 0.45442\n",
      "Batch train [29] loss 0.54551, dsc 0.45449\n",
      "Batch train [30] loss 0.49188, dsc 0.50812\n",
      "Batch train [31] loss 0.67794, dsc 0.32206\n",
      "Batch train [32] loss 0.59196, dsc 0.40804\n",
      "Batch train [33] loss 0.61568, dsc 0.38432\n",
      "Batch train [34] loss 0.49278, dsc 0.50722\n",
      "Batch train [35] loss 0.62750, dsc 0.37250\n",
      "Batch train [36] loss 0.59057, dsc 0.40943\n",
      "Batch train [37] loss 0.52232, dsc 0.47768\n",
      "Batch train [38] loss 0.57369, dsc 0.42631\n",
      "Batch train [39] loss 0.55607, dsc 0.44393\n",
      "Batch train [40] loss 0.49045, dsc 0.50955\n",
      "Epoch [18] train done\n",
      "Batch eval [1] loss 0.55672, dsc 0.44328\n",
      "Batch eval [2] loss 0.69531, dsc 0.30469\n",
      "Batch eval [3] loss 0.62348, dsc 0.37652\n",
      "Batch eval [4] loss 0.62705, dsc 0.37295\n",
      "Batch eval [5] loss 0.64377, dsc 0.35623\n",
      "Epoch [18] valid done\n",
      "Epoch [18] T 1657.30s, deltaT 92.47s, loss: train 0.62597, valid 0.62927, dsc: train 0.37403, valid 0.37073\n",
      "Batch train [1] loss 0.62871, dsc 0.37129\n",
      "Batch train [2] loss 0.49684, dsc 0.50316\n",
      "Batch train [3] loss 0.50723, dsc 0.49277\n",
      "Batch train [4] loss 0.58270, dsc 0.41730\n",
      "Batch train [5] loss 0.55684, dsc 0.44316\n",
      "Batch train [6] loss 0.55972, dsc 0.44028\n",
      "Batch train [7] loss 0.48111, dsc 0.51889\n",
      "Batch train [8] loss 0.53751, dsc 0.46249\n",
      "Batch train [9] loss 0.45867, dsc 0.54133\n",
      "Batch train [10] loss 0.57576, dsc 0.42424\n",
      "Batch train [11] loss 0.54237, dsc 0.45763\n",
      "Batch train [12] loss 0.46944, dsc 0.53056\n",
      "Batch train [13] loss 0.54649, dsc 0.45351\n",
      "Batch train [14] loss 0.49155, dsc 0.50845\n",
      "Batch train [15] loss 0.46617, dsc 0.53383\n",
      "Batch train [16] loss 0.50211, dsc 0.49789\n",
      "Batch train [17] loss 0.43567, dsc 0.56433\n",
      "Batch train [18] loss 0.48289, dsc 0.51711\n",
      "Batch train [19] loss 0.43494, dsc 0.56506\n",
      "Batch train [20] loss 0.63095, dsc 0.36905\n",
      "Batch train [21] loss 0.48349, dsc 0.51651\n",
      "Batch train [22] loss 0.40652, dsc 0.59348\n",
      "Batch train [23] loss 0.54320, dsc 0.45680\n",
      "Batch train [24] loss 0.48572, dsc 0.51428\n",
      "Batch train [25] loss 0.52829, dsc 0.47171\n",
      "Batch train [26] loss 0.38518, dsc 0.61482\n",
      "Batch train [27] loss 0.41643, dsc 0.58357\n",
      "Batch train [28] loss 0.51377, dsc 0.48623\n",
      "Batch train [29] loss 0.40814, dsc 0.59186\n",
      "Batch train [30] loss 0.47820, dsc 0.52180\n",
      "Batch train [31] loss 0.38666, dsc 0.61334\n",
      "Batch train [32] loss 0.34997, dsc 0.65003\n",
      "Batch train [33] loss 0.36258, dsc 0.63742\n",
      "Batch train [34] loss 0.36905, dsc 0.63095\n",
      "Batch train [35] loss 0.44438, dsc 0.55562\n",
      "Batch train [36] loss 0.50649, dsc 0.49351\n",
      "Batch train [37] loss 0.39646, dsc 0.60354\n",
      "Batch train [38] loss 0.47272, dsc 0.52728\n",
      "Batch train [39] loss 0.48687, dsc 0.51313\n",
      "Batch train [40] loss 0.40213, dsc 0.59787\n",
      "Epoch [19] train done\n",
      "Batch eval [1] loss 0.52763, dsc 0.47237\n",
      "Batch eval [2] loss 0.83248, dsc 0.16752\n",
      "Batch eval [3] loss 0.63730, dsc 0.36270\n",
      "Batch eval [4] loss 0.56015, dsc 0.43985\n",
      "Batch eval [5] loss 0.64941, dsc 0.35059\n",
      "Epoch [19] valid done\n",
      "Epoch [19] T 1750.52s, deltaT 93.22s, loss: train 0.48035, valid 0.64139, dsc: train 0.51965, valid 0.35861\n",
      "Batch train [1] loss 0.55126, dsc 0.44874\n",
      "Batch train [2] loss 0.45043, dsc 0.54957\n",
      "Batch train [3] loss 0.40511, dsc 0.59489\n",
      "Batch train [4] loss 0.44007, dsc 0.55993\n",
      "Batch train [5] loss 0.46188, dsc 0.53812\n",
      "Batch train [6] loss 0.36304, dsc 0.63696\n",
      "Batch train [7] loss 0.43164, dsc 0.56836\n",
      "Batch train [8] loss 0.46361, dsc 0.53639\n",
      "Batch train [9] loss 0.46625, dsc 0.53375\n",
      "Batch train [10] loss 0.36896, dsc 0.63104\n",
      "Batch train [11] loss 0.42177, dsc 0.57823\n",
      "Batch train [12] loss 0.42433, dsc 0.57567\n",
      "Batch train [13] loss 0.40170, dsc 0.59830\n",
      "Batch train [14] loss 0.35344, dsc 0.64656\n",
      "Batch train [15] loss 0.32591, dsc 0.67409\n",
      "Batch train [16] loss 0.37902, dsc 0.62098\n",
      "Batch train [17] loss 0.42069, dsc 0.57931\n",
      "Batch train [18] loss 0.37066, dsc 0.62934\n",
      "Batch train [19] loss 0.33529, dsc 0.66471\n",
      "Batch train [20] loss 0.32346, dsc 0.67654\n",
      "Batch train [21] loss 0.36494, dsc 0.63506\n",
      "Batch train [22] loss 0.34860, dsc 0.65140\n",
      "Batch train [23] loss 0.33078, dsc 0.66922\n",
      "Batch train [24] loss 0.33430, dsc 0.66570\n",
      "Batch train [25] loss 0.41094, dsc 0.58906\n",
      "Batch train [26] loss 0.42740, dsc 0.57260\n",
      "Batch train [27] loss 0.34802, dsc 0.65198\n",
      "Batch train [28] loss 0.27711, dsc 0.72289\n",
      "Batch train [29] loss 0.27238, dsc 0.72762\n",
      "Batch train [30] loss 0.37917, dsc 0.62083\n",
      "Batch train [31] loss 0.30604, dsc 0.69396\n",
      "Batch train [32] loss 0.30754, dsc 0.69246\n",
      "Batch train [33] loss 0.40628, dsc 0.59372\n",
      "Batch train [34] loss 0.32757, dsc 0.67243\n",
      "Batch train [35] loss 0.36921, dsc 0.63079\n",
      "Batch train [36] loss 0.38371, dsc 0.61629\n",
      "Batch train [37] loss 0.27958, dsc 0.72042\n",
      "Batch train [38] loss 0.26554, dsc 0.73446\n",
      "Batch train [39] loss 0.34824, dsc 0.65176\n",
      "Batch train [40] loss 0.24321, dsc 0.75679\n",
      "Epoch [20] train done\n",
      "Batch eval [1] loss 0.74561, dsc 0.25439\n",
      "Batch eval [2] loss 0.99926, dsc 0.00074\n",
      "Batch eval [3] loss 0.81917, dsc 0.18083\n",
      "Batch eval [4] loss 0.66479, dsc 0.33521\n",
      "Batch eval [5] loss 0.93759, dsc 0.06241\n",
      "Epoch [20] valid done\n",
      "Epoch [20] T 1841.84s, deltaT 91.32s, loss: train 0.37223, valid 0.83328, dsc: train 0.62777, valid 0.16672\n",
      "Batch train [1] loss 0.40098, dsc 0.59902\n",
      "Batch train [2] loss 0.31740, dsc 0.68260\n",
      "Batch train [3] loss 0.25707, dsc 0.74293\n",
      "Batch train [4] loss 0.23678, dsc 0.76322\n",
      "Batch train [5] loss 0.29617, dsc 0.70383\n",
      "Batch train [6] loss 0.29802, dsc 0.70198\n",
      "Batch train [7] loss 0.27891, dsc 0.72109\n",
      "Batch train [8] loss 0.27701, dsc 0.72299\n",
      "Batch train [9] loss 0.37841, dsc 0.62159\n",
      "Batch train [10] loss 0.28047, dsc 0.71953\n",
      "Batch train [11] loss 0.37143, dsc 0.62857\n",
      "Batch train [12] loss 0.40497, dsc 0.59503\n",
      "Batch train [13] loss 0.35221, dsc 0.64779\n",
      "Batch train [14] loss 0.33148, dsc 0.66852\n",
      "Batch train [15] loss 0.26830, dsc 0.73170\n",
      "Batch train [16] loss 0.35576, dsc 0.64424\n",
      "Batch train [17] loss 0.30686, dsc 0.69314\n",
      "Batch train [18] loss 0.34774, dsc 0.65226\n",
      "Batch train [19] loss 0.29061, dsc 0.70939\n",
      "Batch train [20] loss 0.29455, dsc 0.70545\n",
      "Batch train [21] loss 0.31269, dsc 0.68731\n",
      "Batch train [22] loss 0.30603, dsc 0.69397\n",
      "Batch train [23] loss 0.33449, dsc 0.66551\n",
      "Batch train [24] loss 0.22683, dsc 0.77317\n",
      "Batch train [25] loss 0.26900, dsc 0.73100\n",
      "Batch train [26] loss 0.34596, dsc 0.65404\n",
      "Batch train [27] loss 0.37255, dsc 0.62745\n",
      "Batch train [28] loss 0.30444, dsc 0.69556\n",
      "Batch train [29] loss 0.22034, dsc 0.77966\n",
      "Batch train [30] loss 0.29599, dsc 0.70401\n",
      "Batch train [31] loss 0.25597, dsc 0.74403\n",
      "Batch train [32] loss 0.29849, dsc 0.70151\n",
      "Batch train [33] loss 0.23467, dsc 0.76533\n",
      "Batch train [34] loss 0.31903, dsc 0.68097\n",
      "Batch train [35] loss 0.26602, dsc 0.73398\n",
      "Batch train [36] loss 0.27441, dsc 0.72559\n",
      "Batch train [37] loss 0.25473, dsc 0.74527\n",
      "Batch train [38] loss 0.28517, dsc 0.71483\n",
      "Batch train [39] loss 0.22504, dsc 0.77496\n",
      "Batch train [40] loss 0.26547, dsc 0.73453\n",
      "Epoch [21] train done\n",
      "Batch eval [1] loss 0.24763, dsc 0.75237\n",
      "Batch eval [2] loss 0.30294, dsc 0.69706\n",
      "Batch eval [3] loss 0.20954, dsc 0.79046\n",
      "Batch eval [4] loss 0.21991, dsc 0.78009\n",
      "Batch eval [5] loss 0.23678, dsc 0.76322\n",
      "Epoch [21] valid done\n",
      "Epoch [21] T 1932.52s, deltaT 90.67s, loss: train 0.30031, valid 0.24336, dsc: train 0.69969, valid 0.75664\n",
      "Batch train [1] loss 0.21637, dsc 0.78363\n",
      "Batch train [2] loss 0.36478, dsc 0.63522\n",
      "Batch train [3] loss 0.33995, dsc 0.66005\n",
      "Batch train [4] loss 0.23672, dsc 0.76328\n",
      "Batch train [5] loss 0.27846, dsc 0.72154\n",
      "Batch train [6] loss 0.22645, dsc 0.77355\n",
      "Batch train [7] loss 0.22825, dsc 0.77175\n",
      "Batch train [8] loss 0.23960, dsc 0.76040\n",
      "Batch train [9] loss 0.26992, dsc 0.73008\n",
      "Batch train [10] loss 0.22266, dsc 0.77734\n",
      "Batch train [11] loss 0.38604, dsc 0.61396\n",
      "Batch train [12] loss 0.19029, dsc 0.80971\n",
      "Batch train [13] loss 0.23853, dsc 0.76147\n",
      "Batch train [14] loss 0.20623, dsc 0.79377\n",
      "Batch train [15] loss 0.32469, dsc 0.67531\n",
      "Batch train [16] loss 0.21997, dsc 0.78003\n",
      "Batch train [17] loss 0.26410, dsc 0.73590\n",
      "Batch train [18] loss 0.24881, dsc 0.75119\n",
      "Batch train [19] loss 0.26322, dsc 0.73678\n",
      "Batch train [20] loss 0.20599, dsc 0.79401\n",
      "Batch train [21] loss 0.23653, dsc 0.76347\n",
      "Batch train [22] loss 0.21509, dsc 0.78491\n",
      "Batch train [23] loss 0.27342, dsc 0.72658\n",
      "Batch train [24] loss 0.18933, dsc 0.81067\n",
      "Batch train [25] loss 0.21735, dsc 0.78265\n",
      "Batch train [26] loss 0.22664, dsc 0.77336\n",
      "Batch train [27] loss 0.21820, dsc 0.78180\n",
      "Batch train [28] loss 0.21605, dsc 0.78395\n",
      "Batch train [29] loss 0.20051, dsc 0.79949\n",
      "Batch train [30] loss 0.25569, dsc 0.74431\n",
      "Batch train [31] loss 0.27366, dsc 0.72634\n",
      "Batch train [32] loss 0.24186, dsc 0.75814\n",
      "Batch train [33] loss 0.19156, dsc 0.80844\n",
      "Batch train [34] loss 0.29339, dsc 0.70661\n",
      "Batch train [35] loss 0.30068, dsc 0.69932\n",
      "Batch train [36] loss 0.29002, dsc 0.70998\n",
      "Batch train [37] loss 0.28118, dsc 0.71882\n",
      "Batch train [38] loss 0.20439, dsc 0.79561\n",
      "Batch train [39] loss 0.27506, dsc 0.72494\n",
      "Batch train [40] loss 0.24011, dsc 0.75989\n",
      "Epoch [22] train done\n",
      "Batch eval [1] loss 0.78591, dsc 0.21409\n",
      "Batch eval [2] loss 0.99784, dsc 0.00216\n",
      "Batch eval [3] loss 0.90081, dsc 0.09919\n",
      "Batch eval [4] loss 0.80337, dsc 0.19663\n",
      "Batch eval [5] loss 0.99205, dsc 0.00795\n",
      "Epoch [22] valid done\n",
      "Epoch [22] T 2025.49s, deltaT 92.97s, loss: train 0.25029, valid 0.89599, dsc: train 0.74971, valid 0.10401\n",
      "Batch train [1] loss 0.20046, dsc 0.79954\n",
      "Batch train [2] loss 0.22377, dsc 0.77623\n",
      "Batch train [3] loss 0.16684, dsc 0.83316\n",
      "Batch train [4] loss 0.26344, dsc 0.73656\n",
      "Batch train [5] loss 0.19102, dsc 0.80898\n",
      "Batch train [6] loss 0.27612, dsc 0.72388\n",
      "Batch train [7] loss 0.38074, dsc 0.61926\n",
      "Batch train [8] loss 0.27444, dsc 0.72556\n",
      "Batch train [9] loss 0.24820, dsc 0.75180\n",
      "Batch train [10] loss 0.21673, dsc 0.78327\n",
      "Batch train [11] loss 0.24542, dsc 0.75458\n",
      "Batch train [12] loss 0.21873, dsc 0.78127\n",
      "Batch train [13] loss 0.23994, dsc 0.76006\n",
      "Batch train [14] loss 0.19085, dsc 0.80915\n",
      "Batch train [15] loss 0.19108, dsc 0.80892\n",
      "Batch train [16] loss 0.20597, dsc 0.79403\n",
      "Batch train [17] loss 0.29037, dsc 0.70963\n",
      "Batch train [18] loss 0.33720, dsc 0.66280\n",
      "Batch train [19] loss 0.18352, dsc 0.81648\n",
      "Batch train [20] loss 0.24392, dsc 0.75608\n",
      "Batch train [21] loss 0.17115, dsc 0.82885\n",
      "Batch train [22] loss 0.19458, dsc 0.80542\n",
      "Batch train [23] loss 0.22251, dsc 0.77749\n",
      "Batch train [24] loss 0.17583, dsc 0.82417\n",
      "Batch train [25] loss 0.23591, dsc 0.76409\n",
      "Batch train [26] loss 0.23508, dsc 0.76492\n",
      "Batch train [27] loss 0.26619, dsc 0.73381\n",
      "Batch train [28] loss 0.19761, dsc 0.80239\n",
      "Batch train [29] loss 0.21278, dsc 0.78722\n",
      "Batch train [30] loss 0.17179, dsc 0.82821\n",
      "Batch train [31] loss 0.19790, dsc 0.80210\n",
      "Batch train [32] loss 0.23741, dsc 0.76259\n",
      "Batch train [33] loss 0.28434, dsc 0.71566\n",
      "Batch train [34] loss 0.17044, dsc 0.82956\n",
      "Batch train [35] loss 0.18678, dsc 0.81322\n",
      "Batch train [36] loss 0.27506, dsc 0.72494\n",
      "Batch train [37] loss 0.17431, dsc 0.82569\n",
      "Batch train [38] loss 0.17646, dsc 0.82354\n",
      "Batch train [39] loss 0.19745, dsc 0.80255\n",
      "Batch train [40] loss 0.19286, dsc 0.80714\n",
      "Epoch [23] train done\n",
      "Batch eval [1] loss 0.45715, dsc 0.54285\n",
      "Batch eval [2] loss 0.78028, dsc 0.21972\n",
      "Batch eval [3] loss 0.48049, dsc 0.51951\n",
      "Batch eval [4] loss 0.20463, dsc 0.79537\n",
      "Batch eval [5] loss 0.45614, dsc 0.54386\n",
      "Epoch [23] valid done\n",
      "Epoch [23] T 2117.40s, deltaT 91.91s, loss: train 0.22413, valid 0.47574, dsc: train 0.77587, valid 0.52426\n",
      "Batch train [1] loss 0.17796, dsc 0.82204\n",
      "Batch train [2] loss 0.19895, dsc 0.80105\n",
      "Batch train [3] loss 0.27911, dsc 0.72089\n",
      "Batch train [4] loss 0.19539, dsc 0.80461\n",
      "Batch train [5] loss 0.17010, dsc 0.82990\n",
      "Batch train [6] loss 0.19915, dsc 0.80085\n",
      "Batch train [7] loss 0.18767, dsc 0.81233\n",
      "Batch train [8] loss 0.23509, dsc 0.76491\n",
      "Batch train [9] loss 0.18029, dsc 0.81971\n",
      "Batch train [10] loss 0.19345, dsc 0.80655\n",
      "Batch train [11] loss 0.22518, dsc 0.77482\n",
      "Batch train [12] loss 0.18915, dsc 0.81085\n",
      "Batch train [13] loss 0.19822, dsc 0.80178\n",
      "Batch train [14] loss 0.15002, dsc 0.84998\n",
      "Batch train [15] loss 0.15695, dsc 0.84305\n",
      "Batch train [16] loss 0.18702, dsc 0.81298\n",
      "Batch train [17] loss 0.19173, dsc 0.80827\n",
      "Batch train [18] loss 0.26563, dsc 0.73437\n",
      "Batch train [19] loss 0.19414, dsc 0.80586\n",
      "Batch train [20] loss 0.21117, dsc 0.78883\n",
      "Batch train [21] loss 0.22566, dsc 0.77434\n",
      "Batch train [22] loss 0.23051, dsc 0.76949\n",
      "Batch train [23] loss 0.16990, dsc 0.83010\n",
      "Batch train [24] loss 0.25042, dsc 0.74958\n",
      "Batch train [25] loss 0.22988, dsc 0.77012\n",
      "Batch train [26] loss 0.21269, dsc 0.78731\n",
      "Batch train [27] loss 0.23506, dsc 0.76494\n",
      "Batch train [28] loss 0.13473, dsc 0.86527\n",
      "Batch train [29] loss 0.19461, dsc 0.80539\n",
      "Batch train [30] loss 0.15275, dsc 0.84725\n",
      "Batch train [31] loss 0.19418, dsc 0.80582\n",
      "Batch train [32] loss 0.20182, dsc 0.79818\n",
      "Batch train [33] loss 0.13770, dsc 0.86230\n",
      "Batch train [34] loss 0.26269, dsc 0.73731\n",
      "Batch train [35] loss 0.15272, dsc 0.84728\n",
      "Batch train [36] loss 0.15858, dsc 0.84142\n",
      "Batch train [37] loss 0.19748, dsc 0.80252\n",
      "Batch train [38] loss 0.16171, dsc 0.83829\n",
      "Batch train [39] loss 0.20881, dsc 0.79119\n",
      "Batch train [40] loss 0.14841, dsc 0.85159\n",
      "Epoch [24] train done\n",
      "Batch eval [1] loss 0.20205, dsc 0.79795\n",
      "Batch eval [2] loss 0.21185, dsc 0.78815\n",
      "Batch eval [3] loss 0.21370, dsc 0.78630\n",
      "Batch eval [4] loss 0.20449, dsc 0.79551\n",
      "Batch eval [5] loss 0.22192, dsc 0.77808\n",
      "Epoch [24] valid done\n",
      "Epoch [24] T 2209.28s, deltaT 91.88s, loss: train 0.19617, valid 0.21080, dsc: train 0.80383, valid 0.78920\n",
      "Batch train [1] loss 0.16387, dsc 0.83613\n",
      "Batch train [2] loss 0.23334, dsc 0.76666\n",
      "Batch train [3] loss 0.19349, dsc 0.80651\n",
      "Batch train [4] loss 0.23898, dsc 0.76102\n",
      "Batch train [5] loss 0.21477, dsc 0.78523\n",
      "Batch train [6] loss 0.19118, dsc 0.80882\n",
      "Batch train [7] loss 0.16722, dsc 0.83278\n",
      "Batch train [8] loss 0.17766, dsc 0.82234\n",
      "Batch train [9] loss 0.14408, dsc 0.85592\n",
      "Batch train [10] loss 0.16121, dsc 0.83879\n",
      "Batch train [11] loss 0.17049, dsc 0.82951\n",
      "Batch train [12] loss 0.22101, dsc 0.77899\n",
      "Batch train [13] loss 0.12805, dsc 0.87195\n",
      "Batch train [14] loss 0.16080, dsc 0.83920\n",
      "Batch train [15] loss 0.16183, dsc 0.83817\n",
      "Batch train [16] loss 0.16016, dsc 0.83984\n",
      "Batch train [17] loss 0.15337, dsc 0.84663\n",
      "Batch train [18] loss 0.17201, dsc 0.82799\n",
      "Batch train [19] loss 0.14344, dsc 0.85656\n",
      "Batch train [20] loss 0.20193, dsc 0.79807\n",
      "Batch train [21] loss 0.16301, dsc 0.83699\n",
      "Batch train [22] loss 0.14422, dsc 0.85578\n",
      "Batch train [23] loss 0.17188, dsc 0.82812\n",
      "Batch train [24] loss 0.16805, dsc 0.83195\n",
      "Batch train [25] loss 0.17162, dsc 0.82838\n",
      "Batch train [26] loss 0.14754, dsc 0.85246\n",
      "Batch train [27] loss 0.19611, dsc 0.80389\n",
      "Batch train [28] loss 0.22495, dsc 0.77505\n",
      "Batch train [29] loss 0.19218, dsc 0.80782\n",
      "Batch train [30] loss 0.12967, dsc 0.87033\n",
      "Batch train [31] loss 0.11740, dsc 0.88260\n",
      "Batch train [32] loss 0.23951, dsc 0.76049\n",
      "Batch train [33] loss 0.15593, dsc 0.84407\n",
      "Batch train [34] loss 0.18450, dsc 0.81550\n",
      "Batch train [35] loss 0.14526, dsc 0.85474\n",
      "Batch train [36] loss 0.17152, dsc 0.82848\n",
      "Batch train [37] loss 0.16135, dsc 0.83865\n",
      "Batch train [38] loss 0.18376, dsc 0.81624\n",
      "Batch train [39] loss 0.15397, dsc 0.84603\n",
      "Batch train [40] loss 0.16854, dsc 0.83146\n",
      "Epoch [25] train done\n",
      "Batch eval [1] loss 0.26794, dsc 0.73206\n",
      "Batch eval [2] loss 0.53819, dsc 0.46181\n",
      "Batch eval [3] loss 0.30938, dsc 0.69062\n",
      "Batch eval [4] loss 0.15313, dsc 0.84687\n",
      "Batch eval [5] loss 0.31715, dsc 0.68285\n",
      "Epoch [25] valid done\n",
      "Epoch [25] T 2302.16s, deltaT 92.88s, loss: train 0.17375, valid 0.31716, dsc: train 0.82625, valid 0.68284\n",
      "Batch train [1] loss 0.16181, dsc 0.83819\n",
      "Batch train [2] loss 0.13754, dsc 0.86246\n",
      "Batch train [3] loss 0.17052, dsc 0.82948\n",
      "Batch train [4] loss 0.16683, dsc 0.83317\n",
      "Batch train [5] loss 0.14036, dsc 0.85964\n",
      "Batch train [6] loss 0.13821, dsc 0.86179\n",
      "Batch train [7] loss 0.17546, dsc 0.82454\n",
      "Batch train [8] loss 0.12612, dsc 0.87388\n",
      "Batch train [9] loss 0.14417, dsc 0.85583\n",
      "Batch train [10] loss 0.15534, dsc 0.84466\n",
      "Batch train [11] loss 0.14162, dsc 0.85838\n",
      "Batch train [12] loss 0.19632, dsc 0.80368\n",
      "Batch train [13] loss 0.11048, dsc 0.88952\n",
      "Batch train [14] loss 0.15449, dsc 0.84551\n",
      "Batch train [15] loss 0.20227, dsc 0.79773\n",
      "Batch train [16] loss 0.14348, dsc 0.85652\n",
      "Batch train [17] loss 0.13000, dsc 0.87000\n",
      "Batch train [18] loss 0.14540, dsc 0.85460\n",
      "Batch train [19] loss 0.18160, dsc 0.81840\n",
      "Batch train [20] loss 0.23179, dsc 0.76821\n",
      "Batch train [21] loss 0.16314, dsc 0.83686\n",
      "Batch train [22] loss 0.16954, dsc 0.83046\n",
      "Batch train [23] loss 0.13781, dsc 0.86219\n",
      "Batch train [24] loss 0.15283, dsc 0.84717\n",
      "Batch train [25] loss 0.11741, dsc 0.88259\n",
      "Batch train [26] loss 0.14437, dsc 0.85563\n",
      "Batch train [27] loss 0.14503, dsc 0.85497\n",
      "Batch train [28] loss 0.20195, dsc 0.79805\n",
      "Batch train [29] loss 0.20404, dsc 0.79596\n",
      "Batch train [30] loss 0.20444, dsc 0.79556\n",
      "Batch train [31] loss 0.17598, dsc 0.82402\n",
      "Batch train [32] loss 0.16836, dsc 0.83164\n",
      "Batch train [33] loss 0.13408, dsc 0.86592\n",
      "Batch train [34] loss 0.15226, dsc 0.84774\n",
      "Batch train [35] loss 0.14562, dsc 0.85438\n",
      "Batch train [36] loss 0.14442, dsc 0.85558\n",
      "Batch train [37] loss 0.18659, dsc 0.81341\n",
      "Batch train [38] loss 0.11471, dsc 0.88529\n",
      "Batch train [39] loss 0.11710, dsc 0.88290\n",
      "Batch train [40] loss 0.15270, dsc 0.84730\n",
      "Epoch [26] train done\n",
      "Batch eval [1] loss 0.14092, dsc 0.85908\n",
      "Batch eval [2] loss 0.18134, dsc 0.81866\n",
      "Batch eval [3] loss 0.15307, dsc 0.84693\n",
      "Batch eval [4] loss 0.15613, dsc 0.84387\n",
      "Batch eval [5] loss 0.14336, dsc 0.85664\n",
      "Epoch [26] valid done\n",
      "Epoch [26] T 2394.24s, deltaT 92.07s, loss: train 0.15715, valid 0.15496, dsc: train 0.84285, valid 0.84504\n",
      "Batch train [1] loss 0.15270, dsc 0.84730\n",
      "Batch train [2] loss 0.15418, dsc 0.84582\n",
      "Batch train [3] loss 0.13213, dsc 0.86787\n",
      "Batch train [4] loss 0.16656, dsc 0.83344\n",
      "Batch train [5] loss 0.14693, dsc 0.85307\n",
      "Batch train [6] loss 0.15653, dsc 0.84347\n",
      "Batch train [7] loss 0.12881, dsc 0.87119\n",
      "Batch train [8] loss 0.13065, dsc 0.86935\n",
      "Batch train [9] loss 0.11987, dsc 0.88013\n",
      "Batch train [10] loss 0.17253, dsc 0.82747\n",
      "Batch train [11] loss 0.10674, dsc 0.89326\n",
      "Batch train [12] loss 0.16674, dsc 0.83326\n",
      "Batch train [13] loss 0.16661, dsc 0.83339\n",
      "Batch train [14] loss 0.16055, dsc 0.83945\n",
      "Batch train [15] loss 0.12100, dsc 0.87900\n",
      "Batch train [16] loss 0.17004, dsc 0.82996\n",
      "Batch train [17] loss 0.16053, dsc 0.83947\n",
      "Batch train [18] loss 0.23661, dsc 0.76339\n",
      "Batch train [19] loss 0.12480, dsc 0.87520\n",
      "Batch train [20] loss 0.12477, dsc 0.87523\n",
      "Batch train [21] loss 0.16770, dsc 0.83230\n",
      "Batch train [22] loss 0.12040, dsc 0.87960\n",
      "Batch train [23] loss 0.15790, dsc 0.84210\n",
      "Batch train [24] loss 0.16385, dsc 0.83615\n",
      "Batch train [25] loss 0.15009, dsc 0.84991\n",
      "Batch train [26] loss 0.13704, dsc 0.86296\n",
      "Batch train [27] loss 0.14638, dsc 0.85362\n",
      "Batch train [28] loss 0.11958, dsc 0.88042\n",
      "Batch train [29] loss 0.19574, dsc 0.80426\n",
      "Batch train [30] loss 0.11192, dsc 0.88808\n",
      "Batch train [31] loss 0.14007, dsc 0.85993\n",
      "Batch train [32] loss 0.13767, dsc 0.86233\n",
      "Batch train [33] loss 0.10550, dsc 0.89450\n",
      "Batch train [34] loss 0.12502, dsc 0.87498\n",
      "Batch train [35] loss 0.22838, dsc 0.77162\n",
      "Batch train [36] loss 0.19333, dsc 0.80667\n",
      "Batch train [37] loss 0.20831, dsc 0.79169\n",
      "Batch train [38] loss 0.14421, dsc 0.85579\n",
      "Batch train [39] loss 0.13417, dsc 0.86583\n",
      "Batch train [40] loss 0.14461, dsc 0.85539\n",
      "Epoch [27] train done\n",
      "Batch eval [1] loss 0.15316, dsc 0.84684\n",
      "Batch eval [2] loss 0.15461, dsc 0.84539\n",
      "Batch eval [3] loss 0.12551, dsc 0.87449\n",
      "Batch eval [4] loss 0.15557, dsc 0.84443\n",
      "Batch eval [5] loss 0.16531, dsc 0.83469\n",
      "Epoch [27] valid done\n",
      "Epoch [27] T 2487.00s, deltaT 92.76s, loss: train 0.15078, valid 0.15083, dsc: train 0.84922, valid 0.84917\n",
      "Batch train [1] loss 0.15524, dsc 0.84476\n",
      "Batch train [2] loss 0.12687, dsc 0.87313\n",
      "Batch train [3] loss 0.11329, dsc 0.88671\n",
      "Batch train [4] loss 0.14377, dsc 0.85623\n",
      "Batch train [5] loss 0.14618, dsc 0.85382\n",
      "Batch train [6] loss 0.12757, dsc 0.87243\n",
      "Batch train [7] loss 0.16026, dsc 0.83974\n",
      "Batch train [8] loss 0.12312, dsc 0.87688\n",
      "Batch train [9] loss 0.14525, dsc 0.85475\n",
      "Batch train [10] loss 0.23647, dsc 0.76353\n",
      "Batch train [11] loss 0.16449, dsc 0.83551\n",
      "Batch train [12] loss 0.12161, dsc 0.87839\n",
      "Batch train [13] loss 0.13023, dsc 0.86977\n",
      "Batch train [14] loss 0.18332, dsc 0.81668\n",
      "Batch train [15] loss 0.12002, dsc 0.87998\n",
      "Batch train [16] loss 0.14671, dsc 0.85329\n",
      "Batch train [17] loss 0.14309, dsc 0.85691\n",
      "Batch train [18] loss 0.12936, dsc 0.87064\n",
      "Batch train [19] loss 0.12323, dsc 0.87677\n",
      "Batch train [20] loss 0.14709, dsc 0.85291\n",
      "Batch train [21] loss 0.14717, dsc 0.85283\n",
      "Batch train [22] loss 0.11840, dsc 0.88160\n",
      "Batch train [23] loss 0.14094, dsc 0.85906\n",
      "Batch train [24] loss 0.14816, dsc 0.85184\n",
      "Batch train [25] loss 0.12860, dsc 0.87140\n",
      "Batch train [26] loss 0.10945, dsc 0.89055\n",
      "Batch train [27] loss 0.14330, dsc 0.85670\n",
      "Batch train [28] loss 0.14732, dsc 0.85268\n",
      "Batch train [29] loss 0.12332, dsc 0.87668\n",
      "Batch train [30] loss 0.15436, dsc 0.84564\n",
      "Batch train [31] loss 0.11681, dsc 0.88319\n",
      "Batch train [32] loss 0.12606, dsc 0.87394\n",
      "Batch train [33] loss 0.12216, dsc 0.87784\n",
      "Batch train [34] loss 0.11892, dsc 0.88108\n",
      "Batch train [35] loss 0.13889, dsc 0.86111\n",
      "Batch train [36] loss 0.12024, dsc 0.87976\n",
      "Batch train [37] loss 0.14496, dsc 0.85504\n",
      "Batch train [38] loss 0.20307, dsc 0.79693\n",
      "Batch train [39] loss 0.13905, dsc 0.86095\n",
      "Batch train [40] loss 0.22465, dsc 0.77535\n",
      "Epoch [28] train done\n",
      "Batch eval [1] loss 0.16145, dsc 0.83855\n",
      "Batch eval [2] loss 0.16652, dsc 0.83348\n",
      "Batch eval [3] loss 0.14565, dsc 0.85435\n",
      "Batch eval [4] loss 0.12434, dsc 0.87566\n",
      "Batch eval [5] loss 0.13723, dsc 0.86277\n",
      "Epoch [28] valid done\n",
      "Epoch [28] T 2577.99s, deltaT 90.99s, loss: train 0.14258, valid 0.14704, dsc: train 0.85743, valid 0.85296\n",
      "Batch train [1] loss 0.11641, dsc 0.88359\n",
      "Batch train [2] loss 0.12314, dsc 0.87686\n",
      "Batch train [3] loss 0.10923, dsc 0.89077\n",
      "Batch train [4] loss 0.18185, dsc 0.81815\n",
      "Batch train [5] loss 0.13723, dsc 0.86277\n",
      "Batch train [6] loss 0.12336, dsc 0.87664\n",
      "Batch train [7] loss 0.13842, dsc 0.86158\n",
      "Batch train [8] loss 0.12460, dsc 0.87540\n",
      "Batch train [9] loss 0.12814, dsc 0.87186\n",
      "Batch train [10] loss 0.11197, dsc 0.88803\n",
      "Batch train [11] loss 0.14663, dsc 0.85337\n",
      "Batch train [12] loss 0.12012, dsc 0.87988\n",
      "Batch train [13] loss 0.14323, dsc 0.85677\n",
      "Batch train [14] loss 0.12906, dsc 0.87094\n",
      "Batch train [15] loss 0.22308, dsc 0.77692\n",
      "Batch train [16] loss 0.19529, dsc 0.80471\n",
      "Batch train [17] loss 0.13771, dsc 0.86229\n",
      "Batch train [18] loss 0.15391, dsc 0.84609\n",
      "Batch train [19] loss 0.14319, dsc 0.85681\n",
      "Batch train [20] loss 0.09981, dsc 0.90019\n",
      "Batch train [21] loss 0.12842, dsc 0.87158\n",
      "Batch train [22] loss 0.16790, dsc 0.83210\n",
      "Batch train [23] loss 0.18469, dsc 0.81531\n",
      "Batch train [24] loss 0.14349, dsc 0.85651\n",
      "Batch train [25] loss 0.13183, dsc 0.86817\n",
      "Batch train [26] loss 0.10669, dsc 0.89331\n",
      "Batch train [27] loss 0.13490, dsc 0.86510\n",
      "Batch train [28] loss 0.13021, dsc 0.86979\n",
      "Batch train [29] loss 0.11188, dsc 0.88812\n",
      "Batch train [30] loss 0.17111, dsc 0.82889\n",
      "Batch train [31] loss 0.10873, dsc 0.89127\n",
      "Batch train [32] loss 0.11153, dsc 0.88847\n",
      "Batch train [33] loss 0.11822, dsc 0.88178\n",
      "Batch train [34] loss 0.12263, dsc 0.87737\n",
      "Batch train [35] loss 0.12030, dsc 0.87970\n",
      "Batch train [36] loss 0.15146, dsc 0.84854\n",
      "Batch train [37] loss 0.14068, dsc 0.85932\n",
      "Batch train [38] loss 0.10328, dsc 0.89672\n",
      "Batch train [39] loss 0.17730, dsc 0.82270\n",
      "Batch train [40] loss 0.18411, dsc 0.81589\n",
      "Epoch [29] train done\n",
      "Batch eval [1] loss 0.16519, dsc 0.83481\n",
      "Batch eval [2] loss 0.14757, dsc 0.85243\n",
      "Batch eval [3] loss 0.08997, dsc 0.91003\n",
      "Batch eval [4] loss 0.17202, dsc 0.82798\n",
      "Batch eval [5] loss 0.17669, dsc 0.82331\n",
      "Epoch [29] valid done\n",
      "Epoch [29] T 2671.48s, deltaT 93.48s, loss: train 0.13839, valid 0.15029, dsc: train 0.86161, valid 0.84971\n",
      "Batch train [1] loss 0.11672, dsc 0.88328\n",
      "Batch train [2] loss 0.11240, dsc 0.88760\n",
      "Batch train [3] loss 0.11050, dsc 0.88950\n",
      "Batch train [4] loss 0.16083, dsc 0.83917\n",
      "Batch train [5] loss 0.11964, dsc 0.88036\n",
      "Batch train [6] loss 0.17943, dsc 0.82057\n",
      "Batch train [7] loss 0.14288, dsc 0.85712\n",
      "Batch train [8] loss 0.09865, dsc 0.90135\n",
      "Batch train [9] loss 0.12291, dsc 0.87709\n",
      "Batch train [10] loss 0.15248, dsc 0.84752\n",
      "Batch train [11] loss 0.11207, dsc 0.88793\n",
      "Batch train [12] loss 0.10906, dsc 0.89094\n",
      "Batch train [13] loss 0.11713, dsc 0.88287\n",
      "Batch train [14] loss 0.12319, dsc 0.87681\n",
      "Batch train [15] loss 0.10881, dsc 0.89119\n",
      "Batch train [16] loss 0.19546, dsc 0.80454\n",
      "Batch train [17] loss 0.19876, dsc 0.80124\n",
      "Batch train [18] loss 0.15844, dsc 0.84156\n",
      "Batch train [19] loss 0.16789, dsc 0.83211\n",
      "Batch train [20] loss 0.11956, dsc 0.88044\n",
      "Batch train [21] loss 0.14673, dsc 0.85327\n",
      "Batch train [22] loss 0.13467, dsc 0.86533\n",
      "Batch train [23] loss 0.16233, dsc 0.83767\n",
      "Batch train [24] loss 0.10665, dsc 0.89335\n",
      "Batch train [25] loss 0.13335, dsc 0.86665\n",
      "Batch train [26] loss 0.09138, dsc 0.90862\n",
      "Batch train [27] loss 0.12810, dsc 0.87190\n",
      "Batch train [28] loss 0.17040, dsc 0.82960\n",
      "Batch train [29] loss 0.12487, dsc 0.87513\n",
      "Batch train [30] loss 0.14901, dsc 0.85099\n",
      "Batch train [31] loss 0.11674, dsc 0.88326\n",
      "Batch train [32] loss 0.20055, dsc 0.79945\n",
      "Batch train [33] loss 0.11328, dsc 0.88672\n",
      "Batch train [34] loss 0.15404, dsc 0.84596\n",
      "Batch train [35] loss 0.12242, dsc 0.87758\n",
      "Batch train [36] loss 0.14477, dsc 0.85523\n",
      "Batch train [37] loss 0.09385, dsc 0.90615\n",
      "Batch train [38] loss 0.10480, dsc 0.89520\n",
      "Batch train [39] loss 0.11823, dsc 0.88177\n",
      "Batch train [40] loss 0.12840, dsc 0.87160\n",
      "Epoch [30] train done\n",
      "Batch eval [1] loss 0.99973, dsc 0.00027\n",
      "Batch eval [2] loss 0.99973, dsc 0.00027\n",
      "Batch eval [3] loss 0.99974, dsc 0.00026\n",
      "Batch eval [4] loss 0.99979, dsc 0.00021\n",
      "Batch eval [5] loss 0.99972, dsc 0.00028\n",
      "Epoch [30] valid done\n",
      "Epoch [30] T 2764.06s, deltaT 92.59s, loss: train 0.13428, valid 0.99974, dsc: train 0.86572, valid 0.00026\n",
      "Elapsed time 0:46:04\n"
     ]
    }
   ],
   "source": [
    "cut_model_info = prepare_model(in_channels=8, train_dataset=cut_train_dataset, valid_dataset=cut_valid_dataset, test_dataset=cut_test_dataset)\n",
    "show_model_info(cut_model_info)\n",
    "\n",
    "cut_train_loop_params = {k:v for k,v in cut_model_info.items() if k not in ['model_total_params', 'model_total_trainable_params']}\n",
    "train_loop(**cut_train_loop_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch train [1] loss 0.13134, dsc 0.86866\n",
      "Batch train [2] loss 0.12497, dsc 0.87503\n",
      "Batch train [3] loss 0.11860, dsc 0.88140\n",
      "Batch train [4] loss 0.10048, dsc 0.89952\n",
      "Batch train [5] loss 0.11291, dsc 0.88709\n",
      "Batch train [6] loss 0.13088, dsc 0.86912\n",
      "Batch train [7] loss 0.18721, dsc 0.81279\n",
      "Batch train [8] loss 0.11802, dsc 0.88198\n",
      "Batch train [9] loss 0.13769, dsc 0.86231\n",
      "Batch train [10] loss 0.11462, dsc 0.88538\n",
      "Batch train [11] loss 0.11791, dsc 0.88209\n",
      "Batch train [12] loss 0.14669, dsc 0.85331\n",
      "Batch train [13] loss 0.16311, dsc 0.83689\n",
      "Batch train [14] loss 0.14313, dsc 0.85687\n",
      "Batch train [15] loss 0.14085, dsc 0.85915\n",
      "Batch train [16] loss 0.14378, dsc 0.85622\n",
      "Batch train [17] loss 0.13012, dsc 0.86988\n",
      "Batch train [18] loss 0.11089, dsc 0.88911\n",
      "Batch train [19] loss 0.08718, dsc 0.91282\n",
      "Batch train [20] loss 0.15705, dsc 0.84295\n",
      "Batch train [21] loss 0.11444, dsc 0.88556\n",
      "Batch train [22] loss 0.10427, dsc 0.89573\n",
      "Batch train [23] loss 0.11995, dsc 0.88005\n",
      "Batch train [24] loss 0.09052, dsc 0.90948\n",
      "Batch train [25] loss 0.13932, dsc 0.86068\n",
      "Batch train [26] loss 0.10842, dsc 0.89158\n",
      "Batch train [27] loss 0.14735, dsc 0.85265\n",
      "Batch train [28] loss 0.11092, dsc 0.88908\n",
      "Batch train [29] loss 0.16608, dsc 0.83392\n",
      "Batch train [30] loss 0.12164, dsc 0.87836\n",
      "Batch train [31] loss 0.10506, dsc 0.89494\n",
      "Batch train [32] loss 0.14149, dsc 0.85851\n",
      "Batch train [33] loss 0.11180, dsc 0.88820\n",
      "Batch train [34] loss 0.10780, dsc 0.89220\n",
      "Batch train [35] loss 0.11254, dsc 0.88746\n",
      "Batch train [36] loss 0.12026, dsc 0.87974\n",
      "Batch train [37] loss 0.16263, dsc 0.83737\n",
      "Batch train [38] loss 0.10709, dsc 0.89291\n",
      "Batch train [39] loss 0.12277, dsc 0.87723\n",
      "Batch train [40] loss 0.12419, dsc 0.87581\n"
     ]
    }
   ],
   "source": [
    "from src.training_helpers import iterate_model\n",
    "\n",
    "train_loss, train_dsc = iterate_model(cut_train_loop_params[\"train_dataloader\"], cut_train_loop_params[\"model\"], cut_train_loop_params[\"optimizer\"], cut_train_loop_params[\"criterion\"], cut_train_loop_params[\"device\"], is_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch eval [1] loss 0.18210, dsc 0.81790\n",
      "Batch eval [2] loss 0.42156, dsc 0.57844\n",
      "Batch eval [3] loss 0.24926, dsc 0.75074\n",
      "Batch eval [4] loss 0.11918, dsc 0.88082\n",
      "Batch eval [5] loss 0.27942, dsc 0.72058\n"
     ]
    }
   ],
   "source": [
    "valid_loss, valid_dsc = iterate_model(cut_train_loop_params[\"valid_dataloader\"], cut_train_loop_params[\"model\"], cut_train_loop_params[\"optimizer\"], cut_train_loop_params[\"criterion\"], cut_train_loop_params[\"device\"], is_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "160x128x128 = 2_621_440 \\\n",
    "72x198x168 = 2_395_008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Train Eval"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing number 31\n",
      "loss 0.2326015830039978, dsc 0.7673984169960022, inputs_len 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce205ee9b9c6486bbecd9401ed26c984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=49, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d13d82261048dc88b296590c70062f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG shapes torch.Size([1, 72, 192, 168]) torch.Size([1, 72, 192, 168]) torch.Size([1, 1, 72, 192, 168])\n",
      "DEBUG prediction max 1.0, min 6.080886123527307e-06\n",
      "DEBUG intersection 2587.37060546875\n",
      "DEBUG label sum 3871.0\n",
      "DEBUG prediction sum 2872.236328125\n",
      "DEBUG intersection2 2587.37060546875\n",
      "DEBUG dsc 0.7673972845077515\n",
      "DEBUG MSE 0.0005200004670768976\n"
     ]
    }
   ],
   "source": [
    "max_slices = cut_train_dataset.__getitem__(0)[1].shape[0]\n",
    "\n",
    "display(Markdown(\"#### Train Eval\"))\n",
    "show_model_dataset_pred_preview(cut_model_info, cut_train_dataset, max_slices=max_slices, default_slice=49)\n",
    "\n",
    "# display(Markdown(\"#### Valid Eval\"))\n",
    "# show_model_dataset_pred_preview(cut_model_info, cut_valid_dataset, max_slices=max_slices, default_slice=53)\n",
    "\n",
    "# display(Markdown(\"#### Test Eval\"))\n",
    "# eval_image_dataset(test_dataset, 78, 'test_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.calc_dsc import calc_dsc\n",
    "import pandas as pd\n",
    "\n",
    "def get_rescaled_preds(model, dataset, device):\n",
    "    preds = []\n",
    "    rescaled_preds = []\n",
    "    for i in range(len(dataset)):\n",
    "        data, label = dataset[i]\n",
    "        data_input = torch.from_numpy(np.array([data])).to(device).float()\n",
    "        # data_input.shape => batch, channel, slices, x, y\n",
    "\n",
    "        prediction = model(data_input)[0]\n",
    "        prediction = prediction.cpu().detach().numpy()[0]\n",
    "        rescaled_pred = prediction - prediction.min()\n",
    "        rescaled_pred = rescaled_pred / rescaled_pred.max()\n",
    "        \n",
    "        preds.append(prediction)\n",
    "        rescaled_preds.append(rescaled_pred)\n",
    "        \n",
    "    return preds, rescaled_preds, \n",
    "\n",
    "\n",
    "def get_dataset_threshold_info(dataset, preds, rescaled_preds, index, info_list, is_train=False, is_valid=False, is_test=False):\n",
    "    data, label = dataset[index]\n",
    "    prediction = preds[index]\n",
    "    rescaled_pred = rescaled_preds[index]\n",
    "    \n",
    "    info = {}\n",
    "    info['index'] = index\n",
    "    info['dsc'] = calc_dsc(label, prediction)\n",
    "    info['rescaled_dsc'] = calc_dsc(label, rescaled_pred)\n",
    "    info['is_train'] = is_train\n",
    "    info['is_valid'] = is_valid\n",
    "    info['is_test'] = is_test\n",
    "    \n",
    "    step = 0.01\n",
    "    for thresh in np.arange(0, 1 + step, step):\n",
    "        tmp_text = \"{:.2f}\".format(thresh)\n",
    "        info[f'thres_rescaled_dsc_{tmp_text}'] = calc_dsc(label, (rescaled_pred > thresh) * 1)\n",
    "        \n",
    "    info_list.append(info)\n",
    "    return info_list\n",
    "\n",
    "\n",
    "def get_threshold_info_df(model, dataset, device, train_indices, valid_indices, test_indices):\n",
    "    preds, rescaled_preds = get_rescaled_preds(model, dataset, device)\n",
    "    info_list = []\n",
    "\n",
    "    # get table with dsc, rescaled dsc and treshold dsc with some steps, with subset info\n",
    "    print('starting calc dsc per threshold')\n",
    "    for index in list(sorted(train_indices)):\n",
    "        info_list = get_dataset_threshold_info(dataset, preds, rescaled_preds, index, info_list, is_train=True)\n",
    "    print('done train')\n",
    "    for index in list(sorted(valid_indices)):\n",
    "        info_list = get_dataset_threshold_info(dataset, preds, rescaled_preds, index, info_list, is_valid=True)\n",
    "    print('done valid')\n",
    "    for index in list(sorted(test_indices)):\n",
    "        info_list = get_dataset_threshold_info(dataset, preds, rescaled_preds, index, info_list, is_test=True)\n",
    "    print('done test')\n",
    "    info_df = pd.DataFrame(info_list).set_index('index')\n",
    "    \n",
    "    return info_df, preds, rescaled_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting calc dsc per threshold\n",
      "done train\n",
      "done valid\n",
      "done test\n",
      "----------- Curve\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Best Threshold\n",
      "dsc on train: 0.8147569037995362, threshold 0.01\n",
      "\n",
      "----------- Eval\n",
      "--valid--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index\n",
       "6     0.832812\n",
       "13    0.700683\n",
       "19    0.800531\n",
       "25    0.906688\n",
       "38    0.889212\n",
       "Name: thres_rescaled_dsc_0.01, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results DSC: train 0.8147569037995364 valid 0.8259851907221035\n"
     ]
    }
   ],
   "source": [
    "cut_model_info[\"model\"].eval()\n",
    "with torch.no_grad():\n",
    "    info_df, preds, rescaled_preds = get_threshold_info_df(cut_model_info[\"model\"], cut_full_res_dataset, cut_model_info[\"device\"], cut_train_dataset.indices, cut_valid_dataset.indices, cut_test_dataset.indices)\n",
    "\n",
    "    # getting mean dsc for each treshold\n",
    "    train_tmp_df = info_df[info_df['is_train']]\n",
    "\n",
    "    mean_train_tmp_df = train_tmp_df.mean().copy().drop(['dsc', 'rescaled_dsc', 'is_train', 'is_valid', 'is_test'])\n",
    "    mean_train_tmp_df.index = mean_train_tmp_df.index.str.split('_').map(lambda x: eval(x[-1]))\n",
    "\n",
    "    # print('----------- Sorted by threshold value')\n",
    "    # display(mean_train_tmp_df)\n",
    "    # print()\n",
    "\n",
    "    # print('----------- Sorted by average DSC')\n",
    "    # display(mean_train_tmp_df.sort_values())\n",
    "    # print()\n",
    "\n",
    "    print('----------- Curve')\n",
    "    mean_train_tmp_df.plot()\n",
    "    plt.show()\n",
    "    print()\n",
    "    print('----------- Best Threshold')\n",
    "    best_threshold = mean_train_tmp_df.index[mean_train_tmp_df.argmax()]\n",
    "    print(f'dsc on train: {mean_train_tmp_df.max()}, threshold {best_threshold}')\n",
    "    print()\n",
    "    # final results with best treshold\n",
    "    print('----------- Eval')\n",
    "    best_threshold_column = f'thres_rescaled_dsc_{best_threshold}'\n",
    "    train_tmp = info_df[info_df['is_train']][best_threshold_column]\n",
    "    valid_tmp = info_df[info_df['is_valid']][best_threshold_column]\n",
    "    # test_tmp = info_df[info_df['is_test']][best_threshold_column]\n",
    "\n",
    "    #print('--train--')\n",
    "    #display(train_tmp)\n",
    "    print('--valid--')\n",
    "    display(valid_tmp)\n",
    "    # print('--test--')\n",
    "    # display(test_tmp)\n",
    "\n",
    "    print(f'Results DSC: train {train_tmp.mean()} valid {valid_tmp.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBQAAAF0CAYAAACT02phAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7TedX0n+vd3Z2cnIUAukEC4SJByDVVECgi29bRVwHEU5axWlnPG6VSxZ7Ud2846rZ2udWy7ZpjWKTN26Dky9kit1ipOL8o4ausVHW+VqCARIYEEEiSQOyHZSfble/7YT9INbEh+2Zffs/O8Xmt9V57n91x+n+f37Oe9f/ns7+/3lFprAAAAAJroa7sAAAAAYPbRUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGpu2hkIp5dpSygOllHWllHdN13oAmJgcBmifLAaOZaXWOvVPWsqcJA8meXWSTUm+neTGWusPpnxlADyHHAZonywGjnXTNUPh8iTraq0P11oPJPlYkjdM07oAeC45DNA+WQwc0/qn6XlPT7Jx3PVNSa54vjuXUqZ+mgTA1Nhaa13WdhFHoVEOJ7IY6F611tJ2DUfJPjFwrJhwn3i6GgqHVUq5KclNba0f4Ag90nYB00kWA7RLDgOzxIT7xNPVUHgsyZnjrp/RWXZIrfX9Sd6f6MYCTIPD5nAiiwGmmX1i4Jg2XedQ+HaSc0spZ5dSBpK8Ocmd07QuAJ5LDgO0TxYDx7RpmaFQax0upfxqkr9PMifJ7bXWNdOxLgCeSw4DtE8WA8e6afnayMZFmN4FdK/VtdbL2i5iJshioFvN4pMyNiKHgS424T7xdB3yAAAAABzDNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxjQUAAAAgMY0FAAAAIDGNBQAAACAxo66oVBKObOU8qVSyg9KKWtKKe/sLP+9UspjpZTvdcZrp65cAMaTxQDtksNAL+ufxGOHk/zbWut3SiknJFldSvlc57b/Umv948mXB8BhyGKAdslhoGcddUOh1vp4ksc7l3eXUu5PcvpUFQbA4cligHbJYaCXTck5FEopK5O8LMm3Oot+tZRybynl9lLKkqlYBwAvTBYDtEsOA71m0g2FUsrxSf4mya/XWp9K8r4k5yS5JGPd2lue53E3lVLuLqXcPdkaAHqdLAZolxwGelGptR79g0uZm+RTSf6+1vqfJ7h9ZZJP1VovPszzHH0RANNrda31sraLeCGyGDjW1VpL2zW8EDkM9IAJ94kn8y0PJckHktw/PjhLKSvG3e2NSe472nUA8MJkMUC75DDQyybzLQ9XJ/k/kny/lPK9zrJ/l+TGUsolSWqSDUneMakKmXJjv/eeaTIzVYBWyeJZShbDMUMOAz1rUoc8TFkRpndNq76+vsyfPz+nnXZazjrrrKxatSoLFy7MvHnzsnXr1mzbti3f+MY3snv37uzYsSO1Vju18E+6/pCHqSKLp5cshqPX7Yc8TBU5DHSxCfeJJzNDgS5VSklfX9+hHdX+/v4sXLgwL37xi3PhhRfmqquuyuLFizN//vw89thj2bx5c5588sls27YtAwMDGRoaytDQUPbs2ZORkRE7tFOklJI5c+ZkYGAgAwMDmTNnTvr6+p7xV8rR0dHUWnPgwIEMDw9n//79GR0dbbFq4GjJ4u4kiwHaNTAwkMWLFx/x/UdHR7Njx46MjIxMY1UcLTMUjkGLFy/OySefnF/5lV/J1VdfnUWLFqW/vz8DAwOZN29e5s+fnzlz5qSUkuHh4QwPD2fPnj0ZHh7O0NBQHn744WzcuDF/+qd/mnXr1mXv3r12ZCdpzpw5OeGEE/KiF70oV155Za6++uq86EUvyqJFizJ37twkY1Od9+7dm927d+eb3/xmHnroofzDP/xDduzYkcHBwZZfQU8zQ4GjIou7jyyevcxQgGPHK1/5ynzgAx/InDlzjuj+u3btyi/8wi9k3bp101wZh2GGwrFu4cKFOfvss7Ns2bKcccYZ+fEf//Gce+65OeGEEw77gV2yZMmh6bUDAwM58cQTc+WVV2bp0qX54Q9/mD179uSpp56aoVdybCilZPny5Tn++OOzYsWKLFq0KKeffnouvfTSrFq1KqeddlpOOOGEZ+zEDg4OZs+ePRkcHMyiRYuyZ8+ebN++PTt37sy2bduycePGQ385A7rTwoULs3LlyixfvlwWdwFZDFxwwQVZvnz5UT12dHQ09957r+ydhFJKVq1alaVLlyZJLr/88vzYj/1Y+vqO7PsBdu/enauuuiqnnXZakuRHP/qR5kI3Objj0ubI2MlqjEmMUkq9+OKL69/+7d/We++9t+7fv7+OjIzUozE6OlpHRkbqvn376vr16+sv/uIv1quuuqp2uubGEYy+vr46d+7c+pa3vKW+5z3vqZs3b6579+6tBw4cqMPDw3V0dPSw239oaKju37+/7tixo65evbq++93vrosWLaoDAwOtv74eG3e3nZEzNbpgW8/6IYu7a8jiY2e0nY8zNdrezsfq+NCHPlT3799/VGPXrl31yiuvbP01zObR399f/+f//J+HtumBAwea/kqsBw4cOPT4W2+9tfXX1KNjwn1iMxRmub6+vhx33HH55//8n+fCCy/Mueeem+XLl2dgYOCon7OUklJKBgYGsnTp0lxzzTU5++yzM3fu3ENTcJlYX19f5s6dm5/4iZ/IJZdckiuuuCJnnHFGTjzxxMyfP3/Cs7o/28Htf7BrW0rJihUrctVVV2X//v359re/nXXr1uXxxx/PgQMHpvslAUdAFncXWQy97fTTT8+b3vSmQ7PCVq1addR53NfXlxtvvDEXXHBB7rjjDoc+NXT11VcfmpEwmd+JB2eRJcnLX/7y/Pqv/3o+97nPZc2aNVNRJpPRdidWN3ZyY+7cufWMM86od999d92xY0fjbt+RWrt2bX3ve99bf/Znf7b119zNY2BgoC5durTefPPNdfv27XVoaGhK34fR0dH6J3/yJ/Waa66pixYtav319sgwQ8E47JDF3TVk8bE3ahdk5EyMtrfzsTJ+6qd+qu7fv3+KPvFj1q5dW5ctW9b6a5tt4w//8A+n9H0Y721ve1vrr6/HhhkKx5q+vr687W1vyxVXXJGzzjorCxcunLZ1nXrqqbn22muzb9++DA8P5957782OHTumbX2zTV9fXxYtWpRLL700N910Uy666KIsXLjwiE82c6RKKbn22mtz0UUX5eMf/3jWrVuX//W//leGhoamdD3AkZPF3UMWQ29btGhRfv/3fz8XX3xx+vun9r85p556am677bbs378/SfKxj30sd95555Su41hy2WWX5Td+4zfyspe9bNrW8fa3vz0/8zM/kyTZuHFjfu/3fs8MkhZoKMxSAwMDWbBgQa666qq8+tWvzpIlS6Z8h2m8448/Pueff34uuuiibNq0KevXr8/OnTsPdtN73ty5c3PKKafk4osvPjTF7kim1B6N8847LytXrsyGDRsyf/78rF69OrXWDA8PT8v6gOcni7uLLIbedfzxx+fUU0/N9ddfn7POOmtanv9Nb3rToetr167NV7/61ezevdvnfpy+vr6ceOKJufDCC3PjjTdOWwYnYyd3vPzyy5Mk9913X26++WYNhRZoKMxSP/3TP53rrrsul19+eU466aRp3YF99npf+tKX5sEHH8zWrVszODjY8zuyAwMDOfPMM/Nnf/ZnOeuss6a8Iz6RuXPn5oYbbsjLX/7ybN68OWvXrs33vve9aV8v8EyyuHvIYuhdpZT8x//4H/Oa17zm0DcBTLdf/uVfzhvf+MbcdNNN+eY3vzkj65wNzjjjjHzkIx/JypUrp7WZQPfQUJhlSinp7+/PihUr8uM//uNZsmTJjOw0HXTiiSfmuOOOywUXXJCtW7fmvvvu6+mTUZVS8qIXvSgXXHBBzjvvvJx00kkztt4lS5ZkaGgoL3nJSzI8PJwf/OAHGR4ezujo6IzUAL1MFncXWQy9a8WKFTnzzDPzkpe8JOedd96MrXf58uVZunRpTjjhhBlbZzcrpeT888/PhRdemIsvvjiLFy+e0fUvWLAgl156adavX58NGzbM6Lp73kQnVpjpkfZPMDFrxsDAQD3llFPqzTffXIeGhl7wK6+m04YNG+rnP//5unz58ta3Sdvvxy233FK/8pWvTPnJf47E6Oho3b9/f/3Yxz5WTzvttLpw4cLWt8kxOJyU0XjOkMXdNWTxsT9qF2TkTIy2t/NsHL/9279d9+zZU4eHh6foE33khoaG6qtf/erWt0E3jHnz5tXPf/7zdXBwcMbfh1rHcnjv3r31Ax/4QOvb4hgeE+4Tj30XErPG8uXL87rXvS4XXXRR+vv7W5tKtHjx4px88smZO3fuoa/U6jUHv5bstNNOyxlnnDFjU53HO/iVcueee25uvPHGnHvuuTNeA/QiWdw9ZDH0pjPOOCNvf/vbc9VVV+W4445r5bPf19eXf/bP/lne8pa35Ljjjpvx9XeTUkrmzZuX+fPnt7b+BQsWZNWqVXnHO96RVatWtVJHL3LIwyyzcuXKvPOd78ypp57aah2LFi3KSSedlLlz52bOnDk9ObWzv78/8+bNy1lnnZWzzz671Vpe+tKX5oILLsjevXtz77339uT7ATNJFncPWQy9adWqVXnf+97XSiPhoL6+vrzzne/Mxo0b8+Uvfzl79+5trRbGXHHFFbniiivyb/7Nv8maNWvaLqcn9OafM2ap/v7+HHfccVmxYkVXHK/V39+fs88+O6effnrbpbTi1FNPzUUXXdQVHem+vr4MDAzklFNOydlnn91adxh6gSzuLrIYoF033nhjbrvtthk9hwXdQ0Nhljg4nXLBggVZvHhxV+yk9Pf35/TTT8/JJ5/ck2dxXbp0ac4666yueC8OniDupJNOyplnnpl58+a1XRIck2Rx95HFQNtKKVm4cGEWLFjQdimteMUrXpG3vvWtWb58edul0AINhVliYGAgl1xySc4///yu2WGcP39+fu7nfi5XXHFF19Q0k17ykpfkda97XZYuXdp2KYdcc801ede73pUzzzyz7VLgmCSLu48sBtp2yimn5G/+5m/ynve8p9VDMKANzqEwS/T39+fMM8/Mqaee2jU7jAe/Mu3kk09uu5RWLF68OKeffnpX/QVq2bJl6evr64q/1MGxSBZ3H1kMvWXu3Lk555xzWj9nynhz587NxRdfnMcee6ztUmDGaSjMEgsWLMh1112XCy+8sGt2YufMmZOzzjor69ev75qaZtKSJUu6bkrriSeemP7+/syfPz99fX1OCAZTTBZ3H1kMvWX58uX5xCc+kTPPPNNsAOgCDnmYJfr6+nLSSSflxBNP7JodxlLKodGLuvG1l1J69qvjYCbI4u7Tja9dFsP06evry8KFC7viRKx0r8svvzw33nhjli1b1nYpxzy/7WaJOXPmZPny5V11jGit9dDoRd342g/uxB4cwNSSxd2nG1+7LAZo17/4F/8it99+u2+emAF+y80iBw4cyMjISNtlHDI6Oppt27Zl586dbZfSij179mTbtm0ZGhpqu5QJddsONhwrZHF3kcUA0B4NhVmi1prh4eEMDw+3Xcoho6Oj2b59e3bv3t2TO0x79uzJjh07uuo9ScZ+VhyvC9NDFncfWQzAsw0NDWXfvn1d9QeAY5WGwiwxNDSUtWvX5pFHHumaHcZ9+/bly1/+cu6+++6uqWkmrV27Nl/+8peza9eutks5ZHR0NCMjI3ZkYZrI4u4jiwF4tltvvTXXXntt1qxZ03Ypxzzf8jBLDA8P59FHH+2q43aHh4ezadOmPPHEE22X0ort27fn0Ucfzb59+9ou5ZCRkZEMDQ0d2pEFppYs7j6yGIBn27BhQ771rW+1XUZP0FCYJfbs2ZO//Mu/zGOPPZbrr7++K74mZ//+/VmzZk1X/aVuJq1fvz5PPfVUdu/e3XYph+zcuTNPPPFE1x5LDLOdLO4+3ZzFBw4caLsUAJhWDnmYJUZHR7Nz586uOUb24DTOwcHBnt1h2r9/f55++ukMDw93xXuSJA899FC+/vWvd9XUXziWyOLu081Z/NRTT7VdChxz9u7dm09/+tP56le/2jWfeehlZijMEqOjo9m1a1eefvrpjI6Optba6vduDw8PHzrZSa/uxA4ODqaUcuiM7/397X+cvvjFL+bDH/5wHnvssbZLgWOSLO4+shh6y7Zt2/KOd7wj1157bT71qU91xUwx6GXt/9bliNVas3Xr1nzxi1/Mueeem3POOae1OlavXp37778/+/bt69nu8MGzvX/7299OkrziFa/I3LlzW6ul1ppdu3blySefdMgDTCNZ3F1kMfSmbsq8Xbt25dZbb80999zTkydi/eQnP5nHH388N910U1auXNl2OcwwDYVZZseOHfn617+eefPmtboT+/3vfz+rV6/O/v37W6mhWwwPD+fee+9NX19fLrvsstZ2YkdHR3PgwIHs3r0727dvb6UG6CWyuLvIYqBNTz31VG677baenZX0hS98IV/72tfy2te+tvWGQjd+vfOxzjkUZpkf/ehH+fCHP5zvfve7rXRAa60ZGhrKV77ylXz605/O4ODgjNfQTUZHR/OFL3wh/+N//I/s3bu3ta70t7/97fzar/1a7rrrrlbWD71GFncXWQxAMtbcuOaaa3LnnXe2XUrP0FCYZQYHB/PII49kw4YNefTRR7N3794ZXf/u3buzefPmbNq0KT/60Y8yMjIyo+vvNrXWbN68ORs3bszjjz+enTt3zngNw8PD2bx5c772ta/l8ccfn/H1Qy+Sxd1FFkPvGRwczIYNG1r5vI/3xBNPZNOmTT3/F/GDObxp06ZWmrpDQ0N59NFHc9999+VLX/pSz84WacXB4/3aHEmq0WwsW7asXnzxxfWLX/xinUkf/vCH6/XXX1+XL1/e+jbopnH88cfX66+/vt5yyy11dHR0xt6PAwcO1EceeaS+973vrf39/bWU0vq2OAbH3bULcnImRhds61k3ZHF3DVl87I7aBRk5E6Pt7TybxsDAQD3llFPqH/3RH03RJ7m5oaGh+vM///N12bJlta+vr/Vt0vZYunRpvfLKK+u2bdtm/L24//7764/92I/VRYsWtb4djuEx4T6xGQqz1FNPPZVNmzblO9/5Tv7xH/8x+/btm9b1DQ4OZuPGjXn44Yezbt26Gf9rXLcbGhrKunXrsn79+mzevDlPP/30tK9zeHg4O3bsyJe+9KXcc889XfWVadArZHF3kcXQOw4cOJAnnngi3/nOd/KpT30qTzzxRCt17NixI1u2bOnJkzE+2/bt2/PII4/ks5/9bFavXj2j6x4eHs6WLVt8dXobJuoyzPRI+92WWTvOP//8et1119WNGzdOX8uv1rphw4b6gQ98oF533XWtv+ZuHaWUeuWVV9a/+7u/qw8++OC0vh+11rpr16761a9+tS5evNhfw6Z3mKFgHHbI4u4ZsvjYHLULMnImRtvbeTaOUkrt7++vn/zkJ6foU33khoaG6qtf/erWt0G3jb6+vnrDDTfM6Eyx73//+2YnTP+YcJ/YtzzMclu2bMnIyEje//73Z9WqVbnhhhum9Du4BwcHc88992TNmjW58847s3bt2il77mNNrTWbNm3KRz/60bzqVa/Knj17cs455+SEE06Y0vXs3r07u3btyh133JE1a9ZkcHDw4E4I0BJZ3D1kMfSWWmtGRkbyV3/1V3nggQfytre9LUuWLJn29X7+85/PXXfdlYceemja1zXbjI6OZs2aNXn3u9+d17zmNXnlK185bes6cOBAbr/99txzzz3TPkuQ5zFRl6HJSLIhyfeTfC+drkWSpUk+l2Rt598lh3mOtrsts3r09fXVFStW1De+8Y119+7ddWhoaEo6fSMjI3XLli31lltuqTfeeKO/vDQYN9xwQ7399tvro48+WkdGRqbk/RgdHa0jIyP10Ucfrd/4xjfqBRdc0Prr7JHR9TMUMgU5XGXxpIcs7r4hi4+dUbsgaw83Yp+49XHKKafUtWvXTtnn/fkMDw/X3/qt32r99c6GcfPNN9fh4eEp3f6jo6N1eHi4Dg8P1507d9ZLLrmk9dfZI2PCfeJSx8LrqJVSNiS5rNa6ddyy9yTZXmv9w1LKuzIWnr/9As8xuSLIvHnzsmTJkrz0pS/NK1/5ykPfA7t06dLGzzU8PJx9+/blj//4j3Pffffl/vvvz86dO/OjH/1oGio/Np166qlZvnx5LrzwwqxcuTK/+Zu/mSVLlhz1d6OPjo7mrrvuyoc+9KE88cQT2bFjR+67774ZOT6YrK61XtZ2ES9kKnK48xhZPEmyuLvI4mNHrbW0XcPh2Cdu38DAQF7xilfk8ssvz8033zylM8UO+vrXv55//+//fR544IE8/PDDU/78x5rzzjsv5513Xv7gD/4gL3vZy6bkOT/zmc/k1ltvTTL2u/Jb3/pWnnrqqSl5bl7QhPvE03XIwxuSvKpz+S+SfDnJC+7IMjn79+/P5s2b88QTT6S/vz/nnHNO+vr6sm/fvsyfPz/9/f1ZsGBB+vr60tfXl1LGfi8f7CwNDQ3lwIED2bt3b/bv3589e/bka1/7Wr773e9mx44dTjTT0ObNm7N58+Zs27YtGzduzJve9KYMDg7muOOOy/z587NgwYLMmTPn0PvwbCMjIxkdHc1TTz2VoaGhDA8PZ82aNfn85z+fHTt2ZM+ePTP8ipiF5HALZHF3kcV0AVk8gw4cOJC77ror+/bty6ZNmzIwMJAkWbJkSRYsWHDUz7t169YcOHAgSXL//ffns5/9rEOcjtCDDz6Yhx9+OG95y1tyyimnJEnmz5/fqNFea83WrVszNDSUJPn+97+fz3zmM9NSL81NxQyF9Ul2ZGwaxH+rtb6/lLKz1rq4c3tJsuPg9ed5Dp/IKbRgwYIcf/zxh3aWrr766rz4xS/O9ddfn8WLF2fp0qWHdmYPHDiQwcHB/PCHP8zq1atzxx13ZOfOndmzZ0+2bt2a/fv324GdhDlz5qS/vz8nn3xyTjjhhJx11ll5zWtek+uuuy6nnnpqjj/++MyZMydJUkrJ6OhoRkdHs2XLlmzZsiX/4T/8h6xfvz5btmzJ008/nZ07d2Z0dNQvsZk1G2YoTDqHO/fzgzWFZHH3kMWz3yyZoWCfuEvMmzcvy5cvP9QsvPXWW/P617/+qJ5r//79ufHGGw99a8HevXuzdevWwzyKZ1u2bNmhps5P//RP58///M8P5e7h7Nq1KzfccMOh8wft3r07O3bsmLZaeV7TNkPhlbXWx0opy5N8rpTyw/E31lrrROFYSrkpyU1TsH6eZXBwMIODgymlZGBgIEuXLs3TTz+dZcuW5YQTTsiJJ5546C9jIyMjGRwczPr167NmzZqsX78+u3fv9lVkU2RkZCQjIyN57LHHctxxx+XAgQNZtmxZjj/++EPBOj5Ma60ZHR3Njh07sm3bttx///15/PHHs337djuuvJCjyuFEFk8nWdw9ZDEzxD5xl9i/f382btx46Po3v/nNLFy48Kie68CBA3nwwQfz6KOPTlV5PWnLli2HLt9///35whe+cMQNhd27d2ft2rXegy416RkKz3iyUn4vydNJ3p7kVbXWx0spK5J8udZ6/gs8zm/naVRKecYYv/zg+39wuq2/gE2/g/+BOPhePHuq7fj3xF/AukLXz1AY72hzuPNYP2zTSBZ3F1k8u8yGGQrj2SfuLi90aNORGBkZkQFTqJRyxM2Eg4aHh6epGhqY+hkKpZSFSfpqrbs7l1+T5A+S3JnkrUn+sPPvJyezHibn4A4q3cF/FJhKcnj2kMXdRRYzlWRxdxsZGWm7BMaptWoQHEMme8jDKUn+rtPx60/yV7XWz5ZSvp3k46WUX0rySJKfn+R6AJiYHAZonywGetKUHvJw1EWY3gV0r1l1yMNkyGKgW822Qx6OlhwGutiE+8R9bVQCAAAAzG4aCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBj/Uf7wFLK+UnuGLfoxUn+7ySLk7w9yZbO8n9Xa/30UVcIwPOSxQDtksNALyu11sk/SSlzkjyW5Iokv5jk6VrrHzd4/OSLAJgeq2utl7VdxJGQxcCxqtZa2q7hSMhh4Bg24T7xVB3y8LNJHqq1PjJFzwdAc7IYoF1yGOgpU9VQeHOSj467/qullHtLKbeXUpZM0ToAeGGyGKBdchjoKZNuKJRSBpK8Psl/7yx6X5JzklyS5PEktzzP424qpdxdSrl7sjUA9DpZDNAuOQz0okmfQ6GU8oYkv1Jrfc0Et61M8qla68WHeQ7HiwHdalacQ0EWA8ey2XAOBTkMHOOm7RwKN2bc1K5Syopxt70xyX1TsA4AXpgsBmiXHAZ6zqRmKJRSFiZ5NMmLa627Oss+nLGpXTXJhiTvqLU+fpjn0Y0FulXXz1CQxcCxrttnKMhhoAdMuE88JV8bOVnCE+hiXd9QmCqyGOhW3d5QmCpyGOhi0/q1kQAAAEAP0VAAAAAAGtNQAAAAABrTUAAAAAAa01AAAKb46y4AABIhSURBVAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGtNQAAAAABrTUAAAAAAa01AAAAAAGjuihkIp5fZSypOllPvGLVtaSvlcKWVt598lneWllPJfSynrSin3llIuna7iAXqFHAZonywGeKYjnaHwwSTXPmvZu5J8odZ6bpIvdK4nyXVJzu2Mm5K8b/JlAvS8D0YOA7Ttg5HFAIccUUOh1vqVJNuftfgNSf6ic/kvklw/bvmH6phvJllcSlkxFcUC9Co5DNA+WQzwTJM5h8IptdbHO5c3Jzmlc/n0JBvH3W9TZxkAU0sOA7RPFgM9q38qnqTWWksptcljSik3ZWz6FwCTdDQ5nMhigKlknxjoNZOZofDEwWlbnX+f7Cx/LMmZ4+53RmfZM9Ra319rvazWetkkagDoZZPK4UQWA0wB+8RAz5pMQ+HOJG/tXH5rkk+OW/4vO2e2vTLJrnHTwACYOnIYoH2yGOhZR3TIQynlo0leleTkUsqmJO9O8odJPl5K+aUkjyT5+c7dP53ktUnWJdmb5BenuGaAniOHAdoniwGeqdTa+JDbqS/iKI77BZghq3tlGqosBrpVrbW0XcNMkMNAF5twn3gyhzwAAAAAPUpDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGjssA2FUsrtpZQnSyn3jVv2n0opPyyl3FtK+btSyuLO8pWllMFSyvc647bpLB6gV8higHbJYYDnOpIZCh9Mcu2zln0uycW11pckeTDJ74y77aFa6yWd8ctTUyZAz/tgZDFAmz4YOQzwDIdtKNRav5Jk+7OW/UOtdbhz9ZtJzpiG2gDokMUA7ZLDAM81FedQ+NdJPjPu+tmllO+WUu4qpfzkFDw/AIcniwHaJYeBntM/mQeXUn43yXCSj3QWPZ7kRbXWbaWUlyf5RCllVa31qQkee1OSmyazfgBkMUDb5DDQq456hkIp5V8leV2St9Raa5LUWvfXWrd1Lq9O8lCS8yZ6fK31/bXWy2qtlx1tDQC9ThYDtEsOA73sqBoKpZRrk/xWktfXWveOW76slDKnc/nFSc5N8vBUFArAM8ligHbJYaDXHfaQh1LKR5O8KsnJpZRNSd6dsTPYzkvyuVJKknyzc/ban0ryB6WUoSSjSX651rp9wicG4IjJYoB2yWGA5yqdmVntFlFK+0UATGx1r0xDlcVAt6q1lrZrmAlyGOhiE+4TT8W3PAAAAAA9RkMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaOywDYVSyu2llCdLKfeNW/Z7pZTHSinf64zXjrvtd0op60opD5RSrpmuwgF6iSwGaJccBniuI5mh8MEk106w/L/UWi/pjE8nSSnloiRvTrKq85j/t5QyZ6qKBehhH4wsBmjTByOHAZ7hsA2FWutXkmw/wud7Q5KP1Vr311rXJ1mX5PJJ1AdAZDFA2+QwwHNN5hwKv1pKubcz/WtJZ9npSTaOu8+mzjIApocsBmiXHAZ61tE2FN6X5JwklyR5PMktTZ+glHJTKeXuUsrdR1kDQK+TxQDtksNATzuqhkKt9Yla60itdTTJn+WfpnA9luTMcXc9o7Nsoud4f631slrrZUdTA0Cvk8UA7ZLDQK87qoZCKWXFuKtvTHLwbLd3JnlzKWVeKeXsJOcm+cfJlQjARGQxQLvkMNDr+g93h1LKR5O8KsnJpZRNSd6d5FWllEuS1CQbkrwjSWqta0opH0/ygyTDSX6l1joyPaUD9A5ZDNAuOQzwXKXW2nYNKaW0XwTAxFb3yjRUWQx0q1prabuGmSCHgS424T7xZL7lAQAAAOhRGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAYxoKAAAAQGMaCgAAAEBjGgoAAABAY4dtKJRSbi+lPFlKuW/csjtKKd/rjA2llO91lq8spQyOu+226SweoFfIYoB2yWGA5+o/gvt8MMmfJvnQwQW11l84eLmUckuSXePu/1Ct9ZKpKhCAJLIYoG0fjBwGeIbDNhRqrV8ppayc6LZSSkny80l+ZmrLAmA8WQzQLjkM8FyTPYfCTyZ5ota6dtyys0sp3y2l3FVK+clJPj8AhyeLAdolh4GedCSHPLyQG5N8dNz1x5O8qNa6rZTy8iSfKKWsqrU+9ewHllJuSnLTJNcPgCwGaJscBnrSUc9QKKX0J3lTkjsOLqu17q+1butcXp3koSTnTfT4Wuv7a62X1VovO9oaAHqdLAZolxwGetlkDnn4uSQ/rLVuOriglLKslDKnc/nFSc5N8vDkSgTgBchigHbJYaBnHcnXRn40yTeSnF9K2VRK+aXOTW/OM6d2JclPJbm385U5f53kl2ut26eyYIBeJIsB2iWHAZ6r1FrbriGllPaLAJjY6l6ZhiqLgW5Vay1t1zAT5DDQxSbcJ57stzwAAAAAPUhDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABoTEMBAAAAaExDAQAAAGhMQwEAAABorL/tAjq2JtnT+bcbnJzuqSXprnrUMjG1TKybakmOrp6zpqOQLiWLn59aJtZNtSTdVY9aJiaHX5gcfmHdVI9aJqaWiXVTLckUZnGptU6+nClQSrm71npZ23Uk3VVL0l31qGViaplYN9WSdF893aibtpFaJqaW59dN9ahlYt1US7fqpm3UTbUk3VWPWiamlol1Uy3J1NbjkAcAAACgMQ0FAAAAoLFuaii8v+0CxummWpLuqkctE1PLxLqplqT76ulG3bSN1DIxtTy/bqpHLRPrplq6VTdto26qJemuetQyMbVMrJtqSaawnq45hwIAAAAwe3TTDAUAAABgluiKhkIp5dpSygOllHWllHfN8LrPLKV8qZTyg1LKmlLKOzvLl5ZSPldKWdv5d8kM1jSnlPLdUsqnOtfPLqV8q7N97iilDMxQHYtLKX9dSvlhKeX+Usor2toupZTf6Lw/95VSPlpKmT+T26WUcnsp5clSyn3jlk24LcqY/9qp695SyqUzUMt/6rxP95ZS/q6Usnjcbb/TqeWBUso1013LuNv+bSmlllJO7lyf8e3SWf5rnW2zppTynnHLp227zEZy+Dk1dUUOd9YtiyOHm9Yz7jZZPIvI4ufU1BVZLIefsX5ZfIS1jLvt2M7hWmurI8mcJA8leXGSgST3JLloBte/IsmlncsnJHkwyUVJ3pPkXZ3l70ryRzNY028m+askn+pc/3iSN3cu35bk/5yhOv4iyds6lweSLG5juyQ5Pcn6JAvGbY9/NZPbJclPJbk0yX3jlk24LZK8NslnkpQkVyb51gzU8pok/Z3LfzSulos6n6l5Sc7ufNbmTGctneVnJvn7JI8kObnF7fK/Jfl8knmd68tnYrvMtiGHJ6ypK3K4sz5ZXOVw03o6y2XxLBqyeMKauiKL5fAzapDFR1hLZ/kxn8PT+kN/hC/4FUn+ftz130nyOy3W88kkr07yQJIVnWUrkjwwQ+s/I8kXkvxMkk91ftC2jvtgPGN7TWMdizqBVZ61fMa3Syc8NyZZmqS/s12umentkmTlsz6YE26LJP8tyY0T3W+6annWbW9M8pHO5Wd8njqB9orpriXJXyd5aZIN48JzxrdLxn7B/twE95v27TKbhhx+zvq7Ioc765LFz6xBDjeoRxbPriGLn7P+rshiOTxhHbL4CGvphRzuhkMeDn4wDtrUWTbjSikrk7wsybeSnFJrfbxz0+Ykp8xQGe9N8ltJRjvXT0qys9Y63Lk+U9vn7CRbkvx5Z6rZ/1dKWZgWtkut9bEkf5zk0SSPJ9mVZHXa2S7jPd+2aPtn+l9nrOvZSi2llDckeazWes+zbmpju5yX5Cc70wDvKqX8RIu1dLOu2R5y+Dlk8QuTw89DFs9KXbM9ZPEzyOHDk8UT6JUc7oaGQlcopRyf5G+S/Hqt9anxt9Wxdk2dgRpel+TJWuvq6V7XEejP2FSZ99VaX5ZkT8amMB0yg9tlSZI3ZCzQT0uyMMm1073eJmZqWxxOKeV3kwwn+UhL6z8uyb9L8n+3sf4J9Gesi39lkv8rycdLKaXdkng+cnhCsvgIyeFn1CCLOWqy+DnkcAOy+ND6eyaHu6Gh8FjGji056IzOshlTSpmbseD8SK31bzuLnyilrOjcviLJkzNQytVJXl9K2ZDkYxmb4vUnSRaXUvo795mp7bMpyaZa67c61/86Y2Haxnb5uSTra61baq1DSf42Y9uqje0y3vNti1Z+pksp/yrJ65K8pRPmbdRyTsZ+yd3T+Tk+I8l3SimntlBLMvZz/Ld1zD9m7K8cJ7dUSzdrfXvI4ecli1+YHJ6YLJ6dWt8esnhCcvjwZPFz9UwOd0ND4dtJzi1jZycdSPLmJHfO1Mo7nZkPJLm/1vqfx910Z5K3di6/NWPHkU2rWuvv1FrPqLWuzNh2+GKt9S1JvpTkf5/hWjYn2VhKOb+z6GeT/CAtbJeMTeu6spRyXOf9OljLjG+XZ3m+bXFnkn/ZOYPrlUl2jZsGNi1KKddmbFrg62ute59V45tLKfNKKWcnOTfJP05XHbXW79dal9daV3Z+jjdl7ARPm9PCdknyiYydhCallPMydiKlrZnh7TILyOGObsrhTj2y+IXJ4QnI4llLFnd0UxbL4SMii5+lp3K4NjjhwnSNjJ3p8sGMnVXyd2d43a/M2LSce5N8rzNem7HjtL6QZG3Gzoi5dIbrelX+6Yy2L+68seuS/Pd0zs45AzVckuTuzrb5RJIlbW2XJL+f5IdJ7kvy4YydiXTGtkuSj2bsWLWhjAXCLz3ftsjYSYP+n87P8/eTXDYDtazL2PFPB3+Gbxt3/9/t1PJAkuumu5Zn3b4h/3QCmja2y0CSv+z83Hwnyc/MxHaZjUMOT1hX6zncWbcsrnK4aT3Pul0Wz5Ihiyesq/UslsPPWL8sPsJannX7MZvDpfMkAAAAAEesGw55AAAAAGYZDQUAAACgMQ0FAAAAoDENBQAAAKAxDQUAAACgMQ0FAAAAoDENBQAAAKAxDQUAAACgsf8fXRmZUVasoLcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1296x864 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_index = 40\n",
    "slice_index = 49\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(preds[dataset_index][slice_index], cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(rescaled_preds[dataset_index][slice_index], cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(cut_full_res_dataset[dataset_index][1][slice_index], cmap=\"gray\", vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
