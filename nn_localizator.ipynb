{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset biggest bounding box wihtout spinal cord [56, 177, 156]\n",
      "Cut target size [72, 192, 168]\n",
      "Done Init\n"
     ]
    }
   ],
   "source": [
    "from src.consts import IN_COLAB, DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "from src.dataset.split_dataset import split_dataset, copy_split_dataset\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Found Google Colab')\n",
    "    !pip3 install torch torchvision torchsummary\n",
    "    !pip3 install simpleitk\n",
    "\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import src.dataset.oars_labels_consts as OARS_LABELS\n",
    "from src.helpers.threshold_calc_helpers import get_threshold_info_df\n",
    "from src.helpers.show_model_dataset_pred_preview import show_model_dataset_pred_preview\n",
    "from src.dataset.dataset_cut_helpers import get_full_res_cut, get_cut_lists\n",
    "from src.dataset.get_dataset import get_dataset\n",
    "from src.dataset.get_dataset_info import get_dataset_info\n",
    "from src.dataset.preview_dataset import preview_dataset\n",
    "from src.model_and_training.prepare_model import prepare_model\n",
    "from src.model_and_training.train_loop import train_loop\n",
    "from src.model_and_training.training_helpers import show_model_info\n",
    "\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "torch.manual_seed(20)\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low resolution NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading low res dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 16x dataset\n",
      "normalizing dataset\n",
      "filtering labels\n",
      "dilatating 1x dataset\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n",
      "data type: float64 int8\n",
      "low res dataset RAM sizes in GB 0.06866455078125\n",
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_LIST\n",
    "if OARS_LABELS.SPINAL_CORD in filter_labels:\n",
    "    filter_labels.remove(OARS_LABELS.SPINAL_CORD)\n",
    "\n",
    "low_res_dataset = get_dataset(dataset_size=50, shrink_factor=16, filter_labels=filter_labels, unify_labels=True)\n",
    "low_res_dataset.dilatate_labels(repeat=1)\n",
    "low_res_dataset.to_numpy()\n",
    "low_res_dataset.show_data_type()\n",
    "print('low res dataset RAM sizes in GB', low_res_dataset.get_data_size() / 1024**3)\n",
    "\n",
    "low_res_split_dataset_obj = split_dataset(low_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "get_dataset_info(low_res_dataset, low_res_split_dataset_obj)\n",
    "train_low_res_dataset, valid_low_res_dataset, test_low_res_dataset = itemgetter(\n",
    "    'train_dataset', 'valid_dataset', 'test_dataset')(low_res_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 12.505709639268096, min -0.40698009878688973\n",
      "label max 1, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689b67d1e2224ac1b55928246c00213e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c074b283324809b9b627864b088b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_dataset(low_res_dataset, preview_index=0, show_hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training low res model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device running \"cuda\"\n",
      "max output channels 128\n",
      "Model number of params: 1193537, trainable 1193537\n",
      "Running training loop\n",
      "Batch train [1] loss 0.97630, dsc 0.02370\n",
      "Batch train [2] loss 0.97167, dsc 0.02833\n",
      "Batch train [3] loss 0.97257, dsc 0.02743\n",
      "Batch train [4] loss 0.96968, dsc 0.03032\n",
      "Batch train [5] loss 0.97364, dsc 0.02636\n",
      "Batch train [6] loss 0.96659, dsc 0.03341\n",
      "Batch train [7] loss 0.95242, dsc 0.04758\n",
      "Batch train [8] loss 0.96548, dsc 0.03452\n",
      "Batch train [9] loss 0.96903, dsc 0.03097\n",
      "Batch train [10] loss 0.96693, dsc 0.03307\n",
      "Batch train [11] loss 0.96507, dsc 0.03493\n",
      "Batch train [12] loss 0.97499, dsc 0.02501\n",
      "Batch train [13] loss 0.95384, dsc 0.04616\n",
      "Batch train [14] loss 0.95651, dsc 0.04349\n",
      "Batch train [15] loss 0.95468, dsc 0.04532\n",
      "Batch train [16] loss 0.96143, dsc 0.03857\n",
      "Batch train [17] loss 0.96530, dsc 0.03470\n",
      "Batch train [18] loss 0.96914, dsc 0.03086\n",
      "Batch train [19] loss 0.96070, dsc 0.03930\n",
      "Batch train [20] loss 0.96140, dsc 0.03860\n",
      "Batch train [21] loss 0.95840, dsc 0.04160\n",
      "Batch train [22] loss 0.97222, dsc 0.02778\n",
      "Batch train [23] loss 0.95842, dsc 0.04158\n",
      "Batch train [24] loss 0.96059, dsc 0.03941\n",
      "Batch train [25] loss 0.95558, dsc 0.04442\n",
      "Batch train [26] loss 0.95190, dsc 0.04810\n",
      "Batch train [27] loss 0.96540, dsc 0.03460\n",
      "Batch train [28] loss 0.95481, dsc 0.04519\n",
      "Batch train [29] loss 0.96611, dsc 0.03389\n",
      "Batch train [30] loss 0.95618, dsc 0.04382\n",
      "Batch train [31] loss 0.94886, dsc 0.05114\n",
      "Batch train [32] loss 0.95537, dsc 0.04463\n",
      "Batch train [33] loss 0.94562, dsc 0.05438\n",
      "Batch train [34] loss 0.95596, dsc 0.04404\n",
      "Batch train [35] loss 0.94244, dsc 0.05756\n",
      "Batch train [36] loss 0.95467, dsc 0.04533\n",
      "Batch train [37] loss 0.95559, dsc 0.04441\n",
      "Batch train [38] loss 0.96643, dsc 0.03357\n",
      "Batch train [39] loss 0.95874, dsc 0.04126\n",
      "Batch train [40] loss 0.95800, dsc 0.04200\n",
      "Epoch [1] train done\n",
      "Batch eval [1] loss 0.94855, dsc 0.05145\n",
      "Batch eval [2] loss 0.95686, dsc 0.04314\n",
      "Batch eval [3] loss 0.95186, dsc 0.04814\n",
      "Batch eval [4] loss 0.96014, dsc 0.03986\n",
      "Batch eval [5] loss 0.95077, dsc 0.04923\n",
      "Epoch [1] valid done\n",
      "Epoch [1] T 7.03s, deltaT 7.03s, loss: train 0.96122, valid 0.95364, dsc: train 0.03878, valid 0.04636\n",
      "Batch train [1] loss 0.94790, dsc 0.05210\n",
      "Batch train [2] loss 0.95497, dsc 0.04503\n",
      "Batch train [3] loss 0.96264, dsc 0.03736\n",
      "Batch train [4] loss 0.97084, dsc 0.02916\n",
      "Batch train [5] loss 0.95815, dsc 0.04185\n",
      "Batch train [6] loss 0.95416, dsc 0.04584\n",
      "Batch train [7] loss 0.96328, dsc 0.03672\n",
      "Batch train [8] loss 0.96404, dsc 0.03596\n",
      "Batch train [9] loss 0.94358, dsc 0.05642\n",
      "Batch train [10] loss 0.94780, dsc 0.05220\n",
      "Batch train [11] loss 0.95936, dsc 0.04064\n",
      "Batch train [12] loss 0.96519, dsc 0.03481\n",
      "Batch train [13] loss 0.95131, dsc 0.04869\n",
      "Batch train [14] loss 0.95732, dsc 0.04268\n",
      "Batch train [15] loss 0.95425, dsc 0.04575\n",
      "Batch train [16] loss 0.95651, dsc 0.04349\n",
      "Batch train [17] loss 0.95190, dsc 0.04810\n",
      "Batch train [18] loss 0.94567, dsc 0.05433\n",
      "Batch train [19] loss 0.95237, dsc 0.04763\n",
      "Batch train [20] loss 0.96138, dsc 0.03862\n",
      "Batch train [21] loss 0.96862, dsc 0.03138\n",
      "Batch train [22] loss 0.96446, dsc 0.03554\n",
      "Batch train [23] loss 0.95868, dsc 0.04132\n",
      "Batch train [24] loss 0.96364, dsc 0.03636\n",
      "Batch train [25] loss 0.95251, dsc 0.04749\n",
      "Batch train [26] loss 0.94691, dsc 0.05309\n",
      "Batch train [27] loss 0.95039, dsc 0.04961\n",
      "Batch train [28] loss 0.94749, dsc 0.05251\n",
      "Batch train [29] loss 0.93776, dsc 0.06224\n",
      "Batch train [30] loss 0.94447, dsc 0.05553\n",
      "Batch train [31] loss 0.95490, dsc 0.04510\n",
      "Batch train [32] loss 0.95204, dsc 0.04796\n",
      "Batch train [33] loss 0.95451, dsc 0.04549\n",
      "Batch train [34] loss 0.95426, dsc 0.04574\n",
      "Batch train [35] loss 0.95485, dsc 0.04515\n",
      "Batch train [36] loss 0.95287, dsc 0.04713\n",
      "Batch train [37] loss 0.95236, dsc 0.04764\n",
      "Batch train [38] loss 0.93657, dsc 0.06343\n",
      "Batch train [39] loss 0.95705, dsc 0.04295\n",
      "Batch train [40] loss 0.95491, dsc 0.04509\n",
      "Epoch [2] train done\n",
      "Batch eval [1] loss 0.94443, dsc 0.05557\n",
      "Batch eval [2] loss 0.95315, dsc 0.04685\n",
      "Batch eval [3] loss 0.94646, dsc 0.05354\n",
      "Batch eval [4] loss 0.95730, dsc 0.04270\n",
      "Batch eval [5] loss 0.94488, dsc 0.05512\n",
      "Epoch [2] valid done\n",
      "Epoch [2] T 14.02s, deltaT 7.00s, loss: train 0.95455, valid 0.94925, dsc: train 0.04545, valid 0.05075\n",
      "Batch train [1] loss 0.96266, dsc 0.03734\n",
      "Batch train [2] loss 0.96043, dsc 0.03957\n",
      "Batch train [3] loss 0.95166, dsc 0.04834\n",
      "Batch train [4] loss 0.94557, dsc 0.05443\n",
      "Batch train [5] loss 0.96777, dsc 0.03223\n",
      "Batch train [6] loss 0.94236, dsc 0.05764\n",
      "Batch train [7] loss 0.93527, dsc 0.06473\n",
      "Batch train [8] loss 0.96159, dsc 0.03841\n",
      "Batch train [9] loss 0.93808, dsc 0.06192\n",
      "Batch train [10] loss 0.95291, dsc 0.04709\n",
      "Batch train [11] loss 0.95836, dsc 0.04164\n",
      "Batch train [12] loss 0.96021, dsc 0.03979\n",
      "Batch train [13] loss 0.94816, dsc 0.05184\n",
      "Batch train [14] loss 0.94954, dsc 0.05046\n",
      "Batch train [15] loss 0.94942, dsc 0.05058\n",
      "Batch train [16] loss 0.95488, dsc 0.04512\n",
      "Batch train [17] loss 0.94862, dsc 0.05138\n",
      "Batch train [18] loss 0.95284, dsc 0.04716\n",
      "Batch train [19] loss 0.95248, dsc 0.04752\n",
      "Batch train [20] loss 0.94771, dsc 0.05229\n",
      "Batch train [21] loss 0.95525, dsc 0.04475\n",
      "Batch train [22] loss 0.95248, dsc 0.04752\n",
      "Batch train [23] loss 0.95135, dsc 0.04865\n",
      "Batch train [24] loss 0.94741, dsc 0.05259\n",
      "Batch train [25] loss 0.95158, dsc 0.04842\n",
      "Batch train [26] loss 0.96076, dsc 0.03924\n",
      "Batch train [27] loss 0.94936, dsc 0.05064\n",
      "Batch train [28] loss 0.95471, dsc 0.04529\n",
      "Batch train [29] loss 0.94821, dsc 0.05179\n",
      "Batch train [30] loss 0.93770, dsc 0.06230\n",
      "Batch train [31] loss 0.94105, dsc 0.05895\n",
      "Batch train [32] loss 0.94176, dsc 0.05824\n",
      "Batch train [33] loss 0.95187, dsc 0.04813\n",
      "Batch train [34] loss 0.95610, dsc 0.04390\n",
      "Batch train [35] loss 0.93832, dsc 0.06168\n",
      "Batch train [36] loss 0.94876, dsc 0.05124\n",
      "Batch train [37] loss 0.96440, dsc 0.03560\n",
      "Batch train [38] loss 0.93100, dsc 0.06900\n",
      "Batch train [39] loss 0.94421, dsc 0.05579\n",
      "Batch train [40] loss 0.94959, dsc 0.05041\n",
      "Epoch [3] train done\n",
      "Batch eval [1] loss 0.93884, dsc 0.06116\n",
      "Batch eval [2] loss 0.94895, dsc 0.05105\n",
      "Batch eval [3] loss 0.94390, dsc 0.05610\n",
      "Batch eval [4] loss 0.95369, dsc 0.04631\n",
      "Batch eval [5] loss 0.93966, dsc 0.06034\n",
      "Epoch [3] valid done\n",
      "Epoch [3] T 21.07s, deltaT 7.05s, loss: train 0.95041, valid 0.94501, dsc: train 0.04959, valid 0.05499\n",
      "Batch train [1] loss 0.94439, dsc 0.05561\n",
      "Batch train [2] loss 0.95941, dsc 0.04059\n",
      "Batch train [3] loss 0.93770, dsc 0.06230\n",
      "Batch train [4] loss 0.95282, dsc 0.04718\n",
      "Batch train [5] loss 0.94951, dsc 0.05049\n",
      "Batch train [6] loss 0.95036, dsc 0.04964\n",
      "Batch train [7] loss 0.95137, dsc 0.04863\n",
      "Batch train [8] loss 0.94608, dsc 0.05392\n",
      "Batch train [9] loss 0.94861, dsc 0.05139\n",
      "Batch train [10] loss 0.95232, dsc 0.04768\n",
      "Batch train [11] loss 0.92863, dsc 0.07137\n",
      "Batch train [12] loss 0.94221, dsc 0.05779\n",
      "Batch train [13] loss 0.94408, dsc 0.05592\n",
      "Batch train [14] loss 0.94237, dsc 0.05763\n",
      "Batch train [15] loss 0.94413, dsc 0.05587\n",
      "Batch train [16] loss 0.93512, dsc 0.06488\n",
      "Batch train [17] loss 0.95600, dsc 0.04400\n",
      "Batch train [18] loss 0.96266, dsc 0.03734\n",
      "Batch train [19] loss 0.94428, dsc 0.05572\n",
      "Batch train [20] loss 0.93775, dsc 0.06225\n",
      "Batch train [21] loss 0.96351, dsc 0.03649\n",
      "Batch train [22] loss 0.95706, dsc 0.04294\n",
      "Batch train [23] loss 0.94562, dsc 0.05438\n",
      "Batch train [24] loss 0.95718, dsc 0.04282\n",
      "Batch train [25] loss 0.95482, dsc 0.04518\n",
      "Batch train [26] loss 0.94431, dsc 0.05569\n",
      "Batch train [27] loss 0.94796, dsc 0.05204\n",
      "Batch train [28] loss 0.94359, dsc 0.05641\n",
      "Batch train [29] loss 0.95286, dsc 0.04714\n",
      "Batch train [30] loss 0.94649, dsc 0.05351\n",
      "Batch train [31] loss 0.92614, dsc 0.07386\n",
      "Batch train [32] loss 0.92931, dsc 0.07069\n",
      "Batch train [33] loss 0.95235, dsc 0.04765\n",
      "Batch train [34] loss 0.93361, dsc 0.06639\n",
      "Batch train [35] loss 0.94572, dsc 0.05428\n",
      "Batch train [36] loss 0.94698, dsc 0.05302\n",
      "Batch train [37] loss 0.94385, dsc 0.05615\n",
      "Batch train [38] loss 0.93493, dsc 0.06507\n",
      "Batch train [39] loss 0.93636, dsc 0.06364\n",
      "Batch train [40] loss 0.94481, dsc 0.05519\n",
      "Epoch [4] train done\n",
      "Batch eval [1] loss 0.92985, dsc 0.07015\n",
      "Batch eval [2] loss 0.94170, dsc 0.05830\n",
      "Batch eval [3] loss 0.93416, dsc 0.06584\n",
      "Batch eval [4] loss 0.94660, dsc 0.05340\n",
      "Batch eval [5] loss 0.93222, dsc 0.06778\n",
      "Epoch [4] valid done\n",
      "Epoch [4] T 28.12s, deltaT 7.05s, loss: train 0.94593, valid 0.93691, dsc: train 0.05407, valid 0.06309\n",
      "Batch train [1] loss 0.94545, dsc 0.05455\n",
      "Batch train [2] loss 0.94127, dsc 0.05873\n",
      "Batch train [3] loss 0.96156, dsc 0.03844\n",
      "Batch train [4] loss 0.93181, dsc 0.06819\n",
      "Batch train [5] loss 0.93261, dsc 0.06739\n",
      "Batch train [6] loss 0.94568, dsc 0.05432\n",
      "Batch train [7] loss 0.94369, dsc 0.05631\n",
      "Batch train [8] loss 0.94490, dsc 0.05510\n",
      "Batch train [9] loss 0.93703, dsc 0.06297\n",
      "Batch train [10] loss 0.95205, dsc 0.04795\n",
      "Batch train [11] loss 0.94615, dsc 0.05385\n",
      "Batch train [12] loss 0.94732, dsc 0.05268\n",
      "Batch train [13] loss 0.94080, dsc 0.05920\n",
      "Batch train [14] loss 0.94956, dsc 0.05044\n",
      "Batch train [15] loss 0.94918, dsc 0.05082\n",
      "Batch train [16] loss 0.93299, dsc 0.06701\n",
      "Batch train [17] loss 0.93745, dsc 0.06255\n",
      "Batch train [18] loss 0.94194, dsc 0.05806\n",
      "Batch train [19] loss 0.92977, dsc 0.07023\n",
      "Batch train [20] loss 0.94581, dsc 0.05419\n",
      "Batch train [21] loss 0.95337, dsc 0.04663\n",
      "Batch train [22] loss 0.92652, dsc 0.07348\n",
      "Batch train [23] loss 0.92359, dsc 0.07641\n",
      "Batch train [24] loss 0.93070, dsc 0.06930\n",
      "Batch train [25] loss 0.95063, dsc 0.04937\n",
      "Batch train [26] loss 0.93515, dsc 0.06485\n",
      "Batch train [27] loss 0.91877, dsc 0.08123\n",
      "Batch train [28] loss 0.94128, dsc 0.05872\n",
      "Batch train [29] loss 0.95228, dsc 0.04772\n",
      "Batch train [30] loss 0.95143, dsc 0.04857\n",
      "Batch train [31] loss 0.95776, dsc 0.04224\n",
      "Batch train [32] loss 0.93903, dsc 0.06097\n",
      "Batch train [33] loss 0.93629, dsc 0.06371\n",
      "Batch train [34] loss 0.93709, dsc 0.06291\n",
      "Batch train [35] loss 0.93650, dsc 0.06350\n",
      "Batch train [36] loss 0.93720, dsc 0.06280\n",
      "Batch train [37] loss 0.93982, dsc 0.06018\n",
      "Batch train [38] loss 0.93333, dsc 0.06667\n",
      "Batch train [39] loss 0.94019, dsc 0.05981\n",
      "Batch train [40] loss 0.91706, dsc 0.08294\n",
      "Epoch [5] train done\n",
      "Batch eval [1] loss 0.92244, dsc 0.07756\n",
      "Batch eval [2] loss 0.93499, dsc 0.06501\n",
      "Batch eval [3] loss 0.92742, dsc 0.07258\n",
      "Batch eval [4] loss 0.94012, dsc 0.05988\n",
      "Batch eval [5] loss 0.92417, dsc 0.07583\n",
      "Epoch [5] valid done\n",
      "Epoch [5] T 35.16s, deltaT 7.04s, loss: train 0.94038, valid 0.92982, dsc: train 0.05962, valid 0.07018\n",
      "Batch train [1] loss 0.92007, dsc 0.07993\n",
      "Batch train [2] loss 0.91586, dsc 0.08414\n",
      "Batch train [3] loss 0.91563, dsc 0.08437\n",
      "Batch train [4] loss 0.94541, dsc 0.05459\n",
      "Batch train [5] loss 0.94014, dsc 0.05986\n",
      "Batch train [6] loss 0.92287, dsc 0.07713\n",
      "Batch train [7] loss 0.93086, dsc 0.06914\n",
      "Batch train [8] loss 0.93127, dsc 0.06873\n",
      "Batch train [9] loss 0.94225, dsc 0.05775\n",
      "Batch train [10] loss 0.93721, dsc 0.06279\n",
      "Batch train [11] loss 0.93213, dsc 0.06787\n",
      "Batch train [12] loss 0.92490, dsc 0.07510\n",
      "Batch train [13] loss 0.95640, dsc 0.04360\n",
      "Batch train [14] loss 0.93411, dsc 0.06589\n",
      "Batch train [15] loss 0.92226, dsc 0.07774\n",
      "Batch train [16] loss 0.92627, dsc 0.07373\n",
      "Batch train [17] loss 0.93144, dsc 0.06856\n",
      "Batch train [18] loss 0.94338, dsc 0.05662\n",
      "Batch train [19] loss 0.93568, dsc 0.06432\n",
      "Batch train [20] loss 0.93220, dsc 0.06780\n",
      "Batch train [21] loss 0.92306, dsc 0.07694\n",
      "Batch train [22] loss 0.94699, dsc 0.05301\n",
      "Batch train [23] loss 0.93366, dsc 0.06634\n",
      "Batch train [24] loss 0.93119, dsc 0.06881\n",
      "Batch train [25] loss 0.94778, dsc 0.05222\n",
      "Batch train [26] loss 0.93237, dsc 0.06763\n",
      "Batch train [27] loss 0.93555, dsc 0.06445\n",
      "Batch train [28] loss 0.95279, dsc 0.04721\n",
      "Batch train [29] loss 0.93432, dsc 0.06568\n",
      "Batch train [30] loss 0.94653, dsc 0.05347\n",
      "Batch train [31] loss 0.92937, dsc 0.07063\n",
      "Batch train [32] loss 0.93398, dsc 0.06602\n",
      "Batch train [33] loss 0.93325, dsc 0.06675\n",
      "Batch train [34] loss 0.91770, dsc 0.08230\n",
      "Batch train [35] loss 0.93698, dsc 0.06302\n",
      "Batch train [36] loss 0.94313, dsc 0.05687\n",
      "Batch train [37] loss 0.93319, dsc 0.06681\n",
      "Batch train [38] loss 0.94249, dsc 0.05751\n",
      "Batch train [39] loss 0.92507, dsc 0.07493\n",
      "Batch train [40] loss 0.93569, dsc 0.06431\n",
      "Epoch [6] train done\n",
      "Batch eval [1] loss 0.91823, dsc 0.08177\n",
      "Batch eval [2] loss 0.93075, dsc 0.06925\n",
      "Batch eval [3] loss 0.92126, dsc 0.07874\n",
      "Batch eval [4] loss 0.93721, dsc 0.06279\n",
      "Batch eval [5] loss 0.92048, dsc 0.07952\n",
      "Epoch [6] valid done\n",
      "Epoch [6] T 42.21s, deltaT 7.05s, loss: train 0.93389, valid 0.92559, dsc: train 0.06611, valid 0.07441\n",
      "Batch train [1] loss 0.91144, dsc 0.08856\n",
      "Batch train [2] loss 0.92744, dsc 0.07256\n",
      "Batch train [3] loss 0.94474, dsc 0.05526\n",
      "Batch train [4] loss 0.91686, dsc 0.08314\n",
      "Batch train [5] loss 0.93155, dsc 0.06845\n",
      "Batch train [6] loss 0.94416, dsc 0.05584\n",
      "Batch train [7] loss 0.93201, dsc 0.06799\n",
      "Batch train [8] loss 0.95037, dsc 0.04963\n",
      "Batch train [9] loss 0.93091, dsc 0.06909\n",
      "Batch train [10] loss 0.91162, dsc 0.08838\n",
      "Batch train [11] loss 0.92161, dsc 0.07839\n",
      "Batch train [12] loss 0.94204, dsc 0.05796\n",
      "Batch train [13] loss 0.91599, dsc 0.08401\n",
      "Batch train [14] loss 0.92306, dsc 0.07694\n",
      "Batch train [15] loss 0.90266, dsc 0.09734\n",
      "Batch train [16] loss 0.92276, dsc 0.07724\n",
      "Batch train [17] loss 0.93641, dsc 0.06359\n",
      "Batch train [18] loss 0.91029, dsc 0.08971\n",
      "Batch train [19] loss 0.92738, dsc 0.07262\n",
      "Batch train [20] loss 0.92336, dsc 0.07664\n",
      "Batch train [21] loss 0.92672, dsc 0.07328\n",
      "Batch train [22] loss 0.91520, dsc 0.08480\n",
      "Batch train [23] loss 0.92201, dsc 0.07799\n",
      "Batch train [24] loss 0.93842, dsc 0.06158\n",
      "Batch train [25] loss 0.91835, dsc 0.08165\n",
      "Batch train [26] loss 0.93113, dsc 0.06887\n",
      "Batch train [27] loss 0.92571, dsc 0.07429\n",
      "Batch train [28] loss 0.89805, dsc 0.10195\n",
      "Batch train [29] loss 0.93385, dsc 0.06615\n",
      "Batch train [30] loss 0.92640, dsc 0.07360\n",
      "Batch train [31] loss 0.90767, dsc 0.09233\n",
      "Batch train [32] loss 0.92164, dsc 0.07836\n",
      "Batch train [33] loss 0.92180, dsc 0.07820\n",
      "Batch train [34] loss 0.92856, dsc 0.07144\n",
      "Batch train [35] loss 0.92769, dsc 0.07231\n",
      "Batch train [36] loss 0.92274, dsc 0.07726\n",
      "Batch train [37] loss 0.94619, dsc 0.05381\n",
      "Batch train [38] loss 0.91413, dsc 0.08587\n",
      "Batch train [39] loss 0.93431, dsc 0.06569\n",
      "Batch train [40] loss 0.91798, dsc 0.08202\n",
      "Epoch [7] train done\n",
      "Batch eval [1] loss 0.91189, dsc 0.08811\n",
      "Batch eval [2] loss 0.92397, dsc 0.07603\n",
      "Batch eval [3] loss 0.91338, dsc 0.08662\n",
      "Batch eval [4] loss 0.93205, dsc 0.06795\n",
      "Batch eval [5] loss 0.91357, dsc 0.08643\n",
      "Epoch [7] valid done\n",
      "Epoch [7] T 49.26s, deltaT 7.04s, loss: train 0.92513, valid 0.91897, dsc: train 0.07487, valid 0.08103\n",
      "Batch train [1] loss 0.91881, dsc 0.08119\n",
      "Batch train [2] loss 0.91302, dsc 0.08698\n",
      "Batch train [3] loss 0.93648, dsc 0.06352\n",
      "Batch train [4] loss 0.92079, dsc 0.07921\n",
      "Batch train [5] loss 0.91393, dsc 0.08607\n",
      "Batch train [6] loss 0.92984, dsc 0.07016\n",
      "Batch train [7] loss 0.92080, dsc 0.07920\n",
      "Batch train [8] loss 0.90141, dsc 0.09859\n",
      "Batch train [9] loss 0.91489, dsc 0.08511\n",
      "Batch train [10] loss 0.92006, dsc 0.07994\n",
      "Batch train [11] loss 0.92443, dsc 0.07557\n",
      "Batch train [12] loss 0.92030, dsc 0.07970\n",
      "Batch train [13] loss 0.91585, dsc 0.08415\n",
      "Batch train [14] loss 0.90328, dsc 0.09672\n",
      "Batch train [15] loss 0.92710, dsc 0.07290\n",
      "Batch train [16] loss 0.90405, dsc 0.09595\n",
      "Batch train [17] loss 0.90331, dsc 0.09669\n",
      "Batch train [18] loss 0.93042, dsc 0.06958\n",
      "Batch train [19] loss 0.90993, dsc 0.09007\n",
      "Batch train [20] loss 0.91733, dsc 0.08267\n",
      "Batch train [21] loss 0.94227, dsc 0.05773\n",
      "Batch train [22] loss 0.94041, dsc 0.05959\n",
      "Batch train [23] loss 0.91217, dsc 0.08783\n",
      "Batch train [24] loss 0.90664, dsc 0.09336\n",
      "Batch train [25] loss 0.90973, dsc 0.09027\n",
      "Batch train [26] loss 0.92725, dsc 0.07275\n",
      "Batch train [27] loss 0.93046, dsc 0.06954\n",
      "Batch train [28] loss 0.88398, dsc 0.11602\n",
      "Batch train [29] loss 0.91311, dsc 0.08689\n",
      "Batch train [30] loss 0.92886, dsc 0.07114\n",
      "Batch train [31] loss 0.91410, dsc 0.08590\n",
      "Batch train [32] loss 0.89085, dsc 0.10915\n",
      "Batch train [33] loss 0.90780, dsc 0.09220\n",
      "Batch train [34] loss 0.90690, dsc 0.09310\n",
      "Batch train [35] loss 0.91605, dsc 0.08395\n",
      "Batch train [36] loss 0.91697, dsc 0.08303\n",
      "Batch train [37] loss 0.88471, dsc 0.11529\n",
      "Batch train [38] loss 0.91026, dsc 0.08974\n",
      "Batch train [39] loss 0.88949, dsc 0.11051\n",
      "Batch train [40] loss 0.87764, dsc 0.12236\n",
      "Epoch [8] train done\n",
      "Batch eval [1] loss 0.89736, dsc 0.10264\n",
      "Batch eval [2] loss 0.91076, dsc 0.08924\n",
      "Batch eval [3] loss 0.89847, dsc 0.10153\n",
      "Batch eval [4] loss 0.91927, dsc 0.08073\n",
      "Batch eval [5] loss 0.89783, dsc 0.10217\n",
      "Epoch [8] valid done\n",
      "Epoch [8] T 56.36s, deltaT 7.10s, loss: train 0.91389, valid 0.90474, dsc: train 0.08611, valid 0.09526\n",
      "Batch train [1] loss 0.90890, dsc 0.09110\n",
      "Batch train [2] loss 0.90181, dsc 0.09819\n",
      "Batch train [3] loss 0.89709, dsc 0.10291\n",
      "Batch train [4] loss 0.91459, dsc 0.08541\n",
      "Batch train [5] loss 0.90812, dsc 0.09188\n",
      "Batch train [6] loss 0.90733, dsc 0.09267\n",
      "Batch train [7] loss 0.89178, dsc 0.10822\n",
      "Batch train [8] loss 0.87307, dsc 0.12693\n",
      "Batch train [9] loss 0.91200, dsc 0.08800\n",
      "Batch train [10] loss 0.93194, dsc 0.06806\n",
      "Batch train [11] loss 0.90426, dsc 0.09574\n",
      "Batch train [12] loss 0.89894, dsc 0.10106\n",
      "Batch train [13] loss 0.91474, dsc 0.08526\n",
      "Batch train [14] loss 0.91686, dsc 0.08314\n",
      "Batch train [15] loss 0.87381, dsc 0.12619\n",
      "Batch train [16] loss 0.89655, dsc 0.10345\n",
      "Batch train [17] loss 0.91660, dsc 0.08340\n",
      "Batch train [18] loss 0.89664, dsc 0.10336\n",
      "Batch train [19] loss 0.91884, dsc 0.08116\n",
      "Batch train [20] loss 0.88160, dsc 0.11840\n",
      "Batch train [21] loss 0.89042, dsc 0.10958\n",
      "Batch train [22] loss 0.89278, dsc 0.10722\n",
      "Batch train [23] loss 0.91626, dsc 0.08374\n",
      "Batch train [24] loss 0.86055, dsc 0.13945\n",
      "Batch train [25] loss 0.88824, dsc 0.11176\n",
      "Batch train [26] loss 0.89856, dsc 0.10144\n",
      "Batch train [27] loss 0.88363, dsc 0.11637\n",
      "Batch train [28] loss 0.89447, dsc 0.10553\n",
      "Batch train [29] loss 0.92619, dsc 0.07381\n",
      "Batch train [30] loss 0.89030, dsc 0.10970\n",
      "Batch train [31] loss 0.87094, dsc 0.12906\n",
      "Batch train [32] loss 0.87314, dsc 0.12686\n",
      "Batch train [33] loss 0.89822, dsc 0.10178\n",
      "Batch train [34] loss 0.90437, dsc 0.09563\n",
      "Batch train [35] loss 0.86624, dsc 0.13376\n",
      "Batch train [36] loss 0.88834, dsc 0.11166\n",
      "Batch train [37] loss 0.91079, dsc 0.08921\n",
      "Batch train [38] loss 0.89052, dsc 0.10948\n",
      "Batch train [39] loss 0.86552, dsc 0.13448\n",
      "Batch train [40] loss 0.88247, dsc 0.11753\n",
      "Epoch [9] train done\n",
      "Batch eval [1] loss 0.88124, dsc 0.11876\n",
      "Batch eval [2] loss 0.89537, dsc 0.10463\n",
      "Batch eval [3] loss 0.88256, dsc 0.11744\n",
      "Batch eval [4] loss 0.90680, dsc 0.09320\n",
      "Batch eval [5] loss 0.88172, dsc 0.11828\n",
      "Epoch [9] valid done\n",
      "Epoch [9] T 63.46s, deltaT 7.10s, loss: train 0.89644, valid 0.88954, dsc: train 0.10356, valid 0.11046\n",
      "Batch train [1] loss 0.90573, dsc 0.09427\n",
      "Batch train [2] loss 0.87759, dsc 0.12241\n",
      "Batch train [3] loss 0.91643, dsc 0.08357\n",
      "Batch train [4] loss 0.86227, dsc 0.13773\n",
      "Batch train [5] loss 0.91632, dsc 0.08368\n",
      "Batch train [6] loss 0.85329, dsc 0.14671\n",
      "Batch train [7] loss 0.86124, dsc 0.13876\n",
      "Batch train [8] loss 0.84921, dsc 0.15079\n",
      "Batch train [9] loss 0.87001, dsc 0.12999\n",
      "Batch train [10] loss 0.87540, dsc 0.12460\n",
      "Batch train [11] loss 0.87726, dsc 0.12274\n",
      "Batch train [12] loss 0.86580, dsc 0.13420\n",
      "Batch train [13] loss 0.87229, dsc 0.12771\n",
      "Batch train [14] loss 0.85611, dsc 0.14389\n",
      "Batch train [15] loss 0.87627, dsc 0.12373\n",
      "Batch train [16] loss 0.88278, dsc 0.11722\n",
      "Batch train [17] loss 0.86401, dsc 0.13599\n",
      "Batch train [18] loss 0.87195, dsc 0.12805\n",
      "Batch train [19] loss 0.84432, dsc 0.15568\n",
      "Batch train [20] loss 0.85627, dsc 0.14373\n",
      "Batch train [21] loss 0.88112, dsc 0.11888\n",
      "Batch train [22] loss 0.86038, dsc 0.13962\n",
      "Batch train [23] loss 0.85258, dsc 0.14742\n",
      "Batch train [24] loss 0.82173, dsc 0.17827\n",
      "Batch train [25] loss 0.84640, dsc 0.15360\n",
      "Batch train [26] loss 0.87383, dsc 0.12617\n",
      "Batch train [27] loss 0.80585, dsc 0.19415\n",
      "Batch train [28] loss 0.83370, dsc 0.16630\n",
      "Batch train [29] loss 0.85248, dsc 0.14752\n",
      "Batch train [30] loss 0.84991, dsc 0.15009\n",
      "Batch train [31] loss 0.79890, dsc 0.20110\n",
      "Batch train [32] loss 0.87231, dsc 0.12769\n",
      "Batch train [33] loss 0.85849, dsc 0.14151\n",
      "Batch train [34] loss 0.86172, dsc 0.13828\n",
      "Batch train [35] loss 0.84403, dsc 0.15597\n",
      "Batch train [36] loss 0.80317, dsc 0.19683\n",
      "Batch train [37] loss 0.83529, dsc 0.16471\n",
      "Batch train [38] loss 0.82554, dsc 0.17446\n",
      "Batch train [39] loss 0.82512, dsc 0.17488\n",
      "Batch train [40] loss 0.86528, dsc 0.13472\n",
      "Epoch [10] train done\n",
      "Batch eval [1] loss 0.81294, dsc 0.18706\n",
      "Batch eval [2] loss 0.83461, dsc 0.16539\n",
      "Batch eval [3] loss 0.81789, dsc 0.18211\n",
      "Batch eval [4] loss 0.85966, dsc 0.14034\n",
      "Batch eval [5] loss 0.81776, dsc 0.18224\n",
      "Epoch [10] valid done\n",
      "Epoch [10] T 70.56s, deltaT 7.10s, loss: train 0.85806, valid 0.82857, dsc: train 0.14194, valid 0.17143\n",
      "Batch train [1] loss 0.80968, dsc 0.19032\n",
      "Batch train [2] loss 0.84573, dsc 0.15427\n",
      "Batch train [3] loss 0.80218, dsc 0.19782\n",
      "Batch train [4] loss 0.83476, dsc 0.16524\n",
      "Batch train [5] loss 0.87967, dsc 0.12033\n",
      "Batch train [6] loss 0.82868, dsc 0.17132\n",
      "Batch train [7] loss 0.82583, dsc 0.17417\n",
      "Batch train [8] loss 0.80324, dsc 0.19676\n",
      "Batch train [9] loss 0.84618, dsc 0.15382\n",
      "Batch train [10] loss 0.80824, dsc 0.19176\n",
      "Batch train [11] loss 0.77803, dsc 0.22197\n",
      "Batch train [12] loss 0.84435, dsc 0.15565\n",
      "Batch train [13] loss 0.80588, dsc 0.19412\n",
      "Batch train [14] loss 0.83514, dsc 0.16486\n",
      "Batch train [15] loss 0.75835, dsc 0.24165\n",
      "Batch train [16] loss 0.81628, dsc 0.18372\n",
      "Batch train [17] loss 0.80237, dsc 0.19763\n",
      "Batch train [18] loss 0.82002, dsc 0.17998\n",
      "Batch train [19] loss 0.79866, dsc 0.20134\n",
      "Batch train [20] loss 0.81199, dsc 0.18801\n",
      "Batch train [21] loss 0.80534, dsc 0.19466\n",
      "Batch train [22] loss 0.81378, dsc 0.18622\n",
      "Batch train [23] loss 0.78793, dsc 0.21207\n",
      "Batch train [24] loss 0.81826, dsc 0.18174\n",
      "Batch train [25] loss 0.80144, dsc 0.19856\n",
      "Batch train [26] loss 0.84833, dsc 0.15167\n",
      "Batch train [27] loss 0.82522, dsc 0.17478\n",
      "Batch train [28] loss 0.82474, dsc 0.17526\n",
      "Batch train [29] loss 0.78633, dsc 0.21367\n",
      "Batch train [30] loss 0.75884, dsc 0.24116\n",
      "Batch train [31] loss 0.75391, dsc 0.24609\n",
      "Batch train [32] loss 0.77942, dsc 0.22058\n",
      "Batch train [33] loss 0.78254, dsc 0.21746\n",
      "Batch train [34] loss 0.72712, dsc 0.27288\n",
      "Batch train [35] loss 0.74615, dsc 0.25385\n",
      "Batch train [36] loss 0.76532, dsc 0.23468\n",
      "Batch train [37] loss 0.78147, dsc 0.21853\n",
      "Batch train [38] loss 0.77639, dsc 0.22361\n",
      "Batch train [39] loss 0.81175, dsc 0.18825\n",
      "Batch train [40] loss 0.75724, dsc 0.24276\n",
      "Epoch [11] train done\n",
      "Batch eval [1] loss 0.84495, dsc 0.15505\n",
      "Batch eval [2] loss 0.85365, dsc 0.14635\n",
      "Batch eval [3] loss 0.83872, dsc 0.16128\n",
      "Batch eval [4] loss 0.85904, dsc 0.14096\n",
      "Batch eval [5] loss 0.80159, dsc 0.19841\n",
      "Epoch [11] valid done\n",
      "Epoch [11] T 77.65s, deltaT 7.09s, loss: train 0.80267, valid 0.83959, dsc: train 0.19733, valid 0.16041\n",
      "Batch train [1] loss 0.71642, dsc 0.28358\n",
      "Batch train [2] loss 0.74800, dsc 0.25200\n",
      "Batch train [3] loss 0.73980, dsc 0.26020\n",
      "Batch train [4] loss 0.82925, dsc 0.17075\n",
      "Batch train [5] loss 0.75172, dsc 0.24828\n",
      "Batch train [6] loss 0.76236, dsc 0.23764\n",
      "Batch train [7] loss 0.76029, dsc 0.23971\n",
      "Batch train [8] loss 0.73626, dsc 0.26374\n",
      "Batch train [9] loss 0.74962, dsc 0.25038\n",
      "Batch train [10] loss 0.73563, dsc 0.26437\n",
      "Batch train [11] loss 0.73045, dsc 0.26955\n",
      "Batch train [12] loss 0.75963, dsc 0.24037\n",
      "Batch train [13] loss 0.72879, dsc 0.27121\n",
      "Batch train [14] loss 0.73411, dsc 0.26589\n",
      "Batch train [15] loss 0.69966, dsc 0.30034\n",
      "Batch train [16] loss 0.77758, dsc 0.22242\n",
      "Batch train [17] loss 0.75597, dsc 0.24403\n",
      "Batch train [18] loss 0.70444, dsc 0.29556\n",
      "Batch train [19] loss 0.66235, dsc 0.33765\n",
      "Batch train [20] loss 0.72815, dsc 0.27185\n",
      "Batch train [21] loss 0.72094, dsc 0.27906\n",
      "Batch train [22] loss 0.75800, dsc 0.24200\n",
      "Batch train [23] loss 0.74597, dsc 0.25403\n",
      "Batch train [24] loss 0.74400, dsc 0.25600\n",
      "Batch train [25] loss 0.75531, dsc 0.24469\n",
      "Batch train [26] loss 0.74901, dsc 0.25099\n",
      "Batch train [27] loss 0.69920, dsc 0.30080\n",
      "Batch train [28] loss 0.66502, dsc 0.33498\n",
      "Batch train [29] loss 0.72768, dsc 0.27232\n",
      "Batch train [30] loss 0.65989, dsc 0.34011\n",
      "Batch train [31] loss 0.70526, dsc 0.29474\n",
      "Batch train [32] loss 0.70478, dsc 0.29522\n",
      "Batch train [33] loss 0.69142, dsc 0.30858\n",
      "Batch train [34] loss 0.67880, dsc 0.32120\n",
      "Batch train [35] loss 0.67753, dsc 0.32247\n",
      "Batch train [36] loss 0.74933, dsc 0.25067\n",
      "Batch train [37] loss 0.68720, dsc 0.31280\n",
      "Batch train [38] loss 0.63550, dsc 0.36450\n",
      "Batch train [39] loss 0.77356, dsc 0.22644\n",
      "Batch train [40] loss 0.70344, dsc 0.29656\n",
      "Epoch [12] train done\n",
      "Batch eval [1] loss 0.73580, dsc 0.26420\n",
      "Batch eval [2] loss 0.75300, dsc 0.24700\n",
      "Batch eval [3] loss 0.74081, dsc 0.25919\n",
      "Batch eval [4] loss 0.79667, dsc 0.20333\n",
      "Batch eval [5] loss 0.74206, dsc 0.25794\n",
      "Epoch [12] valid done\n",
      "Epoch [12] T 84.80s, deltaT 7.15s, loss: train 0.72606, valid 0.75367, dsc: train 0.27394, valid 0.24633\n",
      "Batch train [1] loss 0.71783, dsc 0.28217\n",
      "Batch train [2] loss 0.64358, dsc 0.35642\n",
      "Batch train [3] loss 0.71308, dsc 0.28692\n",
      "Batch train [4] loss 0.62298, dsc 0.37702\n",
      "Batch train [5] loss 0.65143, dsc 0.34857\n",
      "Batch train [6] loss 0.70911, dsc 0.29089\n",
      "Batch train [7] loss 0.66717, dsc 0.33283\n",
      "Batch train [8] loss 0.66518, dsc 0.33482\n",
      "Batch train [9] loss 0.65702, dsc 0.34298\n",
      "Batch train [10] loss 0.64725, dsc 0.35275\n",
      "Batch train [11] loss 0.63856, dsc 0.36144\n",
      "Batch train [12] loss 0.60653, dsc 0.39347\n",
      "Batch train [13] loss 0.67112, dsc 0.32888\n",
      "Batch train [14] loss 0.64786, dsc 0.35214\n",
      "Batch train [15] loss 0.67178, dsc 0.32822\n",
      "Batch train [16] loss 0.73134, dsc 0.26866\n",
      "Batch train [17] loss 0.64957, dsc 0.35043\n",
      "Batch train [18] loss 0.61973, dsc 0.38027\n",
      "Batch train [19] loss 0.64330, dsc 0.35670\n",
      "Batch train [20] loss 0.62323, dsc 0.37677\n",
      "Batch train [21] loss 0.64949, dsc 0.35051\n",
      "Batch train [22] loss 0.67439, dsc 0.32561\n",
      "Batch train [23] loss 0.64879, dsc 0.35121\n",
      "Batch train [24] loss 0.61293, dsc 0.38707\n",
      "Batch train [25] loss 0.63228, dsc 0.36772\n",
      "Batch train [26] loss 0.61697, dsc 0.38303\n",
      "Batch train [27] loss 0.59645, dsc 0.40355\n",
      "Batch train [28] loss 0.66538, dsc 0.33462\n",
      "Batch train [29] loss 0.71384, dsc 0.28616\n",
      "Batch train [30] loss 0.60405, dsc 0.39595\n",
      "Batch train [31] loss 0.65123, dsc 0.34877\n",
      "Batch train [32] loss 0.60592, dsc 0.39408\n",
      "Batch train [33] loss 0.58703, dsc 0.41297\n",
      "Batch train [34] loss 0.58091, dsc 0.41909\n",
      "Batch train [35] loss 0.60593, dsc 0.39407\n",
      "Batch train [36] loss 0.52297, dsc 0.47703\n",
      "Batch train [37] loss 0.54578, dsc 0.45422\n",
      "Batch train [38] loss 0.52040, dsc 0.47960\n",
      "Batch train [39] loss 0.53901, dsc 0.46099\n",
      "Batch train [40] loss 0.58688, dsc 0.41312\n",
      "Epoch [13] train done\n",
      "Batch eval [1] loss 0.55140, dsc 0.44860\n",
      "Batch eval [2] loss 0.58743, dsc 0.41257\n",
      "Batch eval [3] loss 0.55999, dsc 0.44001\n",
      "Batch eval [4] loss 0.61840, dsc 0.38160\n",
      "Batch eval [5] loss 0.54808, dsc 0.45192\n",
      "Epoch [13] valid done\n",
      "Epoch [13] T 91.96s, deltaT 7.15s, loss: train 0.63396, valid 0.57306, dsc: train 0.36604, valid 0.42694\n",
      "Batch train [1] loss 0.56096, dsc 0.43904\n",
      "Batch train [2] loss 0.58135, dsc 0.41865\n",
      "Batch train [3] loss 0.51117, dsc 0.48883\n",
      "Batch train [4] loss 0.60917, dsc 0.39083\n",
      "Batch train [5] loss 0.51584, dsc 0.48416\n",
      "Batch train [6] loss 0.52431, dsc 0.47569\n",
      "Batch train [7] loss 0.55792, dsc 0.44208\n",
      "Batch train [8] loss 0.54435, dsc 0.45565\n",
      "Batch train [9] loss 0.54743, dsc 0.45257\n",
      "Batch train [10] loss 0.60954, dsc 0.39046\n",
      "Batch train [11] loss 0.52665, dsc 0.47335\n",
      "Batch train [12] loss 0.55277, dsc 0.44723\n",
      "Batch train [13] loss 0.61487, dsc 0.38513\n",
      "Batch train [14] loss 0.50815, dsc 0.49185\n",
      "Batch train [15] loss 0.59276, dsc 0.40724\n",
      "Batch train [16] loss 0.49920, dsc 0.50080\n",
      "Batch train [17] loss 0.58964, dsc 0.41036\n",
      "Batch train [18] loss 0.54685, dsc 0.45315\n",
      "Batch train [19] loss 0.48128, dsc 0.51872\n",
      "Batch train [20] loss 0.53082, dsc 0.46918\n",
      "Batch train [21] loss 0.53157, dsc 0.46843\n",
      "Batch train [22] loss 0.51687, dsc 0.48313\n",
      "Batch train [23] loss 0.50907, dsc 0.49093\n",
      "Batch train [24] loss 0.51569, dsc 0.48431\n",
      "Batch train [25] loss 0.54266, dsc 0.45734\n",
      "Batch train [26] loss 0.61764, dsc 0.38236\n",
      "Batch train [27] loss 0.52435, dsc 0.47565\n",
      "Batch train [28] loss 0.54598, dsc 0.45402\n",
      "Batch train [29] loss 0.52099, dsc 0.47901\n",
      "Batch train [30] loss 0.48910, dsc 0.51090\n",
      "Batch train [31] loss 0.52646, dsc 0.47354\n",
      "Batch train [32] loss 0.52846, dsc 0.47154\n",
      "Batch train [33] loss 0.49852, dsc 0.50148\n",
      "Batch train [34] loss 0.60566, dsc 0.39434\n",
      "Batch train [35] loss 0.50945, dsc 0.49055\n",
      "Batch train [36] loss 0.53577, dsc 0.46423\n",
      "Batch train [37] loss 0.48063, dsc 0.51937\n",
      "Batch train [38] loss 0.44203, dsc 0.55797\n",
      "Batch train [39] loss 0.49587, dsc 0.50413\n",
      "Batch train [40] loss 0.48773, dsc 0.51227\n",
      "Epoch [14] train done\n",
      "Batch eval [1] loss 0.46764, dsc 0.53236\n",
      "Batch eval [2] loss 0.51231, dsc 0.48769\n",
      "Batch eval [3] loss 0.47393, dsc 0.52607\n",
      "Batch eval [4] loss 0.55005, dsc 0.44995\n",
      "Batch eval [5] loss 0.47263, dsc 0.52737\n",
      "Epoch [14] valid done\n",
      "Epoch [14] T 99.11s, deltaT 7.15s, loss: train 0.53574, valid 0.49531, dsc: train 0.46426, valid 0.50469\n",
      "Batch train [1] loss 0.59860, dsc 0.40140\n",
      "Batch train [2] loss 0.45627, dsc 0.54373\n",
      "Batch train [3] loss 0.43780, dsc 0.56220\n",
      "Batch train [4] loss 0.52701, dsc 0.47299\n",
      "Batch train [5] loss 0.52137, dsc 0.47863\n",
      "Batch train [6] loss 0.45154, dsc 0.54846\n",
      "Batch train [7] loss 0.48963, dsc 0.51037\n",
      "Batch train [8] loss 0.47650, dsc 0.52350\n",
      "Batch train [9] loss 0.43328, dsc 0.56672\n",
      "Batch train [10] loss 0.50460, dsc 0.49540\n",
      "Batch train [11] loss 0.49445, dsc 0.50555\n",
      "Batch train [12] loss 0.45876, dsc 0.54124\n",
      "Batch train [13] loss 0.46561, dsc 0.53439\n",
      "Batch train [14] loss 0.47515, dsc 0.52485\n",
      "Batch train [15] loss 0.43572, dsc 0.56428\n",
      "Batch train [16] loss 0.44042, dsc 0.55958\n",
      "Batch train [17] loss 0.42446, dsc 0.57554\n",
      "Batch train [18] loss 0.44655, dsc 0.55345\n",
      "Batch train [19] loss 0.47714, dsc 0.52286\n",
      "Batch train [20] loss 0.44500, dsc 0.55500\n",
      "Batch train [21] loss 0.44199, dsc 0.55801\n",
      "Batch train [22] loss 0.46832, dsc 0.53168\n",
      "Batch train [23] loss 0.43477, dsc 0.56523\n",
      "Batch train [24] loss 0.39910, dsc 0.60090\n",
      "Batch train [25] loss 0.43423, dsc 0.56577\n",
      "Batch train [26] loss 0.48583, dsc 0.51417\n",
      "Batch train [27] loss 0.47629, dsc 0.52371\n",
      "Batch train [28] loss 0.39191, dsc 0.60809\n",
      "Batch train [29] loss 0.44896, dsc 0.55104\n",
      "Batch train [30] loss 0.36715, dsc 0.63285\n",
      "Batch train [31] loss 0.38417, dsc 0.61583\n",
      "Batch train [32] loss 0.42974, dsc 0.57026\n",
      "Batch train [33] loss 0.50787, dsc 0.49213\n",
      "Batch train [34] loss 0.41229, dsc 0.58771\n",
      "Batch train [35] loss 0.45809, dsc 0.54191\n",
      "Batch train [36] loss 0.39764, dsc 0.60236\n",
      "Batch train [37] loss 0.39360, dsc 0.60640\n",
      "Batch train [38] loss 0.44624, dsc 0.55376\n",
      "Batch train [39] loss 0.39714, dsc 0.60286\n",
      "Batch train [40] loss 0.38559, dsc 0.61441\n",
      "Epoch [15] train done\n",
      "Batch eval [1] loss 0.44523, dsc 0.55477\n",
      "Batch eval [2] loss 0.45002, dsc 0.54998\n",
      "Batch eval [3] loss 0.49014, dsc 0.50986\n",
      "Batch eval [4] loss 0.43544, dsc 0.56456\n",
      "Batch eval [5] loss 0.41302, dsc 0.58698\n",
      "Epoch [15] valid done\n",
      "Epoch [15] T 106.26s, deltaT 7.15s, loss: train 0.45052, valid 0.44677, dsc: train 0.54948, valid 0.55323\n",
      "Batch train [1] loss 0.38559, dsc 0.61441\n",
      "Batch train [2] loss 0.39204, dsc 0.60796\n",
      "Batch train [3] loss 0.39589, dsc 0.60411\n",
      "Batch train [4] loss 0.37727, dsc 0.62273\n",
      "Batch train [5] loss 0.33002, dsc 0.66998\n",
      "Batch train [6] loss 0.44676, dsc 0.55324\n",
      "Batch train [7] loss 0.39902, dsc 0.60098\n",
      "Batch train [8] loss 0.43425, dsc 0.56575\n",
      "Batch train [9] loss 0.41760, dsc 0.58240\n",
      "Batch train [10] loss 0.41060, dsc 0.58940\n",
      "Batch train [11] loss 0.38071, dsc 0.61929\n",
      "Batch train [12] loss 0.38453, dsc 0.61547\n",
      "Batch train [13] loss 0.38263, dsc 0.61737\n",
      "Batch train [14] loss 0.49475, dsc 0.50525\n",
      "Batch train [15] loss 0.40109, dsc 0.59891\n",
      "Batch train [16] loss 0.33720, dsc 0.66280\n",
      "Batch train [17] loss 0.41080, dsc 0.58920\n",
      "Batch train [18] loss 0.34568, dsc 0.65432\n",
      "Batch train [19] loss 0.35327, dsc 0.64673\n",
      "Batch train [20] loss 0.36443, dsc 0.63557\n",
      "Batch train [21] loss 0.34217, dsc 0.65783\n",
      "Batch train [22] loss 0.38146, dsc 0.61854\n",
      "Batch train [23] loss 0.31018, dsc 0.68982\n",
      "Batch train [24] loss 0.35433, dsc 0.64567\n",
      "Batch train [25] loss 0.37829, dsc 0.62171\n",
      "Batch train [26] loss 0.35746, dsc 0.64254\n",
      "Batch train [27] loss 0.32353, dsc 0.67647\n",
      "Batch train [28] loss 0.34737, dsc 0.65263\n",
      "Batch train [29] loss 0.35321, dsc 0.64679\n",
      "Batch train [30] loss 0.38773, dsc 0.61227\n",
      "Batch train [31] loss 0.32088, dsc 0.67912\n",
      "Batch train [32] loss 0.37100, dsc 0.62900\n",
      "Batch train [33] loss 0.32888, dsc 0.67112\n",
      "Batch train [34] loss 0.37853, dsc 0.62147\n",
      "Batch train [35] loss 0.38934, dsc 0.61066\n",
      "Batch train [36] loss 0.34668, dsc 0.65332\n",
      "Batch train [37] loss 0.38398, dsc 0.61602\n",
      "Batch train [38] loss 0.31825, dsc 0.68175\n",
      "Batch train [39] loss 0.33626, dsc 0.66374\n",
      "Batch train [40] loss 0.44206, dsc 0.55794\n",
      "Epoch [16] train done\n",
      "Batch eval [1] loss 0.35897, dsc 0.64103\n",
      "Batch eval [2] loss 0.35810, dsc 0.64190\n",
      "Batch eval [3] loss 0.34931, dsc 0.65069\n",
      "Batch eval [4] loss 0.33184, dsc 0.66816\n",
      "Batch eval [5] loss 0.32338, dsc 0.67662\n",
      "Epoch [16] valid done\n",
      "Epoch [16] T 113.40s, deltaT 7.14s, loss: train 0.37489, valid 0.34432, dsc: train 0.62511, valid 0.65568\n",
      "Batch train [1] loss 0.31817, dsc 0.68183\n",
      "Batch train [2] loss 0.35022, dsc 0.64978\n",
      "Batch train [3] loss 0.33170, dsc 0.66830\n",
      "Batch train [4] loss 0.32540, dsc 0.67460\n",
      "Batch train [5] loss 0.42091, dsc 0.57909\n",
      "Batch train [6] loss 0.32205, dsc 0.67795\n",
      "Batch train [7] loss 0.37161, dsc 0.62839\n",
      "Batch train [8] loss 0.33259, dsc 0.66741\n",
      "Batch train [9] loss 0.31533, dsc 0.68467\n",
      "Batch train [10] loss 0.33340, dsc 0.66660\n",
      "Batch train [11] loss 0.32535, dsc 0.67465\n",
      "Batch train [12] loss 0.32179, dsc 0.67821\n",
      "Batch train [13] loss 0.30808, dsc 0.69192\n",
      "Batch train [14] loss 0.31373, dsc 0.68627\n",
      "Batch train [15] loss 0.34917, dsc 0.65083\n",
      "Batch train [16] loss 0.27376, dsc 0.72624\n",
      "Batch train [17] loss 0.35294, dsc 0.64706\n",
      "Batch train [18] loss 0.32750, dsc 0.67250\n",
      "Batch train [19] loss 0.30467, dsc 0.69533\n",
      "Batch train [20] loss 0.30937, dsc 0.69063\n",
      "Batch train [21] loss 0.29650, dsc 0.70350\n",
      "Batch train [22] loss 0.36860, dsc 0.63140\n",
      "Batch train [23] loss 0.26119, dsc 0.73881\n",
      "Batch train [24] loss 0.29770, dsc 0.70230\n",
      "Batch train [25] loss 0.29460, dsc 0.70540\n",
      "Batch train [26] loss 0.27727, dsc 0.72273\n",
      "Batch train [27] loss 0.31182, dsc 0.68818\n",
      "Batch train [28] loss 0.28893, dsc 0.71107\n",
      "Batch train [29] loss 0.30206, dsc 0.69794\n",
      "Batch train [30] loss 0.33870, dsc 0.66130\n",
      "Batch train [31] loss 0.31834, dsc 0.68166\n",
      "Batch train [32] loss 0.29340, dsc 0.70660\n",
      "Batch train [33] loss 0.29327, dsc 0.70673\n",
      "Batch train [34] loss 0.30332, dsc 0.69668\n",
      "Batch train [35] loss 0.32541, dsc 0.67459\n",
      "Batch train [36] loss 0.34954, dsc 0.65046\n",
      "Batch train [37] loss 0.29133, dsc 0.70867\n",
      "Batch train [38] loss 0.33363, dsc 0.66637\n",
      "Batch train [39] loss 0.41974, dsc 0.58026\n",
      "Batch train [40] loss 0.34617, dsc 0.65383\n",
      "Epoch [17] train done\n",
      "Batch eval [1] loss 0.30438, dsc 0.69562\n",
      "Batch eval [2] loss 0.30936, dsc 0.69064\n",
      "Batch eval [3] loss 0.31968, dsc 0.68032\n",
      "Batch eval [4] loss 0.30497, dsc 0.69503\n",
      "Batch eval [5] loss 0.29540, dsc 0.70460\n",
      "Epoch [17] valid done\n",
      "Epoch [17] T 120.54s, deltaT 7.14s, loss: train 0.32298, valid 0.30676, dsc: train 0.67702, valid 0.69324\n",
      "Batch train [1] loss 0.28564, dsc 0.71436\n",
      "Batch train [2] loss 0.31035, dsc 0.68965\n",
      "Batch train [3] loss 0.26896, dsc 0.73104\n",
      "Batch train [4] loss 0.37475, dsc 0.62525\n",
      "Batch train [5] loss 0.24296, dsc 0.75704\n",
      "Batch train [6] loss 0.27213, dsc 0.72787\n",
      "Batch train [7] loss 0.24094, dsc 0.75906\n",
      "Batch train [8] loss 0.28896, dsc 0.71104\n",
      "Batch train [9] loss 0.31719, dsc 0.68281\n",
      "Batch train [10] loss 0.40788, dsc 0.59212\n",
      "Batch train [11] loss 0.26690, dsc 0.73310\n",
      "Batch train [12] loss 0.31354, dsc 0.68646\n",
      "Batch train [13] loss 0.32151, dsc 0.67849\n",
      "Batch train [14] loss 0.30235, dsc 0.69765\n",
      "Batch train [15] loss 0.29715, dsc 0.70285\n",
      "Batch train [16] loss 0.27428, dsc 0.72572\n",
      "Batch train [17] loss 0.28210, dsc 0.71790\n",
      "Batch train [18] loss 0.30464, dsc 0.69536\n",
      "Batch train [19] loss 0.27222, dsc 0.72778\n",
      "Batch train [20] loss 0.27987, dsc 0.72013\n",
      "Batch train [21] loss 0.30407, dsc 0.69593\n",
      "Batch train [22] loss 0.30120, dsc 0.69880\n",
      "Batch train [23] loss 0.26916, dsc 0.73084\n",
      "Batch train [24] loss 0.26782, dsc 0.73218\n",
      "Batch train [25] loss 0.27458, dsc 0.72542\n",
      "Batch train [26] loss 0.25002, dsc 0.74998\n",
      "Batch train [27] loss 0.27020, dsc 0.72980\n",
      "Batch train [28] loss 0.25991, dsc 0.74009\n",
      "Batch train [29] loss 0.25938, dsc 0.74062\n",
      "Batch train [30] loss 0.25458, dsc 0.74542\n",
      "Batch train [31] loss 0.25579, dsc 0.74421\n",
      "Batch train [32] loss 0.31547, dsc 0.68453\n",
      "Batch train [33] loss 0.28377, dsc 0.71623\n",
      "Batch train [34] loss 0.29797, dsc 0.70203\n",
      "Batch train [35] loss 0.28106, dsc 0.71894\n",
      "Batch train [36] loss 0.26730, dsc 0.73270\n",
      "Batch train [37] loss 0.21041, dsc 0.78959\n",
      "Batch train [38] loss 0.29116, dsc 0.70884\n",
      "Batch train [39] loss 0.24449, dsc 0.75551\n",
      "Batch train [40] loss 0.26248, dsc 0.73752\n",
      "Epoch [18] train done\n",
      "Batch eval [1] loss 0.27675, dsc 0.72325\n",
      "Batch eval [2] loss 0.29678, dsc 0.70322\n",
      "Batch eval [3] loss 0.28740, dsc 0.71260\n",
      "Batch eval [4] loss 0.31248, dsc 0.68752\n",
      "Batch eval [5] loss 0.27739, dsc 0.72261\n",
      "Epoch [18] valid done\n",
      "Epoch [18] T 127.70s, deltaT 7.15s, loss: train 0.28363, valid 0.29016, dsc: train 0.71637, valid 0.70984\n",
      "Batch train [1] loss 0.28587, dsc 0.71413\n",
      "Batch train [2] loss 0.27876, dsc 0.72124\n",
      "Batch train [3] loss 0.25588, dsc 0.74412\n",
      "Batch train [4] loss 0.23862, dsc 0.76138\n",
      "Batch train [5] loss 0.29553, dsc 0.70447\n",
      "Batch train [6] loss 0.28158, dsc 0.71842\n",
      "Batch train [7] loss 0.29065, dsc 0.70935\n",
      "Batch train [8] loss 0.24752, dsc 0.75248\n",
      "Batch train [9] loss 0.24364, dsc 0.75636\n",
      "Batch train [10] loss 0.24583, dsc 0.75417\n",
      "Batch train [11] loss 0.23041, dsc 0.76959\n",
      "Batch train [12] loss 0.34202, dsc 0.65798\n",
      "Batch train [13] loss 0.24847, dsc 0.75153\n",
      "Batch train [14] loss 0.23849, dsc 0.76151\n",
      "Batch train [15] loss 0.26272, dsc 0.73728\n",
      "Batch train [16] loss 0.26943, dsc 0.73057\n",
      "Batch train [17] loss 0.34364, dsc 0.65636\n",
      "Batch train [18] loss 0.24937, dsc 0.75063\n",
      "Batch train [19] loss 0.24169, dsc 0.75831\n",
      "Batch train [20] loss 0.23255, dsc 0.76745\n",
      "Batch train [21] loss 0.26487, dsc 0.73513\n",
      "Batch train [22] loss 0.26368, dsc 0.73632\n",
      "Batch train [23] loss 0.20399, dsc 0.79601\n",
      "Batch train [24] loss 0.24369, dsc 0.75631\n",
      "Batch train [25] loss 0.24242, dsc 0.75758\n",
      "Batch train [26] loss 0.23437, dsc 0.76563\n",
      "Batch train [27] loss 0.27292, dsc 0.72708\n",
      "Batch train [28] loss 0.23027, dsc 0.76973\n",
      "Batch train [29] loss 0.23010, dsc 0.76990\n",
      "Batch train [30] loss 0.24658, dsc 0.75342\n",
      "Batch train [31] loss 0.22834, dsc 0.77166\n",
      "Batch train [32] loss 0.21512, dsc 0.78488\n",
      "Batch train [33] loss 0.24355, dsc 0.75645\n",
      "Batch train [34] loss 0.21345, dsc 0.78655\n",
      "Batch train [35] loss 0.24604, dsc 0.75396\n",
      "Batch train [36] loss 0.26058, dsc 0.73942\n",
      "Batch train [37] loss 0.21767, dsc 0.78233\n",
      "Batch train [38] loss 0.24320, dsc 0.75680\n",
      "Batch train [39] loss 0.22169, dsc 0.77831\n",
      "Batch train [40] loss 0.23254, dsc 0.76746\n",
      "Epoch [19] train done\n",
      "Batch eval [1] loss 0.24731, dsc 0.75269\n",
      "Batch eval [2] loss 0.25622, dsc 0.74378\n",
      "Batch eval [3] loss 0.25586, dsc 0.74414\n",
      "Batch eval [4] loss 0.25256, dsc 0.74744\n",
      "Batch eval [5] loss 0.24424, dsc 0.75576\n",
      "Epoch [19] valid done\n",
      "Epoch [19] T 134.85s, deltaT 7.15s, loss: train 0.25194, valid 0.25124, dsc: train 0.74806, valid 0.74876\n",
      "Batch train [1] loss 0.20642, dsc 0.79358\n",
      "Batch train [2] loss 0.23711, dsc 0.76289\n",
      "Batch train [3] loss 0.22189, dsc 0.77811\n",
      "Batch train [4] loss 0.34683, dsc 0.65317\n",
      "Batch train [5] loss 0.23059, dsc 0.76941\n",
      "Batch train [6] loss 0.24768, dsc 0.75232\n",
      "Batch train [7] loss 0.24468, dsc 0.75532\n",
      "Batch train [8] loss 0.20100, dsc 0.79900\n",
      "Batch train [9] loss 0.22086, dsc 0.77914\n",
      "Batch train [10] loss 0.27571, dsc 0.72429\n",
      "Batch train [11] loss 0.30035, dsc 0.69965\n",
      "Batch train [12] loss 0.24993, dsc 0.75007\n",
      "Batch train [13] loss 0.22337, dsc 0.77663\n",
      "Batch train [14] loss 0.22892, dsc 0.77108\n",
      "Batch train [15] loss 0.23181, dsc 0.76819\n",
      "Batch train [16] loss 0.22985, dsc 0.77015\n",
      "Batch train [17] loss 0.20588, dsc 0.79412\n",
      "Batch train [18] loss 0.17990, dsc 0.82010\n",
      "Batch train [19] loss 0.23194, dsc 0.76806\n",
      "Batch train [20] loss 0.23725, dsc 0.76275\n",
      "Batch train [21] loss 0.19621, dsc 0.80379\n",
      "Batch train [22] loss 0.20275, dsc 0.79725\n",
      "Batch train [23] loss 0.23775, dsc 0.76225\n",
      "Batch train [24] loss 0.18321, dsc 0.81679\n",
      "Batch train [25] loss 0.22939, dsc 0.77061\n",
      "Batch train [26] loss 0.22232, dsc 0.77768\n",
      "Batch train [27] loss 0.22108, dsc 0.77892\n",
      "Batch train [28] loss 0.25146, dsc 0.74854\n",
      "Batch train [29] loss 0.21380, dsc 0.78620\n",
      "Batch train [30] loss 0.20494, dsc 0.79506\n",
      "Batch train [31] loss 0.20309, dsc 0.79691\n",
      "Batch train [32] loss 0.24736, dsc 0.75264\n",
      "Batch train [33] loss 0.20168, dsc 0.79832\n",
      "Batch train [34] loss 0.23322, dsc 0.76678\n",
      "Batch train [35] loss 0.20596, dsc 0.79404\n",
      "Batch train [36] loss 0.22561, dsc 0.77439\n",
      "Batch train [37] loss 0.20889, dsc 0.79111\n",
      "Batch train [38] loss 0.20772, dsc 0.79228\n",
      "Batch train [39] loss 0.21875, dsc 0.78125\n",
      "Batch train [40] loss 0.20681, dsc 0.79319\n",
      "Epoch [20] train done\n",
      "Batch eval [1] loss 0.22190, dsc 0.77810\n",
      "Batch eval [2] loss 0.25660, dsc 0.74340\n",
      "Batch eval [3] loss 0.23477, dsc 0.76523\n",
      "Batch eval [4] loss 0.24420, dsc 0.75580\n",
      "Batch eval [5] loss 0.23297, dsc 0.76703\n",
      "Epoch [20] valid done\n",
      "Epoch [20] T 141.99s, deltaT 7.14s, loss: train 0.22685, valid 0.23809, dsc: train 0.77315, valid 0.76191\n",
      "Batch train [1] loss 0.20239, dsc 0.79761\n",
      "Batch train [2] loss 0.19449, dsc 0.80551\n",
      "Batch train [3] loss 0.18195, dsc 0.81805\n",
      "Batch train [4] loss 0.21330, dsc 0.78670\n",
      "Batch train [5] loss 0.20780, dsc 0.79220\n",
      "Batch train [6] loss 0.18458, dsc 0.81542\n",
      "Batch train [7] loss 0.21971, dsc 0.78029\n",
      "Batch train [8] loss 0.22711, dsc 0.77289\n",
      "Batch train [9] loss 0.21895, dsc 0.78105\n",
      "Batch train [10] loss 0.22540, dsc 0.77460\n",
      "Batch train [11] loss 0.29737, dsc 0.70263\n",
      "Batch train [12] loss 0.19580, dsc 0.80420\n",
      "Batch train [13] loss 0.27855, dsc 0.72145\n",
      "Batch train [14] loss 0.20225, dsc 0.79775\n",
      "Batch train [15] loss 0.20544, dsc 0.79456\n",
      "Batch train [16] loss 0.23875, dsc 0.76125\n",
      "Batch train [17] loss 0.19804, dsc 0.80196\n",
      "Batch train [18] loss 0.19380, dsc 0.80620\n",
      "Batch train [19] loss 0.21002, dsc 0.78998\n",
      "Batch train [20] loss 0.20671, dsc 0.79329\n",
      "Batch train [21] loss 0.21962, dsc 0.78038\n",
      "Batch train [22] loss 0.20583, dsc 0.79417\n",
      "Batch train [23] loss 0.24666, dsc 0.75334\n",
      "Batch train [24] loss 0.18237, dsc 0.81763\n",
      "Batch train [25] loss 0.24743, dsc 0.75257\n",
      "Batch train [26] loss 0.20139, dsc 0.79861\n",
      "Batch train [27] loss 0.18223, dsc 0.81777\n",
      "Batch train [28] loss 0.18233, dsc 0.81767\n",
      "Batch train [29] loss 0.20900, dsc 0.79100\n",
      "Batch train [30] loss 0.17575, dsc 0.82425\n",
      "Batch train [31] loss 0.20112, dsc 0.79888\n",
      "Batch train [32] loss 0.21332, dsc 0.78668\n",
      "Batch train [33] loss 0.20341, dsc 0.79659\n",
      "Batch train [34] loss 0.22957, dsc 0.77043\n",
      "Batch train [35] loss 0.17860, dsc 0.82140\n",
      "Batch train [36] loss 0.21238, dsc 0.78762\n",
      "Batch train [37] loss 0.19683, dsc 0.80317\n",
      "Batch train [38] loss 0.19399, dsc 0.80601\n",
      "Batch train [39] loss 0.18719, dsc 0.81281\n",
      "Batch train [40] loss 0.17671, dsc 0.82329\n",
      "Epoch [21] train done\n",
      "Batch eval [1] loss 0.24215, dsc 0.75785\n",
      "Batch eval [2] loss 0.24125, dsc 0.75875\n",
      "Batch eval [3] loss 0.25070, dsc 0.74930\n",
      "Batch eval [4] loss 0.21412, dsc 0.78588\n",
      "Batch eval [5] loss 0.22366, dsc 0.77634\n",
      "Epoch [21] valid done\n",
      "Epoch [21] T 149.14s, deltaT 7.15s, loss: train 0.20870, valid 0.23437, dsc: train 0.79130, valid 0.76563\n",
      "Batch train [1] loss 0.18931, dsc 0.81069\n",
      "Batch train [2] loss 0.17473, dsc 0.82527\n",
      "Batch train [3] loss 0.19175, dsc 0.80825\n",
      "Batch train [4] loss 0.17810, dsc 0.82190\n",
      "Batch train [5] loss 0.17550, dsc 0.82450\n",
      "Batch train [6] loss 0.20041, dsc 0.79959\n",
      "Batch train [7] loss 0.25966, dsc 0.74034\n",
      "Batch train [8] loss 0.18082, dsc 0.81918\n",
      "Batch train [9] loss 0.18313, dsc 0.81687\n",
      "Batch train [10] loss 0.19247, dsc 0.80753\n",
      "Batch train [11] loss 0.18172, dsc 0.81828\n",
      "Batch train [12] loss 0.18480, dsc 0.81520\n",
      "Batch train [13] loss 0.16966, dsc 0.83034\n",
      "Batch train [14] loss 0.20155, dsc 0.79845\n",
      "Batch train [15] loss 0.18451, dsc 0.81549\n",
      "Batch train [16] loss 0.20701, dsc 0.79299\n",
      "Batch train [17] loss 0.22483, dsc 0.77517\n",
      "Batch train [18] loss 0.19601, dsc 0.80399\n",
      "Batch train [19] loss 0.22235, dsc 0.77765\n",
      "Batch train [20] loss 0.17727, dsc 0.82273\n",
      "Batch train [21] loss 0.19113, dsc 0.80887\n",
      "Batch train [22] loss 0.28771, dsc 0.71229\n",
      "Batch train [23] loss 0.18543, dsc 0.81457\n",
      "Batch train [24] loss 0.17262, dsc 0.82738\n",
      "Batch train [25] loss 0.19182, dsc 0.80818\n",
      "Batch train [26] loss 0.20417, dsc 0.79583\n",
      "Batch train [27] loss 0.18262, dsc 0.81738\n",
      "Batch train [28] loss 0.16603, dsc 0.83397\n",
      "Batch train [29] loss 0.17836, dsc 0.82164\n",
      "Batch train [30] loss 0.18177, dsc 0.81823\n",
      "Batch train [31] loss 0.20978, dsc 0.79022\n",
      "Batch train [32] loss 0.16391, dsc 0.83609\n",
      "Batch train [33] loss 0.20866, dsc 0.79134\n",
      "Batch train [34] loss 0.18726, dsc 0.81274\n",
      "Batch train [35] loss 0.20401, dsc 0.79599\n",
      "Batch train [36] loss 0.21753, dsc 0.78247\n",
      "Batch train [37] loss 0.17166, dsc 0.82834\n",
      "Batch train [38] loss 0.20252, dsc 0.79748\n",
      "Batch train [39] loss 0.19565, dsc 0.80435\n",
      "Batch train [40] loss 0.21496, dsc 0.78504\n",
      "Epoch [22] train done\n",
      "Batch eval [1] loss 0.22876, dsc 0.77124\n",
      "Batch eval [2] loss 0.25936, dsc 0.74064\n",
      "Batch eval [3] loss 0.24087, dsc 0.75913\n",
      "Batch eval [4] loss 0.20253, dsc 0.79747\n",
      "Batch eval [5] loss 0.21662, dsc 0.78338\n",
      "Epoch [22] valid done\n",
      "Epoch [22] T 156.28s, deltaT 7.14s, loss: train 0.19483, valid 0.22963, dsc: train 0.80517, valid 0.77037\n",
      "Batch train [1] loss 0.16955, dsc 0.83045\n",
      "Batch train [2] loss 0.18809, dsc 0.81191\n",
      "Batch train [3] loss 0.18056, dsc 0.81944\n",
      "Batch train [4] loss 0.17738, dsc 0.82262\n",
      "Batch train [5] loss 0.15607, dsc 0.84393\n",
      "Batch train [6] loss 0.22031, dsc 0.77969\n",
      "Batch train [7] loss 0.19294, dsc 0.80706\n",
      "Batch train [8] loss 0.18738, dsc 0.81262\n",
      "Batch train [9] loss 0.17579, dsc 0.82421\n",
      "Batch train [10] loss 0.19642, dsc 0.80358\n",
      "Batch train [11] loss 0.20094, dsc 0.79906\n",
      "Batch train [12] loss 0.16654, dsc 0.83346\n",
      "Batch train [13] loss 0.18295, dsc 0.81705\n",
      "Batch train [14] loss 0.18169, dsc 0.81831\n",
      "Batch train [15] loss 0.17734, dsc 0.82266\n",
      "Batch train [16] loss 0.18160, dsc 0.81840\n",
      "Batch train [17] loss 0.21458, dsc 0.78542\n",
      "Batch train [18] loss 0.18041, dsc 0.81959\n",
      "Batch train [19] loss 0.14914, dsc 0.85086\n",
      "Batch train [20] loss 0.17195, dsc 0.82805\n",
      "Batch train [21] loss 0.18399, dsc 0.81601\n",
      "Batch train [22] loss 0.17527, dsc 0.82473\n",
      "Batch train [23] loss 0.15817, dsc 0.84183\n",
      "Batch train [24] loss 0.18806, dsc 0.81194\n",
      "Batch train [25] loss 0.19111, dsc 0.80889\n",
      "Batch train [26] loss 0.17834, dsc 0.82166\n",
      "Batch train [27] loss 0.18423, dsc 0.81577\n",
      "Batch train [28] loss 0.16044, dsc 0.83956\n",
      "Batch train [29] loss 0.25095, dsc 0.74905\n",
      "Batch train [30] loss 0.19557, dsc 0.80443\n",
      "Batch train [31] loss 0.18683, dsc 0.81317\n",
      "Batch train [32] loss 0.17387, dsc 0.82613\n",
      "Batch train [33] loss 0.17951, dsc 0.82049\n",
      "Batch train [34] loss 0.16818, dsc 0.83182\n",
      "Batch train [35] loss 0.19475, dsc 0.80525\n",
      "Batch train [36] loss 0.18363, dsc 0.81637\n",
      "Batch train [37] loss 0.15381, dsc 0.84619\n",
      "Batch train [38] loss 0.17346, dsc 0.82654\n",
      "Batch train [39] loss 0.23605, dsc 0.76395\n",
      "Batch train [40] loss 0.18113, dsc 0.81887\n",
      "Epoch [23] train done\n",
      "Batch eval [1] loss 0.21098, dsc 0.78902\n",
      "Batch eval [2] loss 0.20797, dsc 0.79203\n",
      "Batch eval [3] loss 0.21437, dsc 0.78563\n",
      "Batch eval [4] loss 0.19412, dsc 0.80588\n",
      "Batch eval [5] loss 0.20806, dsc 0.79194\n",
      "Epoch [23] valid done\n",
      "Epoch [23] T 163.38s, deltaT 7.10s, loss: train 0.18372, valid 0.20710, dsc: train 0.81628, valid 0.79290\n",
      "Batch train [1] loss 0.18257, dsc 0.81743\n",
      "Batch train [2] loss 0.16684, dsc 0.83316\n",
      "Batch train [3] loss 0.16439, dsc 0.83561\n",
      "Batch train [4] loss 0.17844, dsc 0.82156\n",
      "Batch train [5] loss 0.15268, dsc 0.84732\n",
      "Batch train [6] loss 0.15598, dsc 0.84402\n",
      "Batch train [7] loss 0.13925, dsc 0.86075\n",
      "Batch train [8] loss 0.21185, dsc 0.78815\n",
      "Batch train [9] loss 0.18075, dsc 0.81925\n",
      "Batch train [10] loss 0.17790, dsc 0.82210\n",
      "Batch train [11] loss 0.16554, dsc 0.83446\n",
      "Batch train [12] loss 0.15576, dsc 0.84424\n",
      "Batch train [13] loss 0.16481, dsc 0.83519\n",
      "Batch train [14] loss 0.17957, dsc 0.82043\n",
      "Batch train [15] loss 0.16884, dsc 0.83116\n",
      "Batch train [16] loss 0.14125, dsc 0.85875\n",
      "Batch train [17] loss 0.16184, dsc 0.83816\n",
      "Batch train [18] loss 0.15890, dsc 0.84110\n",
      "Batch train [19] loss 0.22130, dsc 0.77870\n",
      "Batch train [20] loss 0.19865, dsc 0.80135\n",
      "Batch train [21] loss 0.18137, dsc 0.81863\n",
      "Batch train [22] loss 0.16337, dsc 0.83663\n",
      "Batch train [23] loss 0.18904, dsc 0.81096\n",
      "Batch train [24] loss 0.14893, dsc 0.85107\n",
      "Batch train [25] loss 0.15814, dsc 0.84186\n",
      "Batch train [26] loss 0.17803, dsc 0.82197\n",
      "Batch train [27] loss 0.16779, dsc 0.83221\n",
      "Batch train [28] loss 0.15584, dsc 0.84416\n",
      "Batch train [29] loss 0.17417, dsc 0.82583\n",
      "Batch train [30] loss 0.15252, dsc 0.84748\n",
      "Batch train [31] loss 0.18315, dsc 0.81685\n",
      "Batch train [32] loss 0.18337, dsc 0.81663\n",
      "Batch train [33] loss 0.17765, dsc 0.82235\n",
      "Batch train [34] loss 0.17038, dsc 0.82962\n",
      "Batch train [35] loss 0.21442, dsc 0.78558\n",
      "Batch train [36] loss 0.16474, dsc 0.83526\n",
      "Batch train [37] loss 0.17941, dsc 0.82059\n",
      "Batch train [38] loss 0.24954, dsc 0.75046\n",
      "Batch train [39] loss 0.17623, dsc 0.82377\n",
      "Batch train [40] loss 0.17345, dsc 0.82655\n",
      "Epoch [24] train done\n",
      "Batch eval [1] loss 0.21635, dsc 0.78365\n",
      "Batch eval [2] loss 0.21566, dsc 0.78434\n",
      "Batch eval [3] loss 0.23612, dsc 0.76388\n",
      "Batch eval [4] loss 0.18885, dsc 0.81115\n",
      "Batch eval [5] loss 0.19727, dsc 0.80273\n",
      "Epoch [24] valid done\n",
      "Epoch [24] T 170.48s, deltaT 7.10s, loss: train 0.17422, valid 0.21085, dsc: train 0.82578, valid 0.78915\n",
      "Batch train [1] loss 0.16387, dsc 0.83613\n",
      "Batch train [2] loss 0.14101, dsc 0.85899\n",
      "Batch train [3] loss 0.23664, dsc 0.76336\n",
      "Batch train [4] loss 0.16238, dsc 0.83762\n",
      "Batch train [5] loss 0.14891, dsc 0.85109\n",
      "Batch train [6] loss 0.16604, dsc 0.83396\n",
      "Batch train [7] loss 0.18915, dsc 0.81085\n",
      "Batch train [8] loss 0.16344, dsc 0.83656\n",
      "Batch train [9] loss 0.16892, dsc 0.83108\n",
      "Batch train [10] loss 0.13633, dsc 0.86367\n",
      "Batch train [11] loss 0.17260, dsc 0.82740\n",
      "Batch train [12] loss 0.13933, dsc 0.86067\n",
      "Batch train [13] loss 0.16440, dsc 0.83560\n",
      "Batch train [14] loss 0.16980, dsc 0.83020\n",
      "Batch train [15] loss 0.18851, dsc 0.81149\n",
      "Batch train [16] loss 0.14803, dsc 0.85197\n",
      "Batch train [17] loss 0.16708, dsc 0.83292\n",
      "Batch train [18] loss 0.17137, dsc 0.82863\n",
      "Batch train [19] loss 0.17309, dsc 0.82691\n",
      "Batch train [20] loss 0.15972, dsc 0.84028\n",
      "Batch train [21] loss 0.14838, dsc 0.85162\n",
      "Batch train [22] loss 0.17181, dsc 0.82819\n",
      "Batch train [23] loss 0.17220, dsc 0.82780\n",
      "Batch train [24] loss 0.17972, dsc 0.82028\n",
      "Batch train [25] loss 0.18114, dsc 0.81886\n",
      "Batch train [26] loss 0.14425, dsc 0.85575\n",
      "Batch train [27] loss 0.15286, dsc 0.84714\n",
      "Batch train [28] loss 0.15825, dsc 0.84175\n",
      "Batch train [29] loss 0.16333, dsc 0.83667\n",
      "Batch train [30] loss 0.16043, dsc 0.83957\n",
      "Batch train [31] loss 0.19817, dsc 0.80183\n",
      "Batch train [32] loss 0.15314, dsc 0.84686\n",
      "Batch train [33] loss 0.17228, dsc 0.82772\n",
      "Batch train [34] loss 0.14927, dsc 0.85073\n",
      "Batch train [35] loss 0.15444, dsc 0.84556\n",
      "Batch train [36] loss 0.17210, dsc 0.82790\n",
      "Batch train [37] loss 0.15666, dsc 0.84334\n",
      "Batch train [38] loss 0.15446, dsc 0.84554\n",
      "Batch train [39] loss 0.16526, dsc 0.83474\n",
      "Batch train [40] loss 0.22908, dsc 0.77092\n",
      "Epoch [25] train done\n",
      "Batch eval [1] loss 0.20945, dsc 0.79055\n",
      "Batch eval [2] loss 0.23067, dsc 0.76933\n",
      "Batch eval [3] loss 0.23932, dsc 0.76068\n",
      "Batch eval [4] loss 0.21250, dsc 0.78750\n",
      "Batch eval [5] loss 0.20573, dsc 0.79427\n",
      "Epoch [25] valid done\n",
      "Epoch [25] T 177.58s, deltaT 7.10s, loss: train 0.16670, valid 0.21953, dsc: train 0.83330, valid 0.78047\n",
      "Batch train [1] loss 0.15586, dsc 0.84414\n",
      "Batch train [2] loss 0.13597, dsc 0.86403\n",
      "Batch train [3] loss 0.17267, dsc 0.82733\n",
      "Batch train [4] loss 0.16643, dsc 0.83357\n",
      "Batch train [5] loss 0.14336, dsc 0.85664\n",
      "Batch train [6] loss 0.14566, dsc 0.85434\n",
      "Batch train [7] loss 0.15382, dsc 0.84618\n",
      "Batch train [8] loss 0.15812, dsc 0.84188\n",
      "Batch train [9] loss 0.15257, dsc 0.84743\n",
      "Batch train [10] loss 0.15106, dsc 0.84894\n",
      "Batch train [11] loss 0.13373, dsc 0.86627\n",
      "Batch train [12] loss 0.14703, dsc 0.85297\n",
      "Batch train [13] loss 0.16475, dsc 0.83525\n",
      "Batch train [14] loss 0.16747, dsc 0.83253\n",
      "Batch train [15] loss 0.15320, dsc 0.84680\n",
      "Batch train [16] loss 0.16231, dsc 0.83769\n",
      "Batch train [17] loss 0.14680, dsc 0.85320\n",
      "Batch train [18] loss 0.13276, dsc 0.86724\n",
      "Batch train [19] loss 0.13608, dsc 0.86392\n",
      "Batch train [20] loss 0.16050, dsc 0.83950\n",
      "Batch train [21] loss 0.16095, dsc 0.83905\n",
      "Batch train [22] loss 0.16197, dsc 0.83803\n",
      "Batch train [23] loss 0.14546, dsc 0.85454\n",
      "Batch train [24] loss 0.14609, dsc 0.85391\n",
      "Batch train [25] loss 0.15590, dsc 0.84410\n",
      "Batch train [26] loss 0.13640, dsc 0.86360\n",
      "Batch train [27] loss 0.15462, dsc 0.84538\n",
      "Batch train [28] loss 0.20806, dsc 0.79194\n",
      "Batch train [29] loss 0.17109, dsc 0.82891\n",
      "Batch train [30] loss 0.16844, dsc 0.83156\n",
      "Batch train [31] loss 0.15110, dsc 0.84890\n",
      "Batch train [32] loss 0.15885, dsc 0.84115\n",
      "Batch train [33] loss 0.15346, dsc 0.84654\n",
      "Batch train [34] loss 0.19852, dsc 0.80148\n",
      "Batch train [35] loss 0.14487, dsc 0.85513\n",
      "Batch train [36] loss 0.14140, dsc 0.85860\n",
      "Batch train [37] loss 0.15009, dsc 0.84991\n",
      "Batch train [38] loss 0.15161, dsc 0.84839\n",
      "Batch train [39] loss 0.23663, dsc 0.76337\n",
      "Batch train [40] loss 0.18094, dsc 0.81906\n",
      "Epoch [26] train done\n",
      "Batch eval [1] loss 0.20657, dsc 0.79343\n",
      "Batch eval [2] loss 0.21445, dsc 0.78555\n",
      "Batch eval [3] loss 0.21092, dsc 0.78908\n",
      "Batch eval [4] loss 0.18927, dsc 0.81073\n",
      "Batch eval [5] loss 0.18607, dsc 0.81393\n",
      "Epoch [26] valid done\n",
      "Epoch [26] T 184.68s, deltaT 7.10s, loss: train 0.15792, valid 0.20146, dsc: train 0.84208, valid 0.79854\n",
      "Batch train [1] loss 0.13478, dsc 0.86522\n",
      "Batch train [2] loss 0.14430, dsc 0.85570\n",
      "Batch train [3] loss 0.15255, dsc 0.84745\n",
      "Batch train [4] loss 0.15876, dsc 0.84124\n",
      "Batch train [5] loss 0.12507, dsc 0.87493\n",
      "Batch train [6] loss 0.15880, dsc 0.84120\n",
      "Batch train [7] loss 0.15169, dsc 0.84831\n",
      "Batch train [8] loss 0.15234, dsc 0.84766\n",
      "Batch train [9] loss 0.14900, dsc 0.85100\n",
      "Batch train [10] loss 0.15758, dsc 0.84242\n",
      "Batch train [11] loss 0.13973, dsc 0.86027\n",
      "Batch train [12] loss 0.15255, dsc 0.84745\n",
      "Batch train [13] loss 0.13712, dsc 0.86288\n",
      "Batch train [14] loss 0.15094, dsc 0.84906\n",
      "Batch train [15] loss 0.17165, dsc 0.82835\n",
      "Batch train [16] loss 0.17977, dsc 0.82023\n",
      "Batch train [17] loss 0.13864, dsc 0.86136\n",
      "Batch train [18] loss 0.13387, dsc 0.86613\n",
      "Batch train [19] loss 0.14523, dsc 0.85477\n",
      "Batch train [20] loss 0.13948, dsc 0.86052\n",
      "Batch train [21] loss 0.14442, dsc 0.85558\n",
      "Batch train [22] loss 0.16372, dsc 0.83628\n",
      "Batch train [23] loss 0.15223, dsc 0.84777\n",
      "Batch train [24] loss 0.19074, dsc 0.80926\n",
      "Batch train [25] loss 0.14795, dsc 0.85205\n",
      "Batch train [26] loss 0.14851, dsc 0.85149\n",
      "Batch train [27] loss 0.14306, dsc 0.85694\n",
      "Batch train [28] loss 0.21968, dsc 0.78032\n",
      "Batch train [29] loss 0.15009, dsc 0.84991\n",
      "Batch train [30] loss 0.16792, dsc 0.83208\n",
      "Batch train [31] loss 0.14723, dsc 0.85277\n",
      "Batch train [32] loss 0.16816, dsc 0.83184\n",
      "Batch train [33] loss 0.15252, dsc 0.84748\n",
      "Batch train [34] loss 0.13908, dsc 0.86092\n",
      "Batch train [35] loss 0.13584, dsc 0.86416\n",
      "Batch train [36] loss 0.14301, dsc 0.85699\n",
      "Batch train [37] loss 0.15691, dsc 0.84309\n",
      "Batch train [38] loss 0.18824, dsc 0.81176\n",
      "Batch train [39] loss 0.15931, dsc 0.84069\n",
      "Batch train [40] loss 0.14186, dsc 0.85814\n",
      "Epoch [27] train done\n",
      "Batch eval [1] loss 0.19019, dsc 0.80981\n",
      "Batch eval [2] loss 0.21116, dsc 0.78884\n",
      "Batch eval [3] loss 0.19409, dsc 0.80591\n",
      "Batch eval [4] loss 0.20036, dsc 0.79964\n",
      "Batch eval [5] loss 0.17442, dsc 0.82558\n",
      "Epoch [27] valid done\n",
      "Epoch [27] T 191.79s, deltaT 7.11s, loss: train 0.15336, valid 0.19404, dsc: train 0.84664, valid 0.80596\n",
      "Batch train [1] loss 0.15620, dsc 0.84380\n",
      "Batch train [2] loss 0.13928, dsc 0.86072\n",
      "Batch train [3] loss 0.14657, dsc 0.85343\n",
      "Batch train [4] loss 0.15049, dsc 0.84951\n",
      "Batch train [5] loss 0.11815, dsc 0.88185\n",
      "Batch train [6] loss 0.13832, dsc 0.86168\n",
      "Batch train [7] loss 0.14155, dsc 0.85845\n",
      "Batch train [8] loss 0.15747, dsc 0.84253\n",
      "Batch train [9] loss 0.16305, dsc 0.83695\n",
      "Batch train [10] loss 0.13399, dsc 0.86601\n",
      "Batch train [11] loss 0.12426, dsc 0.87574\n",
      "Batch train [12] loss 0.14399, dsc 0.85601\n",
      "Batch train [13] loss 0.14299, dsc 0.85701\n",
      "Batch train [14] loss 0.13318, dsc 0.86682\n",
      "Batch train [15] loss 0.13814, dsc 0.86186\n",
      "Batch train [16] loss 0.11733, dsc 0.88267\n",
      "Batch train [17] loss 0.14495, dsc 0.85505\n",
      "Batch train [18] loss 0.14825, dsc 0.85175\n",
      "Batch train [19] loss 0.14744, dsc 0.85256\n",
      "Batch train [20] loss 0.18954, dsc 0.81046\n",
      "Batch train [21] loss 0.14433, dsc 0.85567\n",
      "Batch train [22] loss 0.15115, dsc 0.84885\n",
      "Batch train [23] loss 0.13239, dsc 0.86761\n",
      "Batch train [24] loss 0.14254, dsc 0.85746\n",
      "Batch train [25] loss 0.14816, dsc 0.85184\n",
      "Batch train [26] loss 0.14053, dsc 0.85947\n",
      "Batch train [27] loss 0.14669, dsc 0.85331\n",
      "Batch train [28] loss 0.15014, dsc 0.84986\n",
      "Batch train [29] loss 0.15223, dsc 0.84777\n",
      "Batch train [30] loss 0.16879, dsc 0.83121\n",
      "Batch train [31] loss 0.16215, dsc 0.83785\n",
      "Batch train [32] loss 0.13801, dsc 0.86199\n",
      "Batch train [33] loss 0.16980, dsc 0.83020\n",
      "Batch train [34] loss 0.22750, dsc 0.77250\n",
      "Batch train [35] loss 0.14443, dsc 0.85557\n",
      "Batch train [36] loss 0.15404, dsc 0.84596\n",
      "Batch train [37] loss 0.14608, dsc 0.85392\n",
      "Batch train [38] loss 0.12439, dsc 0.87561\n",
      "Batch train [39] loss 0.15599, dsc 0.84401\n",
      "Batch train [40] loss 0.14718, dsc 0.85282\n",
      "Epoch [28] train done\n",
      "Batch eval [1] loss 0.18961, dsc 0.81039\n",
      "Batch eval [2] loss 0.19848, dsc 0.80152\n",
      "Batch eval [3] loss 0.18329, dsc 0.81671\n",
      "Batch eval [4] loss 0.17333, dsc 0.82667\n",
      "Batch eval [5] loss 0.18279, dsc 0.81721\n",
      "Epoch [28] valid done\n",
      "Epoch [28] T 198.89s, deltaT 7.10s, loss: train 0.14804, valid 0.18550, dsc: train 0.85196, valid 0.81450\n",
      "Batch train [1] loss 0.14698, dsc 0.85302\n",
      "Batch train [2] loss 0.13707, dsc 0.86293\n",
      "Batch train [3] loss 0.13189, dsc 0.86811\n",
      "Batch train [4] loss 0.12435, dsc 0.87565\n",
      "Batch train [5] loss 0.15598, dsc 0.84402\n",
      "Batch train [6] loss 0.14341, dsc 0.85659\n",
      "Batch train [7] loss 0.17505, dsc 0.82495\n",
      "Batch train [8] loss 0.13820, dsc 0.86180\n",
      "Batch train [9] loss 0.14703, dsc 0.85297\n",
      "Batch train [10] loss 0.20345, dsc 0.79655\n",
      "Batch train [11] loss 0.14289, dsc 0.85711\n",
      "Batch train [12] loss 0.13584, dsc 0.86416\n",
      "Batch train [13] loss 0.14419, dsc 0.85581\n",
      "Batch train [14] loss 0.12752, dsc 0.87248\n",
      "Batch train [15] loss 0.16452, dsc 0.83548\n",
      "Batch train [16] loss 0.14905, dsc 0.85095\n",
      "Batch train [17] loss 0.12212, dsc 0.87788\n",
      "Batch train [18] loss 0.14842, dsc 0.85158\n",
      "Batch train [19] loss 0.14137, dsc 0.85863\n",
      "Batch train [20] loss 0.14873, dsc 0.85127\n",
      "Batch train [21] loss 0.13373, dsc 0.86627\n",
      "Batch train [22] loss 0.13712, dsc 0.86288\n",
      "Batch train [23] loss 0.11693, dsc 0.88307\n",
      "Batch train [24] loss 0.14003, dsc 0.85997\n",
      "Batch train [25] loss 0.11371, dsc 0.88629\n",
      "Batch train [26] loss 0.14238, dsc 0.85762\n",
      "Batch train [27] loss 0.13930, dsc 0.86070\n",
      "Batch train [28] loss 0.15653, dsc 0.84347\n",
      "Batch train [29] loss 0.13299, dsc 0.86701\n",
      "Batch train [30] loss 0.13653, dsc 0.86347\n",
      "Batch train [31] loss 0.12652, dsc 0.87348\n",
      "Batch train [32] loss 0.14616, dsc 0.85384\n",
      "Batch train [33] loss 0.15346, dsc 0.84654\n",
      "Batch train [34] loss 0.13124, dsc 0.86876\n",
      "Batch train [35] loss 0.12841, dsc 0.87159\n",
      "Batch train [36] loss 0.14592, dsc 0.85408\n",
      "Batch train [37] loss 0.13134, dsc 0.86866\n",
      "Batch train [38] loss 0.16029, dsc 0.83971\n",
      "Batch train [39] loss 0.15945, dsc 0.84055\n",
      "Batch train [40] loss 0.11878, dsc 0.88122\n",
      "Epoch [29] train done\n",
      "Batch eval [1] loss 0.18588, dsc 0.81412\n",
      "Batch eval [2] loss 0.20949, dsc 0.79051\n",
      "Batch eval [3] loss 0.17672, dsc 0.82328\n",
      "Batch eval [4] loss 0.18329, dsc 0.81671\n",
      "Batch eval [5] loss 0.16086, dsc 0.83914\n",
      "Epoch [29] valid done\n",
      "Epoch [29] T 205.99s, deltaT 7.09s, loss: train 0.14197, valid 0.18325, dsc: train 0.85803, valid 0.81675\n",
      "Batch train [1] loss 0.12876, dsc 0.87124\n",
      "Batch train [2] loss 0.14329, dsc 0.85671\n",
      "Batch train [3] loss 0.13762, dsc 0.86238\n",
      "Batch train [4] loss 0.14273, dsc 0.85727\n",
      "Batch train [5] loss 0.14015, dsc 0.85985\n",
      "Batch train [6] loss 0.13124, dsc 0.86876\n",
      "Batch train [7] loss 0.12511, dsc 0.87489\n",
      "Batch train [8] loss 0.13414, dsc 0.86586\n",
      "Batch train [9] loss 0.13495, dsc 0.86505\n",
      "Batch train [10] loss 0.13504, dsc 0.86496\n",
      "Batch train [11] loss 0.16115, dsc 0.83885\n",
      "Batch train [12] loss 0.12170, dsc 0.87830\n",
      "Batch train [13] loss 0.13099, dsc 0.86901\n",
      "Batch train [14] loss 0.10784, dsc 0.89216\n",
      "Batch train [15] loss 0.12548, dsc 0.87452\n",
      "Batch train [16] loss 0.11327, dsc 0.88673\n",
      "Batch train [17] loss 0.15800, dsc 0.84200\n",
      "Batch train [18] loss 0.11801, dsc 0.88199\n",
      "Batch train [19] loss 0.09739, dsc 0.90261\n",
      "Batch train [20] loss 0.14736, dsc 0.85264\n",
      "Batch train [21] loss 0.13795, dsc 0.86205\n",
      "Batch train [22] loss 0.13187, dsc 0.86813\n",
      "Batch train [23] loss 0.12590, dsc 0.87410\n",
      "Batch train [24] loss 0.18679, dsc 0.81321\n",
      "Batch train [25] loss 0.17144, dsc 0.82856\n",
      "Batch train [26] loss 0.12193, dsc 0.87807\n",
      "Batch train [27] loss 0.14070, dsc 0.85930\n",
      "Batch train [28] loss 0.12322, dsc 0.87678\n",
      "Batch train [29] loss 0.13269, dsc 0.86731\n",
      "Batch train [30] loss 0.15334, dsc 0.84666\n",
      "Batch train [31] loss 0.13737, dsc 0.86263\n",
      "Batch train [32] loss 0.14222, dsc 0.85778\n",
      "Batch train [33] loss 0.10988, dsc 0.89012\n",
      "Batch train [34] loss 0.12876, dsc 0.87124\n",
      "Batch train [35] loss 0.12743, dsc 0.87257\n",
      "Batch train [36] loss 0.13969, dsc 0.86031\n",
      "Batch train [37] loss 0.15460, dsc 0.84540\n",
      "Batch train [38] loss 0.12341, dsc 0.87659\n",
      "Batch train [39] loss 0.13969, dsc 0.86031\n",
      "Batch train [40] loss 0.14902, dsc 0.85098\n",
      "Epoch [30] train done\n",
      "Batch eval [1] loss 0.21053, dsc 0.78947\n",
      "Batch eval [2] loss 0.19939, dsc 0.80061\n",
      "Batch eval [3] loss 0.20547, dsc 0.79453\n",
      "Batch eval [4] loss 0.16472, dsc 0.83528\n",
      "Batch eval [5] loss 0.16662, dsc 0.83338\n",
      "Epoch [30] valid done\n",
      "Epoch [30] T 213.14s, deltaT 7.15s, loss: train 0.13530, valid 0.18935, dsc: train 0.86470, valid 0.81065\n",
      "Elapsed time 0:03:33\n"
     ]
    }
   ],
   "source": [
    "# preparing model loop params\n",
    "low_res_model_info = prepare_model(epochs=30, in_channels=8, train_dataset=train_low_res_dataset, valid_dataset=valid_low_res_dataset, test_dataset=test_low_res_dataset)\n",
    "show_model_info(low_res_model_info)\n",
    "\n",
    "# getting everything necessary for model training\n",
    "low_res_train_loop_params = {k:v for k,v in low_res_model_info.items() if k not in ['model_total_params', 'model_total_trainable_params']}\n",
    "# running training loop\n",
    "train_loop(**low_res_train_loop_params)\n",
    "\n",
    "low_res_model = itemgetter('model')(low_res_model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full resolution cutting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading high/full res dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 1x dataset\n",
      "normalizing dataset\n",
      "filtering labels\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n",
      "data type: float64 int8\n",
      "data max 12.81577046544424, min -0.40489707167932215\n",
      "label max 22, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed166cca06484edf963c15d360ef1508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fb25150a3f44b0820641f8c350c066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset data and label shapes (1, 160, 32, 32) (1, 160, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "full_res_dataset = get_dataset(dataset_size=50, shrink_factor=1, filter_labels=filter_labels, unify_labels=False)\n",
    "full_res_dataset.to_numpy()\n",
    "full_res_dataset.show_data_type()\n",
    "preview_dataset(full_res_dataset, preview_index=0, show_hist=False)\n",
    "\n",
    "print('dataset data and label shapes', low_res_dataset.data_list[0].shape, full_res_dataset.data_list[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing low res network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting bounding box cut in full res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "full_res_split_dataset_obj = copy_split_dataset(full_res_dataset, low_res_split_dataset_obj)\n",
    "get_dataset_info(full_res_dataset, full_res_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### debuging cut algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070\n",
      "Memory Usage before moving model to CPU:\n",
      "Allocated:  0.0 GB\n",
      "Cached:     0.3 GB\n",
      "Max memory: 0.2 GB\n",
      "Max Cached: 0.3 GB\n",
      "moved model to cpu\n",
      "Memory Usage after moving model to CPU:\n",
      "Allocated:  0.0 GB\n",
      "Cached:     0.0 GB\n",
      "Max memory: 0.2 GB\n",
      "Max Cached: 0.3 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print('Memory Usage before moving model to CPU:')\n",
    "print('Allocated: ', round(torch.cuda.memory_allocated(device=None)/1024**3, 1), 'GB')\n",
    "print('Cached:    ', round(torch.cuda.memory_cached(device=None)/1024**3, 1), 'GB')\n",
    "print('Max memory:', round(torch.cuda.max_memory_allocated(device=None)/1024**3, 1), 'GB')\n",
    "print('Max Cached:', round(torch.cuda.max_memory_cached(device=None)/1024**3, 1), 'GB')\n",
    "\n",
    "# moving model to cpu and setting to eval mode, preventing model params changes/training\n",
    "low_res_model = low_res_model.to('cpu')\n",
    "#low_res_model.to(low_res_model_info['device'])\n",
    "low_res_model.eval()\n",
    "\n",
    "low_res_model_info['model'] = low_res_model\n",
    "torch.cuda.empty_cache()\n",
    "print('moved model to cpu')\n",
    "\n",
    "print('Memory Usage after moving model to CPU:')\n",
    "print('Allocated: ', round(torch.cuda.memory_allocated(device=None)/1024**3, 1), 'GB')\n",
    "print('Cached:    ', round(torch.cuda.memory_cached(device=None)/1024**3, 1), 'GB')\n",
    "print('Max memory:', round(torch.cuda.max_memory_allocated(device=None)/1024**3, 1), 'GB')\n",
    "print('Max Cached:', round(torch.cuda.max_memory_cached(device=None)/1024**3, 1), 'GB')\n",
    "\n",
    "# del low_res_dataset\n",
    "# del low_res_split_dataset_obj\n",
    "# del low_res_model\n",
    "# del low_res_model_info\n",
    "# del low_res_train_loop_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug box delta [25 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1311715 1311715\n",
      "debug bounding box sizes (47, 160, 160) (72, 192, 168)\n",
      "debug bounding boxes (45, 91, 160, 319, 176, 335) (33, 104, 144, 335, 172, 339)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b025cb7ef84b4f93b42ffed72af8f80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdfedcd5c9a4e75ba2ce4b78a3a4f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0c70c549ac4bb6a2488e77e0833659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7aa99a17108460d9b0efaae1397b609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_index = 40\n",
    "tmp = get_full_res_cut(low_res_model, low_res_dataset.data_list[dataset_index],\n",
    "                 full_res_dataset.data_list[dataset_index], full_res_dataset.label_list[dataset_index],\n",
    "                 low_res_mask_threshold=0.5,\n",
    "                 desire_bounding_box_size=DESIRE_BOUNDING_BOX_SIZE, \n",
    "                 show_debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Running cut algorithm, creating cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cut_full_res_dataset = full_res_dataset.copy(copy_lists=False)\n",
    "cut_full_res_dataset = get_cut_lists(low_res_model, low_res_dataset, full_res_dataset, cut_full_res_dataset, low_res_mask_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### reviewing full res and cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cut_full_res_dataset.show_data_type()\n",
    "print()\n",
    "print('full res shape', full_res_dataset.data_list[0].shape, full_res_dataset.label_list[0].shape)\n",
    "print('cut full res shape', cut_full_res_dataset.data_list[0].shape, cut_full_res_dataset.label_list[0].shape)\n",
    "print()\n",
    "print('dataset RAM sizes in GB', full_res_dataset.get_data_size() / 1024**3, cut_full_res_dataset.get_data_size() / 1024**3)\n",
    "print('single item RAM in GB', full_res_dataset.label_list[0].nbytes / 1024**3, full_res_dataset.data_list[0].nbytes / 1024**3)\n",
    "print()\n",
    "preview_dataset(cut_full_res_dataset, max_slices=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Full resolution cut model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cut_full_res_dataset.set_output_label([OARS_LABELS.EYE_L, OARS_LABELS.EYE_R, OARS_LABELS.LENS_L, OARS_LABELS.LENS_R])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cut_full_res_dataset.__getitem__(0)[1][47])\n",
    "plt.show()\n",
    "\n",
    "cut_split_dataset_obj = copy_split_dataset(cut_full_res_dataset, low_res_split_dataset_obj)\n",
    "get_dataset_info(cut_full_res_dataset, cut_split_dataset_obj)\n",
    "\n",
    "cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(\n",
    "    'train_dataset', 'valid_dataset', 'test_dataset')(cut_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Cut Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cut_model_info = prepare_model(epochs=50,\n",
    "                               learning_rate=1e-4,\n",
    "                               in_channels=8,\n",
    "                               batch_size=1,\n",
    "                               train_dataset=cut_train_dataset, valid_dataset=cut_valid_dataset, test_dataset=cut_test_dataset)\n",
    "show_model_info(cut_model_info)\n",
    "\n",
    "cut_train_loop_params = {k:v for k,v in cut_model_info.items() if k not in ['model_total_params', 'model_total_trainable_params']}\n",
    "train_loop(**cut_train_loop_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Memory Usage before training cut model:')\n",
    "print('Allocated: ', round(torch.cuda.memory_allocated(device=None)/1024**3, 1), 'GB')\n",
    "print('Cached:    ', round(torch.cuda.memory_cached(device=None)/1024**3, 1), 'GB')\n",
    "print('Max memory:', round(torch.cuda.max_memory_allocated(device=None)/1024**3, 1), 'GB')\n",
    "print('Max Cached:', round(torch.cuda.max_memory_cached(device=None)/1024**3, 1), 'GB')\n",
    "\n",
    "# del cut_model_info\n",
    "# del cut_train_loop_params\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "tmp = gc.collect()\n",
    "print(tmp)\n",
    "\n",
    "print('Memory Usage before training cut model:')\n",
    "print('Allocated: ', round(torch.cuda.memory_allocated(device=None)/1024**3, 1), 'GB')\n",
    "print('Cached:    ', round(torch.cuda.memory_cached(device=None)/1024**3, 1), 'GB')\n",
    "print('Max memory:', round(torch.cuda.max_memory_allocated(device=None)/1024**3, 1), 'GB')\n",
    "print('Max Cached:', round(torch.cuda.max_memory_cached(device=None)/1024**3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "160x128x128 = 2_621_440 \\\n",
    "72x198x168 = 2_395_008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_slices = cut_train_dataset.__getitem__(0)[1].shape[0]\n",
    "\n",
    "display((Markdown(\"### Train Eval\"),))\n",
    "show_model_dataset_pred_preview(cut_model_info, cut_train_dataset, max_slices=max_slices, default_slice=49)\n",
    "\n",
    "# display((Markdown(\"### Valid Eval\"),))\n",
    "# show_model_dataset_pred_preview(cut_model_info, cut_valid_dataset, max_slices=max_slices, default_slice=53)\n",
    "\n",
    "# display((Markdown(\"### Test Eval\"),))\n",
    "# eval_image_dataset(test_dataset, 78, 'test_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_index = 40\n",
    "slice_index = 49\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(preds[dataset_index][slice_index], cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(rescaled_preds[dataset_index][slice_index], cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(cut_full_res_dataset[dataset_index][1][slice_index], cmap=\"gray\", vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
