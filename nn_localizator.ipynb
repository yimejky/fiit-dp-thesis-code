{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset biggest bounding box wihtout spinal cord [56, 177, 156]\n",
      "Cut target size [72, 192, 168]\n",
      "Done Init\n"
     ]
    }
   ],
   "source": [
    "from src.consts import IN_COLAB, DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Found Google Colab')\n",
    "    !pip3 install torch torchvision torchsummary\n",
    "    !pip3 install simpleitk\n",
    "\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import src.dataset.oars_labels_consts as OARS_LABELS\n",
    "from src.helpers.threshold_calc_helpers import get_threshold_info_df\n",
    "from src.helpers.show_model_dataset_pred_preview import show_model_dataset_pred_preview\n",
    "from src.dataset.dataset_cut_helpers import get_full_res_cut, get_cut_lists\n",
    "from src.dataset.get_dataset import get_dataset\n",
    "from src.dataset.get_dataset_info import get_dataset_info\n",
    "from src.dataset.preview_dataset import preview_dataset\n",
    "from src.model_and_training.prepare_model import prepare_model\n",
    "from src.model_and_training.train_loop import train_loop\n",
    "from src.model_and_training.training_helpers import show_model_info\n",
    "from src.helpers.show_cuda_usage import show_cuda_usage\n",
    "from src.helpers.threshold_calc_helpers import get_rescaled_preds\n",
    "from src.dataset.split_dataset import split_dataset, copy_split_dataset\n",
    "from src.helpers.compare_prediction_with_ground_true import compare_prediction_with_ground_true\n",
    "\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "torch.manual_seed(20)\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low resolution NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading low res dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 16x dataset\n",
      "normalizing dataset\n",
      "normalizing done\n",
      "filtering labels\n",
      "filtering labels done\n",
      "dilatating 1x dataset\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n",
      "data type: float64 int8\n",
      "low res dataset RAM sizes in GB 0.06866455078125\n",
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_LIST\n",
    "if OARS_LABELS.SPINAL_CORD in filter_labels:\n",
    "    filter_labels.remove(OARS_LABELS.SPINAL_CORD)\n",
    "\n",
    "low_res_dataset = get_dataset(dataset_size=50, shrink_factor=16, filter_labels=filter_labels, unify_labels=True)\n",
    "low_res_dataset.dilatate_labels(repeat=1)\n",
    "low_res_dataset.to_numpy()\n",
    "low_res_dataset.show_data_type()\n",
    "print('low res dataset RAM sizes in GB', low_res_dataset.get_data_size() / 1024**3)\n",
    "\n",
    "low_res_split_dataset_obj = split_dataset(low_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "get_dataset_info(low_res_dataset, low_res_split_dataset_obj)\n",
    "train_low_res_dataset, valid_low_res_dataset, test_low_res_dataset = itemgetter(\n",
    "    'train_dataset', 'valid_dataset', 'test_dataset')(low_res_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 12.505709639268096, min -0.40698009878688973\n",
      "label max 1, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ce303496b043ca8ddba9b54838abd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325d3a16bf644d0b88cbc302f8cbe146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_dataset(low_res_dataset, preview_index=0, show_hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training low res model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device running \"cuda\"\n",
      "max output channels 128\n",
      "Model number of params: 1193537, trainable 1193537\n",
      "Running training loop\n",
      "Batch train [1] loss 0.97630, dsc 0.02370\n",
      "Batch train [2] loss 0.97167, dsc 0.02833\n",
      "Batch train [3] loss 0.97257, dsc 0.02743\n",
      "Batch train [4] loss 0.96968, dsc 0.03032\n",
      "Batch train [5] loss 0.97364, dsc 0.02636\n",
      "Batch train [6] loss 0.96659, dsc 0.03341\n",
      "Batch train [7] loss 0.95242, dsc 0.04758\n",
      "Batch train [8] loss 0.96548, dsc 0.03452\n",
      "Batch train [9] loss 0.96903, dsc 0.03097\n",
      "Batch train [10] loss 0.96693, dsc 0.03307\n",
      "Batch train [11] loss 0.96507, dsc 0.03493\n",
      "Batch train [12] loss 0.97499, dsc 0.02501\n",
      "Batch train [13] loss 0.95384, dsc 0.04616\n",
      "Batch train [14] loss 0.95651, dsc 0.04349\n",
      "Batch train [15] loss 0.95468, dsc 0.04532\n",
      "Batch train [16] loss 0.96143, dsc 0.03857\n",
      "Batch train [17] loss 0.96530, dsc 0.03470\n",
      "Batch train [18] loss 0.96914, dsc 0.03086\n",
      "Batch train [19] loss 0.96070, dsc 0.03930\n",
      "Batch train [20] loss 0.96140, dsc 0.03860\n",
      "Batch train [21] loss 0.95840, dsc 0.04160\n",
      "Batch train [22] loss 0.97223, dsc 0.02777\n",
      "Batch train [23] loss 0.95842, dsc 0.04158\n",
      "Batch train [24] loss 0.96060, dsc 0.03940\n",
      "Batch train [25] loss 0.95559, dsc 0.04441\n",
      "Batch train [26] loss 0.95190, dsc 0.04810\n",
      "Batch train [27] loss 0.96540, dsc 0.03460\n",
      "Batch train [28] loss 0.95480, dsc 0.04520\n",
      "Batch train [29] loss 0.96610, dsc 0.03390\n",
      "Batch train [30] loss 0.95614, dsc 0.04386\n",
      "Batch train [31] loss 0.94882, dsc 0.05118\n",
      "Batch train [32] loss 0.95541, dsc 0.04459\n",
      "Batch train [33] loss 0.94562, dsc 0.05438\n",
      "Batch train [34] loss 0.95599, dsc 0.04401\n",
      "Batch train [35] loss 0.94250, dsc 0.05750\n",
      "Batch train [36] loss 0.95472, dsc 0.04528\n",
      "Batch train [37] loss 0.95570, dsc 0.04430\n",
      "Batch train [38] loss 0.96641, dsc 0.03359\n",
      "Batch train [39] loss 0.95871, dsc 0.04129\n",
      "Batch train [40] loss 0.95793, dsc 0.04207\n",
      "Epoch [1] train done\n",
      "Batch eval [1] loss 0.95182, dsc 0.04818\n",
      "Batch eval [2] loss 0.96218, dsc 0.03782\n",
      "Batch eval [3] loss 0.96299, dsc 0.03701\n",
      "Batch eval [4] loss 0.96180, dsc 0.03820\n",
      "Batch eval [5] loss 0.95897, dsc 0.04103\n",
      "Epoch [1] valid done\n",
      "Epoch [1] T 7.33s, deltaT 7.33s, loss: train 0.96122, valid 0.95955, dsc: train 0.03878, valid 0.04045\n",
      "Batch train [1] loss 0.94802, dsc 0.05198\n",
      "Batch train [2] loss 0.95494, dsc 0.04506\n",
      "Batch train [3] loss 0.96266, dsc 0.03734\n",
      "Batch train [4] loss 0.97086, dsc 0.02914\n",
      "Batch train [5] loss 0.95813, dsc 0.04187\n",
      "Batch train [6] loss 0.95416, dsc 0.04584\n",
      "Batch train [7] loss 0.96328, dsc 0.03672\n",
      "Batch train [8] loss 0.96404, dsc 0.03596\n",
      "Batch train [9] loss 0.94344, dsc 0.05656\n",
      "Batch train [10] loss 0.94778, dsc 0.05222\n",
      "Batch train [11] loss 0.95964, dsc 0.04036\n",
      "Batch train [12] loss 0.96513, dsc 0.03487\n",
      "Batch train [13] loss 0.95136, dsc 0.04864\n",
      "Batch train [14] loss 0.95748, dsc 0.04252\n",
      "Batch train [15] loss 0.95435, dsc 0.04565\n",
      "Batch train [16] loss 0.95661, dsc 0.04339\n",
      "Batch train [17] loss 0.95201, dsc 0.04799\n",
      "Batch train [18] loss 0.94578, dsc 0.05422\n",
      "Batch train [19] loss 0.95252, dsc 0.04748\n",
      "Batch train [20] loss 0.96154, dsc 0.03846\n",
      "Batch train [21] loss 0.96874, dsc 0.03126\n",
      "Batch train [22] loss 0.96465, dsc 0.03535\n",
      "Batch train [23] loss 0.95881, dsc 0.04119\n",
      "Batch train [24] loss 0.96377, dsc 0.03623\n",
      "Batch train [25] loss 0.95264, dsc 0.04736\n",
      "Batch train [26] loss 0.94716, dsc 0.05284\n",
      "Batch train [27] loss 0.95020, dsc 0.04980\n",
      "Batch train [28] loss 0.94793, dsc 0.05207\n",
      "Batch train [29] loss 0.93757, dsc 0.06243\n",
      "Batch train [30] loss 0.94446, dsc 0.05554\n",
      "Batch train [31] loss 0.95479, dsc 0.04521\n",
      "Batch train [32] loss 0.95220, dsc 0.04780\n",
      "Batch train [33] loss 0.95446, dsc 0.04554\n",
      "Batch train [34] loss 0.95422, dsc 0.04578\n",
      "Batch train [35] loss 0.95481, dsc 0.04519\n",
      "Batch train [36] loss 0.95285, dsc 0.04715\n",
      "Batch train [37] loss 0.95233, dsc 0.04767\n",
      "Batch train [38] loss 0.93690, dsc 0.06310\n",
      "Batch train [39] loss 0.95703, dsc 0.04297\n",
      "Batch train [40] loss 0.95482, dsc 0.04518\n",
      "Epoch [2] train done\n",
      "Batch eval [1] loss 0.94530, dsc 0.05470\n",
      "Batch eval [2] loss 0.95406, dsc 0.04594\n",
      "Batch eval [3] loss 0.94815, dsc 0.05185\n",
      "Batch eval [4] loss 0.95752, dsc 0.04248\n",
      "Batch eval [5] loss 0.94570, dsc 0.05430\n",
      "Epoch [2] valid done\n",
      "Epoch [2] T 14.60s, deltaT 7.27s, loss: train 0.95460, valid 0.95015, dsc: train 0.04540, valid 0.04985\n",
      "Batch train [1] loss 0.96270, dsc 0.03730\n",
      "Batch train [2] loss 0.96075, dsc 0.03925\n",
      "Batch train [3] loss 0.95183, dsc 0.04817\n",
      "Batch train [4] loss 0.94575, dsc 0.05425\n",
      "Batch train [5] loss 0.96774, dsc 0.03226\n",
      "Batch train [6] loss 0.94292, dsc 0.05708\n",
      "Batch train [7] loss 0.93577, dsc 0.06423\n",
      "Batch train [8] loss 0.96165, dsc 0.03835\n",
      "Batch train [9] loss 0.93852, dsc 0.06148\n",
      "Batch train [10] loss 0.95300, dsc 0.04700\n",
      "Batch train [11] loss 0.95850, dsc 0.04150\n",
      "Batch train [12] loss 0.96022, dsc 0.03978\n",
      "Batch train [13] loss 0.94823, dsc 0.05177\n",
      "Batch train [14] loss 0.94970, dsc 0.05030\n",
      "Batch train [15] loss 0.94941, dsc 0.05059\n",
      "Batch train [16] loss 0.95462, dsc 0.04538\n",
      "Batch train [17] loss 0.94851, dsc 0.05149\n",
      "Batch train [18] loss 0.95293, dsc 0.04707\n",
      "Batch train [19] loss 0.95247, dsc 0.04753\n",
      "Batch train [20] loss 0.94837, dsc 0.05163\n",
      "Batch train [21] loss 0.95507, dsc 0.04493\n",
      "Batch train [22] loss 0.95249, dsc 0.04751\n",
      "Batch train [23] loss 0.95140, dsc 0.04860\n",
      "Batch train [24] loss 0.94744, dsc 0.05256\n",
      "Batch train [25] loss 0.95153, dsc 0.04847\n",
      "Batch train [26] loss 0.96081, dsc 0.03919\n",
      "Batch train [27] loss 0.94914, dsc 0.05086\n",
      "Batch train [28] loss 0.95465, dsc 0.04535\n",
      "Batch train [29] loss 0.94816, dsc 0.05184\n",
      "Batch train [30] loss 0.93758, dsc 0.06242\n",
      "Batch train [31] loss 0.94063, dsc 0.05937\n",
      "Batch train [32] loss 0.94162, dsc 0.05838\n",
      "Batch train [33] loss 0.95198, dsc 0.04802\n",
      "Batch train [34] loss 0.95620, dsc 0.04380\n",
      "Batch train [35] loss 0.93816, dsc 0.06184\n",
      "Batch train [36] loss 0.94872, dsc 0.05128\n",
      "Batch train [37] loss 0.96443, dsc 0.03557\n",
      "Batch train [38] loss 0.93128, dsc 0.06872\n",
      "Batch train [39] loss 0.94422, dsc 0.05578\n",
      "Batch train [40] loss 0.94964, dsc 0.05036\n",
      "Epoch [3] train done\n",
      "Batch eval [1] loss 0.93966, dsc 0.06034\n",
      "Batch eval [2] loss 0.94932, dsc 0.05068\n",
      "Batch eval [3] loss 0.94213, dsc 0.05787\n",
      "Batch eval [4] loss 0.95359, dsc 0.04641\n",
      "Batch eval [5] loss 0.94016, dsc 0.05984\n",
      "Epoch [3] valid done\n",
      "Epoch [3] T 21.79s, deltaT 7.19s, loss: train 0.95047, valid 0.94497, dsc: train 0.04953, valid 0.05503\n",
      "Batch train [1] loss 0.94463, dsc 0.05537\n",
      "Batch train [2] loss 0.95973, dsc 0.04027\n",
      "Batch train [3] loss 0.93796, dsc 0.06204\n",
      "Batch train [4] loss 0.95302, dsc 0.04698\n",
      "Batch train [5] loss 0.94991, dsc 0.05009\n",
      "Batch train [6] loss 0.94976, dsc 0.05024\n",
      "Batch train [7] loss 0.95155, dsc 0.04845\n",
      "Batch train [8] loss 0.94605, dsc 0.05395\n",
      "Batch train [9] loss 0.94849, dsc 0.05151\n",
      "Batch train [10] loss 0.95227, dsc 0.04773\n",
      "Batch train [11] loss 0.92982, dsc 0.07018\n",
      "Batch train [12] loss 0.94223, dsc 0.05777\n",
      "Batch train [13] loss 0.94389, dsc 0.05611\n",
      "Batch train [14] loss 0.94273, dsc 0.05727\n",
      "Batch train [15] loss 0.94424, dsc 0.05576\n",
      "Batch train [16] loss 0.93538, dsc 0.06462\n",
      "Batch train [17] loss 0.95627, dsc 0.04373\n",
      "Batch train [18] loss 0.96282, dsc 0.03718\n",
      "Batch train [19] loss 0.94476, dsc 0.05524\n",
      "Batch train [20] loss 0.93691, dsc 0.06309\n",
      "Batch train [21] loss 0.96376, dsc 0.03624\n",
      "Batch train [22] loss 0.95689, dsc 0.04311\n",
      "Batch train [23] loss 0.94512, dsc 0.05488\n",
      "Batch train [24] loss 0.95722, dsc 0.04278\n",
      "Batch train [25] loss 0.95494, dsc 0.04506\n",
      "Batch train [26] loss 0.94448, dsc 0.05552\n",
      "Batch train [27] loss 0.94795, dsc 0.05205\n",
      "Batch train [28] loss 0.94359, dsc 0.05641\n",
      "Batch train [29] loss 0.95277, dsc 0.04723\n",
      "Batch train [30] loss 0.94672, dsc 0.05328\n",
      "Batch train [31] loss 0.92659, dsc 0.07341\n",
      "Batch train [32] loss 0.92968, dsc 0.07032\n",
      "Batch train [33] loss 0.95257, dsc 0.04743\n",
      "Batch train [34] loss 0.93166, dsc 0.06834\n",
      "Batch train [35] loss 0.94579, dsc 0.05421\n",
      "Batch train [36] loss 0.94690, dsc 0.05310\n",
      "Batch train [37] loss 0.94382, dsc 0.05618\n",
      "Batch train [38] loss 0.93538, dsc 0.06462\n",
      "Batch train [39] loss 0.93590, dsc 0.06410\n",
      "Batch train [40] loss 0.94478, dsc 0.05522\n",
      "Epoch [4] train done\n",
      "Batch eval [1] loss 0.94507, dsc 0.05493\n",
      "Batch eval [2] loss 0.95563, dsc 0.04437\n",
      "Batch eval [3] loss 0.95579, dsc 0.04421\n",
      "Batch eval [4] loss 0.95935, dsc 0.04065\n",
      "Batch eval [5] loss 0.94724, dsc 0.05276\n",
      "Epoch [4] valid done\n",
      "Epoch [4] T 29.11s, deltaT 7.31s, loss: train 0.94597, valid 0.95261, dsc: train 0.05403, valid 0.04739\n",
      "Batch train [1] loss 0.94554, dsc 0.05446\n",
      "Batch train [2] loss 0.94101, dsc 0.05899\n",
      "Batch train [3] loss 0.96145, dsc 0.03855\n",
      "Batch train [4] loss 0.93159, dsc 0.06841\n",
      "Batch train [5] loss 0.93389, dsc 0.06611\n",
      "Batch train [6] loss 0.94563, dsc 0.05437\n",
      "Batch train [7] loss 0.94402, dsc 0.05598\n",
      "Batch train [8] loss 0.94520, dsc 0.05480\n",
      "Batch train [9] loss 0.93736, dsc 0.06264\n",
      "Batch train [10] loss 0.95218, dsc 0.04782\n",
      "Batch train [11] loss 0.94644, dsc 0.05356\n",
      "Batch train [12] loss 0.94768, dsc 0.05232\n",
      "Batch train [13] loss 0.94112, dsc 0.05888\n",
      "Batch train [14] loss 0.94976, dsc 0.05024\n",
      "Batch train [15] loss 0.94931, dsc 0.05069\n",
      "Batch train [16] loss 0.93332, dsc 0.06668\n",
      "Batch train [17] loss 0.93779, dsc 0.06221\n",
      "Batch train [18] loss 0.94241, dsc 0.05759\n",
      "Batch train [19] loss 0.92823, dsc 0.07177\n",
      "Batch train [20] loss 0.94594, dsc 0.05406\n",
      "Batch train [21] loss 0.95367, dsc 0.04633\n",
      "Batch train [22] loss 0.92769, dsc 0.07231\n",
      "Batch train [23] loss 0.92349, dsc 0.07651\n",
      "Batch train [24] loss 0.93071, dsc 0.06929\n",
      "Batch train [25] loss 0.95084, dsc 0.04916\n",
      "Batch train [26] loss 0.93530, dsc 0.06470\n",
      "Batch train [27] loss 0.91869, dsc 0.08131\n",
      "Batch train [28] loss 0.94134, dsc 0.05866\n",
      "Batch train [29] loss 0.95233, dsc 0.04767\n",
      "Batch train [30] loss 0.95151, dsc 0.04849\n",
      "Batch train [31] loss 0.95755, dsc 0.04245\n",
      "Batch train [32] loss 0.93946, dsc 0.06054\n",
      "Batch train [33] loss 0.93572, dsc 0.06428\n",
      "Batch train [34] loss 0.93692, dsc 0.06308\n",
      "Batch train [35] loss 0.93634, dsc 0.06366\n",
      "Batch train [36] loss 0.93718, dsc 0.06282\n",
      "Batch train [37] loss 0.93947, dsc 0.06053\n",
      "Batch train [38] loss 0.93319, dsc 0.06681\n",
      "Batch train [39] loss 0.94062, dsc 0.05938\n",
      "Batch train [40] loss 0.91638, dsc 0.08362\n",
      "Epoch [5] train done\n",
      "Batch eval [1] loss 0.92158, dsc 0.07842\n",
      "Batch eval [2] loss 0.93437, dsc 0.06563\n",
      "Batch eval [3] loss 0.92544, dsc 0.07456\n",
      "Batch eval [4] loss 0.93966, dsc 0.06034\n",
      "Batch eval [5] loss 0.92357, dsc 0.07643\n",
      "Epoch [5] valid done\n",
      "Epoch [5] T 36.35s, deltaT 7.24s, loss: train 0.94046, valid 0.92893, dsc: train 0.05954, valid 0.07107\n",
      "Batch train [1] loss 0.92002, dsc 0.07998\n",
      "Batch train [2] loss 0.91563, dsc 0.08437\n",
      "Batch train [3] loss 0.91568, dsc 0.08432\n",
      "Batch train [4] loss 0.94553, dsc 0.05447\n",
      "Batch train [5] loss 0.94028, dsc 0.05972\n",
      "Batch train [6] loss 0.92198, dsc 0.07802\n",
      "Batch train [7] loss 0.93089, dsc 0.06911\n",
      "Batch train [8] loss 0.93117, dsc 0.06883\n",
      "Batch train [9] loss 0.94221, dsc 0.05779\n",
      "Batch train [10] loss 0.93708, dsc 0.06292\n",
      "Batch train [11] loss 0.93202, dsc 0.06798\n",
      "Batch train [12] loss 0.92501, dsc 0.07499\n",
      "Batch train [13] loss 0.95631, dsc 0.04369\n",
      "Batch train [14] loss 0.93458, dsc 0.06542\n",
      "Batch train [15] loss 0.92298, dsc 0.07702\n",
      "Batch train [16] loss 0.92649, dsc 0.07351\n",
      "Batch train [17] loss 0.93179, dsc 0.06821\n",
      "Batch train [18] loss 0.94391, dsc 0.05609\n",
      "Batch train [19] loss 0.93579, dsc 0.06421\n",
      "Batch train [20] loss 0.93258, dsc 0.06742\n",
      "Batch train [21] loss 0.92354, dsc 0.07646\n",
      "Batch train [22] loss 0.94713, dsc 0.05287\n",
      "Batch train [23] loss 0.93366, dsc 0.06634\n",
      "Batch train [24] loss 0.93070, dsc 0.06930\n",
      "Batch train [25] loss 0.94764, dsc 0.05236\n",
      "Batch train [26] loss 0.93405, dsc 0.06595\n",
      "Batch train [27] loss 0.93584, dsc 0.06416\n",
      "Batch train [28] loss 0.95343, dsc 0.04657\n",
      "Batch train [29] loss 0.93415, dsc 0.06585\n",
      "Batch train [30] loss 0.94672, dsc 0.05328\n",
      "Batch train [31] loss 0.92977, dsc 0.07023\n",
      "Batch train [32] loss 0.93442, dsc 0.06558\n",
      "Batch train [33] loss 0.93365, dsc 0.06635\n",
      "Batch train [34] loss 0.91738, dsc 0.08262\n",
      "Batch train [35] loss 0.93784, dsc 0.06216\n",
      "Batch train [36] loss 0.94313, dsc 0.05687\n",
      "Batch train [37] loss 0.93345, dsc 0.06655\n",
      "Batch train [38] loss 0.94235, dsc 0.05765\n",
      "Batch train [39] loss 0.92605, dsc 0.07395\n",
      "Batch train [40] loss 0.93559, dsc 0.06441\n",
      "Epoch [6] train done\n",
      "Batch eval [1] loss 0.91905, dsc 0.08095\n",
      "Batch eval [2] loss 0.93109, dsc 0.06891\n",
      "Batch eval [3] loss 0.92152, dsc 0.07848\n",
      "Batch eval [4] loss 0.93882, dsc 0.06118\n",
      "Batch eval [5] loss 0.92135, dsc 0.07865\n",
      "Epoch [6] valid done\n",
      "Epoch [6] T 43.62s, deltaT 7.28s, loss: train 0.93406, valid 0.92637, dsc: train 0.06594, valid 0.07363\n",
      "Batch train [1] loss 0.91120, dsc 0.08880\n",
      "Batch train [2] loss 0.92734, dsc 0.07266\n",
      "Batch train [3] loss 0.94459, dsc 0.05541\n",
      "Batch train [4] loss 0.91922, dsc 0.08078\n",
      "Batch train [5] loss 0.93215, dsc 0.06785\n",
      "Batch train [6] loss 0.94439, dsc 0.05561\n",
      "Batch train [7] loss 0.93268, dsc 0.06732\n",
      "Batch train [8] loss 0.95068, dsc 0.04932\n",
      "Batch train [9] loss 0.93127, dsc 0.06873\n",
      "Batch train [10] loss 0.91272, dsc 0.08728\n",
      "Batch train [11] loss 0.92172, dsc 0.07828\n",
      "Batch train [12] loss 0.94232, dsc 0.05768\n",
      "Batch train [13] loss 0.91653, dsc 0.08347\n",
      "Batch train [14] loss 0.92338, dsc 0.07662\n",
      "Batch train [15] loss 0.90342, dsc 0.09658\n",
      "Batch train [16] loss 0.92344, dsc 0.07656\n",
      "Batch train [17] loss 0.93668, dsc 0.06332\n",
      "Batch train [18] loss 0.91110, dsc 0.08890\n",
      "Batch train [19] loss 0.92758, dsc 0.07242\n",
      "Batch train [20] loss 0.92377, dsc 0.07623\n",
      "Batch train [21] loss 0.92689, dsc 0.07311\n",
      "Batch train [22] loss 0.91530, dsc 0.08470\n",
      "Batch train [23] loss 0.92269, dsc 0.07731\n",
      "Batch train [24] loss 0.93863, dsc 0.06137\n",
      "Batch train [25] loss 0.91873, dsc 0.08127\n",
      "Batch train [26] loss 0.93143, dsc 0.06857\n",
      "Batch train [27] loss 0.92580, dsc 0.07420\n",
      "Batch train [28] loss 0.89937, dsc 0.10063\n",
      "Batch train [29] loss 0.93390, dsc 0.06610\n",
      "Batch train [30] loss 0.92628, dsc 0.07372\n",
      "Batch train [31] loss 0.90823, dsc 0.09177\n",
      "Batch train [32] loss 0.92202, dsc 0.07798\n",
      "Batch train [33] loss 0.92193, dsc 0.07807\n",
      "Batch train [34] loss 0.92912, dsc 0.07088\n",
      "Batch train [35] loss 0.92786, dsc 0.07214\n",
      "Batch train [36] loss 0.92354, dsc 0.07646\n",
      "Batch train [37] loss 0.94659, dsc 0.05341\n",
      "Batch train [38] loss 0.91472, dsc 0.08528\n",
      "Batch train [39] loss 0.93368, dsc 0.06632\n",
      "Batch train [40] loss 0.91832, dsc 0.08168\n",
      "Epoch [7] train done\n",
      "Batch eval [1] loss 0.90753, dsc 0.09247\n",
      "Batch eval [2] loss 0.91843, dsc 0.08157\n",
      "Batch eval [3] loss 0.90743, dsc 0.09257\n",
      "Batch eval [4] loss 0.92634, dsc 0.07366\n",
      "Batch eval [5] loss 0.90798, dsc 0.09202\n",
      "Epoch [7] valid done\n",
      "Epoch [7] T 50.88s, deltaT 7.26s, loss: train 0.92553, valid 0.91354, dsc: train 0.07447, valid 0.08646\n",
      "Batch train [1] loss 0.91919, dsc 0.08081\n",
      "Batch train [2] loss 0.91327, dsc 0.08673\n",
      "Batch train [3] loss 0.93610, dsc 0.06390\n",
      "Batch train [4] loss 0.92004, dsc 0.07996\n",
      "Batch train [5] loss 0.91346, dsc 0.08654\n",
      "Batch train [6] loss 0.92910, dsc 0.07090\n",
      "Batch train [7] loss 0.92060, dsc 0.07940\n",
      "Batch train [8] loss 0.90325, dsc 0.09675\n",
      "Batch train [9] loss 0.91506, dsc 0.08494\n",
      "Batch train [10] loss 0.91943, dsc 0.08057\n",
      "Batch train [11] loss 0.92494, dsc 0.07506\n",
      "Batch train [12] loss 0.92082, dsc 0.07918\n",
      "Batch train [13] loss 0.91631, dsc 0.08369\n",
      "Batch train [14] loss 0.90427, dsc 0.09573\n",
      "Batch train [15] loss 0.92763, dsc 0.07237\n",
      "Batch train [16] loss 0.90480, dsc 0.09520\n",
      "Batch train [17] loss 0.90254, dsc 0.09746\n",
      "Batch train [18] loss 0.92987, dsc 0.07013\n",
      "Batch train [19] loss 0.90945, dsc 0.09055\n",
      "Batch train [20] loss 0.91628, dsc 0.08372\n",
      "Batch train [21] loss 0.94177, dsc 0.05823\n",
      "Batch train [22] loss 0.93969, dsc 0.06031\n",
      "Batch train [23] loss 0.91037, dsc 0.08963\n",
      "Batch train [24] loss 0.90604, dsc 0.09396\n",
      "Batch train [25] loss 0.90788, dsc 0.09212\n",
      "Batch train [26] loss 0.92626, dsc 0.07374\n",
      "Batch train [27] loss 0.92974, dsc 0.07026\n",
      "Batch train [28] loss 0.88381, dsc 0.11619\n",
      "Batch train [29] loss 0.91289, dsc 0.08711\n",
      "Batch train [30] loss 0.92830, dsc 0.07170\n",
      "Batch train [31] loss 0.91414, dsc 0.08586\n",
      "Batch train [32] loss 0.89040, dsc 0.10960\n",
      "Batch train [33] loss 0.90771, dsc 0.09229\n",
      "Batch train [34] loss 0.90665, dsc 0.09335\n",
      "Batch train [35] loss 0.91523, dsc 0.08477\n",
      "Batch train [36] loss 0.91645, dsc 0.08355\n",
      "Batch train [37] loss 0.88398, dsc 0.11602\n",
      "Batch train [38] loss 0.90948, dsc 0.09052\n",
      "Batch train [39] loss 0.88809, dsc 0.11191\n",
      "Batch train [40] loss 0.87499, dsc 0.12501\n",
      "Epoch [8] train done\n",
      "Batch eval [1] loss 0.89638, dsc 0.10362\n",
      "Batch eval [2] loss 0.90835, dsc 0.09165\n",
      "Batch eval [3] loss 0.90048, dsc 0.09952\n",
      "Batch eval [4] loss 0.91806, dsc 0.08194\n",
      "Batch eval [5] loss 0.89568, dsc 0.10432\n",
      "Epoch [8] valid done\n",
      "Epoch [8] T 58.15s, deltaT 7.26s, loss: train 0.91351, valid 0.90379, dsc: train 0.08649, valid 0.09621\n",
      "Batch train [1] loss 0.90807, dsc 0.09193\n",
      "Batch train [2] loss 0.90037, dsc 0.09963\n",
      "Batch train [3] loss 0.89575, dsc 0.10425\n",
      "Batch train [4] loss 0.91331, dsc 0.08669\n",
      "Batch train [5] loss 0.90728, dsc 0.09272\n",
      "Batch train [6] loss 0.90619, dsc 0.09381\n",
      "Batch train [7] loss 0.89050, dsc 0.10950\n",
      "Batch train [8] loss 0.87083, dsc 0.12917\n",
      "Batch train [9] loss 0.91090, dsc 0.08910\n",
      "Batch train [10] loss 0.93135, dsc 0.06865\n",
      "Batch train [11] loss 0.90298, dsc 0.09702\n",
      "Batch train [12] loss 0.89694, dsc 0.10306\n",
      "Batch train [13] loss 0.91362, dsc 0.08638\n",
      "Batch train [14] loss 0.91628, dsc 0.08372\n",
      "Batch train [15] loss 0.87413, dsc 0.12587\n",
      "Batch train [16] loss 0.89467, dsc 0.10533\n",
      "Batch train [17] loss 0.91617, dsc 0.08383\n",
      "Batch train [18] loss 0.89583, dsc 0.10417\n",
      "Batch train [19] loss 0.91930, dsc 0.08070\n",
      "Batch train [20] loss 0.88205, dsc 0.11795\n",
      "Batch train [21] loss 0.88929, dsc 0.11071\n",
      "Batch train [22] loss 0.89241, dsc 0.10759\n",
      "Batch train [23] loss 0.91685, dsc 0.08315\n",
      "Batch train [24] loss 0.86136, dsc 0.13864\n",
      "Batch train [25] loss 0.88829, dsc 0.11171\n",
      "Batch train [26] loss 0.89725, dsc 0.10275\n",
      "Batch train [27] loss 0.88441, dsc 0.11559\n",
      "Batch train [28] loss 0.89389, dsc 0.10611\n",
      "Batch train [29] loss 0.92614, dsc 0.07386\n",
      "Batch train [30] loss 0.89089, dsc 0.10911\n",
      "Batch train [31] loss 0.86941, dsc 0.13059\n",
      "Batch train [32] loss 0.87542, dsc 0.12458\n",
      "Batch train [33] loss 0.89791, dsc 0.10209\n",
      "Batch train [34] loss 0.90386, dsc 0.09614\n",
      "Batch train [35] loss 0.86678, dsc 0.13322\n",
      "Batch train [36] loss 0.88865, dsc 0.11135\n",
      "Batch train [37] loss 0.91203, dsc 0.08797\n",
      "Batch train [38] loss 0.89126, dsc 0.10874\n",
      "Batch train [39] loss 0.86232, dsc 0.13768\n",
      "Batch train [40] loss 0.88468, dsc 0.11532\n",
      "Epoch [9] train done\n",
      "Batch eval [1] loss 0.86522, dsc 0.13478\n",
      "Batch eval [2] loss 0.88034, dsc 0.11966\n",
      "Batch eval [3] loss 0.86530, dsc 0.13470\n",
      "Batch eval [4] loss 0.89381, dsc 0.10619\n",
      "Batch eval [5] loss 0.86628, dsc 0.13372\n",
      "Epoch [9] valid done\n",
      "Epoch [9] T 65.43s, deltaT 7.28s, loss: train 0.89599, valid 0.87419, dsc: train 0.10401, valid 0.12581\n",
      "Batch train [1] loss 0.90685, dsc 0.09315\n",
      "Batch train [2] loss 0.87810, dsc 0.12190\n",
      "Batch train [3] loss 0.91808, dsc 0.08192\n",
      "Batch train [4] loss 0.86596, dsc 0.13404\n",
      "Batch train [5] loss 0.91887, dsc 0.08113\n",
      "Batch train [6] loss 0.86098, dsc 0.13902\n",
      "Batch train [7] loss 0.86543, dsc 0.13457\n",
      "Batch train [8] loss 0.85416, dsc 0.14584\n",
      "Batch train [9] loss 0.87418, dsc 0.12582\n",
      "Batch train [10] loss 0.87896, dsc 0.12104\n",
      "Batch train [11] loss 0.88010, dsc 0.11990\n",
      "Batch train [12] loss 0.87058, dsc 0.12942\n",
      "Batch train [13] loss 0.87561, dsc 0.12439\n",
      "Batch train [14] loss 0.86309, dsc 0.13691\n",
      "Batch train [15] loss 0.88126, dsc 0.11874\n",
      "Batch train [16] loss 0.88592, dsc 0.11408\n",
      "Batch train [17] loss 0.86829, dsc 0.13171\n",
      "Batch train [18] loss 0.88012, dsc 0.11988\n",
      "Batch train [19] loss 0.84773, dsc 0.15227\n",
      "Batch train [20] loss 0.86265, dsc 0.13735\n",
      "Batch train [21] loss 0.88694, dsc 0.11306\n",
      "Batch train [22] loss 0.86724, dsc 0.13276\n",
      "Batch train [23] loss 0.85799, dsc 0.14201\n",
      "Batch train [24] loss 0.83046, dsc 0.16954\n",
      "Batch train [25] loss 0.85219, dsc 0.14781\n",
      "Batch train [26] loss 0.87807, dsc 0.12193\n",
      "Batch train [27] loss 0.81324, dsc 0.18676\n",
      "Batch train [28] loss 0.84159, dsc 0.15841\n",
      "Batch train [29] loss 0.85930, dsc 0.14070\n",
      "Batch train [30] loss 0.85502, dsc 0.14498\n",
      "Batch train [31] loss 0.80605, dsc 0.19395\n",
      "Batch train [32] loss 0.87921, dsc 0.12079\n",
      "Batch train [33] loss 0.86049, dsc 0.13951\n",
      "Batch train [34] loss 0.86460, dsc 0.13540\n",
      "Batch train [35] loss 0.84835, dsc 0.15165\n",
      "Batch train [36] loss 0.80625, dsc 0.19375\n",
      "Batch train [37] loss 0.83799, dsc 0.16201\n",
      "Batch train [38] loss 0.83158, dsc 0.16842\n",
      "Batch train [39] loss 0.83226, dsc 0.16774\n",
      "Batch train [40] loss 0.86914, dsc 0.13086\n",
      "Epoch [10] train done\n",
      "Batch eval [1] loss 0.81754, dsc 0.18246\n",
      "Batch eval [2] loss 0.83754, dsc 0.16246\n",
      "Batch eval [3] loss 0.81818, dsc 0.18182\n",
      "Batch eval [4] loss 0.85461, dsc 0.14539\n",
      "Batch eval [5] loss 0.81753, dsc 0.18247\n",
      "Epoch [10] valid done\n",
      "Epoch [10] T 72.65s, deltaT 7.22s, loss: train 0.86287, valid 0.82908, dsc: train 0.13713, valid 0.17092\n",
      "Batch train [1] loss 0.81349, dsc 0.18651\n",
      "Batch train [2] loss 0.85098, dsc 0.14902\n",
      "Batch train [3] loss 0.80063, dsc 0.19937\n",
      "Batch train [4] loss 0.83672, dsc 0.16328\n",
      "Batch train [5] loss 0.88268, dsc 0.11732\n",
      "Batch train [6] loss 0.83329, dsc 0.16671\n",
      "Batch train [7] loss 0.82924, dsc 0.17076\n",
      "Batch train [8] loss 0.80741, dsc 0.19259\n",
      "Batch train [9] loss 0.85171, dsc 0.14829\n",
      "Batch train [10] loss 0.80978, dsc 0.19022\n",
      "Batch train [11] loss 0.78129, dsc 0.21871\n",
      "Batch train [12] loss 0.84524, dsc 0.15476\n",
      "Batch train [13] loss 0.80768, dsc 0.19232\n",
      "Batch train [14] loss 0.83818, dsc 0.16182\n",
      "Batch train [15] loss 0.76452, dsc 0.23548\n",
      "Batch train [16] loss 0.81460, dsc 0.18540\n",
      "Batch train [17] loss 0.79976, dsc 0.20024\n",
      "Batch train [18] loss 0.82156, dsc 0.17844\n",
      "Batch train [19] loss 0.80143, dsc 0.19857\n",
      "Batch train [20] loss 0.80672, dsc 0.19328\n",
      "Batch train [21] loss 0.80261, dsc 0.19739\n",
      "Batch train [22] loss 0.81055, dsc 0.18945\n",
      "Batch train [23] loss 0.78385, dsc 0.21615\n",
      "Batch train [24] loss 0.81890, dsc 0.18110\n",
      "Batch train [25] loss 0.79673, dsc 0.20327\n",
      "Batch train [26] loss 0.84697, dsc 0.15303\n",
      "Batch train [27] loss 0.82502, dsc 0.17498\n",
      "Batch train [28] loss 0.82234, dsc 0.17766\n",
      "Batch train [29] loss 0.78150, dsc 0.21850\n",
      "Batch train [30] loss 0.75802, dsc 0.24198\n",
      "Batch train [31] loss 0.74646, dsc 0.25354\n",
      "Batch train [32] loss 0.77758, dsc 0.22242\n",
      "Batch train [33] loss 0.77785, dsc 0.22215\n",
      "Batch train [34] loss 0.71939, dsc 0.28061\n",
      "Batch train [35] loss 0.73702, dsc 0.26298\n",
      "Batch train [36] loss 0.75549, dsc 0.24451\n",
      "Batch train [37] loss 0.77476, dsc 0.22524\n",
      "Batch train [38] loss 0.76646, dsc 0.23354\n",
      "Batch train [39] loss 0.80563, dsc 0.19437\n",
      "Batch train [40] loss 0.75143, dsc 0.24857\n",
      "Epoch [11] train done\n",
      "Batch eval [1] loss 0.75639, dsc 0.24361\n",
      "Batch eval [2] loss 0.79414, dsc 0.20586\n",
      "Batch eval [3] loss 0.83114, dsc 0.16886\n",
      "Batch eval [4] loss 0.80901, dsc 0.19099\n",
      "Batch eval [5] loss 0.75978, dsc 0.24022\n",
      "Epoch [11] valid done\n",
      "Epoch [11] T 79.93s, deltaT 7.28s, loss: train 0.80139, valid 0.79009, dsc: train 0.19861, valid 0.20991\n",
      "Batch train [1] loss 0.70076, dsc 0.29924\n",
      "Batch train [2] loss 0.73995, dsc 0.26005\n",
      "Batch train [3] loss 0.72797, dsc 0.27203\n",
      "Batch train [4] loss 0.81820, dsc 0.18180\n",
      "Batch train [5] loss 0.74199, dsc 0.25801\n",
      "Batch train [6] loss 0.75045, dsc 0.24955\n",
      "Batch train [7] loss 0.75497, dsc 0.24503\n",
      "Batch train [8] loss 0.72762, dsc 0.27238\n",
      "Batch train [9] loss 0.74922, dsc 0.25078\n",
      "Batch train [10] loss 0.71746, dsc 0.28254\n",
      "Batch train [11] loss 0.72621, dsc 0.27379\n",
      "Batch train [12] loss 0.75814, dsc 0.24186\n",
      "Batch train [13] loss 0.72219, dsc 0.27781\n",
      "Batch train [14] loss 0.72506, dsc 0.27494\n",
      "Batch train [15] loss 0.68837, dsc 0.31163\n",
      "Batch train [16] loss 0.77160, dsc 0.22840\n",
      "Batch train [17] loss 0.75076, dsc 0.24924\n",
      "Batch train [18] loss 0.69485, dsc 0.30515\n",
      "Batch train [19] loss 0.65682, dsc 0.34318\n",
      "Batch train [20] loss 0.72187, dsc 0.27813\n",
      "Batch train [21] loss 0.71318, dsc 0.28682\n",
      "Batch train [22] loss 0.75008, dsc 0.24992\n",
      "Batch train [23] loss 0.74309, dsc 0.25691\n",
      "Batch train [24] loss 0.74440, dsc 0.25560\n",
      "Batch train [25] loss 0.75175, dsc 0.24825\n",
      "Batch train [26] loss 0.74983, dsc 0.25017\n",
      "Batch train [27] loss 0.69955, dsc 0.30045\n",
      "Batch train [28] loss 0.65846, dsc 0.34154\n",
      "Batch train [29] loss 0.71708, dsc 0.28292\n",
      "Batch train [30] loss 0.65467, dsc 0.34533\n",
      "Batch train [31] loss 0.70004, dsc 0.29996\n",
      "Batch train [32] loss 0.69867, dsc 0.30133\n",
      "Batch train [33] loss 0.69072, dsc 0.30928\n",
      "Batch train [34] loss 0.68115, dsc 0.31885\n",
      "Batch train [35] loss 0.67768, dsc 0.32232\n",
      "Batch train [36] loss 0.73305, dsc 0.26695\n",
      "Batch train [37] loss 0.68341, dsc 0.31659\n",
      "Batch train [38] loss 0.63685, dsc 0.36315\n",
      "Batch train [39] loss 0.76443, dsc 0.23557\n",
      "Batch train [40] loss 0.68245, dsc 0.31755\n",
      "Epoch [12] train done\n",
      "Batch eval [1] loss 0.65168, dsc 0.34832\n",
      "Batch eval [2] loss 0.68578, dsc 0.31422\n",
      "Batch eval [3] loss 0.65750, dsc 0.34250\n",
      "Batch eval [4] loss 0.71021, dsc 0.28979\n",
      "Batch eval [5] loss 0.64634, dsc 0.35366\n",
      "Epoch [12] valid done\n",
      "Epoch [12] T 87.18s, deltaT 7.26s, loss: train 0.71937, valid 0.67030, dsc: train 0.28063, valid 0.32970\n",
      "Batch train [1] loss 0.70206, dsc 0.29794\n",
      "Batch train [2] loss 0.62388, dsc 0.37612\n",
      "Batch train [3] loss 0.70240, dsc 0.29760\n",
      "Batch train [4] loss 0.62180, dsc 0.37820\n",
      "Batch train [5] loss 0.64197, dsc 0.35803\n",
      "Batch train [6] loss 0.70583, dsc 0.29417\n",
      "Batch train [7] loss 0.65942, dsc 0.34058\n",
      "Batch train [8] loss 0.65764, dsc 0.34236\n",
      "Batch train [9] loss 0.63795, dsc 0.36205\n",
      "Batch train [10] loss 0.64778, dsc 0.35222\n",
      "Batch train [11] loss 0.63399, dsc 0.36601\n",
      "Batch train [12] loss 0.59750, dsc 0.40250\n",
      "Batch train [13] loss 0.66577, dsc 0.33423\n",
      "Batch train [14] loss 0.63674, dsc 0.36326\n",
      "Batch train [15] loss 0.66738, dsc 0.33262\n",
      "Batch train [16] loss 0.72548, dsc 0.27452\n",
      "Batch train [17] loss 0.64498, dsc 0.35502\n",
      "Batch train [18] loss 0.60514, dsc 0.39486\n",
      "Batch train [19] loss 0.63060, dsc 0.36940\n",
      "Batch train [20] loss 0.61679, dsc 0.38321\n",
      "Batch train [21] loss 0.65255, dsc 0.34745\n",
      "Batch train [22] loss 0.67429, dsc 0.32571\n",
      "Batch train [23] loss 0.64586, dsc 0.35414\n",
      "Batch train [24] loss 0.59955, dsc 0.40045\n",
      "Batch train [25] loss 0.62732, dsc 0.37268\n",
      "Batch train [26] loss 0.62151, dsc 0.37849\n",
      "Batch train [27] loss 0.59098, dsc 0.40902\n",
      "Batch train [28] loss 0.65467, dsc 0.34533\n",
      "Batch train [29] loss 0.70640, dsc 0.29360\n",
      "Batch train [30] loss 0.60269, dsc 0.39731\n",
      "Batch train [31] loss 0.64844, dsc 0.35156\n",
      "Batch train [32] loss 0.59384, dsc 0.40616\n",
      "Batch train [33] loss 0.58296, dsc 0.41704\n",
      "Batch train [34] loss 0.58027, dsc 0.41973\n",
      "Batch train [35] loss 0.59583, dsc 0.40417\n",
      "Batch train [36] loss 0.53490, dsc 0.46510\n",
      "Batch train [37] loss 0.53492, dsc 0.46508\n",
      "Batch train [38] loss 0.49974, dsc 0.50026\n",
      "Batch train [39] loss 0.54077, dsc 0.45923\n",
      "Batch train [40] loss 0.59538, dsc 0.40462\n",
      "Epoch [13] train done\n",
      "Batch eval [1] loss 0.53183, dsc 0.46817\n",
      "Batch eval [2] loss 0.56614, dsc 0.43386\n",
      "Batch eval [3] loss 0.52967, dsc 0.47033\n",
      "Batch eval [4] loss 0.60110, dsc 0.39890\n",
      "Batch eval [5] loss 0.52910, dsc 0.47090\n",
      "Epoch [13] valid done\n",
      "Epoch [13] T 94.46s, deltaT 7.27s, loss: train 0.62770, valid 0.55157, dsc: train 0.37230, valid 0.44843\n",
      "Batch train [1] loss 0.55565, dsc 0.44435\n",
      "Batch train [2] loss 0.58153, dsc 0.41847\n",
      "Batch train [3] loss 0.49291, dsc 0.50709\n",
      "Batch train [4] loss 0.60791, dsc 0.39209\n",
      "Batch train [5] loss 0.50981, dsc 0.49019\n",
      "Batch train [6] loss 0.51891, dsc 0.48109\n",
      "Batch train [7] loss 0.54212, dsc 0.45788\n",
      "Batch train [8] loss 0.54361, dsc 0.45639\n",
      "Batch train [9] loss 0.53769, dsc 0.46231\n",
      "Batch train [10] loss 0.60249, dsc 0.39751\n",
      "Batch train [11] loss 0.51664, dsc 0.48336\n",
      "Batch train [12] loss 0.54571, dsc 0.45429\n",
      "Batch train [13] loss 0.60453, dsc 0.39547\n",
      "Batch train [14] loss 0.47957, dsc 0.52043\n",
      "Batch train [15] loss 0.58486, dsc 0.41514\n",
      "Batch train [16] loss 0.49721, dsc 0.50279\n",
      "Batch train [17] loss 0.58365, dsc 0.41635\n",
      "Batch train [18] loss 0.52635, dsc 0.47365\n",
      "Batch train [19] loss 0.46928, dsc 0.53072\n",
      "Batch train [20] loss 0.52344, dsc 0.47656\n",
      "Batch train [21] loss 0.52470, dsc 0.47530\n",
      "Batch train [22] loss 0.51366, dsc 0.48634\n",
      "Batch train [23] loss 0.50191, dsc 0.49809\n",
      "Batch train [24] loss 0.51003, dsc 0.48997\n",
      "Batch train [25] loss 0.53614, dsc 0.46386\n",
      "Batch train [26] loss 0.61058, dsc 0.38942\n",
      "Batch train [27] loss 0.52406, dsc 0.47594\n",
      "Batch train [28] loss 0.53819, dsc 0.46181\n",
      "Batch train [29] loss 0.51728, dsc 0.48272\n",
      "Batch train [30] loss 0.47960, dsc 0.52040\n",
      "Batch train [31] loss 0.51707, dsc 0.48293\n",
      "Batch train [32] loss 0.52405, dsc 0.47595\n",
      "Batch train [33] loss 0.49050, dsc 0.50950\n",
      "Batch train [34] loss 0.59758, dsc 0.40242\n",
      "Batch train [35] loss 0.50967, dsc 0.49033\n",
      "Batch train [36] loss 0.52278, dsc 0.47722\n",
      "Batch train [37] loss 0.47023, dsc 0.52977\n",
      "Batch train [38] loss 0.42212, dsc 0.57788\n",
      "Batch train [39] loss 0.48511, dsc 0.51489\n",
      "Batch train [40] loss 0.48685, dsc 0.51315\n",
      "Epoch [14] train done\n",
      "Batch eval [1] loss 0.47554, dsc 0.52446\n",
      "Batch eval [2] loss 0.51018, dsc 0.48982\n",
      "Batch eval [3] loss 0.47187, dsc 0.52813\n",
      "Batch eval [4] loss 0.55455, dsc 0.44545\n",
      "Batch eval [5] loss 0.47492, dsc 0.52508\n",
      "Epoch [14] valid done\n",
      "Epoch [14] T 101.67s, deltaT 7.21s, loss: train 0.52765, valid 0.49741, dsc: train 0.47235, valid 0.50259\n",
      "Batch train [1] loss 0.60531, dsc 0.39469\n",
      "Batch train [2] loss 0.43643, dsc 0.56357\n",
      "Batch train [3] loss 0.42028, dsc 0.57972\n",
      "Batch train [4] loss 0.51614, dsc 0.48386\n",
      "Batch train [5] loss 0.49473, dsc 0.50527\n",
      "Batch train [6] loss 0.44754, dsc 0.55246\n",
      "Batch train [7] loss 0.46640, dsc 0.53360\n",
      "Batch train [8] loss 0.47074, dsc 0.52926\n",
      "Batch train [9] loss 0.41746, dsc 0.58254\n",
      "Batch train [10] loss 0.49598, dsc 0.50402\n",
      "Batch train [11] loss 0.48754, dsc 0.51246\n",
      "Batch train [12] loss 0.44416, dsc 0.55584\n",
      "Batch train [13] loss 0.45693, dsc 0.54307\n",
      "Batch train [14] loss 0.46441, dsc 0.53559\n",
      "Batch train [15] loss 0.42278, dsc 0.57722\n",
      "Batch train [16] loss 0.44826, dsc 0.55174\n",
      "Batch train [17] loss 0.41889, dsc 0.58111\n",
      "Batch train [18] loss 0.44472, dsc 0.55528\n",
      "Batch train [19] loss 0.47380, dsc 0.52620\n",
      "Batch train [20] loss 0.44600, dsc 0.55400\n",
      "Batch train [21] loss 0.44320, dsc 0.55680\n",
      "Batch train [22] loss 0.47419, dsc 0.52581\n",
      "Batch train [23] loss 0.43463, dsc 0.56537\n",
      "Batch train [24] loss 0.36198, dsc 0.63802\n",
      "Batch train [25] loss 0.42466, dsc 0.57534\n",
      "Batch train [26] loss 0.47761, dsc 0.52239\n",
      "Batch train [27] loss 0.44222, dsc 0.55778\n",
      "Batch train [28] loss 0.38239, dsc 0.61761\n",
      "Batch train [29] loss 0.42351, dsc 0.57649\n",
      "Batch train [30] loss 0.36315, dsc 0.63685\n",
      "Batch train [31] loss 0.38335, dsc 0.61665\n",
      "Batch train [32] loss 0.41606, dsc 0.58394\n",
      "Batch train [33] loss 0.51335, dsc 0.48665\n",
      "Batch train [34] loss 0.40255, dsc 0.59745\n",
      "Batch train [35] loss 0.45582, dsc 0.54418\n",
      "Batch train [36] loss 0.39370, dsc 0.60630\n",
      "Batch train [37] loss 0.38873, dsc 0.61127\n",
      "Batch train [38] loss 0.45190, dsc 0.54810\n",
      "Batch train [39] loss 0.39651, dsc 0.60349\n",
      "Batch train [40] loss 0.36906, dsc 0.63094\n",
      "Epoch [15] train done\n",
      "Batch eval [1] loss 0.40936, dsc 0.59064\n",
      "Batch eval [2] loss 0.43730, dsc 0.56270\n",
      "Batch eval [3] loss 0.42064, dsc 0.57936\n",
      "Batch eval [4] loss 0.46636, dsc 0.53364\n",
      "Batch eval [5] loss 0.40747, dsc 0.59253\n",
      "Epoch [15] valid done\n",
      "Epoch [15] T 108.98s, deltaT 7.32s, loss: train 0.44193, valid 0.42823, dsc: train 0.55807, valid 0.57177\n",
      "Batch train [1] loss 0.36635, dsc 0.63365\n",
      "Batch train [2] loss 0.38955, dsc 0.61045\n",
      "Batch train [3] loss 0.39906, dsc 0.60094\n",
      "Batch train [4] loss 0.38347, dsc 0.61653\n",
      "Batch train [5] loss 0.31927, dsc 0.68073\n",
      "Batch train [6] loss 0.44874, dsc 0.55126\n",
      "Batch train [7] loss 0.38700, dsc 0.61300\n",
      "Batch train [8] loss 0.42633, dsc 0.57367\n",
      "Batch train [9] loss 0.41050, dsc 0.58950\n",
      "Batch train [10] loss 0.41185, dsc 0.58815\n",
      "Batch train [11] loss 0.40356, dsc 0.59644\n",
      "Batch train [12] loss 0.39124, dsc 0.60876\n",
      "Batch train [13] loss 0.37529, dsc 0.62471\n",
      "Batch train [14] loss 0.52442, dsc 0.47558\n",
      "Batch train [15] loss 0.38554, dsc 0.61446\n",
      "Batch train [16] loss 0.32999, dsc 0.67001\n",
      "Batch train [17] loss 0.40597, dsc 0.59403\n",
      "Batch train [18] loss 0.34898, dsc 0.65102\n",
      "Batch train [19] loss 0.35237, dsc 0.64763\n",
      "Batch train [20] loss 0.35862, dsc 0.64138\n",
      "Batch train [21] loss 0.35506, dsc 0.64494\n",
      "Batch train [22] loss 0.37614, dsc 0.62386\n",
      "Batch train [23] loss 0.31257, dsc 0.68743\n",
      "Batch train [24] loss 0.34637, dsc 0.65363\n",
      "Batch train [25] loss 0.35587, dsc 0.64413\n",
      "Batch train [26] loss 0.34247, dsc 0.65753\n",
      "Batch train [27] loss 0.31877, dsc 0.68123\n",
      "Batch train [28] loss 0.35074, dsc 0.64926\n",
      "Batch train [29] loss 0.35279, dsc 0.64721\n",
      "Batch train [30] loss 0.38596, dsc 0.61404\n",
      "Batch train [31] loss 0.33145, dsc 0.66855\n",
      "Batch train [32] loss 0.35967, dsc 0.64033\n",
      "Batch train [33] loss 0.32423, dsc 0.67577\n",
      "Batch train [34] loss 0.36566, dsc 0.63434\n",
      "Batch train [35] loss 0.38219, dsc 0.61781\n",
      "Batch train [36] loss 0.35060, dsc 0.64940\n",
      "Batch train [37] loss 0.36446, dsc 0.63554\n",
      "Batch train [38] loss 0.32916, dsc 0.67084\n",
      "Batch train [39] loss 0.31126, dsc 0.68874\n",
      "Batch train [40] loss 0.44878, dsc 0.55122\n",
      "Epoch [16] train done\n",
      "Batch eval [1] loss 0.31502, dsc 0.68498\n",
      "Batch eval [2] loss 0.35252, dsc 0.64748\n",
      "Batch eval [3] loss 0.32088, dsc 0.67912\n",
      "Batch eval [4] loss 0.37111, dsc 0.62889\n",
      "Batch eval [5] loss 0.31801, dsc 0.68199\n",
      "Epoch [16] valid done\n",
      "Epoch [16] T 116.32s, deltaT 7.33s, loss: train 0.37206, valid 0.33551, dsc: train 0.62794, valid 0.66449\n",
      "Batch train [1] loss 0.30705, dsc 0.69295\n",
      "Batch train [2] loss 0.33507, dsc 0.66493\n",
      "Batch train [3] loss 0.32766, dsc 0.67234\n",
      "Batch train [4] loss 0.31207, dsc 0.68793\n",
      "Batch train [5] loss 0.41678, dsc 0.58322\n",
      "Batch train [6] loss 0.32387, dsc 0.67613\n",
      "Batch train [7] loss 0.35549, dsc 0.64451\n",
      "Batch train [8] loss 0.32194, dsc 0.67806\n",
      "Batch train [9] loss 0.30531, dsc 0.69469\n",
      "Batch train [10] loss 0.32887, dsc 0.67113\n",
      "Batch train [11] loss 0.32546, dsc 0.67454\n",
      "Batch train [12] loss 0.30748, dsc 0.69252\n",
      "Batch train [13] loss 0.30203, dsc 0.69797\n",
      "Batch train [14] loss 0.31027, dsc 0.68973\n",
      "Batch train [15] loss 0.33722, dsc 0.66278\n",
      "Batch train [16] loss 0.26932, dsc 0.73068\n",
      "Batch train [17] loss 0.33944, dsc 0.66056\n",
      "Batch train [18] loss 0.33427, dsc 0.66573\n",
      "Batch train [19] loss 0.30779, dsc 0.69221\n",
      "Batch train [20] loss 0.31051, dsc 0.68949\n",
      "Batch train [21] loss 0.29865, dsc 0.70135\n",
      "Batch train [22] loss 0.32859, dsc 0.67141\n",
      "Batch train [23] loss 0.27314, dsc 0.72686\n",
      "Batch train [24] loss 0.29206, dsc 0.70794\n",
      "Batch train [25] loss 0.28070, dsc 0.71930\n",
      "Batch train [26] loss 0.27187, dsc 0.72813\n",
      "Batch train [27] loss 0.31966, dsc 0.68034\n",
      "Batch train [28] loss 0.27935, dsc 0.72065\n",
      "Batch train [29] loss 0.30025, dsc 0.69975\n",
      "Batch train [30] loss 0.30529, dsc 0.69471\n",
      "Batch train [31] loss 0.31286, dsc 0.68714\n",
      "Batch train [32] loss 0.26710, dsc 0.73290\n",
      "Batch train [33] loss 0.28674, dsc 0.71326\n",
      "Batch train [34] loss 0.29400, dsc 0.70600\n",
      "Batch train [35] loss 0.32633, dsc 0.67367\n",
      "Batch train [36] loss 0.35388, dsc 0.64612\n",
      "Batch train [37] loss 0.28818, dsc 0.71182\n",
      "Batch train [38] loss 0.31554, dsc 0.68446\n",
      "Batch train [39] loss 0.40361, dsc 0.59639\n",
      "Batch train [40] loss 0.32365, dsc 0.67635\n",
      "Epoch [17] train done\n",
      "Batch eval [1] loss 0.34600, dsc 0.65400\n",
      "Batch eval [2] loss 0.34276, dsc 0.65724\n",
      "Batch eval [3] loss 0.36947, dsc 0.63053\n",
      "Batch eval [4] loss 0.34627, dsc 0.65373\n",
      "Batch eval [5] loss 0.33094, dsc 0.66906\n",
      "Epoch [17] valid done\n",
      "Epoch [17] T 123.64s, deltaT 7.32s, loss: train 0.31498, valid 0.34709, dsc: train 0.68502, valid 0.65291\n",
      "Batch train [1] loss 0.28178, dsc 0.71822\n",
      "Batch train [2] loss 0.31256, dsc 0.68744\n",
      "Batch train [3] loss 0.25582, dsc 0.74418\n",
      "Batch train [4] loss 0.37068, dsc 0.62932\n",
      "Batch train [5] loss 0.23681, dsc 0.76319\n",
      "Batch train [6] loss 0.25561, dsc 0.74439\n",
      "Batch train [7] loss 0.24966, dsc 0.75034\n",
      "Batch train [8] loss 0.27234, dsc 0.72766\n",
      "Batch train [9] loss 0.29882, dsc 0.70118\n",
      "Batch train [10] loss 0.38828, dsc 0.61172\n",
      "Batch train [11] loss 0.25429, dsc 0.74571\n",
      "Batch train [12] loss 0.29172, dsc 0.70828\n",
      "Batch train [13] loss 0.31772, dsc 0.68228\n",
      "Batch train [14] loss 0.27890, dsc 0.72110\n",
      "Batch train [15] loss 0.28649, dsc 0.71351\n",
      "Batch train [16] loss 0.27968, dsc 0.72032\n",
      "Batch train [17] loss 0.27708, dsc 0.72292\n",
      "Batch train [18] loss 0.31841, dsc 0.68159\n",
      "Batch train [19] loss 0.27498, dsc 0.72502\n",
      "Batch train [20] loss 0.29047, dsc 0.70953\n",
      "Batch train [21] loss 0.28178, dsc 0.71822\n",
      "Batch train [22] loss 0.29717, dsc 0.70283\n",
      "Batch train [23] loss 0.26348, dsc 0.73652\n",
      "Batch train [24] loss 0.26278, dsc 0.73722\n",
      "Batch train [25] loss 0.26450, dsc 0.73550\n",
      "Batch train [26] loss 0.25651, dsc 0.74349\n",
      "Batch train [27] loss 0.26359, dsc 0.73641\n",
      "Batch train [28] loss 0.24409, dsc 0.75591\n",
      "Batch train [29] loss 0.25558, dsc 0.74442\n",
      "Batch train [30] loss 0.24403, dsc 0.75597\n",
      "Batch train [31] loss 0.24719, dsc 0.75281\n",
      "Batch train [32] loss 0.29504, dsc 0.70496\n",
      "Batch train [33] loss 0.28549, dsc 0.71451\n",
      "Batch train [34] loss 0.28372, dsc 0.71628\n",
      "Batch train [35] loss 0.26122, dsc 0.73878\n",
      "Batch train [36] loss 0.24956, dsc 0.75044\n",
      "Batch train [37] loss 0.22007, dsc 0.77993\n",
      "Batch train [38] loss 0.25099, dsc 0.74901\n",
      "Batch train [39] loss 0.24525, dsc 0.75475\n",
      "Batch train [40] loss 0.25485, dsc 0.74515\n",
      "Epoch [18] train done\n",
      "Batch eval [1] loss 0.27549, dsc 0.72451\n",
      "Batch eval [2] loss 0.29559, dsc 0.70441\n",
      "Batch eval [3] loss 0.28882, dsc 0.71118\n",
      "Batch eval [4] loss 0.30419, dsc 0.69581\n",
      "Batch eval [5] loss 0.28023, dsc 0.71977\n",
      "Epoch [18] valid done\n",
      "Epoch [18] T 131.30s, deltaT 7.66s, loss: train 0.27547, valid 0.28886, dsc: train 0.72453, valid 0.71114\n",
      "Batch train [1] loss 0.29529, dsc 0.70471\n",
      "Batch train [2] loss 0.26919, dsc 0.73081\n",
      "Batch train [3] loss 0.25211, dsc 0.74789\n",
      "Batch train [4] loss 0.24035, dsc 0.75965\n",
      "Batch train [5] loss 0.27529, dsc 0.72471\n",
      "Batch train [6] loss 0.29269, dsc 0.70731\n",
      "Batch train [7] loss 0.27843, dsc 0.72157\n",
      "Batch train [8] loss 0.28013, dsc 0.71987\n",
      "Batch train [9] loss 0.25686, dsc 0.74314\n",
      "Batch train [10] loss 0.22823, dsc 0.77177\n",
      "Batch train [11] loss 0.22372, dsc 0.77628\n",
      "Batch train [12] loss 0.32067, dsc 0.67933\n",
      "Batch train [13] loss 0.27046, dsc 0.72954\n",
      "Batch train [14] loss 0.26073, dsc 0.73927\n",
      "Batch train [15] loss 0.26300, dsc 0.73700\n",
      "Batch train [16] loss 0.27739, dsc 0.72261\n",
      "Batch train [17] loss 0.34574, dsc 0.65426\n",
      "Batch train [18] loss 0.25160, dsc 0.74840\n",
      "Batch train [19] loss 0.23656, dsc 0.76344\n",
      "Batch train [20] loss 0.21908, dsc 0.78092\n",
      "Batch train [21] loss 0.26516, dsc 0.73484\n",
      "Batch train [22] loss 0.26509, dsc 0.73491\n",
      "Batch train [23] loss 0.21498, dsc 0.78502\n",
      "Batch train [24] loss 0.24867, dsc 0.75133\n",
      "Batch train [25] loss 0.23052, dsc 0.76948\n",
      "Batch train [26] loss 0.23936, dsc 0.76064\n",
      "Batch train [27] loss 0.25374, dsc 0.74626\n",
      "Batch train [28] loss 0.23472, dsc 0.76528\n",
      "Batch train [29] loss 0.23084, dsc 0.76916\n",
      "Batch train [30] loss 0.27270, dsc 0.72730\n",
      "Batch train [31] loss 0.21714, dsc 0.78286\n",
      "Batch train [32] loss 0.21693, dsc 0.78307\n",
      "Batch train [33] loss 0.24706, dsc 0.75294\n",
      "Batch train [34] loss 0.21745, dsc 0.78255\n",
      "Batch train [35] loss 0.24340, dsc 0.75660\n",
      "Batch train [36] loss 0.24054, dsc 0.75946\n",
      "Batch train [37] loss 0.21389, dsc 0.78611\n",
      "Batch train [38] loss 0.25183, dsc 0.74817\n",
      "Batch train [39] loss 0.22889, dsc 0.77111\n",
      "Batch train [40] loss 0.24279, dsc 0.75721\n",
      "Epoch [19] train done\n",
      "Batch eval [1] loss 0.23524, dsc 0.76476\n",
      "Batch eval [2] loss 0.27514, dsc 0.72486\n",
      "Batch eval [3] loss 0.25863, dsc 0.74137\n",
      "Batch eval [4] loss 0.28077, dsc 0.71923\n",
      "Batch eval [5] loss 0.25598, dsc 0.74402\n",
      "Epoch [19] valid done\n",
      "Epoch [19] T 138.81s, deltaT 7.51s, loss: train 0.25283, valid 0.26115, dsc: train 0.74717, valid 0.73885\n",
      "Batch train [1] loss 0.20745, dsc 0.79255\n",
      "Batch train [2] loss 0.22753, dsc 0.77247\n",
      "Batch train [3] loss 0.21494, dsc 0.78506\n",
      "Batch train [4] loss 0.33354, dsc 0.66646\n",
      "Batch train [5] loss 0.23506, dsc 0.76494\n",
      "Batch train [6] loss 0.23665, dsc 0.76335\n",
      "Batch train [7] loss 0.26155, dsc 0.73845\n",
      "Batch train [8] loss 0.20179, dsc 0.79821\n",
      "Batch train [9] loss 0.22432, dsc 0.77568\n",
      "Batch train [10] loss 0.26418, dsc 0.73582\n",
      "Batch train [11] loss 0.29603, dsc 0.70397\n",
      "Batch train [12] loss 0.23378, dsc 0.76622\n",
      "Batch train [13] loss 0.22531, dsc 0.77469\n",
      "Batch train [14] loss 0.22780, dsc 0.77220\n",
      "Batch train [15] loss 0.22591, dsc 0.77409\n",
      "Batch train [16] loss 0.23955, dsc 0.76045\n",
      "Batch train [17] loss 0.21183, dsc 0.78817\n",
      "Batch train [18] loss 0.17898, dsc 0.82102\n",
      "Batch train [19] loss 0.23562, dsc 0.76438\n",
      "Batch train [20] loss 0.23082, dsc 0.76918\n",
      "Batch train [21] loss 0.18954, dsc 0.81046\n",
      "Batch train [22] loss 0.21288, dsc 0.78712\n",
      "Batch train [23] loss 0.23544, dsc 0.76456\n",
      "Batch train [24] loss 0.18413, dsc 0.81587\n",
      "Batch train [25] loss 0.24252, dsc 0.75748\n",
      "Batch train [26] loss 0.22793, dsc 0.77207\n",
      "Batch train [27] loss 0.21807, dsc 0.78193\n",
      "Batch train [28] loss 0.24062, dsc 0.75938\n",
      "Batch train [29] loss 0.20875, dsc 0.79125\n",
      "Batch train [30] loss 0.19302, dsc 0.80698\n",
      "Batch train [31] loss 0.21553, dsc 0.78447\n",
      "Batch train [32] loss 0.25802, dsc 0.74198\n",
      "Batch train [33] loss 0.19240, dsc 0.80760\n",
      "Batch train [34] loss 0.24084, dsc 0.75916\n",
      "Batch train [35] loss 0.21299, dsc 0.78701\n",
      "Batch train [36] loss 0.22139, dsc 0.77861\n",
      "Batch train [37] loss 0.21741, dsc 0.78259\n",
      "Batch train [38] loss 0.20045, dsc 0.79955\n",
      "Batch train [39] loss 0.21793, dsc 0.78207\n",
      "Batch train [40] loss 0.21760, dsc 0.78240\n",
      "Epoch [20] train done\n",
      "Batch eval [1] loss 0.22133, dsc 0.77867\n",
      "Batch eval [2] loss 0.25178, dsc 0.74822\n",
      "Batch eval [3] loss 0.23585, dsc 0.76415\n",
      "Batch eval [4] loss 0.25817, dsc 0.74183\n",
      "Batch eval [5] loss 0.23543, dsc 0.76457\n",
      "Epoch [20] valid done\n",
      "Epoch [20] T 146.27s, deltaT 7.45s, loss: train 0.22650, valid 0.24051, dsc: train 0.77350, valid 0.75949\n",
      "Batch train [1] loss 0.20302, dsc 0.79698\n",
      "Batch train [2] loss 0.19411, dsc 0.80589\n",
      "Batch train [3] loss 0.18103, dsc 0.81897\n",
      "Batch train [4] loss 0.20975, dsc 0.79025\n",
      "Batch train [5] loss 0.20579, dsc 0.79421\n",
      "Batch train [6] loss 0.19249, dsc 0.80751\n",
      "Batch train [7] loss 0.21013, dsc 0.78987\n",
      "Batch train [8] loss 0.21868, dsc 0.78132\n",
      "Batch train [9] loss 0.21102, dsc 0.78898\n",
      "Batch train [10] loss 0.22306, dsc 0.77694\n",
      "Batch train [11] loss 0.30951, dsc 0.69049\n",
      "Batch train [12] loss 0.19506, dsc 0.80494\n",
      "Batch train [13] loss 0.26697, dsc 0.73303\n",
      "Batch train [14] loss 0.20701, dsc 0.79299\n",
      "Batch train [15] loss 0.21585, dsc 0.78415\n",
      "Batch train [16] loss 0.21883, dsc 0.78117\n",
      "Batch train [17] loss 0.18822, dsc 0.81178\n",
      "Batch train [18] loss 0.19938, dsc 0.80062\n",
      "Batch train [19] loss 0.22201, dsc 0.77799\n",
      "Batch train [20] loss 0.21097, dsc 0.78903\n",
      "Batch train [21] loss 0.21945, dsc 0.78055\n",
      "Batch train [22] loss 0.20389, dsc 0.79611\n",
      "Batch train [23] loss 0.23930, dsc 0.76070\n",
      "Batch train [24] loss 0.17828, dsc 0.82172\n",
      "Batch train [25] loss 0.24178, dsc 0.75822\n",
      "Batch train [26] loss 0.20331, dsc 0.79669\n",
      "Batch train [27] loss 0.17658, dsc 0.82342\n",
      "Batch train [28] loss 0.17911, dsc 0.82089\n",
      "Batch train [29] loss 0.21437, dsc 0.78563\n",
      "Batch train [30] loss 0.16545, dsc 0.83455\n",
      "Batch train [31] loss 0.22208, dsc 0.77792\n",
      "Batch train [32] loss 0.21292, dsc 0.78708\n",
      "Batch train [33] loss 0.19308, dsc 0.80692\n",
      "Batch train [34] loss 0.22083, dsc 0.77917\n",
      "Batch train [35] loss 0.17154, dsc 0.82846\n",
      "Batch train [36] loss 0.20553, dsc 0.79447\n",
      "Batch train [37] loss 0.20142, dsc 0.79858\n",
      "Batch train [38] loss 0.19406, dsc 0.80594\n",
      "Batch train [39] loss 0.20958, dsc 0.79042\n",
      "Batch train [40] loss 0.18707, dsc 0.81293\n",
      "Epoch [21] train done\n",
      "Batch eval [1] loss 0.23595, dsc 0.76405\n",
      "Batch eval [2] loss 0.25493, dsc 0.74507\n",
      "Batch eval [3] loss 0.27435, dsc 0.72565\n",
      "Batch eval [4] loss 0.24156, dsc 0.75844\n",
      "Batch eval [5] loss 0.24333, dsc 0.75667\n",
      "Epoch [21] valid done\n",
      "Epoch [21] T 153.70s, deltaT 7.43s, loss: train 0.20806, valid 0.25002, dsc: train 0.79194, valid 0.74998\n",
      "Batch train [1] loss 0.20457, dsc 0.79543\n",
      "Batch train [2] loss 0.18315, dsc 0.81685\n",
      "Batch train [3] loss 0.19827, dsc 0.80173\n",
      "Batch train [4] loss 0.17923, dsc 0.82077\n",
      "Batch train [5] loss 0.17127, dsc 0.82873\n",
      "Batch train [6] loss 0.19308, dsc 0.80692\n",
      "Batch train [7] loss 0.24903, dsc 0.75097\n",
      "Batch train [8] loss 0.19089, dsc 0.80911\n",
      "Batch train [9] loss 0.17991, dsc 0.82009\n",
      "Batch train [10] loss 0.19289, dsc 0.80711\n",
      "Batch train [11] loss 0.18556, dsc 0.81444\n",
      "Batch train [12] loss 0.20149, dsc 0.79851\n",
      "Batch train [13] loss 0.19289, dsc 0.80711\n",
      "Batch train [14] loss 0.19424, dsc 0.80576\n",
      "Batch train [15] loss 0.18396, dsc 0.81604\n",
      "Batch train [16] loss 0.20163, dsc 0.79837\n",
      "Batch train [17] loss 0.21858, dsc 0.78142\n",
      "Batch train [18] loss 0.20378, dsc 0.79622\n",
      "Batch train [19] loss 0.21018, dsc 0.78982\n",
      "Batch train [20] loss 0.18831, dsc 0.81169\n",
      "Batch train [21] loss 0.17647, dsc 0.82353\n",
      "Batch train [22] loss 0.28803, dsc 0.71197\n",
      "Batch train [23] loss 0.19283, dsc 0.80717\n",
      "Batch train [24] loss 0.17913, dsc 0.82087\n",
      "Batch train [25] loss 0.18601, dsc 0.81399\n",
      "Batch train [26] loss 0.19309, dsc 0.80691\n",
      "Batch train [27] loss 0.17022, dsc 0.82978\n",
      "Batch train [28] loss 0.17379, dsc 0.82621\n",
      "Batch train [29] loss 0.19099, dsc 0.80901\n",
      "Batch train [30] loss 0.16891, dsc 0.83109\n",
      "Batch train [31] loss 0.20131, dsc 0.79869\n",
      "Batch train [32] loss 0.16698, dsc 0.83302\n",
      "Batch train [33] loss 0.19705, dsc 0.80295\n",
      "Batch train [34] loss 0.19395, dsc 0.80605\n",
      "Batch train [35] loss 0.18685, dsc 0.81315\n",
      "Batch train [36] loss 0.23540, dsc 0.76460\n",
      "Batch train [37] loss 0.18117, dsc 0.81883\n",
      "Batch train [38] loss 0.19382, dsc 0.80618\n",
      "Batch train [39] loss 0.18095, dsc 0.81905\n",
      "Batch train [40] loss 0.20684, dsc 0.79316\n",
      "Epoch [22] train done\n",
      "Batch eval [1] loss 0.18679, dsc 0.81321\n",
      "Batch eval [2] loss 0.23856, dsc 0.76144\n",
      "Batch eval [3] loss 0.22094, dsc 0.77906\n",
      "Batch eval [4] loss 0.21019, dsc 0.78981\n",
      "Batch eval [5] loss 0.21061, dsc 0.78939\n",
      "Epoch [22] valid done\n",
      "Epoch [22] T 161.02s, deltaT 7.33s, loss: train 0.19467, valid 0.21342, dsc: train 0.80533, valid 0.78658\n",
      "Batch train [1] loss 0.16337, dsc 0.83663\n",
      "Batch train [2] loss 0.18589, dsc 0.81411\n",
      "Batch train [3] loss 0.18427, dsc 0.81573\n",
      "Batch train [4] loss 0.18338, dsc 0.81662\n",
      "Batch train [5] loss 0.15827, dsc 0.84173\n",
      "Batch train [6] loss 0.22306, dsc 0.77694\n",
      "Batch train [7] loss 0.20010, dsc 0.79990\n",
      "Batch train [8] loss 0.17751, dsc 0.82249\n",
      "Batch train [9] loss 0.17301, dsc 0.82699\n",
      "Batch train [10] loss 0.20407, dsc 0.79593\n",
      "Batch train [11] loss 0.18763, dsc 0.81237\n",
      "Batch train [12] loss 0.17357, dsc 0.82643\n",
      "Batch train [13] loss 0.18308, dsc 0.81692\n",
      "Batch train [14] loss 0.17525, dsc 0.82475\n",
      "Batch train [15] loss 0.16550, dsc 0.83450\n",
      "Batch train [16] loss 0.17556, dsc 0.82444\n",
      "Batch train [17] loss 0.19461, dsc 0.80539\n",
      "Batch train [18] loss 0.18143, dsc 0.81857\n",
      "Batch train [19] loss 0.17824, dsc 0.82176\n",
      "Batch train [20] loss 0.20537, dsc 0.79463\n",
      "Batch train [21] loss 0.20703, dsc 0.79297\n",
      "Batch train [22] loss 0.17972, dsc 0.82028\n",
      "Batch train [23] loss 0.14906, dsc 0.85094\n",
      "Batch train [24] loss 0.18961, dsc 0.81039\n",
      "Batch train [25] loss 0.18986, dsc 0.81014\n",
      "Batch train [26] loss 0.17943, dsc 0.82057\n",
      "Batch train [27] loss 0.17938, dsc 0.82062\n",
      "Batch train [28] loss 0.15747, dsc 0.84253\n",
      "Batch train [29] loss 0.27435, dsc 0.72565\n",
      "Batch train [30] loss 0.19512, dsc 0.80488\n",
      "Batch train [31] loss 0.17916, dsc 0.82084\n",
      "Batch train [32] loss 0.17942, dsc 0.82058\n",
      "Batch train [33] loss 0.17886, dsc 0.82114\n",
      "Batch train [34] loss 0.17500, dsc 0.82500\n",
      "Batch train [35] loss 0.20517, dsc 0.79483\n",
      "Batch train [36] loss 0.17027, dsc 0.82973\n",
      "Batch train [37] loss 0.14914, dsc 0.85086\n",
      "Batch train [38] loss 0.15954, dsc 0.84046\n",
      "Batch train [39] loss 0.25223, dsc 0.74777\n",
      "Batch train [40] loss 0.18826, dsc 0.81174\n",
      "Epoch [23] train done\n",
      "Batch eval [1] loss 0.19789, dsc 0.80211\n",
      "Batch eval [2] loss 0.22723, dsc 0.77277\n",
      "Batch eval [3] loss 0.24785, dsc 0.75215\n",
      "Batch eval [4] loss 0.22280, dsc 0.77720\n",
      "Batch eval [5] loss 0.20463, dsc 0.79537\n",
      "Epoch [23] valid done\n",
      "Epoch [23] T 168.32s, deltaT 7.30s, loss: train 0.18528, valid 0.22008, dsc: train 0.81472, valid 0.77992\n",
      "Batch train [1] loss 0.19842, dsc 0.80158\n",
      "Batch train [2] loss 0.17147, dsc 0.82853\n",
      "Batch train [3] loss 0.16336, dsc 0.83664\n",
      "Batch train [4] loss 0.17459, dsc 0.82541\n",
      "Batch train [5] loss 0.14549, dsc 0.85451\n",
      "Batch train [6] loss 0.16576, dsc 0.83424\n",
      "Batch train [7] loss 0.13989, dsc 0.86011\n",
      "Batch train [8] loss 0.21174, dsc 0.78826\n",
      "Batch train [9] loss 0.18062, dsc 0.81938\n",
      "Batch train [10] loss 0.18389, dsc 0.81611\n",
      "Batch train [11] loss 0.16666, dsc 0.83334\n",
      "Batch train [12] loss 0.15572, dsc 0.84428\n",
      "Batch train [13] loss 0.17200, dsc 0.82800\n",
      "Batch train [14] loss 0.17283, dsc 0.82717\n",
      "Batch train [15] loss 0.17543, dsc 0.82457\n",
      "Batch train [16] loss 0.14692, dsc 0.85308\n",
      "Batch train [17] loss 0.16954, dsc 0.83046\n",
      "Batch train [18] loss 0.16230, dsc 0.83770\n",
      "Batch train [19] loss 0.21284, dsc 0.78716\n",
      "Batch train [20] loss 0.19161, dsc 0.80839\n",
      "Batch train [21] loss 0.17589, dsc 0.82411\n",
      "Batch train [22] loss 0.16279, dsc 0.83721\n",
      "Batch train [23] loss 0.18173, dsc 0.81827\n",
      "Batch train [24] loss 0.14065, dsc 0.85935\n",
      "Batch train [25] loss 0.15847, dsc 0.84153\n",
      "Batch train [26] loss 0.17874, dsc 0.82126\n",
      "Batch train [27] loss 0.15369, dsc 0.84631\n",
      "Batch train [28] loss 0.15546, dsc 0.84454\n",
      "Batch train [29] loss 0.16400, dsc 0.83600\n",
      "Batch train [30] loss 0.16198, dsc 0.83802\n",
      "Batch train [31] loss 0.16419, dsc 0.83581\n",
      "Batch train [32] loss 0.17066, dsc 0.82934\n",
      "Batch train [33] loss 0.17148, dsc 0.82852\n",
      "Batch train [34] loss 0.18730, dsc 0.81270\n",
      "Batch train [35] loss 0.19017, dsc 0.80983\n",
      "Batch train [36] loss 0.15896, dsc 0.84104\n",
      "Batch train [37] loss 0.16995, dsc 0.83005\n",
      "Batch train [38] loss 0.26400, dsc 0.73600\n",
      "Batch train [39] loss 0.16621, dsc 0.83379\n",
      "Batch train [40] loss 0.16942, dsc 0.83058\n",
      "Epoch [24] train done\n",
      "Batch eval [1] loss 0.19839, dsc 0.80161\n",
      "Batch eval [2] loss 0.21515, dsc 0.78485\n",
      "Batch eval [3] loss 0.23543, dsc 0.76457\n",
      "Batch eval [4] loss 0.21546, dsc 0.78454\n",
      "Batch eval [5] loss 0.19320, dsc 0.80680\n",
      "Epoch [24] valid done\n",
      "Epoch [24] T 175.66s, deltaT 7.33s, loss: train 0.17267, valid 0.21153, dsc: train 0.82733, valid 0.78847\n",
      "Batch train [1] loss 0.15625, dsc 0.84375\n",
      "Batch train [2] loss 0.14027, dsc 0.85973\n",
      "Batch train [3] loss 0.24224, dsc 0.75776\n",
      "Batch train [4] loss 0.15833, dsc 0.84167\n",
      "Batch train [5] loss 0.15496, dsc 0.84504\n",
      "Batch train [6] loss 0.16095, dsc 0.83905\n",
      "Batch train [7] loss 0.17882, dsc 0.82118\n",
      "Batch train [8] loss 0.16554, dsc 0.83446\n",
      "Batch train [9] loss 0.17258, dsc 0.82742\n",
      "Batch train [10] loss 0.13249, dsc 0.86751\n",
      "Batch train [11] loss 0.16174, dsc 0.83826\n",
      "Batch train [12] loss 0.14878, dsc 0.85122\n",
      "Batch train [13] loss 0.16020, dsc 0.83980\n",
      "Batch train [14] loss 0.16682, dsc 0.83318\n",
      "Batch train [15] loss 0.17951, dsc 0.82049\n",
      "Batch train [16] loss 0.15168, dsc 0.84832\n",
      "Batch train [17] loss 0.16767, dsc 0.83233\n",
      "Batch train [18] loss 0.15162, dsc 0.84838\n",
      "Batch train [19] loss 0.16340, dsc 0.83660\n",
      "Batch train [20] loss 0.16236, dsc 0.83764\n",
      "Batch train [21] loss 0.14948, dsc 0.85052\n",
      "Batch train [22] loss 0.16159, dsc 0.83841\n",
      "Batch train [23] loss 0.16574, dsc 0.83426\n",
      "Batch train [24] loss 0.18347, dsc 0.81653\n",
      "Batch train [25] loss 0.16674, dsc 0.83326\n",
      "Batch train [26] loss 0.14910, dsc 0.85090\n",
      "Batch train [27] loss 0.16649, dsc 0.83351\n",
      "Batch train [28] loss 0.15217, dsc 0.84783\n",
      "Batch train [29] loss 0.16182, dsc 0.83818\n",
      "Batch train [30] loss 0.14956, dsc 0.85044\n",
      "Batch train [31] loss 0.19204, dsc 0.80796\n",
      "Batch train [32] loss 0.15636, dsc 0.84364\n",
      "Batch train [33] loss 0.15667, dsc 0.84333\n",
      "Batch train [34] loss 0.15118, dsc 0.84882\n",
      "Batch train [35] loss 0.16575, dsc 0.83425\n",
      "Batch train [36] loss 0.16458, dsc 0.83542\n",
      "Batch train [37] loss 0.16119, dsc 0.83881\n",
      "Batch train [38] loss 0.13316, dsc 0.86684\n",
      "Batch train [39] loss 0.17376, dsc 0.82624\n",
      "Batch train [40] loss 0.19778, dsc 0.80222\n",
      "Epoch [25] train done\n",
      "Batch eval [1] loss 0.20679, dsc 0.79321\n",
      "Batch eval [2] loss 0.22997, dsc 0.77003\n",
      "Batch eval [3] loss 0.26086, dsc 0.73914\n",
      "Batch eval [4] loss 0.21407, dsc 0.78593\n",
      "Batch eval [5] loss 0.20785, dsc 0.79215\n",
      "Epoch [25] valid done\n",
      "Epoch [25] T 183.10s, deltaT 7.44s, loss: train 0.16337, valid 0.22391, dsc: train 0.83663, valid 0.77609\n",
      "Batch train [1] loss 0.16599, dsc 0.83401\n",
      "Batch train [2] loss 0.14566, dsc 0.85434\n",
      "Batch train [3] loss 0.15482, dsc 0.84518\n",
      "Batch train [4] loss 0.16606, dsc 0.83394\n",
      "Batch train [5] loss 0.14666, dsc 0.85334\n",
      "Batch train [6] loss 0.14091, dsc 0.85909\n",
      "Batch train [7] loss 0.14995, dsc 0.85005\n",
      "Batch train [8] loss 0.15209, dsc 0.84791\n",
      "Batch train [9] loss 0.14636, dsc 0.85364\n",
      "Batch train [10] loss 0.13052, dsc 0.86948\n",
      "Batch train [11] loss 0.13376, dsc 0.86624\n",
      "Batch train [12] loss 0.14099, dsc 0.85901\n",
      "Batch train [13] loss 0.15622, dsc 0.84378\n",
      "Batch train [14] loss 0.15657, dsc 0.84343\n",
      "Batch train [15] loss 0.14843, dsc 0.85157\n",
      "Batch train [16] loss 0.15956, dsc 0.84044\n",
      "Batch train [17] loss 0.15151, dsc 0.84849\n",
      "Batch train [18] loss 0.13907, dsc 0.86093\n",
      "Batch train [19] loss 0.12954, dsc 0.87046\n",
      "Batch train [20] loss 0.15992, dsc 0.84008\n",
      "Batch train [21] loss 0.16706, dsc 0.83294\n",
      "Batch train [22] loss 0.15307, dsc 0.84693\n",
      "Batch train [23] loss 0.15062, dsc 0.84938\n",
      "Batch train [24] loss 0.15119, dsc 0.84881\n",
      "Batch train [25] loss 0.14961, dsc 0.85039\n",
      "Batch train [26] loss 0.13834, dsc 0.86166\n",
      "Batch train [27] loss 0.16631, dsc 0.83369\n",
      "Batch train [28] loss 0.19655, dsc 0.80345\n",
      "Batch train [29] loss 0.17229, dsc 0.82771\n",
      "Batch train [30] loss 0.15902, dsc 0.84098\n",
      "Batch train [31] loss 0.15255, dsc 0.84745\n",
      "Batch train [32] loss 0.15728, dsc 0.84272\n",
      "Batch train [33] loss 0.15376, dsc 0.84624\n",
      "Batch train [34] loss 0.19783, dsc 0.80217\n",
      "Batch train [35] loss 0.15320, dsc 0.84680\n",
      "Batch train [36] loss 0.15403, dsc 0.84597\n",
      "Batch train [37] loss 0.15111, dsc 0.84889\n",
      "Batch train [38] loss 0.15484, dsc 0.84516\n",
      "Batch train [39] loss 0.23096, dsc 0.76904\n",
      "Batch train [40] loss 0.18238, dsc 0.81762\n",
      "Epoch [26] train done\n",
      "Batch eval [1] loss 0.19387, dsc 0.80613\n",
      "Batch eval [2] loss 0.21580, dsc 0.78420\n",
      "Batch eval [3] loss 0.23018, dsc 0.76982\n",
      "Batch eval [4] loss 0.19569, dsc 0.80431\n",
      "Batch eval [5] loss 0.18928, dsc 0.81072\n",
      "Epoch [26] valid done\n",
      "Epoch [26] T 190.60s, deltaT 7.51s, loss: train 0.15666, valid 0.20497, dsc: train 0.84334, valid 0.79503\n",
      "Batch train [1] loss 0.13312, dsc 0.86688\n",
      "Batch train [2] loss 0.14566, dsc 0.85434\n",
      "Batch train [3] loss 0.15199, dsc 0.84801\n",
      "Batch train [4] loss 0.15469, dsc 0.84531\n",
      "Batch train [5] loss 0.12876, dsc 0.87124\n",
      "Batch train [6] loss 0.15051, dsc 0.84949\n",
      "Batch train [7] loss 0.14597, dsc 0.85403\n",
      "Batch train [8] loss 0.15335, dsc 0.84665\n",
      "Batch train [9] loss 0.14768, dsc 0.85232\n",
      "Batch train [10] loss 0.15052, dsc 0.84948\n",
      "Batch train [11] loss 0.15468, dsc 0.84532\n",
      "Batch train [12] loss 0.16455, dsc 0.83545\n",
      "Batch train [13] loss 0.13117, dsc 0.86883\n",
      "Batch train [14] loss 0.14659, dsc 0.85341\n",
      "Batch train [15] loss 0.16554, dsc 0.83446\n",
      "Batch train [16] loss 0.16509, dsc 0.83491\n",
      "Batch train [17] loss 0.13981, dsc 0.86019\n",
      "Batch train [18] loss 0.14479, dsc 0.85521\n",
      "Batch train [19] loss 0.14549, dsc 0.85451\n",
      "Batch train [20] loss 0.13439, dsc 0.86561\n",
      "Batch train [21] loss 0.13486, dsc 0.86514\n",
      "Batch train [22] loss 0.14490, dsc 0.85510\n",
      "Batch train [23] loss 0.13330, dsc 0.86670\n",
      "Batch train [24] loss 0.17586, dsc 0.82414\n",
      "Batch train [25] loss 0.13873, dsc 0.86127\n",
      "Batch train [26] loss 0.13972, dsc 0.86028\n",
      "Batch train [27] loss 0.12725, dsc 0.87275\n",
      "Batch train [28] loss 0.22740, dsc 0.77260\n",
      "Batch train [29] loss 0.15005, dsc 0.84995\n",
      "Batch train [30] loss 0.16132, dsc 0.83868\n",
      "Batch train [31] loss 0.14316, dsc 0.85684\n",
      "Batch train [32] loss 0.16455, dsc 0.83545\n",
      "Batch train [33] loss 0.15413, dsc 0.84587\n",
      "Batch train [34] loss 0.14160, dsc 0.85840\n",
      "Batch train [35] loss 0.12774, dsc 0.87226\n",
      "Batch train [36] loss 0.14665, dsc 0.85335\n",
      "Batch train [37] loss 0.15468, dsc 0.84532\n",
      "Batch train [38] loss 0.18694, dsc 0.81306\n",
      "Batch train [39] loss 0.15341, dsc 0.84659\n",
      "Batch train [40] loss 0.14052, dsc 0.85948\n",
      "Epoch [27] train done\n",
      "Batch eval [1] loss 0.19008, dsc 0.80992\n",
      "Batch eval [2] loss 0.23895, dsc 0.76105\n",
      "Batch eval [3] loss 0.23458, dsc 0.76542\n",
      "Batch eval [4] loss 0.22159, dsc 0.77841\n",
      "Batch eval [5] loss 0.18280, dsc 0.81720\n",
      "Epoch [27] valid done\n",
      "Epoch [27] T 198.03s, deltaT 7.42s, loss: train 0.15003, valid 0.21360, dsc: train 0.84997, valid 0.78640\n",
      "Batch train [1] loss 0.15975, dsc 0.84025\n",
      "Batch train [2] loss 0.14477, dsc 0.85523\n",
      "Batch train [3] loss 0.14243, dsc 0.85757\n",
      "Batch train [4] loss 0.14783, dsc 0.85217\n",
      "Batch train [5] loss 0.11502, dsc 0.88498\n",
      "Batch train [6] loss 0.13665, dsc 0.86335\n",
      "Batch train [7] loss 0.12857, dsc 0.87143\n",
      "Batch train [8] loss 0.14721, dsc 0.85279\n",
      "Batch train [9] loss 0.15690, dsc 0.84310\n",
      "Batch train [10] loss 0.12714, dsc 0.87286\n",
      "Batch train [11] loss 0.12542, dsc 0.87458\n",
      "Batch train [12] loss 0.15250, dsc 0.84750\n",
      "Batch train [13] loss 0.12783, dsc 0.87217\n",
      "Batch train [14] loss 0.13904, dsc 0.86096\n",
      "Batch train [15] loss 0.11641, dsc 0.88359\n",
      "Batch train [16] loss 0.12248, dsc 0.87752\n",
      "Batch train [17] loss 0.14286, dsc 0.85714\n",
      "Batch train [18] loss 0.13803, dsc 0.86197\n",
      "Batch train [19] loss 0.12910, dsc 0.87090\n",
      "Batch train [20] loss 0.16649, dsc 0.83351\n",
      "Batch train [21] loss 0.13157, dsc 0.86843\n",
      "Batch train [22] loss 0.14828, dsc 0.85172\n",
      "Batch train [23] loss 0.14156, dsc 0.85844\n",
      "Batch train [24] loss 0.12551, dsc 0.87449\n",
      "Batch train [25] loss 0.15176, dsc 0.84824\n",
      "Batch train [26] loss 0.14329, dsc 0.85671\n",
      "Batch train [27] loss 0.14427, dsc 0.85573\n",
      "Batch train [28] loss 0.16797, dsc 0.83203\n",
      "Batch train [29] loss 0.16371, dsc 0.83629\n",
      "Batch train [30] loss 0.18243, dsc 0.81757\n",
      "Batch train [31] loss 0.17073, dsc 0.82927\n",
      "Batch train [32] loss 0.13508, dsc 0.86492\n",
      "Batch train [33] loss 0.17736, dsc 0.82264\n",
      "Batch train [34] loss 0.22144, dsc 0.77856\n",
      "Batch train [35] loss 0.14713, dsc 0.85287\n",
      "Batch train [36] loss 0.14974, dsc 0.85026\n",
      "Batch train [37] loss 0.14718, dsc 0.85282\n",
      "Batch train [38] loss 0.12887, dsc 0.87113\n",
      "Batch train [39] loss 0.13478, dsc 0.86522\n",
      "Batch train [40] loss 0.13965, dsc 0.86035\n",
      "Epoch [28] train done\n",
      "Batch eval [1] loss 0.20269, dsc 0.79731\n",
      "Batch eval [2] loss 0.20570, dsc 0.79430\n",
      "Batch eval [3] loss 0.19826, dsc 0.80174\n",
      "Batch eval [4] loss 0.19009, dsc 0.80991\n",
      "Batch eval [5] loss 0.18240, dsc 0.81760\n",
      "Epoch [28] valid done\n",
      "Epoch [28] T 205.41s, deltaT 7.38s, loss: train 0.14547, valid 0.19583, dsc: train 0.85453, valid 0.80417\n",
      "Batch train [1] loss 0.15090, dsc 0.84910\n",
      "Batch train [2] loss 0.13448, dsc 0.86552\n",
      "Batch train [3] loss 0.12679, dsc 0.87321\n",
      "Batch train [4] loss 0.12196, dsc 0.87804\n",
      "Batch train [5] loss 0.15484, dsc 0.84516\n",
      "Batch train [6] loss 0.15345, dsc 0.84655\n",
      "Batch train [7] loss 0.16736, dsc 0.83264\n",
      "Batch train [8] loss 0.13543, dsc 0.86457\n",
      "Batch train [9] loss 0.14571, dsc 0.85429\n",
      "Batch train [10] loss 0.20378, dsc 0.79622\n",
      "Batch train [11] loss 0.14011, dsc 0.85989\n",
      "Batch train [12] loss 0.13740, dsc 0.86260\n",
      "Batch train [13] loss 0.14465, dsc 0.85535\n",
      "Batch train [14] loss 0.14232, dsc 0.85768\n",
      "Batch train [15] loss 0.15687, dsc 0.84313\n",
      "Batch train [16] loss 0.14135, dsc 0.85865\n",
      "Batch train [17] loss 0.13574, dsc 0.86426\n",
      "Batch train [18] loss 0.14864, dsc 0.85136\n",
      "Batch train [19] loss 0.14050, dsc 0.85950\n",
      "Batch train [20] loss 0.14565, dsc 0.85435\n",
      "Batch train [21] loss 0.12629, dsc 0.87371\n",
      "Batch train [22] loss 0.14014, dsc 0.85986\n",
      "Batch train [23] loss 0.13287, dsc 0.86713\n",
      "Batch train [24] loss 0.14200, dsc 0.85800\n",
      "Batch train [25] loss 0.11948, dsc 0.88052\n",
      "Batch train [26] loss 0.12993, dsc 0.87007\n",
      "Batch train [27] loss 0.13396, dsc 0.86604\n",
      "Batch train [28] loss 0.15456, dsc 0.84544\n",
      "Batch train [29] loss 0.14100, dsc 0.85900\n",
      "Batch train [30] loss 0.13903, dsc 0.86097\n",
      "Batch train [31] loss 0.11426, dsc 0.88574\n",
      "Batch train [32] loss 0.15698, dsc 0.84302\n",
      "Batch train [33] loss 0.15027, dsc 0.84973\n",
      "Batch train [34] loss 0.13232, dsc 0.86768\n",
      "Batch train [35] loss 0.13122, dsc 0.86878\n",
      "Batch train [36] loss 0.12041, dsc 0.87959\n",
      "Batch train [37] loss 0.13328, dsc 0.86672\n",
      "Batch train [38] loss 0.16172, dsc 0.83828\n",
      "Batch train [39] loss 0.14811, dsc 0.85189\n",
      "Batch train [40] loss 0.13056, dsc 0.86944\n",
      "Epoch [29] train done\n",
      "Batch eval [1] loss 0.18067, dsc 0.81933\n",
      "Batch eval [2] loss 0.20893, dsc 0.79107\n",
      "Batch eval [3] loss 0.18781, dsc 0.81219\n",
      "Batch eval [4] loss 0.20001, dsc 0.79999\n",
      "Batch eval [5] loss 0.17223, dsc 0.82777\n",
      "Epoch [29] valid done\n",
      "Epoch [29] T 212.66s, deltaT 7.24s, loss: train 0.14166, valid 0.18993, dsc: train 0.85834, valid 0.81007\n",
      "Batch train [1] loss 0.13267, dsc 0.86733\n",
      "Batch train [2] loss 0.12649, dsc 0.87351\n",
      "Batch train [3] loss 0.14574, dsc 0.85426\n",
      "Batch train [4] loss 0.14234, dsc 0.85766\n",
      "Batch train [5] loss 0.13113, dsc 0.86887\n",
      "Batch train [6] loss 0.14125, dsc 0.85875\n",
      "Batch train [7] loss 0.12043, dsc 0.87957\n",
      "Batch train [8] loss 0.13751, dsc 0.86249\n",
      "Batch train [9] loss 0.12873, dsc 0.87127\n",
      "Batch train [10] loss 0.13221, dsc 0.86779\n",
      "Batch train [11] loss 0.17343, dsc 0.82657\n",
      "Batch train [12] loss 0.12631, dsc 0.87369\n",
      "Batch train [13] loss 0.13702, dsc 0.86298\n",
      "Batch train [14] loss 0.10537, dsc 0.89463\n",
      "Batch train [15] loss 0.11187, dsc 0.88813\n",
      "Batch train [16] loss 0.11054, dsc 0.88946\n",
      "Batch train [17] loss 0.15936, dsc 0.84064\n",
      "Batch train [18] loss 0.13226, dsc 0.86774\n",
      "Batch train [19] loss 0.11135, dsc 0.88865\n",
      "Batch train [20] loss 0.15247, dsc 0.84753\n",
      "Batch train [21] loss 0.14669, dsc 0.85331\n",
      "Batch train [22] loss 0.14769, dsc 0.85231\n",
      "Batch train [23] loss 0.13584, dsc 0.86416\n",
      "Batch train [24] loss 0.20531, dsc 0.79469\n",
      "Batch train [25] loss 0.17586, dsc 0.82414\n",
      "Batch train [26] loss 0.12953, dsc 0.87047\n",
      "Batch train [27] loss 0.14598, dsc 0.85402\n",
      "Batch train [28] loss 0.13340, dsc 0.86660\n",
      "Batch train [29] loss 0.14061, dsc 0.85939\n",
      "Batch train [30] loss 0.15641, dsc 0.84359\n",
      "Batch train [31] loss 0.13729, dsc 0.86271\n",
      "Batch train [32] loss 0.14421, dsc 0.85579\n",
      "Batch train [33] loss 0.12119, dsc 0.87881\n",
      "Batch train [34] loss 0.13050, dsc 0.86950\n",
      "Batch train [35] loss 0.14307, dsc 0.85693\n",
      "Batch train [36] loss 0.14592, dsc 0.85408\n",
      "Batch train [37] loss 0.16802, dsc 0.83198\n",
      "Batch train [38] loss 0.14401, dsc 0.85599\n",
      "Batch train [39] loss 0.12453, dsc 0.87547\n",
      "Batch train [40] loss 0.15512, dsc 0.84488\n",
      "Epoch [30] train done\n",
      "Batch eval [1] loss 0.20197, dsc 0.79803\n",
      "Batch eval [2] loss 0.20261, dsc 0.79739\n",
      "Batch eval [3] loss 0.21551, dsc 0.78449\n",
      "Batch eval [4] loss 0.20965, dsc 0.79035\n",
      "Batch eval [5] loss 0.19290, dsc 0.80710\n",
      "Epoch [30] valid done\n",
      "Epoch [30] T 219.95s, deltaT 7.29s, loss: train 0.13974, valid 0.20453, dsc: train 0.86026, valid 0.79547\n",
      "Elapsed time 0:03:39\n"
     ]
    }
   ],
   "source": [
    "# preparing model loop params\n",
    "low_res_model_info = prepare_model(epochs=30, in_channels=8, train_dataset=train_low_res_dataset, valid_dataset=valid_low_res_dataset, test_dataset=test_low_res_dataset)\n",
    "show_model_info(low_res_model_info)\n",
    "\n",
    "# getting everything necessary for model training\n",
    "low_res_train_loop_params = {k:v for k,v in low_res_model_info.items() if k not in ['model_total_params', 'model_total_trainable_params']}\n",
    "# running training loop\n",
    "train_loop(**low_res_train_loop_params)\n",
    "\n",
    "low_res_model = itemgetter('model')(low_res_model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full resolution cutting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading high/full res dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 1x dataset\n",
      "normalizing dataset\n",
      "normalizing done\n",
      "filtering labels\n",
      "filtering labels done\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n",
      "data type: float64 int8\n",
      "data max 12.81577046544424, min -0.40489707167932215\n",
      "label max 22, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53f651fa0af4c4580f984532e0360fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419fac61b0654c3faab876cf7df054de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset data and label shapes (1, 160, 32, 32) (1, 160, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "full_res_dataset = get_dataset(dataset_size=50, shrink_factor=1, filter_labels=filter_labels, unify_labels=False)\n",
    "full_res_dataset.to_numpy()\n",
    "full_res_dataset.show_data_type()\n",
    "preview_dataset(full_res_dataset, preview_index=0, show_hist=False)\n",
    "\n",
    "print('dataset data and label shapes', low_res_dataset.data_list[0].shape, full_res_dataset.data_list[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing low res network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting bounding box cut in full res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "full_res_split_dataset_obj = copy_split_dataset(full_res_dataset, low_res_split_dataset_obj)\n",
    "get_dataset_info(full_res_dataset, full_res_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### debuging cut algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved model to cpu\n",
      "CUDA Memory Usage\n",
      "GPU:        GeForce RTX 2070\n",
      "Allocated:  0.0 GB\n",
      "Cached:     0.0 GB\n",
      "Max memory: 0.2 GB\n",
      "Max Cached: 0.3 GB\n"
     ]
    }
   ],
   "source": [
    "# moving model to cpu and setting to eval mode, preventing model params changes/training\n",
    "low_res_model = low_res_model.to('cpu')\n",
    "#low_res_model.to(low_res_model_info['device'])\n",
    "low_res_model.eval()\n",
    "\n",
    "low_res_model_info['model'] = low_res_model\n",
    "torch.cuda.empty_cache()\n",
    "print('moved model to cpu')\n",
    "\n",
    "show_cuda_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_res_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug box delta [25 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1311715 1311715\n",
      "debug bounding box sizes (47, 160, 160) (72, 192, 168)\n",
      "debug bounding boxes (45, 91, 160, 319, 176, 335) (33, 104, 144, 335, 172, 339)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4ba67e4b5c422a86bf574f6d3c1b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6be72b2ec741e4911a810375282d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9730cbd82ec04734aa062197ac3770f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de62b6e887934e77890e5c0cd5769e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_index = 40\n",
    "tmp = get_full_res_cut(low_res_model, low_res_dataset.data_list[dataset_index],\n",
    "                 full_res_dataset.data_list[dataset_index], full_res_dataset.label_list[dataset_index],\n",
    "                 low_res_mask_threshold=0.5,\n",
    "                 desire_bounding_box_size=DESIRE_BOUNDING_BOX_SIZE, \n",
    "                 show_debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Running cut algorithm, creating cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting cut index 0\n",
      "debug box delta [22 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1223526 1223526\n",
      "getting cut index 1\n",
      "debug box delta [24 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1326052 1326052\n",
      "getting cut index 2\n",
      "debug box delta [21  0 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1890464 1890464\n",
      "getting cut index 3\n",
      "debug box delta [-42   0   8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1560217 1560217\n",
      "getting cut index 4\n",
      "debug box delta [21 32 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1451227 1451227\n",
      "getting cut index 5\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1262651 1262651\n",
      "getting cut index 6\n",
      "debug box delta [22 48  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1566938 1566938\n",
      "getting cut index 7\n",
      "debug box delta [27 48 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 869847 869847\n",
      "getting cut index 8\n",
      "debug box delta [19 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1397249 1397249\n",
      "getting cut index 9\n",
      "debug box delta [-3 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? False 1319795 1350330\n",
      "getting cut index 10\n",
      "debug box delta [22  0  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1635868 1635868\n",
      "getting cut index 11\n",
      "debug box delta [24 32 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1283062 1283062\n",
      "getting cut index 12\n",
      "debug box delta [23 16 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1518406 1518406\n",
      "getting cut index 13\n",
      "debug box delta [23 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1504194 1504194\n",
      "getting cut index 14\n",
      "debug box delta [26 48  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1084254 1084254\n",
      "getting cut index 15\n",
      "debug box delta [26 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1221257 1221257\n",
      "getting cut index 16\n",
      "debug box delta [22 48 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 945639 945639\n",
      "getting cut index 17\n",
      "debug box delta [ 24  16 -24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1469035 1469035\n",
      "getting cut index 18\n",
      "debug box delta [25 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1322571 1322571\n",
      "getting cut index 19\n",
      "debug box delta [16 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1593516 1593516\n",
      "getting cut index 20\n",
      "debug box delta [ 1 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1390348 1390348\n",
      "getting cut index 21\n",
      "debug box delta [ 23  16 -24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1463017 1463017\n",
      "getting cut index 22\n",
      "debug box delta [26 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1162215 1162215\n",
      "getting cut index 23\n",
      "debug box delta [23 48 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1029805 1029805\n",
      "getting cut index 24\n",
      "debug box delta [  8 -16   8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1689537 1689537\n",
      "getting cut index 25\n",
      "debug box delta [-22  32   8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1141739 1141739\n",
      "getting cut index 26\n",
      "debug box delta [20 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1167835 1167835\n",
      "getting cut index 27\n",
      "debug box delta [24  0 -8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1783264 1783264\n",
      "getting cut index 28\n",
      "debug box delta [ 21   0 -24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1944758 1944758\n",
      "getting cut index 29\n",
      "debug box delta [22 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1583396 1583396\n",
      "getting cut index 30\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1248609 1248609\n",
      "getting cut index 31\n",
      "debug box delta [25 48 40]\n",
      "debug, does cut and original label contain the same amount of pixels? True 947124 947124\n",
      "getting cut index 32\n",
      "debug box delta [-16  16   8]\n",
      "debug, does cut and original label contain the same amount of pixels? False 1417672 1648187\n",
      "getting cut index 33\n",
      "debug box delta [20 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1214697 1214697\n",
      "getting cut index 34\n",
      "debug box delta [25 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1445951 1445951\n",
      "getting cut index 35\n",
      "debug box delta [ 13  32 -24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1963068 1963068\n",
      "getting cut index 36\n",
      "debug box delta [ 0 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1241941 1241941\n",
      "getting cut index 37\n",
      "debug box delta [20 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1298886 1298886\n",
      "getting cut index 38\n",
      "debug box delta [-14  16   8]\n",
      "debug, does cut and original label contain the same amount of pixels? False 1445392 1731533\n",
      "getting cut index 39\n",
      "debug box delta [20 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1067335 1067335\n",
      "getting cut index 40\n",
      "debug box delta [25 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1311715 1311715\n",
      "getting cut index 41\n",
      "debug box delta [ 20  32 -24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1411792 1411792\n",
      "getting cut index 42\n",
      "debug box delta [29 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 951804 951804\n",
      "getting cut index 43\n",
      "debug box delta [31 48 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1024831 1024831\n",
      "getting cut index 44\n",
      "debug box delta [13 48  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1763923 1763923\n",
      "getting cut index 45\n",
      "debug box delta [ 5 32 24]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1115633 1115633\n",
      "getting cut index 46\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1670156 1670156\n",
      "getting cut index 47\n",
      "debug box delta [21 32  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1413179 1413179\n",
      "getting cut index 48\n",
      "debug box delta [-3 64 24]\n",
      "debug, does cut and original label contain the same amount of pixels? False 769376 781269\n",
      "getting cut index 49\n",
      "debug box delta [20 16  8]\n",
      "debug, does cut and original label contain the same amount of pixels? True 1756965 1756965\n"
     ]
    }
   ],
   "source": [
    "cut_full_res_dataset = full_res_dataset.copy(copy_lists=False)\n",
    "cut_full_res_dataset = get_cut_lists(low_res_model, low_res_dataset, full_res_dataset, cut_full_res_dataset, low_res_mask_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### reviewing full res and cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type: float64 int8\n",
      "\n",
      "full res shape (1, 160, 512, 512) (160, 512, 512)\n",
      "cut full res shape (1, 72, 192, 168) (72, 192, 168)\n",
      "\n",
      "dataset RAM sizes in GB 17.578125 0.9733200073242188\n",
      "single item RAM in GB 0.0390625 0.3125\n",
      "\n",
      "data max 11.780218856954171, min -0.42423961281850314\n",
      "label max 22, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30d6da09eb84472bda892490f359f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9468fb7d4d614cd6847b9262b27611c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cut_full_res_dataset.show_data_type()\n",
    "print()\n",
    "print('full res shape', full_res_dataset.data_list[0].shape, full_res_dataset.label_list[0].shape)\n",
    "print('cut full res shape', cut_full_res_dataset.data_list[0].shape, cut_full_res_dataset.label_list[0].shape)\n",
    "print()\n",
    "print('dataset RAM sizes in GB', full_res_dataset.get_data_size() / 1024**3, cut_full_res_dataset.get_data_size() / 1024**3)\n",
    "print('single item RAM in GB', full_res_dataset.label_list[0].nbytes / 1024**3, full_res_dataset.data_list[0].nbytes / 1024**3)\n",
    "print()\n",
    "preview_dataset(cut_full_res_dataset, max_slices=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Full resolution cut model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "cut_full_res_dataset.set_output_label([OARS_LABELS.EYE_L, OARS_LABELS.EYE_R, OARS_LABELS.LENS_L, OARS_LABELS.LENS_R])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cut_full_res_dataset.__getitem__(0)[1][47])\n",
    "plt.show()\n",
    "\n",
    "cut_split_dataset_obj = copy_split_dataset(cut_full_res_dataset, low_res_split_dataset_obj)\n",
    "get_dataset_info(cut_full_res_dataset, cut_split_dataset_obj)\n",
    "\n",
    "cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(\n",
    "    'train_dataset', 'valid_dataset', 'test_dataset')(cut_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Cut Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device running \"cuda\"\n",
      "max output channels 128\n",
      "Model number of params: 1193537, trainable 1193537\n",
      "Running training loop\n",
      "Batch train [1] loss 0.99691, dsc 0.00620\n",
      "Batch train [2] loss 0.99554, dsc 0.00871\n",
      "Batch train [3] loss 0.99431, dsc 0.01091\n",
      "Batch train [4] loss 0.99307, dsc 0.01358\n",
      "Batch train [5] loss 0.99542, dsc 0.00900\n",
      "Batch train [6] loss 0.99140, dsc 0.01682\n",
      "Batch train [7] loss 0.99277, dsc 0.01335\n",
      "Batch train [8] loss 0.99259, dsc 0.01263\n",
      "Batch train [9] loss 0.99229, dsc 0.01480\n",
      "Batch train [10] loss 0.99329, dsc 0.01296\n",
      "Batch train [11] loss 0.99160, dsc 0.01431\n",
      "Batch train [12] loss 0.99250, dsc 0.01219\n",
      "Batch train [13] loss 0.99149, dsc 0.01673\n",
      "Batch train [14] loss 0.98967, dsc 0.01788\n",
      "Batch train [15] loss 0.99201, dsc 0.01287\n",
      "Batch train [16] loss 0.99006, dsc 0.01981\n",
      "Batch train [17] loss 0.99132, dsc 0.01495\n",
      "Batch train [18] loss 0.99109, dsc 0.01581\n",
      "Batch train [19] loss 0.99048, dsc 0.01492\n",
      "Batch train [20] loss 0.99230, dsc 0.01247\n",
      "Epoch [1] train done\n",
      "Batch eval [1] loss 0.99721, dsc 0.00279\n",
      "Batch eval [2] loss 0.99638, dsc 0.00362\n",
      "Batch eval [3] loss 0.99572, dsc 0.00428\n",
      "Batch eval [4] loss 0.99534, dsc 0.00466\n",
      "Batch eval [5] loss 0.99297, dsc 0.00703\n",
      "Epoch [1] valid done\n",
      "Epoch [1] T 65.99s, deltaT 65.99s, loss: train 0.99250, valid 0.99552, dsc: train 0.01355, valid 0.00448\n",
      "Batch train [1] loss 0.98869, dsc 0.01870\n",
      "Batch train [2] loss 0.99150, dsc 0.01521\n",
      "Batch train [3] loss 0.99134, dsc 0.01533\n",
      "Batch train [4] loss 0.98931, dsc 0.02109\n",
      "Batch train [5] loss 0.99071, dsc 0.01834\n",
      "Batch train [6] loss 0.99251, dsc 0.01217\n",
      "Batch train [7] loss 0.99036, dsc 0.01488\n",
      "Batch train [8] loss 0.98881, dsc 0.02201\n",
      "Batch train [9] loss 0.99076, dsc 0.01563\n",
      "Batch train [10] loss 0.99006, dsc 0.01603\n",
      "Batch train [11] loss 0.98930, dsc 0.01732\n",
      "Batch train [12] loss 0.99062, dsc 0.01797\n",
      "Batch train [13] loss 0.99168, dsc 0.01303\n",
      "Batch train [14] loss 0.99146, dsc 0.01289\n",
      "Batch train [15] loss 0.99149, dsc 0.01296\n",
      "Batch train [16] loss 0.99214, dsc 0.01566\n",
      "Batch train [17] loss 0.99179, dsc 0.01609\n",
      "Batch train [18] loss 0.99066, dsc 0.01495\n",
      "Batch train [19] loss 0.99324, dsc 0.01007\n",
      "Batch train [20] loss 0.99116, dsc 0.01632\n",
      "Epoch [2] train done\n",
      "Batch eval [1] loss 0.99484, dsc 0.00516\n",
      "Batch eval [2] loss 0.99371, dsc 0.00629\n",
      "Batch eval [3] loss 0.99189, dsc 0.00811\n",
      "Batch eval [4] loss 0.99099, dsc 0.00901\n",
      "Batch eval [5] loss 0.98646, dsc 0.01354\n",
      "Epoch [2] valid done\n",
      "Epoch [2] T 127.54s, deltaT 61.55s, loss: train 0.99088, valid 0.99158, dsc: train 0.01583, valid 0.00842\n",
      "Batch train [1] loss 0.99058, dsc 0.01675\n",
      "Batch train [2] loss 0.99200, dsc 0.01567\n",
      "Batch train [3] loss 0.99063, dsc 0.01796\n",
      "Batch train [4] loss 0.99252, dsc 0.01271\n",
      "Batch train [5] loss 0.99019, dsc 0.01842\n",
      "Batch train [6] loss 0.99134, dsc 0.01280\n",
      "Batch train [7] loss 0.98922, dsc 0.01747\n",
      "Batch train [8] loss 0.98996, dsc 0.01426\n",
      "Batch train [9] loss 0.98972, dsc 0.01565\n",
      "Batch train [10] loss 0.99170, dsc 0.01352\n",
      "Batch train [11] loss 0.99035, dsc 0.01548\n",
      "Batch train [12] loss 0.98820, dsc 0.02233\n",
      "Batch train [13] loss 0.99328, dsc 0.01238\n",
      "Batch train [14] loss 0.98849, dsc 0.02298\n",
      "Batch train [15] loss 0.99093, dsc 0.01770\n",
      "Batch train [16] loss 0.99062, dsc 0.01750\n",
      "Batch train [17] loss 0.99166, dsc 0.01549\n",
      "Batch train [18] loss 0.98947, dsc 0.02033\n",
      "Batch train [19] loss 0.99044, dsc 0.01745\n",
      "Batch train [20] loss 0.99024, dsc 0.01532\n",
      "Epoch [3] train done\n",
      "Batch eval [1] loss 0.99440, dsc 0.00560\n",
      "Batch eval [2] loss 0.99314, dsc 0.00686\n",
      "Batch eval [3] loss 0.99121, dsc 0.00879\n",
      "Batch eval [4] loss 0.99017, dsc 0.00983\n",
      "Batch eval [5] loss 0.98523, dsc 0.01477\n",
      "Epoch [3] valid done\n",
      "Epoch [3] T 186.75s, deltaT 59.22s, loss: train 0.99058, valid 0.99083, dsc: train 0.01661, valid 0.00917\n",
      "Batch train [1] loss 0.98893, dsc 0.01823\n",
      "Batch train [2] loss 0.99157, dsc 0.01452\n",
      "Batch train [3] loss 0.99110, dsc 0.01488\n",
      "Batch train [4] loss 0.99051, dsc 0.01747\n",
      "Batch train [5] loss 0.99048, dsc 0.01592\n",
      "Batch train [6] loss 0.99157, dsc 0.01203\n",
      "Batch train [7] loss 0.99221, dsc 0.01373\n",
      "Batch train [8] loss 0.98985, dsc 0.01514\n",
      "Batch train [9] loss 0.98801, dsc 0.02165\n",
      "Batch train [10] loss 0.99084, dsc 0.01709\n",
      "Batch train [11] loss 0.99238, dsc 0.01401\n",
      "Batch train [12] loss 0.98916, dsc 0.01741\n",
      "Batch train [13] loss 0.98886, dsc 0.01699\n",
      "Batch train [14] loss 0.99066, dsc 0.01513\n",
      "Batch train [15] loss 0.99023, dsc 0.01439\n",
      "Batch train [16] loss 0.99211, dsc 0.01560\n",
      "Batch train [17] loss 0.99108, dsc 0.01534\n",
      "Batch train [18] loss 0.98873, dsc 0.02204\n",
      "Batch train [19] loss 0.98869, dsc 0.02195\n",
      "Batch train [20] loss 0.99006, dsc 0.01716\n",
      "Epoch [4] train done\n",
      "Batch eval [1] loss 0.99627, dsc 0.00373\n",
      "Batch eval [2] loss 0.99314, dsc 0.00686\n",
      "Batch eval [3] loss 0.99112, dsc 0.00888\n",
      "Batch eval [4] loss 0.99009, dsc 0.00991\n",
      "Batch eval [5] loss 0.98511, dsc 0.01489\n",
      "Epoch [4] valid done\n",
      "Epoch [4] T 245.68s, deltaT 58.93s, loss: train 0.99035, valid 0.99115, dsc: train 0.01653, valid 0.00885\n",
      "Batch train [1] loss 0.98829, dsc 0.02171\n",
      "Batch train [2] loss 0.99063, dsc 0.01400\n",
      "Batch train [3] loss 0.98954, dsc 0.01618\n",
      "Batch train [4] loss 0.99031, dsc 0.01417\n",
      "Batch train [5] loss 0.99020, dsc 0.01701\n",
      "Batch train [6] loss 0.99063, dsc 0.01324\n",
      "Batch train [7] loss 0.98879, dsc 0.01717\n",
      "Batch train [8] loss 0.98983, dsc 0.01554\n",
      "Batch train [9] loss 0.99059, dsc 0.01403\n",
      "Batch train [10] loss 0.98835, dsc 0.02210\n",
      "Batch train [11] loss 0.99109, dsc 0.01560\n",
      "Batch train [12] loss 0.99101, dsc 0.01382\n",
      "Batch train [13] loss 0.99061, dsc 0.01379\n",
      "Batch train [14] loss 0.99241, dsc 0.01512\n",
      "Batch train [15] loss 0.99122, dsc 0.01491\n",
      "Batch train [16] loss 0.98911, dsc 0.01732\n",
      "Batch train [17] loss 0.99163, dsc 0.01654\n",
      "Batch train [18] loss 0.98827, dsc 0.02269\n",
      "Batch train [19] loss 0.98838, dsc 0.02280\n",
      "Batch train [20] loss 0.99121, dsc 0.01593\n",
      "Epoch [5] train done\n",
      "Batch eval [1] loss 0.99410, dsc 0.00590\n",
      "Batch eval [2] loss 0.99297, dsc 0.00703\n",
      "Batch eval [3] loss 0.99099, dsc 0.00901\n",
      "Batch eval [4] loss 0.98996, dsc 0.01004\n",
      "Batch eval [5] loss 0.98492, dsc 0.01508\n",
      "Epoch [5] valid done\n",
      "Epoch [5] T 304.99s, deltaT 59.31s, loss: train 0.99011, valid 0.99059, dsc: train 0.01668, valid 0.00941\n",
      "Batch train [1] loss 0.98806, dsc 0.02203\n",
      "Batch train [2] loss 0.99071, dsc 0.01421\n",
      "Batch train [3] loss 0.99029, dsc 0.01519\n",
      "Batch train [4] loss 0.99024, dsc 0.01673\n",
      "Batch train [5] loss 0.99276, dsc 0.01381\n",
      "Batch train [6] loss 0.98883, dsc 0.01663\n",
      "Batch train [7] loss 0.99184, dsc 0.01570\n",
      "Batch train [8] loss 0.98761, dsc 0.02363\n",
      "Batch train [9] loss 0.99229, dsc 0.01236\n",
      "Batch train [10] loss 0.99000, dsc 0.01813\n",
      "Batch train [11] loss 0.98752, dsc 0.01923\n",
      "Batch train [12] loss 0.98914, dsc 0.01679\n",
      "Batch train [13] loss 0.98893, dsc 0.02043\n",
      "Batch train [14] loss 0.98981, dsc 0.01918\n",
      "Batch train [15] loss 0.99086, dsc 0.01452\n",
      "Batch train [16] loss 0.99229, dsc 0.01490\n",
      "Batch train [17] loss 0.99093, dsc 0.01296\n",
      "Batch train [18] loss 0.98812, dsc 0.01852\n",
      "Batch train [19] loss 0.98942, dsc 0.01821\n",
      "Batch train [20] loss 0.98777, dsc 0.01754\n",
      "Epoch [6] train done\n",
      "Batch eval [1] loss 0.99393, dsc 0.00607\n",
      "Batch eval [2] loss 0.99268, dsc 0.00732\n",
      "Batch eval [3] loss 0.99064, dsc 0.00936\n",
      "Batch eval [4] loss 0.98953, dsc 0.01047\n",
      "Batch eval [5] loss 0.98425, dsc 0.01575\n",
      "Epoch [6] valid done\n",
      "Epoch [6] T 365.75s, deltaT 60.76s, loss: train 0.98987, valid 0.99021, dsc: train 0.01704, valid 0.00979\n",
      "Batch train [1] loss 0.98692, dsc 0.01906\n",
      "Batch train [2] loss 0.99184, dsc 0.01609\n",
      "Batch train [3] loss 0.99029, dsc 0.01929\n",
      "Batch train [4] loss 0.99059, dsc 0.01291\n",
      "Batch train [5] loss 0.99056, dsc 0.01386\n",
      "Batch train [6] loss 0.99013, dsc 0.01577\n",
      "Batch train [7] loss 0.99085, dsc 0.01521\n",
      "Batch train [8] loss 0.99250, dsc 0.01155\n",
      "Batch train [9] loss 0.98780, dsc 0.02352\n",
      "Batch train [10] loss 0.98854, dsc 0.01719\n",
      "Batch train [11] loss 0.98939, dsc 0.01847\n",
      "Batch train [12] loss 0.98967, dsc 0.01840\n",
      "Batch train [13] loss 0.98833, dsc 0.01805\n",
      "Batch train [14] loss 0.98613, dsc 0.02556\n",
      "Batch train [15] loss 0.98964, dsc 0.01812\n",
      "Batch train [16] loss 0.99129, dsc 0.01375\n",
      "Batch train [17] loss 0.99012, dsc 0.01496\n",
      "Batch train [18] loss 0.99101, dsc 0.01272\n",
      "Batch train [19] loss 0.98733, dsc 0.01939\n",
      "Batch train [20] loss 0.98915, dsc 0.01760\n",
      "Epoch [7] train done\n",
      "Batch eval [1] loss 0.99445, dsc 0.00555\n",
      "Batch eval [2] loss 0.99254, dsc 0.00746\n",
      "Batch eval [3] loss 0.99045, dsc 0.00955\n",
      "Batch eval [4] loss 0.98930, dsc 0.01070\n",
      "Batch eval [5] loss 0.98391, dsc 0.01609\n",
      "Epoch [7] valid done\n",
      "Epoch [7] T 425.91s, deltaT 60.16s, loss: train 0.98960, valid 0.99013, dsc: train 0.01707, valid 0.00987\n",
      "Batch train [1] loss 0.98986, dsc 0.01740\n",
      "Batch train [2] loss 0.98605, dsc 0.02780\n",
      "Batch train [3] loss 0.99075, dsc 0.01353\n",
      "Batch train [4] loss 0.99145, dsc 0.01283\n",
      "Batch train [5] loss 0.99154, dsc 0.01407\n",
      "Batch train [6] loss 0.99039, dsc 0.01856\n",
      "Batch train [7] loss 0.99161, dsc 0.01595\n",
      "Batch train [8] loss 0.99050, dsc 0.01327\n",
      "Batch train [9] loss 0.98978, dsc 0.01555\n",
      "Batch train [10] loss 0.99094, dsc 0.01337\n",
      "Batch train [11] loss 0.99069, dsc 0.01278\n",
      "Batch train [12] loss 0.98866, dsc 0.01677\n",
      "Batch train [13] loss 0.98910, dsc 0.01669\n",
      "Batch train [14] loss 0.98715, dsc 0.02381\n",
      "Batch train [15] loss 0.98796, dsc 0.01913\n",
      "Batch train [16] loss 0.98813, dsc 0.02150\n",
      "Batch train [17] loss 0.98891, dsc 0.01835\n",
      "Batch train [18] loss 0.98652, dsc 0.02046\n",
      "Batch train [19] loss 0.98901, dsc 0.01576\n",
      "Batch train [20] loss 0.98744, dsc 0.02500\n",
      "Epoch [8] train done\n",
      "Batch eval [1] loss 0.99418, dsc 0.00582\n",
      "Batch eval [2] loss 0.99228, dsc 0.00772\n",
      "Batch eval [3] loss 0.99015, dsc 0.00985\n",
      "Batch eval [4] loss 0.98895, dsc 0.01105\n",
      "Batch eval [5] loss 0.98339, dsc 0.01661\n",
      "Epoch [8] valid done\n",
      "Epoch [8] T 486.89s, deltaT 60.98s, loss: train 0.98932, valid 0.98979, dsc: train 0.01763, valid 0.01021\n",
      "Batch train [1] loss 0.99017, dsc 0.01380\n",
      "Batch train [2] loss 0.98652, dsc 0.02615\n",
      "Batch train [3] loss 0.98695, dsc 0.02299\n",
      "Batch train [4] loss 0.99026, dsc 0.01821\n",
      "Batch train [5] loss 0.98845, dsc 0.01903\n",
      "Batch train [6] loss 0.99111, dsc 0.01600\n",
      "Batch train [7] loss 0.98636, dsc 0.02522\n",
      "Batch train [8] loss 0.98861, dsc 0.01728\n",
      "Batch train [9] loss 0.98573, dsc 0.02371\n",
      "Batch train [10] loss 0.99052, dsc 0.01749\n",
      "Batch train [11] loss 0.99055, dsc 0.01420\n",
      "Batch train [12] loss 0.98980, dsc 0.01420\n",
      "Batch train [13] loss 0.99108, dsc 0.01292\n",
      "Batch train [14] loss 0.98721, dsc 0.02238\n",
      "Batch train [15] loss 0.98776, dsc 0.02164\n",
      "Batch train [16] loss 0.99163, dsc 0.01418\n",
      "Batch train [17] loss 0.98959, dsc 0.01526\n",
      "Batch train [18] loss 0.98858, dsc 0.01646\n",
      "Batch train [19] loss 0.99093, dsc 0.01404\n",
      "Batch train [20] loss 0.98883, dsc 0.01939\n",
      "Epoch [9] train done\n",
      "Batch eval [1] loss 0.99439, dsc 0.00561\n",
      "Batch eval [2] loss 0.99199, dsc 0.00801\n",
      "Batch eval [3] loss 0.98981, dsc 0.01019\n",
      "Batch eval [4] loss 0.98855, dsc 0.01145\n",
      "Batch eval [5] loss 0.98275, dsc 0.01725\n",
      "Epoch [9] valid done\n",
      "Epoch [9] T 549.78s, deltaT 62.89s, loss: train 0.98903, valid 0.98950, dsc: train 0.01823, valid 0.01050\n",
      "Batch train [1] loss 0.98849, dsc 0.01867\n",
      "Batch train [2] loss 0.98859, dsc 0.01611\n",
      "Batch train [3] loss 0.98836, dsc 0.02012\n",
      "Batch train [4] loss 0.98816, dsc 0.01525\n",
      "Batch train [5] loss 0.99026, dsc 0.01365\n",
      "Batch train [6] loss 0.98960, dsc 0.01442\n",
      "Batch train [7] loss 0.98834, dsc 0.01750\n",
      "Batch train [8] loss 0.98948, dsc 0.01690\n",
      "Batch train [9] loss 0.98776, dsc 0.01896\n",
      "Batch train [10] loss 0.98975, dsc 0.01969\n",
      "Batch train [11] loss 0.98899, dsc 0.01632\n",
      "Batch train [12] loss 0.98773, dsc 0.01661\n",
      "Batch train [13] loss 0.98707, dsc 0.02533\n",
      "Batch train [14] loss 0.98817, dsc 0.01701\n",
      "Batch train [15] loss 0.99104, dsc 0.01676\n",
      "Batch train [16] loss 0.98898, dsc 0.01934\n",
      "Batch train [17] loss 0.98869, dsc 0.01979\n",
      "Batch train [18] loss 0.98745, dsc 0.01822\n",
      "Batch train [19] loss 0.99073, dsc 0.01725\n",
      "Batch train [20] loss 0.98622, dsc 0.02076\n",
      "Epoch [10] train done\n",
      "Batch eval [1] loss 0.99329, dsc 0.00671\n",
      "Batch eval [2] loss 0.99190, dsc 0.00810\n",
      "Batch eval [3] loss 0.98970, dsc 0.01030\n",
      "Batch eval [4] loss 0.98841, dsc 0.01159\n",
      "Batch eval [5] loss 0.98256, dsc 0.01744\n",
      "Epoch [10] valid done\n",
      "Epoch [10] T 611.10s, deltaT 61.32s, loss: train 0.98869, valid 0.98917, dsc: train 0.01793, valid 0.01083\n",
      "Batch train [1] loss 0.98833, dsc 0.01819\n",
      "Batch train [2] loss 0.98854, dsc 0.01482\n",
      "Batch train [3] loss 0.98995, dsc 0.01598\n",
      "Batch train [4] loss 0.98631, dsc 0.02444\n",
      "Batch train [5] loss 0.98783, dsc 0.01582\n",
      "Batch train [6] loss 0.98617, dsc 0.02417\n",
      "Batch train [7] loss 0.98565, dsc 0.02362\n",
      "Batch train [8] loss 0.98677, dsc 0.02611\n",
      "Batch train [9] loss 0.99153, dsc 0.01527\n",
      "Batch train [10] loss 0.98583, dsc 0.01837\n",
      "Batch train [11] loss 0.98778, dsc 0.02017\n",
      "Batch train [12] loss 0.99001, dsc 0.01935\n",
      "Batch train [13] loss 0.98748, dsc 0.01830\n",
      "Batch train [14] loss 0.98938, dsc 0.01847\n",
      "Batch train [15] loss 0.99087, dsc 0.01638\n",
      "Batch train [16] loss 0.98619, dsc 0.02722\n",
      "Batch train [17] loss 0.98988, dsc 0.01461\n",
      "Batch train [18] loss 0.98845, dsc 0.01535\n",
      "Batch train [19] loss 0.98778, dsc 0.01670\n",
      "Batch train [20] loss 0.99204, dsc 0.01208\n",
      "Epoch [11] train done\n",
      "Batch eval [1] loss 0.99413, dsc 0.00587\n",
      "Batch eval [2] loss 0.99149, dsc 0.00851\n",
      "Batch eval [3] loss 0.98919, dsc 0.01081\n",
      "Batch eval [4] loss 0.98782, dsc 0.01218\n",
      "Batch eval [5] loss 0.98167, dsc 0.01833\n",
      "Epoch [11] valid done\n",
      "Epoch [11] T 672.26s, deltaT 61.16s, loss: train 0.98834, valid 0.98886, dsc: train 0.01877, valid 0.01114\n",
      "Batch train [1] loss 0.98828, dsc 0.01957\n",
      "Batch train [2] loss 0.98989, dsc 0.01370\n",
      "Batch train [3] loss 0.99056, dsc 0.01671\n",
      "Batch train [4] loss 0.98783, dsc 0.02040\n",
      "Batch train [5] loss 0.98692, dsc 0.02019\n",
      "Batch train [6] loss 0.98680, dsc 0.01926\n",
      "Batch train [7] loss 0.98845, dsc 0.01496\n",
      "Batch train [8] loss 0.98535, dsc 0.01963\n",
      "Batch train [9] loss 0.98799, dsc 0.01669\n",
      "Batch train [10] loss 0.98801, dsc 0.02188\n",
      "Batch train [11] loss 0.98730, dsc 0.02254\n",
      "Batch train [12] loss 0.98830, dsc 0.01725\n",
      "Batch train [13] loss 0.99144, dsc 0.01463\n",
      "Batch train [14] loss 0.98385, dsc 0.02299\n",
      "Batch train [15] loss 0.98722, dsc 0.01643\n",
      "Batch train [16] loss 0.99098, dsc 0.01384\n",
      "Batch train [17] loss 0.98693, dsc 0.01855\n",
      "Batch train [18] loss 0.98877, dsc 0.01888\n",
      "Batch train [19] loss 0.98610, dsc 0.01983\n",
      "Batch train [20] loss 0.98797, dsc 0.01939\n",
      "Epoch [12] train done\n",
      "Batch eval [1] loss 0.99502, dsc 0.00498\n",
      "Batch eval [2] loss 0.99121, dsc 0.00879\n",
      "Batch eval [3] loss 0.98887, dsc 0.01113\n",
      "Batch eval [4] loss 0.98742, dsc 0.01258\n",
      "Batch eval [5] loss 0.98111, dsc 0.01889\n",
      "Epoch [12] valid done\n",
      "Epoch [12] T 734.39s, deltaT 62.12s, loss: train 0.98795, valid 0.98873, dsc: train 0.01837, valid 0.01127\n",
      "Batch train [1] loss 0.98951, dsc 0.01823\n",
      "Batch train [2] loss 0.98589, dsc 0.02648\n",
      "Batch train [3] loss 0.98520, dsc 0.02679\n",
      "Batch train [4] loss 0.98592, dsc 0.02674\n",
      "Batch train [5] loss 0.98765, dsc 0.02302\n",
      "Batch train [6] loss 0.98960, dsc 0.02052\n",
      "Batch train [7] loss 0.98923, dsc 0.01427\n",
      "Batch train [8] loss 0.98634, dsc 0.01930\n",
      "Batch train [9] loss 0.98692, dsc 0.01972\n",
      "Batch train [10] loss 0.99050, dsc 0.01803\n",
      "Batch train [11] loss 0.99040, dsc 0.01393\n",
      "Batch train [12] loss 0.98840, dsc 0.01550\n",
      "Batch train [13] loss 0.98780, dsc 0.01843\n",
      "Batch train [14] loss 0.98733, dsc 0.01689\n",
      "Batch train [15] loss 0.98600, dsc 0.02122\n",
      "Batch train [16] loss 0.98545, dsc 0.01929\n",
      "Batch train [17] loss 0.98930, dsc 0.01434\n",
      "Batch train [18] loss 0.98928, dsc 0.02002\n",
      "Batch train [19] loss 0.98504, dsc 0.02151\n",
      "Batch train [20] loss 0.98480, dsc 0.02798\n",
      "Epoch [13] train done\n",
      "Batch eval [1] loss 0.99405, dsc 0.00595\n",
      "Batch eval [2] loss 0.99117, dsc 0.00883\n",
      "Batch eval [3] loss 0.98881, dsc 0.01119\n",
      "Batch eval [4] loss 0.98743, dsc 0.01257\n",
      "Batch eval [5] loss 0.98113, dsc 0.01887\n",
      "Epoch [13] valid done\n",
      "Epoch [13] T 796.85s, deltaT 62.46s, loss: train 0.98753, valid 0.98852, dsc: train 0.02011, valid 0.01148\n",
      "Batch train [1] loss 0.98984, dsc 0.01706\n",
      "Batch train [2] loss 0.98720, dsc 0.01710\n",
      "Batch train [3] loss 0.98740, dsc 0.02163\n",
      "Batch train [4] loss 0.98424, dsc 0.02319\n",
      "Batch train [5] loss 0.98608, dsc 0.02515\n",
      "Batch train [6] loss 0.98783, dsc 0.02220\n",
      "Batch train [7] loss 0.98477, dsc 0.02929\n",
      "Batch train [8] loss 0.98973, dsc 0.01667\n",
      "Batch train [9] loss 0.98724, dsc 0.02247\n",
      "Batch train [10] loss 0.98776, dsc 0.01553\n",
      "Batch train [11] loss 0.98992, dsc 0.01458\n",
      "Batch train [12] loss 0.98658, dsc 0.01687\n",
      "Batch train [13] loss 0.98775, dsc 0.01573\n",
      "Batch train [14] loss 0.98926, dsc 0.01334\n",
      "Batch train [15] loss 0.98554, dsc 0.02542\n",
      "Batch train [16] loss 0.98573, dsc 0.01854\n",
      "Batch train [17] loss 0.98525, dsc 0.01915\n",
      "Batch train [18] loss 0.98627, dsc 0.02242\n",
      "Batch train [19] loss 0.98521, dsc 0.02108\n",
      "Batch train [20] loss 0.98692, dsc 0.02087\n",
      "Epoch [14] train done\n",
      "Batch eval [1] loss 0.99253, dsc 0.00747\n",
      "Batch eval [2] loss 0.99062, dsc 0.00938\n",
      "Batch eval [3] loss 0.98824, dsc 0.01176\n",
      "Batch eval [4] loss 0.98667, dsc 0.01333\n",
      "Batch eval [5] loss 0.97999, dsc 0.02001\n",
      "Epoch [14] valid done\n",
      "Epoch [14] T 857.05s, deltaT 60.19s, loss: train 0.98703, valid 0.98761, dsc: train 0.01992, valid 0.01239\n",
      "Batch train [1] loss 0.98683, dsc 0.01827\n",
      "Batch train [2] loss 0.98580, dsc 0.02024\n",
      "Batch train [3] loss 0.98878, dsc 0.01516\n",
      "Batch train [4] loss 0.98649, dsc 0.02063\n",
      "Batch train [5] loss 0.98705, dsc 0.02409\n",
      "Batch train [6] loss 0.98896, dsc 0.01494\n",
      "Batch train [7] loss 0.98179, dsc 0.02272\n",
      "Batch train [8] loss 0.98986, dsc 0.01695\n",
      "Batch train [9] loss 0.98620, dsc 0.02425\n",
      "Batch train [10] loss 0.98360, dsc 0.02752\n",
      "Batch train [11] loss 0.98602, dsc 0.01930\n",
      "Batch train [12] loss 0.98556, dsc 0.02349\n",
      "Batch train [13] loss 0.98537, dsc 0.01968\n",
      "Batch train [14] loss 0.98596, dsc 0.01936\n",
      "Batch train [15] loss 0.98599, dsc 0.02161\n",
      "Batch train [16] loss 0.98858, dsc 0.01469\n",
      "Batch train [17] loss 0.98502, dsc 0.01893\n",
      "Batch train [18] loss 0.98723, dsc 0.01652\n",
      "Batch train [19] loss 0.98301, dsc 0.03137\n",
      "Batch train [20] loss 0.99100, dsc 0.01723\n",
      "Epoch [15] train done\n",
      "Batch eval [1] loss 0.99209, dsc 0.00791\n",
      "Batch eval [2] loss 0.99026, dsc 0.00974\n",
      "Batch eval [3] loss 0.98773, dsc 0.01227\n",
      "Batch eval [4] loss 0.98629, dsc 0.01371\n",
      "Batch eval [5] loss 0.97931, dsc 0.02069\n",
      "Epoch [15] valid done\n",
      "Epoch [15] T 917.06s, deltaT 60.01s, loss: train 0.98645, valid 0.98714, dsc: train 0.02035, valid 0.01286\n",
      "Batch train [1] loss 0.98606, dsc 0.02165\n",
      "Batch train [2] loss 0.98641, dsc 0.01710\n",
      "Batch train [3] loss 0.98872, dsc 0.01932\n",
      "Batch train [4] loss 0.98865, dsc 0.02021\n",
      "Batch train [5] loss 0.98400, dsc 0.03108\n",
      "Batch train [6] loss 0.98455, dsc 0.01975\n",
      "Batch train [7] loss 0.98425, dsc 0.02348\n",
      "Batch train [8] loss 0.98502, dsc 0.01830\n",
      "Batch train [9] loss 0.98455, dsc 0.01990\n",
      "Batch train [10] loss 0.98674, dsc 0.02630\n",
      "Batch train [11] loss 0.98462, dsc 0.01998\n",
      "Batch train [12] loss 0.98698, dsc 0.02565\n",
      "Batch train [13] loss 0.98484, dsc 0.02204\n",
      "Batch train [14] loss 0.98786, dsc 0.02360\n",
      "Batch train [15] loss 0.98199, dsc 0.02757\n",
      "Batch train [16] loss 0.99091, dsc 0.01496\n",
      "Batch train [17] loss 0.98554, dsc 0.01827\n",
      "Batch train [18] loss 0.98468, dsc 0.02081\n",
      "Batch train [19] loss 0.98704, dsc 0.01625\n",
      "Batch train [20] loss 0.98396, dsc 0.03123\n",
      "Epoch [16] train done\n",
      "Batch eval [1] loss 0.99265, dsc 0.00735\n",
      "Batch eval [2] loss 0.98985, dsc 0.01015\n",
      "Batch eval [3] loss 0.98735, dsc 0.01265\n",
      "Batch eval [4] loss 0.98571, dsc 0.01429\n",
      "Batch eval [5] loss 0.97845, dsc 0.02155\n",
      "Epoch [16] valid done\n",
      "Epoch [16] T 978.01s, deltaT 60.95s, loss: train 0.98587, valid 0.98680, dsc: train 0.02187, valid 0.01320\n",
      "Batch train [1] loss 0.98550, dsc 0.01805\n",
      "Batch train [2] loss 0.98063, dsc 0.03855\n",
      "Batch train [3] loss 0.98594, dsc 0.02387\n",
      "Batch train [4] loss 0.98682, dsc 0.02519\n",
      "Batch train [5] loss 0.98235, dsc 0.02415\n",
      "Batch train [6] loss 0.98474, dsc 0.02313\n",
      "Batch train [7] loss 0.98342, dsc 0.02202\n",
      "Batch train [8] loss 0.98499, dsc 0.02206\n",
      "Batch train [9] loss 0.98252, dsc 0.02266\n",
      "Batch train [10] loss 0.98529, dsc 0.02608\n",
      "Batch train [11] loss 0.98672, dsc 0.01669\n",
      "Batch train [12] loss 0.98743, dsc 0.01706\n",
      "Batch train [13] loss 0.98857, dsc 0.02275\n",
      "Batch train [14] loss 0.98416, dsc 0.02119\n",
      "Batch train [15] loss 0.98896, dsc 0.01995\n",
      "Batch train [16] loss 0.98538, dsc 0.02347\n",
      "Batch train [17] loss 0.98517, dsc 0.02856\n",
      "Batch train [18] loss 0.98758, dsc 0.01503\n",
      "Batch train [19] loss 0.98399, dsc 0.02052\n",
      "Batch train [20] loss 0.98282, dsc 0.02248\n",
      "Epoch [17] train done\n",
      "Batch eval [1] loss 0.99141, dsc 0.00859\n",
      "Batch eval [2] loss 0.98959, dsc 0.01041\n",
      "Batch eval [3] loss 0.98685, dsc 0.01315\n",
      "Batch eval [4] loss 0.98534, dsc 0.01466\n",
      "Batch eval [5] loss 0.97798, dsc 0.02202\n",
      "Epoch [17] valid done\n",
      "Epoch [17] T 1037.81s, deltaT 59.81s, loss: train 0.98515, valid 0.98624, dsc: train 0.02267, valid 0.01376\n",
      "Batch train [1] loss 0.98080, dsc 0.02599\n",
      "Batch train [2] loss 0.98389, dsc 0.01999\n",
      "Batch train [3] loss 0.98154, dsc 0.02924\n",
      "Batch train [4] loss 0.98377, dsc 0.02610\n",
      "Batch train [5] loss 0.98837, dsc 0.01887\n",
      "Batch train [6] loss 0.98759, dsc 0.01672\n",
      "Batch train [7] loss 0.98763, dsc 0.01898\n",
      "Batch train [8] loss 0.98538, dsc 0.02645\n",
      "Batch train [9] loss 0.98619, dsc 0.02116\n",
      "Batch train [10] loss 0.98492, dsc 0.02180\n",
      "Batch train [11] loss 0.98442, dsc 0.01867\n",
      "Batch train [12] loss 0.98594, dsc 0.02616\n",
      "Batch train [13] loss 0.98193, dsc 0.02470\n",
      "Batch train [14] loss 0.98362, dsc 0.02828\n",
      "Batch train [15] loss 0.98016, dsc 0.03577\n",
      "Batch train [16] loss 0.98609, dsc 0.01685\n",
      "Batch train [17] loss 0.98216, dsc 0.02340\n",
      "Batch train [18] loss 0.98696, dsc 0.01663\n",
      "Batch train [19] loss 0.98338, dsc 0.02910\n",
      "Batch train [20] loss 0.98206, dsc 0.02661\n",
      "Epoch [18] train done\n",
      "Batch eval [1] loss 0.99234, dsc 0.00766\n",
      "Batch eval [2] loss 0.98827, dsc 0.01173\n",
      "Batch eval [3] loss 0.98521, dsc 0.01479\n",
      "Batch eval [4] loss 0.98344, dsc 0.01656\n",
      "Batch eval [5] loss 0.97521, dsc 0.02479\n",
      "Epoch [18] valid done\n",
      "Epoch [18] T 1098.02s, deltaT 60.20s, loss: train 0.98434, valid 0.98489, dsc: train 0.02357, valid 0.01511\n",
      "Batch train [1] loss 0.98480, dsc 0.01904\n",
      "Batch train [2] loss 0.98441, dsc 0.02181\n",
      "Batch train [3] loss 0.98589, dsc 0.01685\n",
      "Batch train [4] loss 0.98298, dsc 0.02095\n",
      "Batch train [5] loss 0.98324, dsc 0.02175\n",
      "Batch train [6] loss 0.98412, dsc 0.02711\n",
      "Batch train [7] loss 0.98454, dsc 0.01881\n",
      "Batch train [8] loss 0.98343, dsc 0.02057\n",
      "Batch train [9] loss 0.98141, dsc 0.02362\n",
      "Batch train [10] loss 0.98468, dsc 0.02305\n",
      "Batch train [11] loss 0.98206, dsc 0.02694\n",
      "Batch train [12] loss 0.98651, dsc 0.02553\n",
      "Batch train [13] loss 0.98266, dsc 0.02208\n",
      "Batch train [14] loss 0.98431, dsc 0.02589\n",
      "Batch train [15] loss 0.98404, dsc 0.02831\n",
      "Batch train [16] loss 0.97835, dsc 0.03899\n",
      "Batch train [17] loss 0.97893, dsc 0.04119\n",
      "Batch train [18] loss 0.98571, dsc 0.01845\n",
      "Batch train [19] loss 0.98394, dsc 0.02696\n",
      "Batch train [20] loss 0.98353, dsc 0.03053\n",
      "Epoch [19] train done\n",
      "Batch eval [1] loss 0.99329, dsc 0.00671\n",
      "Batch eval [2] loss 0.98331, dsc 0.01669\n",
      "Batch eval [3] loss 0.97826, dsc 0.02174\n",
      "Batch eval [4] loss 0.97752, dsc 0.02248\n",
      "Batch eval [5] loss 0.96692, dsc 0.03308\n",
      "Epoch [19] valid done\n",
      "Epoch [19] T 1157.73s, deltaT 59.71s, loss: train 0.98348, valid 0.97986, dsc: train 0.02492, valid 0.02014\n",
      "Batch train [1] loss 0.98843, dsc 0.01956\n",
      "Batch train [2] loss 0.98314, dsc 0.02706\n",
      "Batch train [3] loss 0.98271, dsc 0.02451\n",
      "Batch train [4] loss 0.98606, dsc 0.02515\n",
      "Batch train [5] loss 0.98360, dsc 0.02682\n",
      "Batch train [6] loss 0.98561, dsc 0.02391\n",
      "Batch train [7] loss 0.98138, dsc 0.02814\n",
      "Batch train [8] loss 0.98180, dsc 0.02138\n",
      "Batch train [9] loss 0.98425, dsc 0.02084\n",
      "Batch train [10] loss 0.97921, dsc 0.03455\n",
      "Batch train [11] loss 0.98155, dsc 0.02290\n",
      "Batch train [12] loss 0.97893, dsc 0.04170\n",
      "Batch train [13] loss 0.98643, dsc 0.01741\n",
      "Batch train [14] loss 0.97933, dsc 0.02723\n",
      "Batch train [15] loss 0.98005, dsc 0.02558\n",
      "Batch train [16] loss 0.97902, dsc 0.02547\n",
      "Batch train [17] loss 0.98403, dsc 0.02922\n",
      "Batch train [18] loss 0.98577, dsc 0.02109\n",
      "Batch train [19] loss 0.97758, dsc 0.04358\n",
      "Batch train [20] loss 0.98068, dsc 0.02877\n",
      "Epoch [20] train done\n",
      "Batch eval [1] loss 0.99540, dsc 0.00460\n",
      "Batch eval [2] loss 0.98502, dsc 0.01498\n",
      "Batch eval [3] loss 0.98077, dsc 0.01923\n",
      "Batch eval [4] loss 0.97943, dsc 0.02057\n",
      "Batch eval [5] loss 0.96902, dsc 0.03098\n",
      "Epoch [20] valid done\n",
      "Epoch [20] T 1218.82s, deltaT 61.09s, loss: train 0.98248, valid 0.98193, dsc: train 0.02674, valid 0.01807\n",
      "Batch train [1] loss 0.98349, dsc 0.01928\n",
      "Batch train [2] loss 0.97574, dsc 0.04099\n",
      "Batch train [3] loss 0.98123, dsc 0.02755\n",
      "Batch train [4] loss 0.97873, dsc 0.03547\n",
      "Batch train [5] loss 0.98298, dsc 0.02006\n",
      "Batch train [6] loss 0.98835, dsc 0.01803\n",
      "Batch train [7] loss 0.97650, dsc 0.02923\n",
      "Batch train [8] loss 0.98374, dsc 0.01928\n",
      "Batch train [9] loss 0.98425, dsc 0.01916\n",
      "Batch train [10] loss 0.97819, dsc 0.03915\n",
      "Batch train [11] loss 0.98304, dsc 0.02064\n",
      "Batch train [12] loss 0.98091, dsc 0.02791\n",
      "Batch train [13] loss 0.97952, dsc 0.02705\n",
      "Batch train [14] loss 0.98470, dsc 0.02781\n",
      "Batch train [15] loss 0.98041, dsc 0.02965\n",
      "Batch train [16] loss 0.98330, dsc 0.01939\n",
      "Batch train [17] loss 0.98215, dsc 0.03393\n",
      "Batch train [18] loss 0.97959, dsc 0.02586\n",
      "Batch train [19] loss 0.98119, dsc 0.03531\n",
      "Batch train [20] loss 0.97947, dsc 0.03391\n",
      "Epoch [21] train done\n",
      "Batch eval [1] loss 0.99543, dsc 0.00457\n",
      "Batch eval [2] loss 0.98341, dsc 0.01659\n",
      "Batch eval [3] loss 0.97846, dsc 0.02154\n",
      "Batch eval [4] loss 0.97759, dsc 0.02241\n",
      "Batch eval [5] loss 0.96583, dsc 0.03417\n",
      "Epoch [21] valid done\n",
      "Epoch [21] T 1278.44s, deltaT 59.62s, loss: train 0.98137, valid 0.98014, dsc: train 0.02748, valid 0.01986\n",
      "Batch train [1] loss 0.97892, dsc 0.02563\n",
      "Batch train [2] loss 0.98179, dsc 0.03610\n",
      "Batch train [3] loss 0.98030, dsc 0.03133\n",
      "Batch train [4] loss 0.97583, dsc 0.04571\n",
      "Batch train [5] loss 0.97888, dsc 0.03062\n",
      "Batch train [6] loss 0.98251, dsc 0.02300\n",
      "Batch train [7] loss 0.97831, dsc 0.03397\n",
      "Batch train [8] loss 0.97996, dsc 0.02472\n",
      "Batch train [9] loss 0.98307, dsc 0.02736\n",
      "Batch train [10] loss 0.97874, dsc 0.03661\n",
      "Batch train [11] loss 0.98005, dsc 0.02969\n",
      "Batch train [12] loss 0.97805, dsc 0.02877\n",
      "Batch train [13] loss 0.97948, dsc 0.02356\n",
      "Batch train [14] loss 0.97922, dsc 0.02511\n",
      "Batch train [15] loss 0.98056, dsc 0.02979\n",
      "Batch train [16] loss 0.98565, dsc 0.02694\n",
      "Batch train [17] loss 0.97677, dsc 0.02968\n",
      "Batch train [18] loss 0.98384, dsc 0.02031\n",
      "Batch train [19] loss 0.98129, dsc 0.02961\n",
      "Batch train [20] loss 0.98257, dsc 0.02812\n",
      "Epoch [22] train done\n",
      "Batch eval [1] loss 0.98924, dsc 0.01076\n",
      "Batch eval [2] loss 0.98500, dsc 0.01500\n",
      "Batch eval [3] loss 0.98091, dsc 0.01909\n",
      "Batch eval [4] loss 0.97826, dsc 0.02174\n",
      "Batch eval [5] loss 0.96831, dsc 0.03169\n",
      "Epoch [22] valid done\n",
      "Epoch [22] T 1337.71s, deltaT 59.27s, loss: train 0.98029, valid 0.98034, dsc: train 0.02933, valid 0.01966\n",
      "Batch train [1] loss 0.97878, dsc 0.02546\n",
      "Batch train [2] loss 0.97796, dsc 0.04022\n",
      "Batch train [3] loss 0.97911, dsc 0.02653\n",
      "Batch train [4] loss 0.98438, dsc 0.01994\n",
      "Batch train [5] loss 0.98071, dsc 0.02225\n",
      "Batch train [6] loss 0.97393, dsc 0.03125\n",
      "Batch train [7] loss 0.97783, dsc 0.02918\n",
      "Batch train [8] loss 0.97655, dsc 0.03942\n",
      "Batch train [9] loss 0.98407, dsc 0.03023\n",
      "Batch train [10] loss 0.97818, dsc 0.03638\n",
      "Batch train [11] loss 0.97861, dsc 0.03976\n",
      "Batch train [12] loss 0.97772, dsc 0.02944\n",
      "Batch train [13] loss 0.98352, dsc 0.02002\n",
      "Batch train [14] loss 0.98095, dsc 0.02203\n",
      "Batch train [15] loss 0.97656, dsc 0.03509\n",
      "Batch train [16] loss 0.97876, dsc 0.02608\n",
      "Batch train [17] loss 0.97822, dsc 0.03067\n",
      "Batch train [18] loss 0.97952, dsc 0.02639\n",
      "Batch train [19] loss 0.97491, dsc 0.04924\n",
      "Batch train [20] loss 0.97526, dsc 0.03333\n",
      "Epoch [23] train done\n",
      "Batch eval [1] loss 0.98650, dsc 0.01350\n",
      "Batch eval [2] loss 0.98371, dsc 0.01629\n",
      "Batch eval [3] loss 0.97921, dsc 0.02079\n",
      "Batch eval [4] loss 0.97756, dsc 0.02244\n",
      "Batch eval [5] loss 0.96597, dsc 0.03403\n",
      "Epoch [23] valid done\n",
      "Epoch [23] T 1396.91s, deltaT 59.21s, loss: train 0.97878, valid 0.97859, dsc: train 0.03065, valid 0.02141\n",
      "Batch train [1] loss 0.97500, dsc 0.02901\n",
      "Batch train [2] loss 0.98260, dsc 0.01988\n",
      "Batch train [3] loss 0.97924, dsc 0.02402\n",
      "Batch train [4] loss 0.97723, dsc 0.04104\n",
      "Batch train [5] loss 0.97741, dsc 0.03522\n",
      "Batch train [6] loss 0.97625, dsc 0.03115\n",
      "Batch train [7] loss 0.97773, dsc 0.04412\n",
      "Batch train [8] loss 0.97189, dsc 0.03561\n",
      "Batch train [9] loss 0.97859, dsc 0.02584\n",
      "Batch train [10] loss 0.97858, dsc 0.02426\n",
      "Batch train [11] loss 0.98208, dsc 0.03363\n",
      "Batch train [12] loss 0.98084, dsc 0.02431\n",
      "Batch train [13] loss 0.97673, dsc 0.03304\n",
      "Batch train [14] loss 0.97593, dsc 0.04011\n",
      "Batch train [15] loss 0.97799, dsc 0.02646\n",
      "Batch train [16] loss 0.98026, dsc 0.03731\n",
      "Batch train [17] loss 0.97619, dsc 0.03005\n",
      "Batch train [18] loss 0.96989, dsc 0.03521\n",
      "Batch train [19] loss 0.97311, dsc 0.03227\n",
      "Batch train [20] loss 0.97807, dsc 0.02807\n",
      "Epoch [24] train done\n",
      "Batch eval [1] loss 0.99547, dsc 0.00453\n",
      "Batch eval [2] loss 0.98217, dsc 0.01783\n",
      "Batch eval [3] loss 0.97727, dsc 0.02273\n",
      "Batch eval [4] loss 0.97389, dsc 0.02611\n",
      "Batch eval [5] loss 0.96240, dsc 0.03760\n",
      "Epoch [24] valid done\n",
      "Epoch [24] T 1456.57s, deltaT 59.66s, loss: train 0.97728, valid 0.97824, dsc: train 0.03153, valid 0.02176\n",
      "Batch train [1] loss 0.97761, dsc 0.02645\n",
      "Batch train [2] loss 0.97852, dsc 0.02941\n",
      "Batch train [3] loss 0.97460, dsc 0.03060\n",
      "Batch train [4] loss 0.97431, dsc 0.03848\n",
      "Batch train [5] loss 0.97201, dsc 0.03887\n",
      "Batch train [6] loss 0.98004, dsc 0.03263\n",
      "Batch train [7] loss 0.97222, dsc 0.03242\n",
      "Batch train [8] loss 0.97565, dsc 0.02736\n",
      "Batch train [9] loss 0.97278, dsc 0.03069\n",
      "Batch train [10] loss 0.97552, dsc 0.03622\n",
      "Batch train [11] loss 0.97497, dsc 0.02805\n",
      "Batch train [12] loss 0.97557, dsc 0.03850\n",
      "Batch train [13] loss 0.97225, dsc 0.05029\n",
      "Batch train [14] loss 0.97954, dsc 0.02624\n",
      "Batch train [15] loss 0.97288, dsc 0.03583\n",
      "Batch train [16] loss 0.97435, dsc 0.03154\n",
      "Batch train [17] loss 0.97903, dsc 0.04063\n",
      "Batch train [18] loss 0.97321, dsc 0.04031\n",
      "Batch train [19] loss 0.97073, dsc 0.03822\n",
      "Batch train [20] loss 0.97629, dsc 0.02821\n",
      "Epoch [25] train done\n",
      "Batch eval [1] loss 0.98964, dsc 0.01036\n",
      "Batch eval [2] loss 0.98109, dsc 0.01891\n",
      "Batch eval [3] loss 0.97658, dsc 0.02342\n",
      "Batch eval [4] loss 0.97335, dsc 0.02665\n",
      "Batch eval [5] loss 0.95940, dsc 0.04060\n",
      "Epoch [25] valid done\n",
      "Epoch [25] T 1516.33s, deltaT 59.76s, loss: train 0.97511, valid 0.97601, dsc: train 0.03405, valid 0.02399\n",
      "Batch train [1] loss 0.96995, dsc 0.03741\n",
      "Batch train [2] loss 0.97859, dsc 0.03274\n",
      "Batch train [3] loss 0.96811, dsc 0.05649\n",
      "Batch train [4] loss 0.96729, dsc 0.04539\n",
      "Batch train [5] loss 0.97367, dsc 0.03004\n",
      "Batch train [6] loss 0.97473, dsc 0.02908\n",
      "Batch train [7] loss 0.97151, dsc 0.03247\n",
      "Batch train [8] loss 0.97383, dsc 0.03719\n",
      "Batch train [9] loss 0.97182, dsc 0.04398\n",
      "Batch train [10] loss 0.97841, dsc 0.02548\n",
      "Batch train [11] loss 0.97449, dsc 0.04965\n",
      "Batch train [12] loss 0.97260, dsc 0.04256\n",
      "Batch train [13] loss 0.96414, dsc 0.06017\n",
      "Batch train [14] loss 0.97649, dsc 0.04190\n",
      "Batch train [15] loss 0.97202, dsc 0.03276\n",
      "Batch train [16] loss 0.97468, dsc 0.04051\n",
      "Batch train [17] loss 0.96059, dsc 0.05335\n",
      "Batch train [18] loss 0.95895, dsc 0.06816\n",
      "Batch train [19] loss 0.96776, dsc 0.03885\n",
      "Batch train [20] loss 0.96708, dsc 0.06426\n",
      "Epoch [26] train done\n",
      "Batch eval [1] loss 0.98457, dsc 0.01543\n",
      "Batch eval [2] loss 0.98146, dsc 0.01854\n",
      "Batch eval [3] loss 0.97469, dsc 0.02531\n",
      "Batch eval [4] loss 0.97540, dsc 0.02460\n",
      "Batch eval [5] loss 0.96291, dsc 0.03709\n",
      "Epoch [26] valid done\n",
      "Epoch [26] T 1575.40s, deltaT 59.07s, loss: train 0.97083, valid 0.97581, dsc: train 0.04312, valid 0.02419\n",
      "Batch train [1] loss 0.96640, dsc 0.05812\n",
      "Batch train [2] loss 0.96132, dsc 0.05731\n",
      "Batch train [3] loss 0.97217, dsc 0.03156\n",
      "Batch train [4] loss 0.96212, dsc 0.05828\n",
      "Batch train [5] loss 0.96493, dsc 0.05148\n",
      "Batch train [6] loss 0.96304, dsc 0.04646\n",
      "Batch train [7] loss 0.97019, dsc 0.05780\n",
      "Batch train [8] loss 0.96565, dsc 0.04629\n",
      "Batch train [9] loss 0.96930, dsc 0.04052\n",
      "Batch train [10] loss 0.95457, dsc 0.05146\n",
      "Batch train [11] loss 0.95796, dsc 0.08008\n",
      "Batch train [12] loss 0.96486, dsc 0.06016\n",
      "Batch train [13] loss 0.97040, dsc 0.05019\n",
      "Batch train [14] loss 0.96247, dsc 0.04949\n",
      "Batch train [15] loss 0.96488, dsc 0.04009\n",
      "Batch train [16] loss 0.95961, dsc 0.04906\n",
      "Batch train [17] loss 0.95745, dsc 0.04755\n",
      "Batch train [18] loss 0.96772, dsc 0.03596\n",
      "Batch train [19] loss 0.96808, dsc 0.03548\n",
      "Batch train [20] loss 0.94992, dsc 0.08775\n",
      "Epoch [27] train done\n",
      "Batch eval [1] loss 0.97178, dsc 0.02822\n",
      "Batch eval [2] loss 0.96682, dsc 0.03318\n",
      "Batch eval [3] loss 0.95730, dsc 0.04270\n",
      "Batch eval [4] loss 0.95409, dsc 0.04591\n",
      "Batch eval [5] loss 0.93350, dsc 0.06650\n",
      "Epoch [27] valid done\n",
      "Epoch [27] T 1636.16s, deltaT 60.76s, loss: train 0.96365, valid 0.95670, dsc: train 0.05175, valid 0.04330\n",
      "Batch train [1] loss 0.96410, dsc 0.03956\n",
      "Batch train [2] loss 0.94937, dsc 0.09498\n",
      "Batch train [3] loss 0.95801, dsc 0.06075\n",
      "Batch train [4] loss 0.95657, dsc 0.04851\n",
      "Batch train [5] loss 0.97064, dsc 0.03831\n",
      "Batch train [6] loss 0.95596, dsc 0.07575\n",
      "Batch train [7] loss 0.96030, dsc 0.04368\n",
      "Batch train [8] loss 0.96378, dsc 0.03960\n",
      "Batch train [9] loss 0.94835, dsc 0.08543\n",
      "Batch train [10] loss 0.94916, dsc 0.05585\n",
      "Batch train [11] loss 0.95543, dsc 0.05566\n",
      "Batch train [12] loss 0.95840, dsc 0.05516\n",
      "Batch train [13] loss 0.95701, dsc 0.07487\n",
      "Batch train [14] loss 0.96316, dsc 0.06300\n",
      "Batch train [15] loss 0.95397, dsc 0.07881\n",
      "Batch train [16] loss 0.94522, dsc 0.10149\n",
      "Batch train [17] loss 0.94563, dsc 0.06820\n",
      "Batch train [18] loss 0.95461, dsc 0.05894\n",
      "Batch train [19] loss 0.95649, dsc 0.04835\n",
      "Batch train [20] loss 0.96177, dsc 0.07140\n",
      "Epoch [28] train done\n",
      "Batch eval [1] loss 0.98093, dsc 0.01907\n",
      "Batch eval [2] loss 0.97363, dsc 0.02637\n",
      "Batch eval [3] loss 0.96587, dsc 0.03413\n",
      "Batch eval [4] loss 0.96409, dsc 0.03591\n",
      "Batch eval [5] loss 0.94438, dsc 0.05562\n",
      "Epoch [28] valid done\n",
      "Epoch [28] T 1696.82s, deltaT 60.66s, loss: train 0.95640, valid 0.96578, dsc: train 0.06292, valid 0.03422\n",
      "Batch train [1] loss 0.94761, dsc 0.05969\n",
      "Batch train [2] loss 0.94829, dsc 0.06444\n",
      "Batch train [3] loss 0.95111, dsc 0.07012\n",
      "Batch train [4] loss 0.94252, dsc 0.09776\n",
      "Batch train [5] loss 0.94043, dsc 0.09106\n",
      "Batch train [6] loss 0.94716, dsc 0.05767\n",
      "Batch train [7] loss 0.95599, dsc 0.06713\n",
      "Batch train [8] loss 0.96242, dsc 0.04994\n",
      "Batch train [9] loss 0.95147, dsc 0.05270\n",
      "Batch train [10] loss 0.94754, dsc 0.10045\n",
      "Batch train [11] loss 0.94621, dsc 0.05823\n",
      "Batch train [12] loss 0.95920, dsc 0.04382\n",
      "Batch train [13] loss 0.94440, dsc 0.06040\n",
      "Batch train [14] loss 0.94636, dsc 0.06034\n",
      "Batch train [15] loss 0.94824, dsc 0.09621\n",
      "Batch train [16] loss 0.94013, dsc 0.08622\n",
      "Batch train [17] loss 0.92992, dsc 0.07528\n",
      "Batch train [18] loss 0.94643, dsc 0.05787\n",
      "Batch train [19] loss 0.94123, dsc 0.08338\n",
      "Batch train [20] loss 0.94556, dsc 0.05858\n",
      "Epoch [29] train done\n",
      "Batch eval [1] loss 0.95960, dsc 0.04040\n",
      "Batch eval [2] loss 0.95322, dsc 0.04678\n",
      "Batch eval [3] loss 0.94061, dsc 0.05939\n",
      "Batch eval [4] loss 0.93445, dsc 0.06555\n",
      "Batch eval [5] loss 0.90459, dsc 0.09541\n",
      "Epoch [29] valid done\n",
      "Epoch [29] T 1756.39s, deltaT 59.57s, loss: train 0.94711, valid 0.93850, dsc: train 0.06957, valid 0.06150\n",
      "Batch train [1] loss 0.93289, dsc 0.11791\n",
      "Batch train [2] loss 0.93892, dsc 0.09897\n",
      "Batch train [3] loss 0.94147, dsc 0.06282\n",
      "Batch train [4] loss 0.93900, dsc 0.08529\n",
      "Batch train [5] loss 0.93590, dsc 0.06882\n",
      "Batch train [6] loss 0.93294, dsc 0.07243\n",
      "Batch train [7] loss 0.93601, dsc 0.08664\n",
      "Batch train [8] loss 0.93897, dsc 0.10791\n",
      "Batch train [9] loss 0.94188, dsc 0.06209\n",
      "Batch train [10] loss 0.93208, dsc 0.07250\n",
      "Batch train [11] loss 0.93631, dsc 0.08953\n",
      "Batch train [12] loss 0.94147, dsc 0.06259\n",
      "Batch train [13] loss 0.92750, dsc 0.11381\n",
      "Batch train [14] loss 0.94644, dsc 0.07518\n",
      "Batch train [15] loss 0.94325, dsc 0.10169\n",
      "Batch train [16] loss 0.94297, dsc 0.06760\n",
      "Batch train [17] loss 0.91877, dsc 0.10066\n",
      "Batch train [18] loss 0.94691, dsc 0.07280\n",
      "Batch train [19] loss 0.91783, dsc 0.09852\n",
      "Batch train [20] loss 0.92509, dsc 0.07886\n",
      "Epoch [30] train done\n",
      "Batch eval [1] loss 0.96584, dsc 0.03416\n",
      "Batch eval [2] loss 0.95072, dsc 0.04928\n",
      "Batch eval [3] loss 0.93657, dsc 0.06343\n",
      "Batch eval [4] loss 0.93124, dsc 0.06876\n",
      "Batch eval [5] loss 0.90368, dsc 0.09632\n",
      "Epoch [30] valid done\n",
      "Epoch [30] T 1816.35s, deltaT 59.96s, loss: train 0.93583, valid 0.93761, dsc: train 0.08483, valid 0.06239\n",
      "Elapsed time 0:30:16\n"
     ]
    }
   ],
   "source": [
    "cut_model_info = prepare_model(epochs=30,\n",
    "                               learning_rate=5e-4,\n",
    "                               in_channels=8,\n",
    "                               batch_size=2,\n",
    "                               train_dataset=cut_train_dataset, valid_dataset=cut_valid_dataset, test_dataset=cut_test_dataset)\n",
    "show_model_info(cut_model_info)\n",
    "\n",
    "cut_train_loop_params = {k:v for k,v in cut_model_info.items() if k not in ['model_total_params', 'model_total_trainable_params']}\n",
    "train_loop(**cut_train_loop_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Usage\n",
      "GPU:        GeForce RTX 2070\n",
      "Allocated:  0.0 GB\n",
      "Cached:     0.9 GB\n",
      "Max memory: 5.1 GB\n",
      "Max Cached: 6.0 GB\n"
     ]
    }
   ],
   "source": [
    "show_cuda_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "160x128x128 = 2_621_440 \\\n",
    "72x198x168 = 2_395_008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<IPython.core.display.Markdown object>,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing number 31\n",
      "loss 0.9662920236587524, dsc 0.033707957714796066, inputs_len 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7463b965886141c5a77211842ec6fa42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=49, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2160c3917b3442c6b7ae16c0c4f95c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG shapes torch.Size([1, 72, 192, 168]) torch.Size([1, 72, 192, 168]) torch.Size([1, 1, 72, 192, 168])\n",
      "DEBUG prediction max 0.9999998807907104, min 0.024152379482984543\n",
      "DEBUG intersection 2401.641845703125\n",
      "DEBUG label sum 3871.0\n",
      "DEBUG prediction sum 138626.015625\n",
      "DEBUG intersection2 2401.641845703125\n",
      "DEBUG dsc 0.033707961440086365\n",
      "DEBUG MSE 0.00416009034961462\n"
     ]
    }
   ],
   "source": [
    "max_slices = cut_train_dataset.__getitem__(0)[1].shape[0]\n",
    "\n",
    "display((Markdown(\"### Train Eval\"),))\n",
    "show_model_dataset_pred_preview(cut_model_info, cut_train_dataset, max_slices=max_slices, default_slice=49)\n",
    "\n",
    "# display((Markdown(\"### Valid Eval\"),))\n",
    "# show_model_dataset_pred_preview(cut_model_info, cut_valid_dataset, max_slices=max_slices, default_slice=53)\n",
    "\n",
    "# display((Markdown(\"### Test Eval\"),))\n",
    "# eval_image_dataset(test_dataset, 78, 'test_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing label prediction comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, rescaled_preds = get_rescaled_preds(cut_model_info[\"model\"], cut_full_res_dataset, cut_model_info[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41573be6e9e84f1a8477170622b57380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=49, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bdef25fdd347b58347115098d293d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_prediction_with_ground_true(cut_full_res_dataset, rescaled_preds, dataset_index=0, default_slice=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
