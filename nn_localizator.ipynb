{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n",
      "Dataset biggest bounding box wihtout spinal cord [56, 177, 156]\n",
      "Cut target size [72, 192, 168]\n",
      "Done Init\n"
     ]
    }
   ],
   "source": [
    "from src.consts import IN_COLAB, DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Found Google Colab')\n",
    "    !pip3 install torch torchvision torchsummary\n",
    "    !pip3 install simpleitk\n",
    "\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from torchio import RandomAffine, Compose, ZNormalization\n",
    "\n",
    "import src.dataset.oars_labels_consts as OARS_LABELS\n",
    "from src.helpers.threshold_calc_helpers import get_threshold_info_df\n",
    "from src.helpers.show_model_dataset_pred_preview import show_model_dataset_pred_preview\n",
    "from src.dataset.get_cut_lists import get_cut_lists\n",
    "from src.dataset.get_full_res_cut import get_full_res_cut\n",
    "from src.dataset.get_dataset import get_dataset\n",
    "from src.dataset.get_dataset_info import get_dataset_info\n",
    "from src.dataset.preview_dataset import preview_dataset\n",
    "from src.dataset.get_dataset_transform import get_dataset_transform\n",
    "from src.model_and_training.prepare_model import prepare_model\n",
    "from src.model_and_training.train_loop import train_loop\n",
    "from src.model_and_training.show_model_info import show_model_info\n",
    "from src.helpers.show_cuda_usage import show_cuda_usage\n",
    "from src.helpers.threshold_calc_helpers import get_rescaled_preds\n",
    "from src.dataset.split_dataset import split_dataset, copy_split_dataset\n",
    "from src.helpers.compare_prediction_with_ground_true import compare_prediction_with_ground_true\n",
    "from src.helpers.get_img_outliers_pixels import get_img_outliers_pixels\n",
    "\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "torch.manual_seed(20)\n",
    "logging.basicConfig(filename='nn_local.log', level=logging.DEBUG)\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low resolution NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading low res dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 16x dataset\n",
      "filtering labels\n",
      "filtering labels done\n",
      "dilatating 1x dataset\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n",
      "data type: int16 int8\n",
      "low res dataset RAM sizes in GB 0.02288818359375\n",
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_LIST\n",
    "if OARS_LABELS.SPINAL_CORD in filter_labels:\n",
    "    filter_labels.remove(OARS_LABELS.SPINAL_CORD)\n",
    "\n",
    "low_res_dataset = get_dataset(dataset_size=50, shrink_factor=16, filter_labels=filter_labels, unify_labels=True)\n",
    "low_res_dataset.transform = Compose([ZNormalization()])\n",
    "# low_res_dataset.transform = get_dataset_transform()\n",
    "low_res_dataset.dilatate_labels(repeat=1)\n",
    "low_res_dataset.to_numpy()\n",
    "low_res_dataset.show_data_type()\n",
    "print('low res dataset RAM sizes in GB', low_res_dataset.get_data_size() / 1024**3)\n",
    "\n",
    "low_res_split_dataset_obj = split_dataset(low_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "get_dataset_info(low_res_dataset, low_res_split_dataset_obj)\n",
    "train_low_res_dataset, valid_low_res_dataset, test_low_res_dataset = itemgetter(\n",
    "    'train_dataset', 'valid_dataset', 'test_dataset')(low_res_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 12.505672454833984, min -0.4069788455963135\n",
      "label max 1, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d834fe5d4b4749188b37683c68829def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b8b6cb45794f7da4e3a1f26eb52f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_dataset(low_res_dataset, preview_index=0, show_hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training low res model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device running \"cuda\"\n",
      "max output channels 64\n",
      "Model number of params: 298881, trainable 298881\n",
      "Running training loop\n",
      "Batch train [1] loss 0.97868, dsc 0.02132\n",
      "Batch train [2] loss 0.97697, dsc 0.02303\n",
      "Batch train [3] loss 0.97572, dsc 0.02428\n",
      "Batch train [4] loss 0.97679, dsc 0.02321\n",
      "Batch train [5] loss 0.97250, dsc 0.02750\n",
      "Batch train [6] loss 0.97422, dsc 0.02578\n",
      "Batch train [7] loss 0.97080, dsc 0.02920\n",
      "Batch train [8] loss 0.97474, dsc 0.02526\n",
      "Batch train [9] loss 0.97486, dsc 0.02514\n",
      "Batch train [10] loss 0.97237, dsc 0.02763\n",
      "Epoch [1] train done\n",
      "Batch eval [1] loss 0.97934, dsc 0.02066\n",
      "Batch eval [2] loss 0.98218, dsc 0.01782\n",
      "Batch eval [3] loss 0.97965, dsc 0.02035\n",
      "Batch eval [4] loss 0.98409, dsc 0.01591\n",
      "Batch eval [5] loss 0.97903, dsc 0.02097\n",
      "Epoch [1] valid done\n",
      "Epoch [1] T 4.04s, deltaT 4.04s, loss: train 0.97476, valid 0.98086, dsc: train 0.02524, valid 0.01914\n",
      "Batch train [1] loss 0.97021, dsc 0.02979\n",
      "Batch train [2] loss 0.97051, dsc 0.02949\n",
      "Batch train [3] loss 0.97254, dsc 0.02746\n",
      "Batch train [4] loss 0.96946, dsc 0.03054\n",
      "Batch train [5] loss 0.97233, dsc 0.02767\n",
      "Batch train [6] loss 0.97130, dsc 0.02870\n",
      "Batch train [7] loss 0.97114, dsc 0.02886\n",
      "Batch train [8] loss 0.97027, dsc 0.02973\n",
      "Batch train [9] loss 0.96923, dsc 0.03077\n",
      "Batch train [10] loss 0.96366, dsc 0.03634\n",
      "Epoch [2] train done\n",
      "Batch eval [1] loss 0.97221, dsc 0.02779\n",
      "Batch eval [2] loss 0.97578, dsc 0.02422\n",
      "Batch eval [3] loss 0.97239, dsc 0.02761\n",
      "Batch eval [4] loss 0.97798, dsc 0.02202\n",
      "Batch eval [5] loss 0.97154, dsc 0.02846\n",
      "Epoch [2] valid done\n",
      "Epoch [2] T 8.09s, deltaT 4.05s, loss: train 0.97006, valid 0.97398, dsc: train 0.02994, valid 0.02602\n",
      "Batch train [1] loss 0.96923, dsc 0.03077\n",
      "Batch train [2] loss 0.96696, dsc 0.03304\n",
      "Batch train [3] loss 0.96844, dsc 0.03156\n",
      "Batch train [4] loss 0.96604, dsc 0.03396\n",
      "Batch train [5] loss 0.97383, dsc 0.02617\n",
      "Batch train [6] loss 0.97202, dsc 0.02798\n",
      "Batch train [7] loss 0.97017, dsc 0.02983\n",
      "Batch train [8] loss 0.96645, dsc 0.03355\n",
      "Batch train [9] loss 0.96559, dsc 0.03441\n",
      "Batch train [10] loss 0.96615, dsc 0.03385\n",
      "Epoch [3] train done\n",
      "Batch eval [1] loss 0.96588, dsc 0.03412\n",
      "Batch eval [2] loss 0.97046, dsc 0.02954\n",
      "Batch eval [3] loss 0.96733, dsc 0.03267\n",
      "Batch eval [4] loss 0.97324, dsc 0.02676\n",
      "Batch eval [5] loss 0.96586, dsc 0.03414\n",
      "Epoch [3] valid done\n",
      "Epoch [3] T 12.14s, deltaT 4.05s, loss: train 0.96849, valid 0.96855, dsc: train 0.03151, valid 0.03145\n",
      "Batch train [1] loss 0.97038, dsc 0.02962\n",
      "Batch train [2] loss 0.96388, dsc 0.03612\n",
      "Batch train [3] loss 0.96949, dsc 0.03051\n",
      "Batch train [4] loss 0.96746, dsc 0.03254\n",
      "Batch train [5] loss 0.97145, dsc 0.02855\n",
      "Batch train [6] loss 0.96515, dsc 0.03485\n",
      "Batch train [7] loss 0.97044, dsc 0.02956\n",
      "Batch train [8] loss 0.96617, dsc 0.03383\n",
      "Batch train [9] loss 0.96654, dsc 0.03346\n",
      "Batch train [10] loss 0.96455, dsc 0.03545\n",
      "Epoch [4] train done\n",
      "Batch eval [1] loss 0.96226, dsc 0.03774\n",
      "Batch eval [2] loss 0.96747, dsc 0.03253\n",
      "Batch eval [3] loss 0.96306, dsc 0.03694\n",
      "Batch eval [4] loss 0.97124, dsc 0.02876\n",
      "Batch eval [5] loss 0.96209, dsc 0.03791\n",
      "Epoch [4] valid done\n",
      "Epoch [4] T 16.17s, deltaT 4.03s, loss: train 0.96755, valid 0.96522, dsc: train 0.03245, valid 0.03478\n",
      "Batch train [1] loss 0.96796, dsc 0.03204\n",
      "Batch train [2] loss 0.96527, dsc 0.03473\n",
      "Batch train [3] loss 0.96892, dsc 0.03108\n",
      "Batch train [4] loss 0.96414, dsc 0.03586\n",
      "Batch train [5] loss 0.97118, dsc 0.02882\n",
      "Batch train [6] loss 0.96494, dsc 0.03506\n",
      "Batch train [7] loss 0.96504, dsc 0.03496\n",
      "Batch train [8] loss 0.96668, dsc 0.03332\n",
      "Batch train [9] loss 0.96524, dsc 0.03476\n",
      "Batch train [10] loss 0.97104, dsc 0.02896\n",
      "Epoch [5] train done\n",
      "Batch eval [1] loss 0.96297, dsc 0.03703\n",
      "Batch eval [2] loss 0.96843, dsc 0.03157\n",
      "Batch eval [3] loss 0.96603, dsc 0.03397\n",
      "Batch eval [4] loss 0.97255, dsc 0.02745\n",
      "Batch eval [5] loss 0.96457, dsc 0.03543\n",
      "Epoch [5] valid done\n",
      "Epoch [5] T 20.21s, deltaT 4.03s, loss: train 0.96704, valid 0.96691, dsc: train 0.03296, valid 0.03309\n",
      "Batch train [1] loss 0.96670, dsc 0.03330\n",
      "Batch train [2] loss 0.96233, dsc 0.03767\n",
      "Batch train [3] loss 0.96795, dsc 0.03205\n",
      "Batch train [4] loss 0.96235, dsc 0.03765\n",
      "Batch train [5] loss 0.96848, dsc 0.03152\n",
      "Batch train [6] loss 0.96891, dsc 0.03109\n",
      "Batch train [7] loss 0.96477, dsc 0.03523\n",
      "Batch train [8] loss 0.96999, dsc 0.03001\n",
      "Batch train [9] loss 0.96593, dsc 0.03407\n",
      "Batch train [10] loss 0.97040, dsc 0.02960\n",
      "Epoch [6] train done\n",
      "Batch eval [1] loss 0.96333, dsc 0.03667\n",
      "Batch eval [2] loss 0.96835, dsc 0.03165\n",
      "Batch eval [3] loss 0.96572, dsc 0.03428\n",
      "Batch eval [4] loss 0.97259, dsc 0.02741\n",
      "Batch eval [5] loss 0.96429, dsc 0.03571\n",
      "Epoch [6] valid done\n",
      "Epoch [6] T 24.25s, deltaT 4.04s, loss: train 0.96678, valid 0.96686, dsc: train 0.03322, valid 0.03314\n",
      "Batch train [1] loss 0.97007, dsc 0.02993\n",
      "Batch train [2] loss 0.96398, dsc 0.03602\n",
      "Batch train [3] loss 0.97011, dsc 0.02989\n",
      "Batch train [4] loss 0.96614, dsc 0.03386\n",
      "Batch train [5] loss 0.96995, dsc 0.03005\n",
      "Batch train [6] loss 0.96881, dsc 0.03119\n",
      "Batch train [7] loss 0.96151, dsc 0.03849\n",
      "Batch train [8] loss 0.96730, dsc 0.03270\n",
      "Batch train [9] loss 0.95785, dsc 0.04215\n",
      "Batch train [10] loss 0.97021, dsc 0.02979\n",
      "Epoch [7] train done\n",
      "Batch eval [1] loss 0.96149, dsc 0.03851\n",
      "Batch eval [2] loss 0.96678, dsc 0.03322\n",
      "Batch eval [3] loss 0.96239, dsc 0.03761\n",
      "Batch eval [4] loss 0.97094, dsc 0.02906\n",
      "Batch eval [5] loss 0.96144, dsc 0.03856\n",
      "Epoch [7] valid done\n",
      "Epoch [7] T 28.30s, deltaT 4.04s, loss: train 0.96659, valid 0.96461, dsc: train 0.03341, valid 0.03539\n",
      "Batch train [1] loss 0.96777, dsc 0.03223\n",
      "Batch train [2] loss 0.96263, dsc 0.03737\n",
      "Batch train [3] loss 0.96493, dsc 0.03507\n",
      "Batch train [4] loss 0.96537, dsc 0.03463\n",
      "Batch train [5] loss 0.96603, dsc 0.03397\n",
      "Batch train [6] loss 0.96705, dsc 0.03295\n",
      "Batch train [7] loss 0.96553, dsc 0.03447\n",
      "Batch train [8] loss 0.97130, dsc 0.02870\n",
      "Batch train [9] loss 0.96653, dsc 0.03347\n",
      "Batch train [10] loss 0.96773, dsc 0.03227\n",
      "Epoch [8] train done\n",
      "Batch eval [1] loss 0.96777, dsc 0.03223\n",
      "Batch eval [2] loss 0.97367, dsc 0.02633\n",
      "Batch eval [3] loss 0.97265, dsc 0.02735\n",
      "Batch eval [4] loss 0.97929, dsc 0.02071\n",
      "Batch eval [5] loss 0.96985, dsc 0.03015\n",
      "Epoch [8] valid done\n",
      "Epoch [8] T 32.28s, deltaT 3.99s, loss: train 0.96649, valid 0.97265, dsc: train 0.03351, valid 0.02735\n",
      "Batch train [1] loss 0.96759, dsc 0.03241\n",
      "Batch train [2] loss 0.96781, dsc 0.03219\n",
      "Batch train [3] loss 0.96437, dsc 0.03563\n",
      "Batch train [4] loss 0.95965, dsc 0.04035\n",
      "Batch train [5] loss 0.97051, dsc 0.02949\n",
      "Batch train [6] loss 0.97053, dsc 0.02947\n",
      "Batch train [7] loss 0.96818, dsc 0.03182\n",
      "Batch train [8] loss 0.96283, dsc 0.03717\n",
      "Batch train [9] loss 0.96809, dsc 0.03191\n",
      "Batch train [10] loss 0.96315, dsc 0.03685\n",
      "Epoch [9] train done\n",
      "Batch eval [1] loss 0.96155, dsc 0.03845\n",
      "Batch eval [2] loss 0.96678, dsc 0.03322\n",
      "Batch eval [3] loss 0.96317, dsc 0.03683\n",
      "Batch eval [4] loss 0.97106, dsc 0.02894\n",
      "Batch eval [5] loss 0.96163, dsc 0.03837\n",
      "Epoch [9] valid done\n",
      "Epoch [9] T 36.28s, deltaT 4.00s, loss: train 0.96627, valid 0.96484, dsc: train 0.03373, valid 0.03516\n",
      "Batch train [1] loss 0.96855, dsc 0.03145\n",
      "Batch train [2] loss 0.96740, dsc 0.03260\n",
      "Batch train [3] loss 0.96947, dsc 0.03053\n",
      "Batch train [4] loss 0.96373, dsc 0.03627\n",
      "Batch train [5] loss 0.96850, dsc 0.03150\n",
      "Batch train [6] loss 0.96338, dsc 0.03662\n",
      "Batch train [7] loss 0.96173, dsc 0.03827\n",
      "Batch train [8] loss 0.96779, dsc 0.03221\n",
      "Batch train [9] loss 0.96573, dsc 0.03427\n",
      "Batch train [10] loss 0.96530, dsc 0.03470\n",
      "Epoch [10] train done\n",
      "Batch eval [1] loss 0.96095, dsc 0.03905\n",
      "Batch eval [2] loss 0.96633, dsc 0.03367\n",
      "Batch eval [3] loss 0.96169, dsc 0.03831\n",
      "Batch eval [4] loss 0.97020, dsc 0.02980\n",
      "Batch eval [5] loss 0.96064, dsc 0.03936\n",
      "Epoch [10] valid done\n",
      "Epoch [10] T 40.27s, deltaT 3.99s, loss: train 0.96616, valid 0.96396, dsc: train 0.03384, valid 0.03604\n",
      "Batch train [1] loss 0.96677, dsc 0.03323\n",
      "Batch train [2] loss 0.96422, dsc 0.03578\n",
      "Batch train [3] loss 0.96687, dsc 0.03313\n",
      "Batch train [4] loss 0.96694, dsc 0.03306\n",
      "Batch train [5] loss 0.96717, dsc 0.03283\n",
      "Batch train [6] loss 0.96850, dsc 0.03150\n",
      "Batch train [7] loss 0.96394, dsc 0.03606\n",
      "Batch train [8] loss 0.96547, dsc 0.03453\n",
      "Batch train [9] loss 0.96726, dsc 0.03274\n",
      "Batch train [10] loss 0.96286, dsc 0.03714\n",
      "Epoch [11] train done\n",
      "Batch eval [1] loss 0.96383, dsc 0.03617\n",
      "Batch eval [2] loss 0.96812, dsc 0.03188\n",
      "Batch eval [3] loss 0.96650, dsc 0.03350\n",
      "Batch eval [4] loss 0.97369, dsc 0.02631\n",
      "Batch eval [5] loss 0.96396, dsc 0.03604\n",
      "Epoch [11] valid done\n",
      "Epoch [11] T 44.22s, deltaT 3.95s, loss: train 0.96600, valid 0.96722, dsc: train 0.03400, valid 0.03278\n",
      "Batch train [1] loss 0.96560, dsc 0.03440\n",
      "Batch train [2] loss 0.97058, dsc 0.02942\n",
      "Batch train [3] loss 0.96404, dsc 0.03596\n",
      "Batch train [4] loss 0.96290, dsc 0.03710\n",
      "Batch train [5] loss 0.96234, dsc 0.03766\n",
      "Batch train [6] loss 0.96877, dsc 0.03123\n",
      "Batch train [7] loss 0.96268, dsc 0.03732\n",
      "Batch train [8] loss 0.96937, dsc 0.03063\n",
      "Batch train [9] loss 0.96386, dsc 0.03614\n",
      "Batch train [10] loss 0.96858, dsc 0.03142\n",
      "Epoch [12] train done\n",
      "Batch eval [1] loss 0.96227, dsc 0.03773\n",
      "Batch eval [2] loss 0.96729, dsc 0.03271\n",
      "Batch eval [3] loss 0.96459, dsc 0.03541\n",
      "Batch eval [4] loss 0.97234, dsc 0.02766\n",
      "Batch eval [5] loss 0.96234, dsc 0.03766\n",
      "Epoch [12] valid done\n",
      "Epoch [12] T 48.23s, deltaT 4.00s, loss: train 0.96587, valid 0.96577, dsc: train 0.03413, valid 0.03423\n",
      "Batch train [1] loss 0.96457, dsc 0.03543\n",
      "Batch train [2] loss 0.97003, dsc 0.02997\n",
      "Batch train [3] loss 0.96634, dsc 0.03366\n",
      "Batch train [4] loss 0.96640, dsc 0.03360\n",
      "Batch train [5] loss 0.96345, dsc 0.03655\n",
      "Batch train [6] loss 0.96795, dsc 0.03205\n",
      "Batch train [7] loss 0.96463, dsc 0.03537\n",
      "Batch train [8] loss 0.96554, dsc 0.03446\n",
      "Batch train [9] loss 0.96499, dsc 0.03501\n",
      "Batch train [10] loss 0.96342, dsc 0.03658\n",
      "Epoch [13] train done\n",
      "Batch eval [1] loss 0.96076, dsc 0.03924\n",
      "Batch eval [2] loss 0.96610, dsc 0.03390\n",
      "Batch eval [3] loss 0.96176, dsc 0.03824\n",
      "Batch eval [4] loss 0.97015, dsc 0.02985\n",
      "Batch eval [5] loss 0.96047, dsc 0.03953\n",
      "Epoch [13] valid done\n",
      "Epoch [13] T 52.30s, deltaT 4.07s, loss: train 0.96573, valid 0.96385, dsc: train 0.03427, valid 0.03615\n",
      "Batch train [1] loss 0.96457, dsc 0.03543\n",
      "Batch train [2] loss 0.96903, dsc 0.03097\n",
      "Batch train [3] loss 0.96790, dsc 0.03210\n",
      "Batch train [4] loss 0.96436, dsc 0.03564\n",
      "Batch train [5] loss 0.96492, dsc 0.03508\n",
      "Batch train [6] loss 0.96383, dsc 0.03617\n",
      "Batch train [7] loss 0.96456, dsc 0.03544\n",
      "Batch train [8] loss 0.96448, dsc 0.03552\n",
      "Batch train [9] loss 0.96411, dsc 0.03589\n",
      "Batch train [10] loss 0.96837, dsc 0.03163\n",
      "Epoch [14] train done\n",
      "Batch eval [1] loss 0.96093, dsc 0.03907\n",
      "Batch eval [2] loss 0.96619, dsc 0.03381\n",
      "Batch eval [3] loss 0.96266, dsc 0.03734\n",
      "Batch eval [4] loss 0.97050, dsc 0.02950\n",
      "Batch eval [5] loss 0.96069, dsc 0.03931\n",
      "Epoch [14] valid done\n",
      "Epoch [14] T 56.30s, deltaT 4.00s, loss: train 0.96561, valid 0.96419, dsc: train 0.03439, valid 0.03581\n",
      "Batch train [1] loss 0.96926, dsc 0.03074\n",
      "Batch train [2] loss 0.96445, dsc 0.03555\n",
      "Batch train [3] loss 0.96253, dsc 0.03747\n",
      "Batch train [4] loss 0.96668, dsc 0.03332\n",
      "Batch train [5] loss 0.96409, dsc 0.03591\n",
      "Batch train [6] loss 0.96778, dsc 0.03222\n",
      "Batch train [7] loss 0.96257, dsc 0.03743\n",
      "Batch train [8] loss 0.96377, dsc 0.03623\n",
      "Batch train [9] loss 0.96833, dsc 0.03167\n",
      "Batch train [10] loss 0.96552, dsc 0.03448\n",
      "Epoch [15] train done\n",
      "Batch eval [1] loss 0.96089, dsc 0.03911\n",
      "Batch eval [2] loss 0.96621, dsc 0.03379\n",
      "Batch eval [3] loss 0.96272, dsc 0.03728\n",
      "Batch eval [4] loss 0.97060, dsc 0.02940\n",
      "Batch eval [5] loss 0.96070, dsc 0.03930\n",
      "Epoch [15] valid done\n",
      "Epoch [15] T 60.33s, deltaT 4.03s, loss: train 0.96550, valid 0.96422, dsc: train 0.03450, valid 0.03578\n",
      "Batch train [1] loss 0.96488, dsc 0.03512\n",
      "Batch train [2] loss 0.96843, dsc 0.03157\n",
      "Batch train [3] loss 0.96340, dsc 0.03660\n",
      "Batch train [4] loss 0.95974, dsc 0.04026\n",
      "Batch train [5] loss 0.96466, dsc 0.03534\n",
      "Batch train [6] loss 0.96703, dsc 0.03297\n",
      "Batch train [7] loss 0.96652, dsc 0.03348\n",
      "Batch train [8] loss 0.96368, dsc 0.03632\n",
      "Batch train [9] loss 0.96723, dsc 0.03277\n",
      "Batch train [10] loss 0.96830, dsc 0.03170\n",
      "Epoch [16] train done\n",
      "Batch eval [1] loss 0.96146, dsc 0.03854\n",
      "Batch eval [2] loss 0.96677, dsc 0.03323\n",
      "Batch eval [3] loss 0.96407, dsc 0.03593\n",
      "Batch eval [4] loss 0.97219, dsc 0.02781\n",
      "Batch eval [5] loss 0.96154, dsc 0.03846\n",
      "Epoch [16] valid done\n",
      "Epoch [16] T 64.35s, deltaT 4.02s, loss: train 0.96539, valid 0.96521, dsc: train 0.03461, valid 0.03479\n",
      "Batch train [1] loss 0.96266, dsc 0.03734\n",
      "Batch train [2] loss 0.96487, dsc 0.03513\n",
      "Batch train [3] loss 0.96451, dsc 0.03549\n",
      "Batch train [4] loss 0.96703, dsc 0.03297\n",
      "Batch train [5] loss 0.96618, dsc 0.03382\n",
      "Batch train [6] loss 0.96794, dsc 0.03206\n",
      "Batch train [7] loss 0.96676, dsc 0.03324\n",
      "Batch train [8] loss 0.96231, dsc 0.03769\n",
      "Batch train [9] loss 0.96705, dsc 0.03295\n",
      "Batch train [10] loss 0.96343, dsc 0.03657\n",
      "Epoch [17] train done\n",
      "Batch eval [1] loss 0.96092, dsc 0.03908\n",
      "Batch eval [2] loss 0.96594, dsc 0.03406\n",
      "Batch eval [3] loss 0.96236, dsc 0.03764\n",
      "Batch eval [4] loss 0.97043, dsc 0.02957\n",
      "Batch eval [5] loss 0.96033, dsc 0.03967\n",
      "Epoch [17] valid done\n",
      "Epoch [17] T 68.32s, deltaT 3.97s, loss: train 0.96527, valid 0.96400, dsc: train 0.03473, valid 0.03600\n",
      "Batch train [1] loss 0.96802, dsc 0.03198\n",
      "Batch train [2] loss 0.96235, dsc 0.03765\n",
      "Batch train [3] loss 0.96507, dsc 0.03493\n",
      "Batch train [4] loss 0.97138, dsc 0.02862\n",
      "Batch train [5] loss 0.96682, dsc 0.03318\n",
      "Batch train [6] loss 0.96220, dsc 0.03780\n",
      "Batch train [7] loss 0.96103, dsc 0.03897\n",
      "Batch train [8] loss 0.96580, dsc 0.03420\n",
      "Batch train [9] loss 0.96374, dsc 0.03626\n",
      "Batch train [10] loss 0.96507, dsc 0.03493\n",
      "Epoch [18] train done\n",
      "Batch eval [1] loss 0.96024, dsc 0.03976\n",
      "Batch eval [2] loss 0.96562, dsc 0.03438\n",
      "Batch eval [3] loss 0.96153, dsc 0.03847\n",
      "Batch eval [4] loss 0.96986, dsc 0.03014\n",
      "Batch eval [5] loss 0.95978, dsc 0.04022\n",
      "Epoch [18] valid done\n",
      "Epoch [18] T 72.37s, deltaT 4.05s, loss: train 0.96515, valid 0.96341, dsc: train 0.03485, valid 0.03659\n",
      "Batch train [1] loss 0.96554, dsc 0.03446\n",
      "Batch train [2] loss 0.96487, dsc 0.03513\n",
      "Batch train [3] loss 0.96119, dsc 0.03881\n",
      "Batch train [4] loss 0.96733, dsc 0.03267\n",
      "Batch train [5] loss 0.96160, dsc 0.03840\n",
      "Batch train [6] loss 0.96596, dsc 0.03404\n",
      "Batch train [7] loss 0.96996, dsc 0.03004\n",
      "Batch train [8] loss 0.96165, dsc 0.03835\n",
      "Batch train [9] loss 0.96750, dsc 0.03250\n",
      "Batch train [10] loss 0.96457, dsc 0.03543\n",
      "Epoch [19] train done\n",
      "Batch eval [1] loss 0.96024, dsc 0.03976\n",
      "Batch eval [2] loss 0.96553, dsc 0.03447\n",
      "Batch eval [3] loss 0.96159, dsc 0.03841\n",
      "Batch eval [4] loss 0.97007, dsc 0.02993\n",
      "Batch eval [5] loss 0.95973, dsc 0.04027\n",
      "Epoch [19] valid done\n",
      "Epoch [19] T 76.50s, deltaT 4.13s, loss: train 0.96502, valid 0.96343, dsc: train 0.03498, valid 0.03657\n",
      "Batch train [1] loss 0.96680, dsc 0.03320\n",
      "Batch train [2] loss 0.95999, dsc 0.04001\n",
      "Batch train [3] loss 0.96215, dsc 0.03785\n",
      "Batch train [4] loss 0.96383, dsc 0.03617\n",
      "Batch train [5] loss 0.96705, dsc 0.03295\n",
      "Batch train [6] loss 0.96599, dsc 0.03401\n",
      "Batch train [7] loss 0.96651, dsc 0.03349\n",
      "Batch train [8] loss 0.97272, dsc 0.02728\n",
      "Batch train [9] loss 0.96326, dsc 0.03674\n",
      "Batch train [10] loss 0.96089, dsc 0.03911\n",
      "Epoch [20] train done\n",
      "Batch eval [1] loss 0.96081, dsc 0.03919\n",
      "Batch eval [2] loss 0.96597, dsc 0.03403\n",
      "Batch eval [3] loss 0.96254, dsc 0.03746\n",
      "Batch eval [4] loss 0.97121, dsc 0.02879\n",
      "Batch eval [5] loss 0.96026, dsc 0.03974\n",
      "Epoch [20] valid done\n",
      "Epoch [20] T 80.63s, deltaT 4.13s, loss: train 0.96492, valid 0.96416, dsc: train 0.03508, valid 0.03584\n",
      "Batch train [1] loss 0.96406, dsc 0.03594\n",
      "Batch train [2] loss 0.96660, dsc 0.03340\n",
      "Batch train [3] loss 0.96132, dsc 0.03868\n",
      "Batch train [4] loss 0.96404, dsc 0.03596\n",
      "Batch train [5] loss 0.97181, dsc 0.02819\n",
      "Batch train [6] loss 0.96216, dsc 0.03784\n",
      "Batch train [7] loss 0.96236, dsc 0.03764\n",
      "Batch train [8] loss 0.96408, dsc 0.03592\n",
      "Batch train [9] loss 0.96871, dsc 0.03129\n",
      "Batch train [10] loss 0.96270, dsc 0.03730\n",
      "Epoch [21] train done\n",
      "Batch eval [1] loss 0.95977, dsc 0.04023\n",
      "Batch eval [2] loss 0.96520, dsc 0.03480\n",
      "Batch eval [3] loss 0.96068, dsc 0.03932\n",
      "Batch eval [4] loss 0.96933, dsc 0.03067\n",
      "Batch eval [5] loss 0.95935, dsc 0.04065\n",
      "Epoch [21] valid done\n",
      "Epoch [21] T 84.67s, deltaT 4.03s, loss: train 0.96478, valid 0.96287, dsc: train 0.03522, valid 0.03713\n",
      "Batch train [1] loss 0.96834, dsc 0.03166\n",
      "Batch train [2] loss 0.96556, dsc 0.03444\n",
      "Batch train [3] loss 0.96343, dsc 0.03657\n",
      "Batch train [4] loss 0.96834, dsc 0.03166\n",
      "Batch train [5] loss 0.96387, dsc 0.03613\n",
      "Batch train [6] loss 0.95862, dsc 0.04138\n",
      "Batch train [7] loss 0.96272, dsc 0.03728\n",
      "Batch train [8] loss 0.96365, dsc 0.03635\n",
      "Batch train [9] loss 0.96957, dsc 0.03043\n",
      "Batch train [10] loss 0.96245, dsc 0.03755\n",
      "Epoch [22] train done\n",
      "Batch eval [1] loss 0.95970, dsc 0.04030\n",
      "Batch eval [2] loss 0.96535, dsc 0.03465\n",
      "Batch eval [3] loss 0.96064, dsc 0.03936\n",
      "Batch eval [4] loss 0.96933, dsc 0.03067\n",
      "Batch eval [5] loss 0.95932, dsc 0.04068\n",
      "Epoch [22] valid done\n",
      "Epoch [22] T 88.68s, deltaT 4.02s, loss: train 0.96465, valid 0.96287, dsc: train 0.03535, valid 0.03713\n",
      "Batch train [1] loss 0.96591, dsc 0.03409\n",
      "Batch train [2] loss 0.96606, dsc 0.03394\n",
      "Batch train [3] loss 0.96433, dsc 0.03567\n",
      "Batch train [4] loss 0.96577, dsc 0.03423\n",
      "Batch train [5] loss 0.96821, dsc 0.03179\n",
      "Batch train [6] loss 0.96531, dsc 0.03469\n",
      "Batch train [7] loss 0.96474, dsc 0.03526\n",
      "Batch train [8] loss 0.96188, dsc 0.03812\n",
      "Batch train [9] loss 0.96523, dsc 0.03477\n",
      "Batch train [10] loss 0.95793, dsc 0.04207\n",
      "Epoch [23] train done\n",
      "Batch eval [1] loss 0.95926, dsc 0.04074\n",
      "Batch eval [2] loss 0.96487, dsc 0.03513\n",
      "Batch eval [3] loss 0.96019, dsc 0.03981\n",
      "Batch eval [4] loss 0.96901, dsc 0.03099\n",
      "Batch eval [5] loss 0.95899, dsc 0.04101\n",
      "Epoch [23] valid done\n",
      "Epoch [23] T 92.67s, deltaT 3.99s, loss: train 0.96454, valid 0.96246, dsc: train 0.03546, valid 0.03754\n",
      "Batch train [1] loss 0.96474, dsc 0.03526\n",
      "Batch train [2] loss 0.96809, dsc 0.03191\n",
      "Batch train [3] loss 0.96354, dsc 0.03646\n",
      "Batch train [4] loss 0.96210, dsc 0.03790\n",
      "Batch train [5] loss 0.96631, dsc 0.03369\n",
      "Batch train [6] loss 0.96420, dsc 0.03580\n",
      "Batch train [7] loss 0.96327, dsc 0.03673\n",
      "Batch train [8] loss 0.96343, dsc 0.03657\n",
      "Batch train [9] loss 0.96337, dsc 0.03663\n",
      "Batch train [10] loss 0.96496, dsc 0.03504\n",
      "Epoch [24] train done\n",
      "Batch eval [1] loss 0.95909, dsc 0.04091\n",
      "Batch eval [2] loss 0.96474, dsc 0.03526\n",
      "Batch eval [3] loss 0.95988, dsc 0.04012\n",
      "Batch eval [4] loss 0.96873, dsc 0.03127\n",
      "Batch eval [5] loss 0.95877, dsc 0.04123\n",
      "Epoch [24] valid done\n",
      "Epoch [24] T 96.75s, deltaT 4.08s, loss: train 0.96440, valid 0.96224, dsc: train 0.03560, valid 0.03776\n",
      "Batch train [1] loss 0.96684, dsc 0.03316\n",
      "Batch train [2] loss 0.95809, dsc 0.04191\n",
      "Batch train [3] loss 0.96046, dsc 0.03954\n",
      "Batch train [4] loss 0.96653, dsc 0.03347\n",
      "Batch train [5] loss 0.96466, dsc 0.03534\n",
      "Batch train [6] loss 0.96487, dsc 0.03513\n",
      "Batch train [7] loss 0.96425, dsc 0.03575\n",
      "Batch train [8] loss 0.96225, dsc 0.03775\n",
      "Batch train [9] loss 0.96419, dsc 0.03581\n",
      "Batch train [10] loss 0.97077, dsc 0.02923\n",
      "Epoch [25] train done\n",
      "Batch eval [1] loss 0.95917, dsc 0.04083\n",
      "Batch eval [2] loss 0.96484, dsc 0.03516\n",
      "Batch eval [3] loss 0.96033, dsc 0.03967\n",
      "Batch eval [4] loss 0.96911, dsc 0.03089\n",
      "Batch eval [5] loss 0.95890, dsc 0.04110\n",
      "Epoch [25] valid done\n",
      "Epoch [25] T 100.90s, deltaT 4.15s, loss: train 0.96429, valid 0.96247, dsc: train 0.03571, valid 0.03753\n",
      "Batch train [1] loss 0.96705, dsc 0.03295\n",
      "Batch train [2] loss 0.96417, dsc 0.03583\n",
      "Batch train [3] loss 0.96383, dsc 0.03617\n",
      "Batch train [4] loss 0.96136, dsc 0.03864\n",
      "Batch train [5] loss 0.96372, dsc 0.03628\n",
      "Batch train [6] loss 0.96547, dsc 0.03453\n",
      "Batch train [7] loss 0.96840, dsc 0.03160\n",
      "Batch train [8] loss 0.96107, dsc 0.03893\n",
      "Batch train [9] loss 0.96587, dsc 0.03413\n",
      "Batch train [10] loss 0.96056, dsc 0.03944\n",
      "Epoch [26] train done\n",
      "Batch eval [1] loss 0.95926, dsc 0.04074\n",
      "Batch eval [2] loss 0.96480, dsc 0.03520\n",
      "Batch eval [3] loss 0.96048, dsc 0.03952\n",
      "Batch eval [4] loss 0.96939, dsc 0.03061\n",
      "Batch eval [5] loss 0.95897, dsc 0.04103\n",
      "Epoch [26] valid done\n",
      "Epoch [26] T 104.99s, deltaT 4.09s, loss: train 0.96415, valid 0.96258, dsc: train 0.03585, valid 0.03742\n",
      "Batch train [1] loss 0.96016, dsc 0.03984\n",
      "Batch train [2] loss 0.96331, dsc 0.03669\n",
      "Batch train [3] loss 0.96299, dsc 0.03701\n",
      "Batch train [4] loss 0.96167, dsc 0.03833\n",
      "Batch train [5] loss 0.96588, dsc 0.03412\n",
      "Batch train [6] loss 0.96364, dsc 0.03636\n",
      "Batch train [7] loss 0.96495, dsc 0.03505\n",
      "Batch train [8] loss 0.96434, dsc 0.03566\n",
      "Batch train [9] loss 0.96693, dsc 0.03307\n",
      "Batch train [10] loss 0.96639, dsc 0.03361\n",
      "Epoch [27] train done\n",
      "Batch eval [1] loss 0.95933, dsc 0.04067\n",
      "Batch eval [2] loss 0.96470, dsc 0.03530\n",
      "Batch eval [3] loss 0.96058, dsc 0.03942\n",
      "Batch eval [4] loss 0.96968, dsc 0.03032\n",
      "Batch eval [5] loss 0.95886, dsc 0.04114\n",
      "Epoch [27] valid done\n",
      "Epoch [27] T 109.05s, deltaT 4.06s, loss: train 0.96403, valid 0.96263, dsc: train 0.03597, valid 0.03737\n",
      "Batch train [1] loss 0.96769, dsc 0.03231\n",
      "Batch train [2] loss 0.96098, dsc 0.03902\n",
      "Batch train [3] loss 0.96388, dsc 0.03612\n",
      "Batch train [4] loss 0.96247, dsc 0.03753\n",
      "Batch train [5] loss 0.96667, dsc 0.03333\n",
      "Batch train [6] loss 0.96531, dsc 0.03469\n",
      "Batch train [7] loss 0.95957, dsc 0.04043\n",
      "Batch train [8] loss 0.96439, dsc 0.03561\n",
      "Batch train [9] loss 0.96338, dsc 0.03662\n",
      "Batch train [10] loss 0.96481, dsc 0.03519\n",
      "Epoch [28] train done\n",
      "Batch eval [1] loss 0.95879, dsc 0.04121\n",
      "Batch eval [2] loss 0.96442, dsc 0.03558\n",
      "Batch eval [3] loss 0.96007, dsc 0.03993\n",
      "Batch eval [4] loss 0.96892, dsc 0.03108\n",
      "Batch eval [5] loss 0.95842, dsc 0.04158\n",
      "Epoch [28] valid done\n",
      "Epoch [28] T 113.16s, deltaT 4.11s, loss: train 0.96391, valid 0.96212, dsc: train 0.03609, valid 0.03788\n",
      "Batch train [1] loss 0.96714, dsc 0.03286\n",
      "Batch train [2] loss 0.96236, dsc 0.03764\n",
      "Batch train [3] loss 0.95826, dsc 0.04174\n",
      "Batch train [4] loss 0.96551, dsc 0.03449\n",
      "Batch train [5] loss 0.96147, dsc 0.03853\n",
      "Batch train [6] loss 0.96535, dsc 0.03465\n",
      "Batch train [7] loss 0.96882, dsc 0.03118\n",
      "Batch train [8] loss 0.96262, dsc 0.03738\n",
      "Batch train [9] loss 0.96446, dsc 0.03554\n",
      "Batch train [10] loss 0.96174, dsc 0.03826\n",
      "Epoch [29] train done\n",
      "Batch eval [1] loss 0.95899, dsc 0.04101\n",
      "Batch eval [2] loss 0.96440, dsc 0.03560\n",
      "Batch eval [3] loss 0.96013, dsc 0.03987\n",
      "Batch eval [4] loss 0.96902, dsc 0.03098\n",
      "Batch eval [5] loss 0.95839, dsc 0.04161\n",
      "Epoch [29] valid done\n",
      "Epoch [29] T 117.23s, deltaT 4.06s, loss: train 0.96377, valid 0.96218, dsc: train 0.03623, valid 0.03782\n",
      "Batch train [1] loss 0.96387, dsc 0.03613\n",
      "Batch train [2] loss 0.96474, dsc 0.03526\n",
      "Batch train [3] loss 0.96538, dsc 0.03462\n",
      "Batch train [4] loss 0.96104, dsc 0.03896\n",
      "Batch train [5] loss 0.96506, dsc 0.03494\n",
      "Batch train [6] loss 0.96361, dsc 0.03639\n",
      "Batch train [7] loss 0.96332, dsc 0.03668\n",
      "Batch train [8] loss 0.96157, dsc 0.03843\n",
      "Batch train [9] loss 0.96089, dsc 0.03911\n",
      "Batch train [10] loss 0.96696, dsc 0.03304\n",
      "Epoch [30] train done\n",
      "Batch eval [1] loss 0.95860, dsc 0.04140\n",
      "Batch eval [2] loss 0.96416, dsc 0.03584\n",
      "Batch eval [3] loss 0.95992, dsc 0.04008\n",
      "Batch eval [4] loss 0.96851, dsc 0.03149\n",
      "Batch eval [5] loss 0.95823, dsc 0.04177\n",
      "Epoch [30] valid done\n",
      "Epoch [30] T 121.33s, deltaT 4.10s, loss: train 0.96364, valid 0.96188, dsc: train 0.03636, valid 0.03812\n",
      "Batch train [1] loss 0.96351, dsc 0.03649\n",
      "Batch train [2] loss 0.96218, dsc 0.03782\n",
      "Batch train [3] loss 0.95982, dsc 0.04018\n",
      "Batch train [4] loss 0.96536, dsc 0.03464\n",
      "Batch train [5] loss 0.96125, dsc 0.03875\n",
      "Batch train [6] loss 0.96403, dsc 0.03597\n",
      "Batch train [7] loss 0.96177, dsc 0.03823\n",
      "Batch train [8] loss 0.96132, dsc 0.03868\n",
      "Batch train [9] loss 0.97008, dsc 0.02992\n",
      "Batch train [10] loss 0.96583, dsc 0.03417\n",
      "Epoch [31] train done\n",
      "Batch eval [1] loss 0.95820, dsc 0.04180\n",
      "Batch eval [2] loss 0.96405, dsc 0.03595\n",
      "Batch eval [3] loss 0.95966, dsc 0.04034\n",
      "Batch eval [4] loss 0.96863, dsc 0.03137\n",
      "Batch eval [5] loss 0.95804, dsc 0.04196\n",
      "Epoch [31] valid done\n",
      "Epoch [31] T 125.45s, deltaT 4.12s, loss: train 0.96351, valid 0.96172, dsc: train 0.03649, valid 0.03828\n",
      "Batch train [1] loss 0.96509, dsc 0.03491\n",
      "Batch train [2] loss 0.96412, dsc 0.03588\n",
      "Batch train [3] loss 0.96296, dsc 0.03704\n",
      "Batch train [4] loss 0.96402, dsc 0.03598\n",
      "Batch train [5] loss 0.96097, dsc 0.03903\n",
      "Batch train [6] loss 0.96224, dsc 0.03776\n",
      "Batch train [7] loss 0.96448, dsc 0.03552\n",
      "Batch train [8] loss 0.96193, dsc 0.03807\n",
      "Batch train [9] loss 0.96364, dsc 0.03636\n",
      "Batch train [10] loss 0.96425, dsc 0.03575\n",
      "Epoch [32] train done\n",
      "Batch eval [1] loss 0.95816, dsc 0.04184\n",
      "Batch eval [2] loss 0.96393, dsc 0.03607\n",
      "Batch eval [3] loss 0.95929, dsc 0.04071\n",
      "Batch eval [4] loss 0.96831, dsc 0.03169\n",
      "Batch eval [5] loss 0.95776, dsc 0.04224\n",
      "Epoch [32] valid done\n",
      "Epoch [32] T 129.40s, deltaT 3.95s, loss: train 0.96337, valid 0.96149, dsc: train 0.03663, valid 0.03851\n",
      "Batch train [1] loss 0.96156, dsc 0.03844\n",
      "Batch train [2] loss 0.96284, dsc 0.03716\n",
      "Batch train [3] loss 0.96610, dsc 0.03390\n",
      "Batch train [4] loss 0.96201, dsc 0.03799\n",
      "Batch train [5] loss 0.95897, dsc 0.04103\n",
      "Batch train [6] loss 0.96233, dsc 0.03767\n",
      "Batch train [7] loss 0.96254, dsc 0.03746\n",
      "Batch train [8] loss 0.96643, dsc 0.03357\n",
      "Batch train [9] loss 0.96692, dsc 0.03308\n",
      "Batch train [10] loss 0.96272, dsc 0.03728\n",
      "Epoch [33] train done\n",
      "Batch eval [1] loss 0.95791, dsc 0.04209\n",
      "Batch eval [2] loss 0.96378, dsc 0.03622\n",
      "Batch eval [3] loss 0.95917, dsc 0.04083\n",
      "Batch eval [4] loss 0.96805, dsc 0.03195\n",
      "Batch eval [5] loss 0.95762, dsc 0.04238\n",
      "Epoch [33] valid done\n",
      "Epoch [33] T 133.39s, deltaT 3.99s, loss: train 0.96324, valid 0.96131, dsc: train 0.03676, valid 0.03869\n",
      "Batch train [1] loss 0.96232, dsc 0.03768\n",
      "Batch train [2] loss 0.96225, dsc 0.03775\n",
      "Batch train [3] loss 0.96457, dsc 0.03543\n",
      "Batch train [4] loss 0.96491, dsc 0.03509\n",
      "Batch train [5] loss 0.96882, dsc 0.03118\n",
      "Batch train [6] loss 0.96518, dsc 0.03482\n",
      "Batch train [7] loss 0.96368, dsc 0.03632\n",
      "Batch train [8] loss 0.95628, dsc 0.04372\n",
      "Batch train [9] loss 0.96003, dsc 0.03997\n",
      "Batch train [10] loss 0.96309, dsc 0.03691\n",
      "Epoch [34] train done\n",
      "Batch eval [1] loss 0.95763, dsc 0.04237\n",
      "Batch eval [2] loss 0.96355, dsc 0.03645\n",
      "Batch eval [3] loss 0.95914, dsc 0.04086\n",
      "Batch eval [4] loss 0.96784, dsc 0.03216\n",
      "Batch eval [5] loss 0.95743, dsc 0.04257\n",
      "Epoch [34] valid done\n",
      "Epoch [34] T 137.45s, deltaT 4.05s, loss: train 0.96311, valid 0.96112, dsc: train 0.03689, valid 0.03888\n",
      "Batch train [1] loss 0.96319, dsc 0.03681\n",
      "Batch train [2] loss 0.96013, dsc 0.03987\n",
      "Batch train [3] loss 0.96679, dsc 0.03321\n",
      "Batch train [4] loss 0.96168, dsc 0.03832\n",
      "Batch train [5] loss 0.96471, dsc 0.03529\n",
      "Batch train [6] loss 0.96234, dsc 0.03766\n",
      "Batch train [7] loss 0.96335, dsc 0.03665\n",
      "Batch train [8] loss 0.96142, dsc 0.03858\n",
      "Batch train [9] loss 0.96352, dsc 0.03648\n",
      "Batch train [10] loss 0.96257, dsc 0.03743\n",
      "Epoch [35] train done\n",
      "Batch eval [1] loss 0.95747, dsc 0.04253\n",
      "Batch eval [2] loss 0.96344, dsc 0.03656\n",
      "Batch eval [3] loss 0.95889, dsc 0.04111\n",
      "Batch eval [4] loss 0.96756, dsc 0.03244\n",
      "Batch eval [5] loss 0.95725, dsc 0.04275\n",
      "Epoch [35] valid done\n",
      "Epoch [35] T 141.43s, deltaT 3.99s, loss: train 0.96297, valid 0.96092, dsc: train 0.03703, valid 0.03908\n",
      "Batch train [1] loss 0.96188, dsc 0.03812\n",
      "Batch train [2] loss 0.96656, dsc 0.03344\n",
      "Batch train [3] loss 0.96269, dsc 0.03731\n",
      "Batch train [4] loss 0.96414, dsc 0.03586\n",
      "Batch train [5] loss 0.96257, dsc 0.03743\n",
      "Batch train [6] loss 0.96297, dsc 0.03703\n",
      "Batch train [7] loss 0.95943, dsc 0.04057\n",
      "Batch train [8] loss 0.95874, dsc 0.04126\n",
      "Batch train [9] loss 0.96416, dsc 0.03584\n",
      "Batch train [10] loss 0.96522, dsc 0.03478\n",
      "Epoch [36] train done\n",
      "Batch eval [1] loss 0.95748, dsc 0.04252\n",
      "Batch eval [2] loss 0.96327, dsc 0.03673\n",
      "Batch eval [3] loss 0.95902, dsc 0.04098\n",
      "Batch eval [4] loss 0.96767, dsc 0.03233\n",
      "Batch eval [5] loss 0.95717, dsc 0.04283\n",
      "Epoch [36] valid done\n",
      "Epoch [36] T 145.40s, deltaT 3.96s, loss: train 0.96284, valid 0.96092, dsc: train 0.03716, valid 0.03908\n",
      "Batch train [1] loss 0.96339, dsc 0.03661\n",
      "Batch train [2] loss 0.96166, dsc 0.03834\n",
      "Batch train [3] loss 0.95520, dsc 0.04480\n",
      "Batch train [4] loss 0.96009, dsc 0.03991\n",
      "Batch train [5] loss 0.96730, dsc 0.03270\n",
      "Batch train [6] loss 0.96149, dsc 0.03851\n",
      "Batch train [7] loss 0.96573, dsc 0.03427\n",
      "Batch train [8] loss 0.96376, dsc 0.03624\n",
      "Batch train [9] loss 0.96378, dsc 0.03622\n",
      "Batch train [10] loss 0.96456, dsc 0.03544\n",
      "Epoch [37] train done\n",
      "Batch eval [1] loss 0.95760, dsc 0.04240\n",
      "Batch eval [2] loss 0.96329, dsc 0.03671\n",
      "Batch eval [3] loss 0.95906, dsc 0.04094\n",
      "Batch eval [4] loss 0.96773, dsc 0.03227\n",
      "Batch eval [5] loss 0.95712, dsc 0.04288\n",
      "Epoch [37] valid done\n",
      "Epoch [37] T 149.37s, deltaT 3.98s, loss: train 0.96270, valid 0.96096, dsc: train 0.03730, valid 0.03904\n",
      "Batch train [1] loss 0.96098, dsc 0.03902\n",
      "Batch train [2] loss 0.96532, dsc 0.03468\n",
      "Batch train [3] loss 0.95905, dsc 0.04095\n",
      "Batch train [4] loss 0.96618, dsc 0.03382\n",
      "Batch train [5] loss 0.95916, dsc 0.04084\n",
      "Batch train [6] loss 0.96819, dsc 0.03181\n",
      "Batch train [7] loss 0.95983, dsc 0.04017\n",
      "Batch train [8] loss 0.96129, dsc 0.03871\n",
      "Batch train [9] loss 0.96741, dsc 0.03259\n",
      "Batch train [10] loss 0.95803, dsc 0.04197\n",
      "Epoch [38] train done\n",
      "Batch eval [1] loss 0.95723, dsc 0.04277\n",
      "Batch eval [2] loss 0.96308, dsc 0.03692\n",
      "Batch eval [3] loss 0.95864, dsc 0.04136\n",
      "Batch eval [4] loss 0.96742, dsc 0.03258\n",
      "Batch eval [5] loss 0.95683, dsc 0.04317\n",
      "Epoch [38] valid done\n",
      "Epoch [38] T 153.42s, deltaT 4.05s, loss: train 0.96254, valid 0.96064, dsc: train 0.03746, valid 0.03936\n",
      "Batch train [1] loss 0.96290, dsc 0.03710\n",
      "Batch train [2] loss 0.96145, dsc 0.03855\n",
      "Batch train [3] loss 0.95955, dsc 0.04045\n",
      "Batch train [4] loss 0.96607, dsc 0.03393\n",
      "Batch train [5] loss 0.95586, dsc 0.04414\n",
      "Batch train [6] loss 0.96684, dsc 0.03316\n",
      "Batch train [7] loss 0.96544, dsc 0.03456\n",
      "Batch train [8] loss 0.96019, dsc 0.03981\n",
      "Batch train [9] loss 0.96758, dsc 0.03242\n",
      "Batch train [10] loss 0.95817, dsc 0.04183\n",
      "Epoch [39] train done\n",
      "Batch eval [1] loss 0.95696, dsc 0.04304\n",
      "Batch eval [2] loss 0.96282, dsc 0.03718\n",
      "Batch eval [3] loss 0.95850, dsc 0.04150\n",
      "Batch eval [4] loss 0.96722, dsc 0.03278\n",
      "Batch eval [5] loss 0.95666, dsc 0.04334\n",
      "Epoch [39] valid done\n",
      "Epoch [39] T 157.46s, deltaT 4.04s, loss: train 0.96241, valid 0.96043, dsc: train 0.03759, valid 0.03957\n",
      "Batch train [1] loss 0.95963, dsc 0.04037\n",
      "Batch train [2] loss 0.96210, dsc 0.03790\n",
      "Batch train [3] loss 0.96689, dsc 0.03311\n",
      "Batch train [4] loss 0.95991, dsc 0.04009\n",
      "Batch train [5] loss 0.96031, dsc 0.03969\n",
      "Batch train [6] loss 0.96135, dsc 0.03865\n",
      "Batch train [7] loss 0.95787, dsc 0.04213\n",
      "Batch train [8] loss 0.96545, dsc 0.03455\n",
      "Batch train [9] loss 0.96540, dsc 0.03460\n",
      "Batch train [10] loss 0.96366, dsc 0.03634\n",
      "Epoch [40] train done\n",
      "Batch eval [1] loss 0.95734, dsc 0.04266\n",
      "Batch eval [2] loss 0.96309, dsc 0.03691\n",
      "Batch eval [3] loss 0.95885, dsc 0.04115\n",
      "Batch eval [4] loss 0.96800, dsc 0.03200\n",
      "Batch eval [5] loss 0.95674, dsc 0.04326\n",
      "Epoch [40] valid done\n",
      "Epoch [40] T 161.45s, deltaT 3.98s, loss: train 0.96226, valid 0.96080, dsc: train 0.03774, valid 0.03920\n",
      "Batch train [1] loss 0.96235, dsc 0.03765\n",
      "Batch train [2] loss 0.96182, dsc 0.03818\n",
      "Batch train [3] loss 0.96371, dsc 0.03629\n",
      "Batch train [4] loss 0.96471, dsc 0.03529\n",
      "Batch train [5] loss 0.96344, dsc 0.03656\n",
      "Batch train [6] loss 0.96280, dsc 0.03720\n",
      "Batch train [7] loss 0.96502, dsc 0.03498\n",
      "Batch train [8] loss 0.96257, dsc 0.03743\n",
      "Batch train [9] loss 0.95789, dsc 0.04211\n",
      "Batch train [10] loss 0.95684, dsc 0.04316\n",
      "Epoch [41] train done\n",
      "Batch eval [1] loss 0.95670, dsc 0.04330\n",
      "Batch eval [2] loss 0.96257, dsc 0.03743\n",
      "Batch eval [3] loss 0.95751, dsc 0.04249\n",
      "Batch eval [4] loss 0.96688, dsc 0.03312\n",
      "Batch eval [5] loss 0.95617, dsc 0.04383\n",
      "Epoch [41] valid done\n",
      "Epoch [41] T 165.49s, deltaT 4.04s, loss: train 0.96212, valid 0.95997, dsc: train 0.03788, valid 0.04003\n",
      "Batch train [1] loss 0.96118, dsc 0.03882\n",
      "Batch train [2] loss 0.96501, dsc 0.03499\n",
      "Batch train [3] loss 0.96398, dsc 0.03602\n",
      "Batch train [4] loss 0.95967, dsc 0.04033\n",
      "Batch train [5] loss 0.96023, dsc 0.03977\n",
      "Batch train [6] loss 0.96625, dsc 0.03375\n",
      "Batch train [7] loss 0.96009, dsc 0.03991\n",
      "Batch train [8] loss 0.96068, dsc 0.03932\n",
      "Batch train [9] loss 0.96151, dsc 0.03849\n",
      "Batch train [10] loss 0.96147, dsc 0.03853\n",
      "Epoch [42] train done\n",
      "Batch eval [1] loss 0.95786, dsc 0.04214\n",
      "Batch eval [2] loss 0.96350, dsc 0.03650\n",
      "Batch eval [3] loss 0.95977, dsc 0.04023\n",
      "Batch eval [4] loss 0.96874, dsc 0.03126\n",
      "Batch eval [5] loss 0.95728, dsc 0.04272\n",
      "Epoch [42] valid done\n",
      "Epoch [42] T 169.54s, deltaT 4.05s, loss: train 0.96201, valid 0.96143, dsc: train 0.03799, valid 0.03857\n",
      "Batch train [1] loss 0.95983, dsc 0.04017\n",
      "Batch train [2] loss 0.96142, dsc 0.03858\n",
      "Batch train [3] loss 0.96188, dsc 0.03812\n",
      "Batch train [4] loss 0.95963, dsc 0.04037\n",
      "Batch train [5] loss 0.96167, dsc 0.03833\n",
      "Batch train [6] loss 0.96558, dsc 0.03442\n",
      "Batch train [7] loss 0.95888, dsc 0.04112\n",
      "Batch train [8] loss 0.96091, dsc 0.03909\n",
      "Batch train [9] loss 0.96823, dsc 0.03177\n",
      "Batch train [10] loss 0.96047, dsc 0.03953\n",
      "Epoch [43] train done\n",
      "Batch eval [1] loss 0.95732, dsc 0.04268\n",
      "Batch eval [2] loss 0.96267, dsc 0.03733\n",
      "Batch eval [3] loss 0.95859, dsc 0.04141\n",
      "Batch eval [4] loss 0.96805, dsc 0.03195\n",
      "Batch eval [5] loss 0.95662, dsc 0.04338\n",
      "Epoch [43] valid done\n",
      "Epoch [43] T 173.63s, deltaT 4.09s, loss: train 0.96185, valid 0.96065, dsc: train 0.03815, valid 0.03935\n",
      "Batch train [1] loss 0.96100, dsc 0.03900\n",
      "Batch train [2] loss 0.95552, dsc 0.04448\n",
      "Batch train [3] loss 0.96621, dsc 0.03379\n",
      "Batch train [4] loss 0.96470, dsc 0.03530\n",
      "Batch train [5] loss 0.96686, dsc 0.03314\n",
      "Batch train [6] loss 0.96118, dsc 0.03882\n",
      "Batch train [7] loss 0.96038, dsc 0.03962\n",
      "Batch train [8] loss 0.95799, dsc 0.04201\n",
      "Batch train [9] loss 0.96591, dsc 0.03409\n",
      "Batch train [10] loss 0.95698, dsc 0.04302\n",
      "Epoch [44] train done\n",
      "Batch eval [1] loss 0.95617, dsc 0.04383\n",
      "Batch eval [2] loss 0.96211, dsc 0.03789\n",
      "Batch eval [3] loss 0.95716, dsc 0.04284\n",
      "Batch eval [4] loss 0.96641, dsc 0.03359\n",
      "Batch eval [5] loss 0.95572, dsc 0.04428\n",
      "Epoch [44] valid done\n",
      "Epoch [44] T 177.63s, deltaT 4.00s, loss: train 0.96167, valid 0.95952, dsc: train 0.03833, valid 0.04048\n",
      "Batch train [1] loss 0.96520, dsc 0.03480\n",
      "Batch train [2] loss 0.96180, dsc 0.03820\n",
      "Batch train [3] loss 0.96030, dsc 0.03970\n",
      "Batch train [4] loss 0.96307, dsc 0.03693\n",
      "Batch train [5] loss 0.96028, dsc 0.03972\n",
      "Batch train [6] loss 0.96074, dsc 0.03926\n",
      "Batch train [7] loss 0.96246, dsc 0.03754\n",
      "Batch train [8] loss 0.96171, dsc 0.03829\n",
      "Batch train [9] loss 0.96048, dsc 0.03952\n",
      "Batch train [10] loss 0.95898, dsc 0.04102\n",
      "Epoch [45] train done\n",
      "Batch eval [1] loss 0.95651, dsc 0.04349\n",
      "Batch eval [2] loss 0.96215, dsc 0.03785\n",
      "Batch eval [3] loss 0.95786, dsc 0.04214\n",
      "Batch eval [4] loss 0.96675, dsc 0.03325\n",
      "Batch eval [5] loss 0.95605, dsc 0.04395\n",
      "Epoch [45] valid done\n",
      "Epoch [45] T 181.62s, deltaT 3.98s, loss: train 0.96150, valid 0.95987, dsc: train 0.03850, valid 0.04013\n",
      "Batch train [1] loss 0.96200, dsc 0.03800\n",
      "Batch train [2] loss 0.96116, dsc 0.03884\n",
      "Batch train [3] loss 0.95953, dsc 0.04047\n",
      "Batch train [4] loss 0.95969, dsc 0.04031\n",
      "Batch train [5] loss 0.95922, dsc 0.04078\n",
      "Batch train [6] loss 0.96259, dsc 0.03741\n",
      "Batch train [7] loss 0.96059, dsc 0.03941\n",
      "Batch train [8] loss 0.96070, dsc 0.03930\n",
      "Batch train [9] loss 0.96149, dsc 0.03851\n",
      "Batch train [10] loss 0.96636, dsc 0.03364\n",
      "Epoch [46] train done\n",
      "Batch eval [1] loss 0.95589, dsc 0.04411\n",
      "Batch eval [2] loss 0.96193, dsc 0.03807\n",
      "Batch eval [3] loss 0.95737, dsc 0.04263\n",
      "Batch eval [4] loss 0.96632, dsc 0.03368\n",
      "Batch eval [5] loss 0.95554, dsc 0.04446\n",
      "Epoch [46] valid done\n",
      "Epoch [46] T 185.62s, deltaT 4.00s, loss: train 0.96133, valid 0.95941, dsc: train 0.03867, valid 0.04059\n",
      "Batch train [1] loss 0.95962, dsc 0.04038\n",
      "Batch train [2] loss 0.96079, dsc 0.03921\n",
      "Batch train [3] loss 0.96041, dsc 0.03959\n",
      "Batch train [4] loss 0.95945, dsc 0.04055\n",
      "Batch train [5] loss 0.96061, dsc 0.03939\n",
      "Batch train [6] loss 0.96425, dsc 0.03575\n",
      "Batch train [7] loss 0.96167, dsc 0.03833\n",
      "Batch train [8] loss 0.96276, dsc 0.03724\n",
      "Batch train [9] loss 0.95817, dsc 0.04183\n",
      "Batch train [10] loss 0.96383, dsc 0.03617\n",
      "Epoch [47] train done\n",
      "Batch eval [1] loss 0.95588, dsc 0.04412\n",
      "Batch eval [2] loss 0.96171, dsc 0.03829\n",
      "Batch eval [3] loss 0.95734, dsc 0.04266\n",
      "Batch eval [4] loss 0.96624, dsc 0.03376\n",
      "Batch eval [5] loss 0.95543, dsc 0.04457\n",
      "Epoch [47] valid done\n",
      "Epoch [47] T 189.60s, deltaT 3.98s, loss: train 0.96116, valid 0.95932, dsc: train 0.03884, valid 0.04068\n",
      "Batch train [1] loss 0.96138, dsc 0.03862\n",
      "Batch train [2] loss 0.96744, dsc 0.03256\n",
      "Batch train [3] loss 0.95801, dsc 0.04199\n",
      "Batch train [4] loss 0.96138, dsc 0.03862\n",
      "Batch train [5] loss 0.95930, dsc 0.04070\n",
      "Batch train [6] loss 0.96763, dsc 0.03237\n",
      "Batch train [7] loss 0.95889, dsc 0.04111\n",
      "Batch train [8] loss 0.95962, dsc 0.04038\n",
      "Batch train [9] loss 0.95772, dsc 0.04228\n",
      "Batch train [10] loss 0.95864, dsc 0.04136\n",
      "Epoch [48] train done\n",
      "Batch eval [1] loss 0.95598, dsc 0.04402\n",
      "Batch eval [2] loss 0.96182, dsc 0.03818\n",
      "Batch eval [3] loss 0.95700, dsc 0.04300\n",
      "Batch eval [4] loss 0.96613, dsc 0.03387\n",
      "Batch eval [5] loss 0.95514, dsc 0.04486\n",
      "Epoch [48] valid done\n",
      "Epoch [48] T 193.60s, deltaT 4.00s, loss: train 0.96100, valid 0.95921, dsc: train 0.03900, valid 0.04079\n",
      "Batch train [1] loss 0.95746, dsc 0.04254\n",
      "Batch train [2] loss 0.95689, dsc 0.04311\n",
      "Batch train [3] loss 0.95980, dsc 0.04020\n",
      "Batch train [4] loss 0.96138, dsc 0.03862\n",
      "Batch train [5] loss 0.96661, dsc 0.03339\n",
      "Batch train [6] loss 0.96195, dsc 0.03805\n",
      "Batch train [7] loss 0.95871, dsc 0.04129\n",
      "Batch train [8] loss 0.96262, dsc 0.03738\n",
      "Batch train [9] loss 0.95985, dsc 0.04015\n",
      "Batch train [10] loss 0.96310, dsc 0.03690\n",
      "Epoch [49] train done\n",
      "Batch eval [1] loss 0.95614, dsc 0.04386\n",
      "Batch eval [2] loss 0.96174, dsc 0.03826\n",
      "Batch eval [3] loss 0.95782, dsc 0.04218\n",
      "Batch eval [4] loss 0.96667, dsc 0.03333\n",
      "Batch eval [5] loss 0.95554, dsc 0.04446\n",
      "Epoch [49] valid done\n",
      "Epoch [49] T 197.60s, deltaT 3.99s, loss: train 0.96084, valid 0.95958, dsc: train 0.03916, valid 0.04042\n",
      "Batch train [1] loss 0.95952, dsc 0.04048\n",
      "Batch train [2] loss 0.96074, dsc 0.03926\n",
      "Batch train [3] loss 0.96217, dsc 0.03783\n",
      "Batch train [4] loss 0.96164, dsc 0.03836\n",
      "Batch train [5] loss 0.96053, dsc 0.03947\n",
      "Batch train [6] loss 0.96126, dsc 0.03874\n",
      "Batch train [7] loss 0.96450, dsc 0.03550\n",
      "Batch train [8] loss 0.95842, dsc 0.04158\n",
      "Batch train [9] loss 0.96113, dsc 0.03887\n",
      "Batch train [10] loss 0.95660, dsc 0.04340\n",
      "Epoch [50] train done\n",
      "Batch eval [1] loss 0.95502, dsc 0.04498\n",
      "Batch eval [2] loss 0.96130, dsc 0.03870\n",
      "Batch eval [3] loss 0.95660, dsc 0.04340\n",
      "Batch eval [4] loss 0.96563, dsc 0.03437\n",
      "Batch eval [5] loss 0.95483, dsc 0.04517\n",
      "Epoch [50] valid done\n",
      "Epoch [50] T 201.61s, deltaT 4.01s, loss: train 0.96065, valid 0.95868, dsc: train 0.03935, valid 0.04132\n",
      "Batch train [1] loss 0.95904, dsc 0.04096\n",
      "Batch train [2] loss 0.95930, dsc 0.04070\n",
      "Batch train [3] loss 0.95643, dsc 0.04357\n",
      "Batch train [4] loss 0.96239, dsc 0.03761\n",
      "Batch train [5] loss 0.96165, dsc 0.03835\n",
      "Batch train [6] loss 0.96069, dsc 0.03931\n",
      "Batch train [7] loss 0.96431, dsc 0.03569\n",
      "Batch train [8] loss 0.96033, dsc 0.03967\n",
      "Batch train [9] loss 0.96391, dsc 0.03609\n",
      "Batch train [10] loss 0.95698, dsc 0.04302\n",
      "Epoch [51] train done\n",
      "Batch eval [1] loss 0.95503, dsc 0.04497\n",
      "Batch eval [2] loss 0.96104, dsc 0.03896\n",
      "Batch eval [3] loss 0.95645, dsc 0.04355\n",
      "Batch eval [4] loss 0.96555, dsc 0.03445\n",
      "Batch eval [5] loss 0.95458, dsc 0.04542\n",
      "Epoch [51] valid done\n",
      "Epoch [51] T 205.62s, deltaT 4.01s, loss: train 0.96050, valid 0.95853, dsc: train 0.03950, valid 0.04147\n",
      "Batch train [1] loss 0.96470, dsc 0.03530\n",
      "Batch train [2] loss 0.95666, dsc 0.04334\n",
      "Batch train [3] loss 0.95918, dsc 0.04082\n",
      "Batch train [4] loss 0.96205, dsc 0.03795\n",
      "Batch train [5] loss 0.95564, dsc 0.04436\n",
      "Batch train [6] loss 0.96267, dsc 0.03733\n",
      "Batch train [7] loss 0.95864, dsc 0.04136\n",
      "Batch train [8] loss 0.95943, dsc 0.04057\n",
      "Batch train [9] loss 0.96262, dsc 0.03738\n",
      "Batch train [10] loss 0.96174, dsc 0.03826\n",
      "Epoch [52] train done\n",
      "Batch eval [1] loss 0.95653, dsc 0.04347\n",
      "Batch eval [2] loss 0.96153, dsc 0.03847\n",
      "Batch eval [3] loss 0.95797, dsc 0.04203\n",
      "Batch eval [4] loss 0.96647, dsc 0.03353\n",
      "Batch eval [5] loss 0.95545, dsc 0.04455\n",
      "Epoch [52] valid done\n",
      "Epoch [52] T 209.64s, deltaT 4.02s, loss: train 0.96033, valid 0.95959, dsc: train 0.03967, valid 0.04041\n",
      "Batch train [1] loss 0.95890, dsc 0.04110\n",
      "Batch train [2] loss 0.96070, dsc 0.03930\n",
      "Batch train [3] loss 0.96144, dsc 0.03856\n",
      "Batch train [4] loss 0.95425, dsc 0.04575\n",
      "Batch train [5] loss 0.95771, dsc 0.04229\n",
      "Batch train [6] loss 0.96842, dsc 0.03158\n",
      "Batch train [7] loss 0.95505, dsc 0.04495\n",
      "Batch train [8] loss 0.96197, dsc 0.03803\n",
      "Batch train [9] loss 0.96167, dsc 0.03833\n",
      "Batch train [10] loss 0.96164, dsc 0.03836\n",
      "Epoch [53] train done\n",
      "Batch eval [1] loss 0.95540, dsc 0.04460\n",
      "Batch eval [2] loss 0.96102, dsc 0.03898\n",
      "Batch eval [3] loss 0.95634, dsc 0.04366\n",
      "Batch eval [4] loss 0.96537, dsc 0.03463\n",
      "Batch eval [5] loss 0.95443, dsc 0.04557\n",
      "Epoch [53] valid done\n",
      "Epoch [53] T 213.66s, deltaT 4.01s, loss: train 0.96017, valid 0.95851, dsc: train 0.03983, valid 0.04149\n",
      "Batch train [1] loss 0.96146, dsc 0.03854\n",
      "Batch train [2] loss 0.95810, dsc 0.04190\n",
      "Batch train [3] loss 0.95955, dsc 0.04045\n",
      "Batch train [4] loss 0.95707, dsc 0.04293\n",
      "Batch train [5] loss 0.96204, dsc 0.03796\n",
      "Batch train [6] loss 0.96316, dsc 0.03684\n",
      "Batch train [7] loss 0.95809, dsc 0.04191\n",
      "Batch train [8] loss 0.96219, dsc 0.03781\n",
      "Batch train [9] loss 0.96225, dsc 0.03775\n",
      "Batch train [10] loss 0.95585, dsc 0.04415\n",
      "Epoch [54] train done\n",
      "Batch eval [1] loss 0.95397, dsc 0.04603\n",
      "Batch eval [2] loss 0.96038, dsc 0.03962\n",
      "Batch eval [3] loss 0.95566, dsc 0.04434\n",
      "Batch eval [4] loss 0.96463, dsc 0.03537\n",
      "Batch eval [5] loss 0.95391, dsc 0.04609\n",
      "Epoch [54] valid done\n",
      "Epoch [54] T 217.69s, deltaT 4.03s, loss: train 0.95997, valid 0.95771, dsc: train 0.04003, valid 0.04229\n",
      "Batch train [1] loss 0.96154, dsc 0.03846\n",
      "Batch train [2] loss 0.95994, dsc 0.04006\n",
      "Batch train [3] loss 0.95644, dsc 0.04356\n",
      "Batch train [4] loss 0.96371, dsc 0.03629\n",
      "Batch train [5] loss 0.96216, dsc 0.03784\n",
      "Batch train [6] loss 0.96711, dsc 0.03289\n",
      "Batch train [7] loss 0.95926, dsc 0.04074\n",
      "Batch train [8] loss 0.95512, dsc 0.04488\n",
      "Batch train [9] loss 0.95536, dsc 0.04464\n",
      "Batch train [10] loss 0.95742, dsc 0.04258\n",
      "Epoch [55] train done\n",
      "Batch eval [1] loss 0.95372, dsc 0.04628\n",
      "Batch eval [2] loss 0.96015, dsc 0.03985\n",
      "Batch eval [3] loss 0.95479, dsc 0.04521\n",
      "Batch eval [4] loss 0.96441, dsc 0.03559\n",
      "Batch eval [5] loss 0.95344, dsc 0.04656\n",
      "Epoch [55] valid done\n",
      "Epoch [55] T 221.70s, deltaT 4.01s, loss: train 0.95981, valid 0.95730, dsc: train 0.04019, valid 0.04270\n",
      "Batch train [1] loss 0.96299, dsc 0.03701\n",
      "Batch train [2] loss 0.96073, dsc 0.03927\n",
      "Batch train [3] loss 0.95955, dsc 0.04045\n",
      "Batch train [4] loss 0.95230, dsc 0.04770\n",
      "Batch train [5] loss 0.96187, dsc 0.03813\n",
      "Batch train [6] loss 0.95642, dsc 0.04358\n",
      "Batch train [7] loss 0.95951, dsc 0.04049\n",
      "Batch train [8] loss 0.96006, dsc 0.03994\n",
      "Batch train [9] loss 0.96249, dsc 0.03751\n",
      "Batch train [10] loss 0.96053, dsc 0.03947\n",
      "Epoch [56] train done\n",
      "Batch eval [1] loss 0.95373, dsc 0.04627\n",
      "Batch eval [2] loss 0.96016, dsc 0.03984\n",
      "Batch eval [3] loss 0.95476, dsc 0.04524\n",
      "Batch eval [4] loss 0.96460, dsc 0.03540\n",
      "Batch eval [5] loss 0.95339, dsc 0.04661\n",
      "Epoch [56] valid done\n",
      "Epoch [56] T 225.72s, deltaT 4.02s, loss: train 0.95965, valid 0.95733, dsc: train 0.04035, valid 0.04267\n",
      "Batch train [1] loss 0.95973, dsc 0.04027\n",
      "Batch train [2] loss 0.95579, dsc 0.04421\n",
      "Batch train [3] loss 0.95841, dsc 0.04159\n",
      "Batch train [4] loss 0.95683, dsc 0.04317\n",
      "Batch train [5] loss 0.95948, dsc 0.04052\n",
      "Batch train [6] loss 0.96066, dsc 0.03934\n",
      "Batch train [7] loss 0.96207, dsc 0.03793\n",
      "Batch train [8] loss 0.96181, dsc 0.03819\n",
      "Batch train [9] loss 0.95774, dsc 0.04226\n",
      "Batch train [10] loss 0.96193, dsc 0.03807\n",
      "Epoch [57] train done\n",
      "Batch eval [1] loss 0.95399, dsc 0.04601\n",
      "Batch eval [2] loss 0.96032, dsc 0.03968\n",
      "Batch eval [3] loss 0.95527, dsc 0.04473\n",
      "Batch eval [4] loss 0.96450, dsc 0.03550\n",
      "Batch eval [5] loss 0.95372, dsc 0.04628\n",
      "Epoch [57] valid done\n",
      "Epoch [57] T 229.71s, deltaT 3.99s, loss: train 0.95945, valid 0.95756, dsc: train 0.04055, valid 0.04244\n",
      "Batch train [1] loss 0.96038, dsc 0.03962\n",
      "Batch train [2] loss 0.95454, dsc 0.04546\n",
      "Batch train [3] loss 0.95917, dsc 0.04083\n",
      "Batch train [4] loss 0.96140, dsc 0.03860\n",
      "Batch train [5] loss 0.96030, dsc 0.03970\n",
      "Batch train [6] loss 0.95927, dsc 0.04073\n",
      "Batch train [7] loss 0.95631, dsc 0.04369\n",
      "Batch train [8] loss 0.96460, dsc 0.03540\n",
      "Batch train [9] loss 0.95869, dsc 0.04131\n",
      "Batch train [10] loss 0.95786, dsc 0.04214\n",
      "Epoch [58] train done\n",
      "Batch eval [1] loss 0.95546, dsc 0.04454\n",
      "Batch eval [2] loss 0.96082, dsc 0.03918\n",
      "Batch eval [3] loss 0.95695, dsc 0.04305\n",
      "Batch eval [4] loss 0.96493, dsc 0.03507\n",
      "Batch eval [5] loss 0.95470, dsc 0.04530\n",
      "Epoch [58] valid done\n",
      "Epoch [58] T 233.71s, deltaT 4.00s, loss: train 0.95925, valid 0.95857, dsc: train 0.04075, valid 0.04143\n",
      "Batch train [1] loss 0.95658, dsc 0.04342\n",
      "Batch train [2] loss 0.96046, dsc 0.03954\n",
      "Batch train [3] loss 0.96317, dsc 0.03683\n",
      "Batch train [4] loss 0.95277, dsc 0.04723\n",
      "Batch train [5] loss 0.96232, dsc 0.03768\n",
      "Batch train [6] loss 0.95874, dsc 0.04126\n",
      "Batch train [7] loss 0.95580, dsc 0.04420\n",
      "Batch train [8] loss 0.95717, dsc 0.04283\n",
      "Batch train [9] loss 0.95884, dsc 0.04116\n",
      "Batch train [10] loss 0.96494, dsc 0.03506\n",
      "Epoch [59] train done\n",
      "Batch eval [1] loss 0.95460, dsc 0.04540\n",
      "Batch eval [2] loss 0.96019, dsc 0.03981\n",
      "Batch eval [3] loss 0.95595, dsc 0.04405\n",
      "Batch eval [4] loss 0.96445, dsc 0.03555\n",
      "Batch eval [5] loss 0.95416, dsc 0.04584\n",
      "Epoch [59] valid done\n",
      "Epoch [59] T 237.89s, deltaT 4.18s, loss: train 0.95908, valid 0.95787, dsc: train 0.04092, valid 0.04213\n",
      "Batch train [1] loss 0.95580, dsc 0.04420\n",
      "Batch train [2] loss 0.95893, dsc 0.04107\n",
      "Batch train [3] loss 0.95865, dsc 0.04135\n",
      "Batch train [4] loss 0.96216, dsc 0.03784\n",
      "Batch train [5] loss 0.95382, dsc 0.04618\n",
      "Batch train [6] loss 0.96245, dsc 0.03755\n",
      "Batch train [7] loss 0.96009, dsc 0.03991\n",
      "Batch train [8] loss 0.96705, dsc 0.03295\n",
      "Batch train [9] loss 0.95425, dsc 0.04575\n",
      "Batch train [10] loss 0.95555, dsc 0.04445\n",
      "Epoch [60] train done\n",
      "Batch eval [1] loss 0.95322, dsc 0.04678\n",
      "Batch eval [2] loss 0.95949, dsc 0.04051\n",
      "Batch eval [3] loss 0.95476, dsc 0.04524\n",
      "Batch eval [4] loss 0.96395, dsc 0.03605\n",
      "Batch eval [5] loss 0.95288, dsc 0.04712\n",
      "Epoch [60] valid done\n",
      "Epoch [60] T 241.91s, deltaT 4.02s, loss: train 0.95887, valid 0.95686, dsc: train 0.04113, valid 0.04314\n",
      "Batch train [1] loss 0.96160, dsc 0.03840\n",
      "Batch train [2] loss 0.95425, dsc 0.04575\n",
      "Batch train [3] loss 0.96548, dsc 0.03452\n",
      "Batch train [4] loss 0.95508, dsc 0.04492\n",
      "Batch train [5] loss 0.96090, dsc 0.03910\n",
      "Batch train [6] loss 0.95901, dsc 0.04099\n",
      "Batch train [7] loss 0.95709, dsc 0.04291\n",
      "Batch train [8] loss 0.95824, dsc 0.04176\n",
      "Batch train [9] loss 0.95561, dsc 0.04439\n",
      "Batch train [10] loss 0.95930, dsc 0.04070\n",
      "Epoch [61] train done\n",
      "Batch eval [1] loss 0.95266, dsc 0.04734\n",
      "Batch eval [2] loss 0.95922, dsc 0.04078\n",
      "Batch eval [3] loss 0.95401, dsc 0.04599\n",
      "Batch eval [4] loss 0.96359, dsc 0.03641\n",
      "Batch eval [5] loss 0.95239, dsc 0.04761\n",
      "Epoch [61] valid done\n",
      "Epoch [61] T 246.01s, deltaT 4.09s, loss: train 0.95866, valid 0.95637, dsc: train 0.04134, valid 0.04363\n",
      "Batch train [1] loss 0.96338, dsc 0.03662\n",
      "Batch train [2] loss 0.96094, dsc 0.03906\n",
      "Batch train [3] loss 0.96380, dsc 0.03620\n",
      "Batch train [4] loss 0.95987, dsc 0.04013\n",
      "Batch train [5] loss 0.95280, dsc 0.04720\n",
      "Batch train [6] loss 0.96052, dsc 0.03948\n",
      "Batch train [7] loss 0.95373, dsc 0.04627\n",
      "Batch train [8] loss 0.96143, dsc 0.03857\n",
      "Batch train [9] loss 0.95132, dsc 0.04868\n",
      "Batch train [10] loss 0.95704, dsc 0.04296\n",
      "Epoch [62] train done\n",
      "Batch eval [1] loss 0.95248, dsc 0.04752\n",
      "Batch eval [2] loss 0.95908, dsc 0.04092\n",
      "Batch eval [3] loss 0.95396, dsc 0.04604\n",
      "Batch eval [4] loss 0.96344, dsc 0.03656\n",
      "Batch eval [5] loss 0.95229, dsc 0.04771\n",
      "Epoch [62] valid done\n",
      "Epoch [62] T 250.12s, deltaT 4.12s, loss: train 0.95848, valid 0.95625, dsc: train 0.04152, valid 0.04375\n",
      "Batch train [1] loss 0.95960, dsc 0.04040\n",
      "Batch train [2] loss 0.95343, dsc 0.04657\n",
      "Batch train [3] loss 0.96208, dsc 0.03792\n",
      "Batch train [4] loss 0.95860, dsc 0.04140\n",
      "Batch train [5] loss 0.96114, dsc 0.03886\n",
      "Batch train [6] loss 0.96421, dsc 0.03579\n",
      "Batch train [7] loss 0.95825, dsc 0.04175\n",
      "Batch train [8] loss 0.95247, dsc 0.04753\n",
      "Batch train [9] loss 0.95359, dsc 0.04641\n",
      "Batch train [10] loss 0.95924, dsc 0.04076\n",
      "Epoch [63] train done\n",
      "Batch eval [1] loss 0.95287, dsc 0.04713\n",
      "Batch eval [2] loss 0.95920, dsc 0.04080\n",
      "Batch eval [3] loss 0.95490, dsc 0.04510\n",
      "Batch eval [4] loss 0.96365, dsc 0.03635\n",
      "Batch eval [5] loss 0.95274, dsc 0.04726\n",
      "Epoch [63] valid done\n",
      "Epoch [63] T 254.17s, deltaT 4.04s, loss: train 0.95826, valid 0.95667, dsc: train 0.04174, valid 0.04333\n",
      "Batch train [1] loss 0.95619, dsc 0.04381\n",
      "Batch train [2] loss 0.96274, dsc 0.03726\n",
      "Batch train [3] loss 0.95969, dsc 0.04031\n",
      "Batch train [4] loss 0.96028, dsc 0.03972\n",
      "Batch train [5] loss 0.95386, dsc 0.04614\n",
      "Batch train [6] loss 0.96016, dsc 0.03985\n",
      "Batch train [7] loss 0.95360, dsc 0.04640\n",
      "Batch train [8] loss 0.96083, dsc 0.03917\n",
      "Batch train [9] loss 0.95840, dsc 0.04160\n",
      "Batch train [10] loss 0.95504, dsc 0.04496\n",
      "Epoch [64] train done\n",
      "Batch eval [1] loss 0.95224, dsc 0.04776\n",
      "Batch eval [2] loss 0.95861, dsc 0.04139\n",
      "Batch eval [3] loss 0.95360, dsc 0.04640\n",
      "Batch eval [4] loss 0.96308, dsc 0.03692\n",
      "Batch eval [5] loss 0.95170, dsc 0.04830\n",
      "Epoch [64] valid done\n",
      "Epoch [64] T 258.21s, deltaT 4.04s, loss: train 0.95808, valid 0.95585, dsc: train 0.04192, valid 0.04415\n",
      "Batch train [1] loss 0.95556, dsc 0.04444\n",
      "Batch train [2] loss 0.96289, dsc 0.03711\n",
      "Batch train [3] loss 0.95916, dsc 0.04084\n",
      "Batch train [4] loss 0.95773, dsc 0.04227\n",
      "Batch train [5] loss 0.95786, dsc 0.04214\n",
      "Batch train [6] loss 0.95359, dsc 0.04641\n",
      "Batch train [7] loss 0.95615, dsc 0.04385\n",
      "Batch train [8] loss 0.95344, dsc 0.04656\n",
      "Batch train [9] loss 0.96396, dsc 0.03604\n",
      "Batch train [10] loss 0.95807, dsc 0.04193\n",
      "Epoch [65] train done\n",
      "Batch eval [1] loss 0.95204, dsc 0.04796\n",
      "Batch eval [2] loss 0.95857, dsc 0.04143\n",
      "Batch eval [3] loss 0.95298, dsc 0.04702\n",
      "Batch eval [4] loss 0.96297, dsc 0.03703\n",
      "Batch eval [5] loss 0.95171, dsc 0.04829\n",
      "Epoch [65] valid done\n",
      "Epoch [65] T 262.40s, deltaT 4.19s, loss: train 0.95784, valid 0.95565, dsc: train 0.04216, valid 0.04435\n",
      "Batch train [1] loss 0.95447, dsc 0.04553\n",
      "Batch train [2] loss 0.96042, dsc 0.03958\n",
      "Batch train [3] loss 0.95571, dsc 0.04429\n",
      "Batch train [4] loss 0.96187, dsc 0.03813\n",
      "Batch train [5] loss 0.95685, dsc 0.04315\n",
      "Batch train [6] loss 0.95567, dsc 0.04433\n",
      "Batch train [7] loss 0.96027, dsc 0.03973\n",
      "Batch train [8] loss 0.95197, dsc 0.04803\n",
      "Batch train [9] loss 0.96096, dsc 0.03904\n",
      "Batch train [10] loss 0.95817, dsc 0.04183\n",
      "Epoch [66] train done\n",
      "Batch eval [1] loss 0.95209, dsc 0.04791\n",
      "Batch eval [2] loss 0.95865, dsc 0.04135\n",
      "Batch eval [3] loss 0.95399, dsc 0.04601\n",
      "Batch eval [4] loss 0.96315, dsc 0.03685\n",
      "Batch eval [5] loss 0.95212, dsc 0.04788\n",
      "Epoch [66] valid done\n",
      "Epoch [66] T 266.49s, deltaT 4.09s, loss: train 0.95763, valid 0.95600, dsc: train 0.04237, valid 0.04400\n",
      "Batch train [1] loss 0.95422, dsc 0.04578\n",
      "Batch train [2] loss 0.96203, dsc 0.03797\n",
      "Batch train [3] loss 0.95601, dsc 0.04399\n",
      "Batch train [4] loss 0.95451, dsc 0.04549\n",
      "Batch train [5] loss 0.96029, dsc 0.03971\n",
      "Batch train [6] loss 0.95931, dsc 0.04069\n",
      "Batch train [7] loss 0.95767, dsc 0.04233\n",
      "Batch train [8] loss 0.95842, dsc 0.04158\n",
      "Batch train [9] loss 0.95169, dsc 0.04831\n",
      "Batch train [10] loss 0.95974, dsc 0.04026\n",
      "Epoch [67] train done\n",
      "Batch eval [1] loss 0.95174, dsc 0.04826\n",
      "Batch eval [2] loss 0.95821, dsc 0.04179\n",
      "Batch eval [3] loss 0.95263, dsc 0.04737\n",
      "Batch eval [4] loss 0.96255, dsc 0.03745\n",
      "Batch eval [5] loss 0.95122, dsc 0.04878\n",
      "Epoch [67] valid done\n",
      "Epoch [67] T 270.52s, deltaT 4.03s, loss: train 0.95739, valid 0.95527, dsc: train 0.04261, valid 0.04473\n",
      "Batch train [1] loss 0.95607, dsc 0.04393\n",
      "Batch train [2] loss 0.95910, dsc 0.04090\n",
      "Batch train [3] loss 0.95234, dsc 0.04766\n",
      "Batch train [4] loss 0.95675, dsc 0.04325\n",
      "Batch train [5] loss 0.95966, dsc 0.04034\n",
      "Batch train [6] loss 0.95772, dsc 0.04228\n",
      "Batch train [7] loss 0.95900, dsc 0.04100\n",
      "Batch train [8] loss 0.95865, dsc 0.04135\n",
      "Batch train [9] loss 0.96258, dsc 0.03742\n",
      "Batch train [10] loss 0.94989, dsc 0.05011\n",
      "Epoch [68] train done\n",
      "Batch eval [1] loss 0.95181, dsc 0.04819\n",
      "Batch eval [2] loss 0.95825, dsc 0.04175\n",
      "Batch eval [3] loss 0.95345, dsc 0.04654\n",
      "Batch eval [4] loss 0.96281, dsc 0.03719\n",
      "Batch eval [5] loss 0.95163, dsc 0.04837\n",
      "Epoch [68] valid done\n",
      "Epoch [68] T 274.57s, deltaT 4.04s, loss: train 0.95718, valid 0.95559, dsc: train 0.04282, valid 0.04441\n",
      "Batch train [1] loss 0.95876, dsc 0.04124\n",
      "Batch train [2] loss 0.95750, dsc 0.04250\n",
      "Batch train [3] loss 0.95503, dsc 0.04497\n",
      "Batch train [4] loss 0.95481, dsc 0.04519\n",
      "Batch train [5] loss 0.95578, dsc 0.04422\n",
      "Batch train [6] loss 0.95705, dsc 0.04295\n",
      "Batch train [7] loss 0.96457, dsc 0.03543\n",
      "Batch train [8] loss 0.95917, dsc 0.04083\n",
      "Batch train [9] loss 0.95209, dsc 0.04791\n",
      "Batch train [10] loss 0.95477, dsc 0.04523\n",
      "Epoch [69] train done\n",
      "Batch eval [1] loss 0.95109, dsc 0.04891\n",
      "Batch eval [2] loss 0.95776, dsc 0.04224\n",
      "Batch eval [3] loss 0.95289, dsc 0.04711\n",
      "Batch eval [4] loss 0.96238, dsc 0.03762\n",
      "Batch eval [5] loss 0.95102, dsc 0.04898\n",
      "Epoch [69] valid done\n",
      "Epoch [69] T 278.63s, deltaT 4.06s, loss: train 0.95695, valid 0.95503, dsc: train 0.04305, valid 0.04497\n",
      "Batch train [1] loss 0.95689, dsc 0.04311\n",
      "Batch train [2] loss 0.95558, dsc 0.04442\n",
      "Batch train [3] loss 0.95473, dsc 0.04527\n",
      "Batch train [4] loss 0.95713, dsc 0.04287\n",
      "Batch train [5] loss 0.95809, dsc 0.04191\n",
      "Batch train [6] loss 0.95827, dsc 0.04173\n",
      "Batch train [7] loss 0.95663, dsc 0.04337\n",
      "Batch train [8] loss 0.96257, dsc 0.03743\n",
      "Batch train [9] loss 0.95836, dsc 0.04164\n",
      "Batch train [10] loss 0.94905, dsc 0.05095\n",
      "Epoch [70] train done\n",
      "Batch eval [1] loss 0.95118, dsc 0.04882\n",
      "Batch eval [2] loss 0.95762, dsc 0.04238\n",
      "Batch eval [3] loss 0.95253, dsc 0.04747\n",
      "Batch eval [4] loss 0.96251, dsc 0.03749\n",
      "Batch eval [5] loss 0.95061, dsc 0.04939\n",
      "Epoch [70] valid done\n",
      "Epoch [70] T 282.74s, deltaT 4.11s, loss: train 0.95673, valid 0.95489, dsc: train 0.04327, valid 0.04511\n",
      "Batch train [1] loss 0.95669, dsc 0.04331\n",
      "Batch train [2] loss 0.95554, dsc 0.04446\n",
      "Batch train [3] loss 0.95774, dsc 0.04226\n",
      "Batch train [4] loss 0.95792, dsc 0.04208\n",
      "Batch train [5] loss 0.95565, dsc 0.04435\n",
      "Batch train [6] loss 0.95844, dsc 0.04156\n",
      "Batch train [7] loss 0.95644, dsc 0.04356\n",
      "Batch train [8] loss 0.95587, dsc 0.04413\n",
      "Batch train [9] loss 0.95380, dsc 0.04620\n",
      "Batch train [10] loss 0.95665, dsc 0.04335\n",
      "Epoch [71] train done\n",
      "Batch eval [1] loss 0.95076, dsc 0.04924\n",
      "Batch eval [2] loss 0.95749, dsc 0.04251\n",
      "Batch eval [3] loss 0.95236, dsc 0.04764\n",
      "Batch eval [4] loss 0.96212, dsc 0.03788\n",
      "Batch eval [5] loss 0.95063, dsc 0.04937\n",
      "Epoch [71] valid done\n",
      "Epoch [71] T 286.87s, deltaT 4.13s, loss: train 0.95647, valid 0.95467, dsc: train 0.04353, valid 0.04533\n",
      "Batch train [1] loss 0.95910, dsc 0.04090\n",
      "Batch train [2] loss 0.95634, dsc 0.04366\n",
      "Batch train [3] loss 0.95909, dsc 0.04091\n",
      "Batch train [4] loss 0.95179, dsc 0.04821\n",
      "Batch train [5] loss 0.96110, dsc 0.03890\n",
      "Batch train [6] loss 0.95351, dsc 0.04649\n",
      "Batch train [7] loss 0.95539, dsc 0.04461\n",
      "Batch train [8] loss 0.95698, dsc 0.04302\n",
      "Batch train [9] loss 0.96169, dsc 0.03831\n",
      "Batch train [10] loss 0.94772, dsc 0.05228\n",
      "Epoch [72] train done\n",
      "Batch eval [1] loss 0.95019, dsc 0.04981\n",
      "Batch eval [2] loss 0.95698, dsc 0.04302\n",
      "Batch eval [3] loss 0.95172, dsc 0.04828\n",
      "Batch eval [4] loss 0.96143, dsc 0.03857\n",
      "Batch eval [5] loss 0.94993, dsc 0.05007\n",
      "Epoch [72] valid done\n",
      "Epoch [72] T 290.93s, deltaT 4.06s, loss: train 0.95627, valid 0.95405, dsc: train 0.04373, valid 0.04595\n",
      "Batch train [1] loss 0.95526, dsc 0.04474\n",
      "Batch train [2] loss 0.95265, dsc 0.04735\n",
      "Batch train [3] loss 0.95068, dsc 0.04932\n",
      "Batch train [4] loss 0.95774, dsc 0.04226\n",
      "Batch train [5] loss 0.95586, dsc 0.04414\n",
      "Batch train [6] loss 0.95512, dsc 0.04488\n",
      "Batch train [7] loss 0.95996, dsc 0.04004\n",
      "Batch train [8] loss 0.95766, dsc 0.04234\n",
      "Batch train [9] loss 0.95956, dsc 0.04044\n",
      "Batch train [10] loss 0.95547, dsc 0.04453\n",
      "Epoch [73] train done\n",
      "Batch eval [1] loss 0.95128, dsc 0.04872\n",
      "Batch eval [2] loss 0.95741, dsc 0.04259\n",
      "Batch eval [3] loss 0.95239, dsc 0.04761\n",
      "Batch eval [4] loss 0.96216, dsc 0.03784\n",
      "Batch eval [5] loss 0.95033, dsc 0.04967\n",
      "Epoch [73] valid done\n",
      "Epoch [73] T 294.96s, deltaT 4.03s, loss: train 0.95600, valid 0.95471, dsc: train 0.04400, valid 0.04529\n",
      "Batch train [1] loss 0.96109, dsc 0.03891\n",
      "Batch train [2] loss 0.94774, dsc 0.05226\n",
      "Batch train [3] loss 0.95071, dsc 0.04929\n",
      "Batch train [4] loss 0.95703, dsc 0.04297\n",
      "Batch train [5] loss 0.95350, dsc 0.04650\n",
      "Batch train [6] loss 0.96363, dsc 0.03637\n",
      "Batch train [7] loss 0.95619, dsc 0.04381\n",
      "Batch train [8] loss 0.96117, dsc 0.03883\n",
      "Batch train [9] loss 0.95901, dsc 0.04099\n",
      "Batch train [10] loss 0.94776, dsc 0.05224\n",
      "Epoch [74] train done\n",
      "Batch eval [1] loss 0.95057, dsc 0.04943\n",
      "Batch eval [2] loss 0.95706, dsc 0.04294\n",
      "Batch eval [3] loss 0.95228, dsc 0.04772\n",
      "Batch eval [4] loss 0.96188, dsc 0.03812\n",
      "Batch eval [5] loss 0.95012, dsc 0.04988\n",
      "Epoch [74] valid done\n",
      "Epoch [74] T 299.12s, deltaT 4.16s, loss: train 0.95578, valid 0.95438, dsc: train 0.04422, valid 0.04562\n",
      "Batch train [1] loss 0.94862, dsc 0.05138\n",
      "Batch train [2] loss 0.95839, dsc 0.04161\n",
      "Batch train [3] loss 0.95452, dsc 0.04548\n",
      "Batch train [4] loss 0.95080, dsc 0.04920\n",
      "Batch train [5] loss 0.96092, dsc 0.03908\n",
      "Batch train [6] loss 0.95663, dsc 0.04337\n",
      "Batch train [7] loss 0.95709, dsc 0.04291\n",
      "Batch train [8] loss 0.95389, dsc 0.04611\n",
      "Batch train [9] loss 0.95352, dsc 0.04648\n",
      "Batch train [10] loss 0.96099, dsc 0.03901\n",
      "Epoch [75] train done\n",
      "Batch eval [1] loss 0.95020, dsc 0.04980\n",
      "Batch eval [2] loss 0.95661, dsc 0.04339\n",
      "Batch eval [3] loss 0.95092, dsc 0.04908\n",
      "Batch eval [4] loss 0.96118, dsc 0.03882\n",
      "Batch eval [5] loss 0.94947, dsc 0.05053\n",
      "Epoch [75] valid done\n",
      "Epoch [75] T 303.31s, deltaT 4.20s, loss: train 0.95554, valid 0.95368, dsc: train 0.04446, valid 0.04632\n",
      "Batch train [1] loss 0.96125, dsc 0.03875\n",
      "Batch train [2] loss 0.95252, dsc 0.04748\n",
      "Batch train [3] loss 0.95485, dsc 0.04515\n",
      "Batch train [4] loss 0.96125, dsc 0.03875\n",
      "Batch train [5] loss 0.95614, dsc 0.04386\n",
      "Batch train [6] loss 0.95400, dsc 0.04600\n",
      "Batch train [7] loss 0.95406, dsc 0.04594\n",
      "Batch train [8] loss 0.95049, dsc 0.04951\n",
      "Batch train [9] loss 0.95226, dsc 0.04774\n",
      "Batch train [10] loss 0.95598, dsc 0.04402\n",
      "Epoch [76] train done\n",
      "Batch eval [1] loss 0.94920, dsc 0.05080\n",
      "Batch eval [2] loss 0.95617, dsc 0.04383\n",
      "Batch eval [3] loss 0.95010, dsc 0.04990\n",
      "Batch eval [4] loss 0.96071, dsc 0.03929\n",
      "Batch eval [5] loss 0.94886, dsc 0.05114\n",
      "Epoch [76] valid done\n",
      "Epoch [76] T 307.33s, deltaT 4.01s, loss: train 0.95528, valid 0.95301, dsc: train 0.04472, valid 0.04699\n",
      "Batch train [1] loss 0.95990, dsc 0.04010\n",
      "Batch train [2] loss 0.95280, dsc 0.04720\n",
      "Batch train [3] loss 0.95764, dsc 0.04236\n",
      "Batch train [4] loss 0.95640, dsc 0.04360\n",
      "Batch train [5] loss 0.95637, dsc 0.04363\n",
      "Batch train [6] loss 0.95438, dsc 0.04562\n",
      "Batch train [7] loss 0.94814, dsc 0.05186\n",
      "Batch train [8] loss 0.95522, dsc 0.04478\n",
      "Batch train [9] loss 0.95563, dsc 0.04437\n",
      "Batch train [10] loss 0.95391, dsc 0.04609\n",
      "Epoch [77] train done\n",
      "Batch eval [1] loss 0.95079, dsc 0.04921\n",
      "Batch eval [2] loss 0.95679, dsc 0.04321\n",
      "Batch eval [3] loss 0.95233, dsc 0.04767\n",
      "Batch eval [4] loss 0.96189, dsc 0.03811\n",
      "Batch eval [5] loss 0.94982, dsc 0.05018\n",
      "Epoch [77] valid done\n",
      "Epoch [77] T 311.32s, deltaT 3.99s, loss: train 0.95504, valid 0.95432, dsc: train 0.04496, valid 0.04568\n",
      "Batch train [1] loss 0.95133, dsc 0.04867\n",
      "Batch train [2] loss 0.96019, dsc 0.03981\n",
      "Batch train [3] loss 0.95536, dsc 0.04464\n",
      "Batch train [4] loss 0.95203, dsc 0.04797\n",
      "Batch train [5] loss 0.95659, dsc 0.04341\n",
      "Batch train [6] loss 0.95817, dsc 0.04183\n",
      "Batch train [7] loss 0.95157, dsc 0.04843\n",
      "Batch train [8] loss 0.95737, dsc 0.04263\n",
      "Batch train [9] loss 0.95494, dsc 0.04506\n",
      "Batch train [10] loss 0.95010, dsc 0.04990\n",
      "Epoch [78] train done\n",
      "Batch eval [1] loss 0.94890, dsc 0.05110\n",
      "Batch eval [2] loss 0.95569, dsc 0.04431\n",
      "Batch eval [3] loss 0.94973, dsc 0.05027\n",
      "Batch eval [4] loss 0.96066, dsc 0.03934\n",
      "Batch eval [5] loss 0.94859, dsc 0.05141\n",
      "Epoch [78] valid done\n",
      "Epoch [78] T 315.31s, deltaT 3.99s, loss: train 0.95477, valid 0.95271, dsc: train 0.04523, valid 0.04729\n",
      "Batch train [1] loss 0.95633, dsc 0.04367\n",
      "Batch train [2] loss 0.95646, dsc 0.04354\n",
      "Batch train [3] loss 0.95162, dsc 0.04838\n",
      "Batch train [4] loss 0.95214, dsc 0.04786\n",
      "Batch train [5] loss 0.95729, dsc 0.04271\n",
      "Batch train [6] loss 0.96044, dsc 0.03956\n",
      "Batch train [7] loss 0.95053, dsc 0.04947\n",
      "Batch train [8] loss 0.95923, dsc 0.04077\n",
      "Batch train [9] loss 0.95270, dsc 0.04730\n",
      "Batch train [10] loss 0.94825, dsc 0.05175\n",
      "Epoch [79] train done\n",
      "Batch eval [1] loss 0.94878, dsc 0.05122\n",
      "Batch eval [2] loss 0.95548, dsc 0.04452\n",
      "Batch eval [3] loss 0.95029, dsc 0.04971\n",
      "Batch eval [4] loss 0.96056, dsc 0.03944\n",
      "Batch eval [5] loss 0.94843, dsc 0.05157\n",
      "Epoch [79] valid done\n",
      "Epoch [79] T 319.32s, deltaT 4.00s, loss: train 0.95450, valid 0.95271, dsc: train 0.04550, valid 0.04729\n",
      "Batch train [1] loss 0.95409, dsc 0.04591\n",
      "Batch train [2] loss 0.96242, dsc 0.03758\n",
      "Batch train [3] loss 0.95884, dsc 0.04116\n",
      "Batch train [4] loss 0.94871, dsc 0.05129\n",
      "Batch train [5] loss 0.94855, dsc 0.05145\n",
      "Batch train [6] loss 0.95395, dsc 0.04605\n",
      "Batch train [7] loss 0.95641, dsc 0.04359\n",
      "Batch train [8] loss 0.95174, dsc 0.04826\n",
      "Batch train [9] loss 0.95775, dsc 0.04225\n",
      "Batch train [10] loss 0.94986, dsc 0.05014\n",
      "Epoch [80] train done\n",
      "Batch eval [1] loss 0.94876, dsc 0.05124\n",
      "Batch eval [2] loss 0.95552, dsc 0.04448\n",
      "Batch eval [3] loss 0.94961, dsc 0.05039\n",
      "Batch eval [4] loss 0.96040, dsc 0.03960\n",
      "Batch eval [5] loss 0.94814, dsc 0.05186\n",
      "Epoch [80] valid done\n",
      "Epoch [80] T 323.25s, deltaT 3.93s, loss: train 0.95423, valid 0.95249, dsc: train 0.04577, valid 0.04751\n",
      "Batch train [1] loss 0.95078, dsc 0.04922\n",
      "Batch train [2] loss 0.95655, dsc 0.04345\n",
      "Batch train [3] loss 0.95534, dsc 0.04466\n",
      "Batch train [4] loss 0.94961, dsc 0.05039\n",
      "Batch train [5] loss 0.95349, dsc 0.04651\n",
      "Batch train [6] loss 0.95650, dsc 0.04350\n",
      "Batch train [7] loss 0.95947, dsc 0.04053\n",
      "Batch train [8] loss 0.95579, dsc 0.04421\n",
      "Batch train [9] loss 0.95128, dsc 0.04872\n",
      "Batch train [10] loss 0.95072, dsc 0.04928\n",
      "Epoch [81] train done\n",
      "Batch eval [1] loss 0.94812, dsc 0.05188\n",
      "Batch eval [2] loss 0.95500, dsc 0.04500\n",
      "Batch eval [3] loss 0.94941, dsc 0.05059\n",
      "Batch eval [4] loss 0.96012, dsc 0.03988\n",
      "Batch eval [5] loss 0.94798, dsc 0.05202\n",
      "Epoch [81] valid done\n",
      "Epoch [81] T 327.28s, deltaT 4.03s, loss: train 0.95395, valid 0.95213, dsc: train 0.04605, valid 0.04787\n",
      "Batch train [1] loss 0.95106, dsc 0.04894\n",
      "Batch train [2] loss 0.95203, dsc 0.04797\n",
      "Batch train [3] loss 0.95753, dsc 0.04247\n",
      "Batch train [4] loss 0.95182, dsc 0.04818\n",
      "Batch train [5] loss 0.95522, dsc 0.04478\n",
      "Batch train [6] loss 0.95088, dsc 0.04912\n",
      "Batch train [7] loss 0.95196, dsc 0.04804\n",
      "Batch train [8] loss 0.95378, dsc 0.04622\n",
      "Batch train [9] loss 0.95517, dsc 0.04483\n",
      "Batch train [10] loss 0.95716, dsc 0.04284\n",
      "Epoch [82] train done\n",
      "Batch eval [1] loss 0.94786, dsc 0.05214\n",
      "Batch eval [2] loss 0.95473, dsc 0.04527\n",
      "Batch eval [3] loss 0.94931, dsc 0.05069\n",
      "Batch eval [4] loss 0.95941, dsc 0.04059\n",
      "Batch eval [5] loss 0.94738, dsc 0.05262\n",
      "Epoch [82] valid done\n",
      "Epoch [82] T 331.33s, deltaT 4.05s, loss: train 0.95366, valid 0.95174, dsc: train 0.04634, valid 0.04826\n",
      "Batch train [1] loss 0.95402, dsc 0.04598\n",
      "Batch train [2] loss 0.95658, dsc 0.04342\n",
      "Batch train [3] loss 0.94881, dsc 0.05119\n",
      "Batch train [4] loss 0.95702, dsc 0.04298\n",
      "Batch train [5] loss 0.95307, dsc 0.04693\n",
      "Batch train [6] loss 0.95561, dsc 0.04439\n",
      "Batch train [7] loss 0.95384, dsc 0.04616\n",
      "Batch train [8] loss 0.95529, dsc 0.04471\n",
      "Batch train [9] loss 0.95474, dsc 0.04526\n",
      "Batch train [10] loss 0.94502, dsc 0.05498\n",
      "Epoch [83] train done\n",
      "Batch eval [1] loss 0.94728, dsc 0.05272\n",
      "Batch eval [2] loss 0.95422, dsc 0.04578\n",
      "Batch eval [3] loss 0.94862, dsc 0.05138\n",
      "Batch eval [4] loss 0.95899, dsc 0.04101\n",
      "Batch eval [5] loss 0.94693, dsc 0.05307\n",
      "Epoch [83] valid done\n",
      "Epoch [83] T 335.36s, deltaT 4.04s, loss: train 0.95340, valid 0.95121, dsc: train 0.04660, valid 0.04879\n",
      "Batch train [1] loss 0.95649, dsc 0.04351\n",
      "Batch train [2] loss 0.95140, dsc 0.04860\n",
      "Batch train [3] loss 0.94949, dsc 0.05051\n",
      "Batch train [4] loss 0.94486, dsc 0.05514\n",
      "Batch train [5] loss 0.95330, dsc 0.04670\n",
      "Batch train [6] loss 0.95854, dsc 0.04146\n",
      "Batch train [7] loss 0.95777, dsc 0.04223\n",
      "Batch train [8] loss 0.95890, dsc 0.04110\n",
      "Batch train [9] loss 0.95033, dsc 0.04967\n",
      "Batch train [10] loss 0.95036, dsc 0.04964\n",
      "Epoch [84] train done\n",
      "Batch eval [1] loss 0.94827, dsc 0.05173\n",
      "Batch eval [2] loss 0.95425, dsc 0.04575\n",
      "Batch eval [3] loss 0.94922, dsc 0.05078\n",
      "Batch eval [4] loss 0.95977, dsc 0.04023\n",
      "Batch eval [5] loss 0.94712, dsc 0.05288\n",
      "Epoch [84] valid done\n",
      "Epoch [84] T 339.40s, deltaT 4.04s, loss: train 0.95314, valid 0.95173, dsc: train 0.04686, valid 0.04827\n",
      "Batch train [1] loss 0.95626, dsc 0.04374\n",
      "Batch train [2] loss 0.95658, dsc 0.04342\n",
      "Batch train [3] loss 0.95912, dsc 0.04088\n",
      "Batch train [4] loss 0.95389, dsc 0.04611\n",
      "Batch train [5] loss 0.94629, dsc 0.05371\n",
      "Batch train [6] loss 0.95204, dsc 0.04796\n",
      "Batch train [7] loss 0.94969, dsc 0.05031\n",
      "Batch train [8] loss 0.94931, dsc 0.05069\n",
      "Batch train [9] loss 0.94892, dsc 0.05108\n",
      "Batch train [10] loss 0.95668, dsc 0.04332\n",
      "Epoch [85] train done\n",
      "Batch eval [1] loss 0.94762, dsc 0.05238\n",
      "Batch eval [2] loss 0.95409, dsc 0.04591\n",
      "Batch eval [3] loss 0.94877, dsc 0.05123\n",
      "Batch eval [4] loss 0.95904, dsc 0.04096\n",
      "Batch eval [5] loss 0.94696, dsc 0.05304\n",
      "Epoch [85] valid done\n",
      "Epoch [85] T 343.42s, deltaT 4.01s, loss: train 0.95288, valid 0.95130, dsc: train 0.04712, valid 0.04870\n",
      "Batch train [1] loss 0.95018, dsc 0.04982\n",
      "Batch train [2] loss 0.95221, dsc 0.04779\n",
      "Batch train [3] loss 0.94392, dsc 0.05608\n",
      "Batch train [4] loss 0.95581, dsc 0.04419\n",
      "Batch train [5] loss 0.95258, dsc 0.04742\n",
      "Batch train [6] loss 0.95363, dsc 0.04637\n",
      "Batch train [7] loss 0.95358, dsc 0.04642\n",
      "Batch train [8] loss 0.95417, dsc 0.04583\n",
      "Batch train [9] loss 0.95317, dsc 0.04683\n",
      "Batch train [10] loss 0.95648, dsc 0.04352\n",
      "Epoch [86] train done\n",
      "Batch eval [1] loss 0.94714, dsc 0.05286\n",
      "Batch eval [2] loss 0.95362, dsc 0.04638\n",
      "Batch eval [3] loss 0.94808, dsc 0.05192\n",
      "Batch eval [4] loss 0.95867, dsc 0.04133\n",
      "Batch eval [5] loss 0.94620, dsc 0.05380\n",
      "Epoch [86] valid done\n",
      "Epoch [86] T 347.41s, deltaT 3.99s, loss: train 0.95257, valid 0.95074, dsc: train 0.04743, valid 0.04926\n",
      "Batch train [1] loss 0.94895, dsc 0.05105\n",
      "Batch train [2] loss 0.95075, dsc 0.04925\n",
      "Batch train [3] loss 0.95083, dsc 0.04917\n",
      "Batch train [4] loss 0.95779, dsc 0.04221\n",
      "Batch train [5] loss 0.95191, dsc 0.04809\n",
      "Batch train [6] loss 0.95660, dsc 0.04340\n",
      "Batch train [7] loss 0.94951, dsc 0.05049\n",
      "Batch train [8] loss 0.95154, dsc 0.04846\n",
      "Batch train [9] loss 0.94937, dsc 0.05063\n",
      "Batch train [10] loss 0.95519, dsc 0.04481\n",
      "Epoch [87] train done\n",
      "Batch eval [1] loss 0.94627, dsc 0.05373\n",
      "Batch eval [2] loss 0.95329, dsc 0.04671\n",
      "Batch eval [3] loss 0.94679, dsc 0.05321\n",
      "Batch eval [4] loss 0.95812, dsc 0.04188\n",
      "Batch eval [5] loss 0.94566, dsc 0.05434\n",
      "Epoch [87] valid done\n",
      "Epoch [87] T 351.44s, deltaT 4.03s, loss: train 0.95224, valid 0.95003, dsc: train 0.04776, valid 0.04997\n",
      "Batch train [1] loss 0.95791, dsc 0.04209\n",
      "Batch train [2] loss 0.95743, dsc 0.04257\n",
      "Batch train [3] loss 0.95134, dsc 0.04866\n",
      "Batch train [4] loss 0.95362, dsc 0.04638\n",
      "Batch train [5] loss 0.94784, dsc 0.05216\n",
      "Batch train [6] loss 0.95864, dsc 0.04136\n",
      "Batch train [7] loss 0.94662, dsc 0.05338\n",
      "Batch train [8] loss 0.94922, dsc 0.05078\n",
      "Batch train [9] loss 0.95122, dsc 0.04878\n",
      "Batch train [10] loss 0.94569, dsc 0.05431\n",
      "Epoch [88] train done\n",
      "Batch eval [1] loss 0.94585, dsc 0.05415\n",
      "Batch eval [2] loss 0.95279, dsc 0.04721\n",
      "Batch eval [3] loss 0.94724, dsc 0.05276\n",
      "Batch eval [4] loss 0.95810, dsc 0.04190\n",
      "Batch eval [5] loss 0.94551, dsc 0.05449\n",
      "Epoch [88] valid done\n",
      "Epoch [88] T 355.44s, deltaT 4.00s, loss: train 0.95195, valid 0.94990, dsc: train 0.04805, valid 0.05010\n",
      "Batch train [1] loss 0.95085, dsc 0.04915\n",
      "Batch train [2] loss 0.95051, dsc 0.04949\n",
      "Batch train [3] loss 0.95134, dsc 0.04866\n",
      "Batch train [4] loss 0.95367, dsc 0.04633\n",
      "Batch train [5] loss 0.94692, dsc 0.05308\n",
      "Batch train [6] loss 0.94826, dsc 0.05174\n",
      "Batch train [7] loss 0.95199, dsc 0.04801\n",
      "Batch train [8] loss 0.94973, dsc 0.05027\n",
      "Batch train [9] loss 0.95799, dsc 0.04201\n",
      "Batch train [10] loss 0.95507, dsc 0.04493\n",
      "Epoch [89] train done\n",
      "Batch eval [1] loss 0.94639, dsc 0.05361\n",
      "Batch eval [2] loss 0.95266, dsc 0.04734\n",
      "Batch eval [3] loss 0.94741, dsc 0.05259\n",
      "Batch eval [4] loss 0.95787, dsc 0.04213\n",
      "Batch eval [5] loss 0.94544, dsc 0.05456\n",
      "Epoch [89] valid done\n",
      "Epoch [89] T 359.41s, deltaT 3.97s, loss: train 0.95163, valid 0.94995, dsc: train 0.04837, valid 0.05005\n",
      "Batch train [1] loss 0.95049, dsc 0.04951\n",
      "Batch train [2] loss 0.94890, dsc 0.05110\n",
      "Batch train [3] loss 0.94937, dsc 0.05063\n",
      "Batch train [4] loss 0.95640, dsc 0.04360\n",
      "Batch train [5] loss 0.94715, dsc 0.05285\n",
      "Batch train [6] loss 0.95299, dsc 0.04701\n",
      "Batch train [7] loss 0.95236, dsc 0.04764\n",
      "Batch train [8] loss 0.95259, dsc 0.04741\n",
      "Batch train [9] loss 0.95482, dsc 0.04518\n",
      "Batch train [10] loss 0.94858, dsc 0.05142\n",
      "Epoch [90] train done\n",
      "Batch eval [1] loss 0.94437, dsc 0.05563\n",
      "Batch eval [2] loss 0.95158, dsc 0.04842\n",
      "Batch eval [3] loss 0.94546, dsc 0.05454\n",
      "Batch eval [4] loss 0.95696, dsc 0.04304\n",
      "Batch eval [5] loss 0.94397, dsc 0.05603\n",
      "Epoch [90] valid done\n",
      "Epoch [90] T 363.43s, deltaT 4.02s, loss: train 0.95137, valid 0.94847, dsc: train 0.04863, valid 0.05153\n",
      "Batch train [1] loss 0.94729, dsc 0.05271\n",
      "Batch train [2] loss 0.94965, dsc 0.05035\n",
      "Batch train [3] loss 0.95157, dsc 0.04843\n",
      "Batch train [4] loss 0.95541, dsc 0.04459\n",
      "Batch train [5] loss 0.94584, dsc 0.05416\n",
      "Batch train [6] loss 0.95560, dsc 0.04440\n",
      "Batch train [7] loss 0.95297, dsc 0.04703\n",
      "Batch train [8] loss 0.94720, dsc 0.05280\n",
      "Batch train [9] loss 0.95059, dsc 0.04941\n",
      "Batch train [10] loss 0.95414, dsc 0.04586\n",
      "Epoch [91] train done\n",
      "Batch eval [1] loss 0.94450, dsc 0.05550\n",
      "Batch eval [2] loss 0.95158, dsc 0.04842\n",
      "Batch eval [3] loss 0.94505, dsc 0.05495\n",
      "Batch eval [4] loss 0.95683, dsc 0.04317\n",
      "Batch eval [5] loss 0.94369, dsc 0.05631\n",
      "Epoch [91] valid done\n",
      "Epoch [91] T 367.43s, deltaT 4.00s, loss: train 0.95102, valid 0.94833, dsc: train 0.04898, valid 0.05167\n",
      "Batch train [1] loss 0.96031, dsc 0.03969\n",
      "Batch train [2] loss 0.94250, dsc 0.05750\n",
      "Batch train [3] loss 0.95402, dsc 0.04598\n",
      "Batch train [4] loss 0.94671, dsc 0.05329\n",
      "Batch train [5] loss 0.94711, dsc 0.05289\n",
      "Batch train [6] loss 0.95301, dsc 0.04699\n",
      "Batch train [7] loss 0.95300, dsc 0.04700\n",
      "Batch train [8] loss 0.94770, dsc 0.05230\n",
      "Batch train [9] loss 0.95021, dsc 0.04979\n",
      "Batch train [10] loss 0.95254, dsc 0.04746\n",
      "Epoch [92] train done\n",
      "Batch eval [1] loss 0.94398, dsc 0.05602\n",
      "Batch eval [2] loss 0.95142, dsc 0.04858\n",
      "Batch eval [3] loss 0.94510, dsc 0.05490\n",
      "Batch eval [4] loss 0.95643, dsc 0.04357\n",
      "Batch eval [5] loss 0.94346, dsc 0.05654\n",
      "Epoch [92] valid done\n",
      "Epoch [92] T 371.45s, deltaT 4.02s, loss: train 0.95071, valid 0.94808, dsc: train 0.04929, valid 0.05192\n",
      "Batch train [1] loss 0.94433, dsc 0.05567\n",
      "Batch train [2] loss 0.95410, dsc 0.04590\n",
      "Batch train [3] loss 0.94685, dsc 0.05315\n",
      "Batch train [4] loss 0.95316, dsc 0.04684\n",
      "Batch train [5] loss 0.94576, dsc 0.05424\n",
      "Batch train [6] loss 0.95154, dsc 0.04846\n",
      "Batch train [7] loss 0.94504, dsc 0.05496\n",
      "Batch train [8] loss 0.95722, dsc 0.04278\n",
      "Batch train [9] loss 0.94987, dsc 0.05013\n",
      "Batch train [10] loss 0.95601, dsc 0.04399\n",
      "Epoch [93] train done\n",
      "Batch eval [1] loss 0.94452, dsc 0.05548\n",
      "Batch eval [2] loss 0.95124, dsc 0.04876\n",
      "Batch eval [3] loss 0.94507, dsc 0.05493\n",
      "Batch eval [4] loss 0.95631, dsc 0.04369\n",
      "Batch eval [5] loss 0.94336, dsc 0.05664\n",
      "Epoch [93] valid done\n",
      "Epoch [93] T 375.45s, deltaT 4.00s, loss: train 0.95039, valid 0.94810, dsc: train 0.04961, valid 0.05190\n",
      "Batch train [1] loss 0.94603, dsc 0.05397\n",
      "Batch train [2] loss 0.95331, dsc 0.04669\n",
      "Batch train [3] loss 0.95123, dsc 0.04877\n",
      "Batch train [4] loss 0.95297, dsc 0.04703\n",
      "Batch train [5] loss 0.95125, dsc 0.04875\n",
      "Batch train [6] loss 0.94540, dsc 0.05460\n",
      "Batch train [7] loss 0.94555, dsc 0.05445\n",
      "Batch train [8] loss 0.95377, dsc 0.04623\n",
      "Batch train [9] loss 0.94942, dsc 0.05058\n",
      "Batch train [10] loss 0.95175, dsc 0.04825\n",
      "Epoch [94] train done\n",
      "Batch eval [1] loss 0.94346, dsc 0.05654\n",
      "Batch eval [2] loss 0.95063, dsc 0.04937\n",
      "Batch eval [3] loss 0.94429, dsc 0.05571\n",
      "Batch eval [4] loss 0.95590, dsc 0.04410\n",
      "Batch eval [5] loss 0.94284, dsc 0.05716\n",
      "Epoch [94] valid done\n",
      "Epoch [94] T 379.49s, deltaT 4.04s, loss: train 0.95007, valid 0.94742, dsc: train 0.04993, valid 0.05258\n",
      "Batch train [1] loss 0.94795, dsc 0.05205\n",
      "Batch train [2] loss 0.95368, dsc 0.04632\n",
      "Batch train [3] loss 0.94496, dsc 0.05504\n",
      "Batch train [4] loss 0.94425, dsc 0.05575\n",
      "Batch train [5] loss 0.95254, dsc 0.04746\n",
      "Batch train [6] loss 0.95257, dsc 0.04743\n",
      "Batch train [7] loss 0.95452, dsc 0.04548\n",
      "Batch train [8] loss 0.94501, dsc 0.05499\n",
      "Batch train [9] loss 0.95474, dsc 0.04526\n",
      "Batch train [10] loss 0.94696, dsc 0.05304\n",
      "Epoch [95] train done\n",
      "Batch eval [1] loss 0.94418, dsc 0.05582\n",
      "Batch eval [2] loss 0.95052, dsc 0.04948\n",
      "Batch eval [3] loss 0.94436, dsc 0.05564\n",
      "Batch eval [4] loss 0.95592, dsc 0.04408\n",
      "Batch eval [5] loss 0.94267, dsc 0.05733\n",
      "Epoch [95] valid done\n",
      "Epoch [95] T 383.50s, deltaT 4.00s, loss: train 0.94972, valid 0.94753, dsc: train 0.05028, valid 0.05247\n",
      "Batch train [1] loss 0.94602, dsc 0.05398\n",
      "Batch train [2] loss 0.94751, dsc 0.05249\n",
      "Batch train [3] loss 0.94321, dsc 0.05679\n",
      "Batch train [4] loss 0.95423, dsc 0.04577\n",
      "Batch train [5] loss 0.95079, dsc 0.04921\n",
      "Batch train [6] loss 0.94999, dsc 0.05001\n",
      "Batch train [7] loss 0.95575, dsc 0.04425\n",
      "Batch train [8] loss 0.94718, dsc 0.05282\n",
      "Batch train [9] loss 0.94773, dsc 0.05227\n",
      "Batch train [10] loss 0.95108, dsc 0.04892\n",
      "Epoch [96] train done\n",
      "Batch eval [1] loss 0.94232, dsc 0.05768\n",
      "Batch eval [2] loss 0.94968, dsc 0.05032\n",
      "Batch eval [3] loss 0.94279, dsc 0.05721\n",
      "Batch eval [4] loss 0.95495, dsc 0.04505\n",
      "Batch eval [5] loss 0.94190, dsc 0.05810\n",
      "Epoch [96] valid done\n",
      "Epoch [96] T 387.53s, deltaT 4.03s, loss: train 0.94935, valid 0.94633, dsc: train 0.05065, valid 0.05367\n",
      "Batch train [1] loss 0.95229, dsc 0.04771\n",
      "Batch train [2] loss 0.95476, dsc 0.04524\n",
      "Batch train [3] loss 0.95026, dsc 0.04974\n",
      "Batch train [4] loss 0.95207, dsc 0.04793\n",
      "Batch train [5] loss 0.94565, dsc 0.05435\n",
      "Batch train [6] loss 0.94527, dsc 0.05473\n",
      "Batch train [7] loss 0.95050, dsc 0.04950\n",
      "Batch train [8] loss 0.94740, dsc 0.05260\n",
      "Batch train [9] loss 0.94788, dsc 0.05212\n",
      "Batch train [10] loss 0.94384, dsc 0.05616\n",
      "Epoch [97] train done\n",
      "Batch eval [1] loss 0.94243, dsc 0.05757\n",
      "Batch eval [2] loss 0.94971, dsc 0.05029\n",
      "Batch eval [3] loss 0.94300, dsc 0.05700\n",
      "Batch eval [4] loss 0.95516, dsc 0.04484\n",
      "Batch eval [5] loss 0.94155, dsc 0.05845\n",
      "Epoch [97] valid done\n",
      "Epoch [97] T 391.58s, deltaT 4.05s, loss: train 0.94899, valid 0.94637, dsc: train 0.05101, valid 0.05363\n",
      "Batch train [1] loss 0.94061, dsc 0.05939\n",
      "Batch train [2] loss 0.94951, dsc 0.05049\n",
      "Batch train [3] loss 0.94849, dsc 0.05151\n",
      "Batch train [4] loss 0.95019, dsc 0.04981\n",
      "Batch train [5] loss 0.95450, dsc 0.04550\n",
      "Batch train [6] loss 0.95114, dsc 0.04886\n",
      "Batch train [7] loss 0.94165, dsc 0.05835\n",
      "Batch train [8] loss 0.95081, dsc 0.04919\n",
      "Batch train [9] loss 0.95262, dsc 0.04738\n",
      "Batch train [10] loss 0.94732, dsc 0.05268\n",
      "Epoch [98] train done\n",
      "Batch eval [1] loss 0.94187, dsc 0.05813\n",
      "Batch eval [2] loss 0.94938, dsc 0.05062\n",
      "Batch eval [3] loss 0.94283, dsc 0.05717\n",
      "Batch eval [4] loss 0.95458, dsc 0.04542\n",
      "Batch eval [5] loss 0.94128, dsc 0.05872\n",
      "Epoch [98] valid done\n",
      "Epoch [98] T 395.57s, deltaT 3.99s, loss: train 0.94868, valid 0.94599, dsc: train 0.05132, valid 0.05401\n",
      "Batch train [1] loss 0.94876, dsc 0.05124\n",
      "Batch train [2] loss 0.95170, dsc 0.04830\n",
      "Batch train [3] loss 0.94064, dsc 0.05936\n",
      "Batch train [4] loss 0.94828, dsc 0.05172\n",
      "Batch train [5] loss 0.95380, dsc 0.04620\n",
      "Batch train [6] loss 0.94761, dsc 0.05239\n",
      "Batch train [7] loss 0.95253, dsc 0.04747\n",
      "Batch train [8] loss 0.94682, dsc 0.05318\n",
      "Batch train [9] loss 0.94322, dsc 0.05678\n",
      "Batch train [10] loss 0.94991, dsc 0.05009\n",
      "Epoch [99] train done\n",
      "Batch eval [1] loss 0.94239, dsc 0.05761\n",
      "Batch eval [2] loss 0.94942, dsc 0.05058\n",
      "Batch eval [3] loss 0.94306, dsc 0.05694\n",
      "Batch eval [4] loss 0.95453, dsc 0.04547\n",
      "Batch eval [5] loss 0.94144, dsc 0.05856\n",
      "Epoch [99] valid done\n",
      "Epoch [99] T 399.52s, deltaT 3.95s, loss: train 0.94833, valid 0.94617, dsc: train 0.05167, valid 0.05383\n",
      "Batch train [1] loss 0.95094, dsc 0.04906\n",
      "Batch train [2] loss 0.95623, dsc 0.04377\n",
      "Batch train [3] loss 0.94811, dsc 0.05189\n",
      "Batch train [4] loss 0.95152, dsc 0.04848\n",
      "Batch train [5] loss 0.94515, dsc 0.05485\n",
      "Batch train [6] loss 0.93983, dsc 0.06017\n",
      "Batch train [7] loss 0.95304, dsc 0.04696\n",
      "Batch train [8] loss 0.94847, dsc 0.05153\n",
      "Batch train [9] loss 0.94602, dsc 0.05398\n",
      "Batch train [10] loss 0.94027, dsc 0.05973\n",
      "Epoch [100] train done\n",
      "Batch eval [1] loss 0.94191, dsc 0.05809\n",
      "Batch eval [2] loss 0.94895, dsc 0.05105\n",
      "Batch eval [3] loss 0.94246, dsc 0.05754\n",
      "Batch eval [4] loss 0.95447, dsc 0.04553\n",
      "Batch eval [5] loss 0.94075, dsc 0.05925\n",
      "Epoch [100] valid done\n",
      "Epoch [100] T 403.53s, deltaT 4.01s, loss: train 0.94796, valid 0.94571, dsc: train 0.05204, valid 0.05429\n",
      "Batch train [1] loss 0.95526, dsc 0.04474\n",
      "Batch train [2] loss 0.95038, dsc 0.04962\n",
      "Batch train [3] loss 0.94956, dsc 0.05044\n",
      "Batch train [4] loss 0.94051, dsc 0.05949\n",
      "Batch train [5] loss 0.94982, dsc 0.05018\n",
      "Batch train [6] loss 0.94363, dsc 0.05637\n",
      "Batch train [7] loss 0.94985, dsc 0.05015\n",
      "Batch train [8] loss 0.94641, dsc 0.05359\n",
      "Batch train [9] loss 0.94277, dsc 0.05723\n",
      "Batch train [10] loss 0.94747, dsc 0.05253\n",
      "Epoch [101] train done\n",
      "Batch eval [1] loss 0.94181, dsc 0.05819\n",
      "Batch eval [2] loss 0.94878, dsc 0.05122\n",
      "Batch eval [3] loss 0.94271, dsc 0.05729\n",
      "Batch eval [4] loss 0.95377, dsc 0.04623\n",
      "Batch eval [5] loss 0.94064, dsc 0.05936\n",
      "Epoch [101] valid done\n",
      "Epoch [101] T 407.57s, deltaT 4.05s, loss: train 0.94757, valid 0.94554, dsc: train 0.05243, valid 0.05446\n",
      "Batch train [1] loss 0.94768, dsc 0.05232\n",
      "Batch train [2] loss 0.94688, dsc 0.05312\n",
      "Batch train [3] loss 0.94386, dsc 0.05614\n",
      "Batch train [4] loss 0.94646, dsc 0.05354\n",
      "Batch train [5] loss 0.94041, dsc 0.05959\n",
      "Batch train [6] loss 0.94992, dsc 0.05008\n",
      "Batch train [7] loss 0.94756, dsc 0.05244\n",
      "Batch train [8] loss 0.95017, dsc 0.04983\n",
      "Batch train [9] loss 0.94983, dsc 0.05017\n",
      "Batch train [10] loss 0.94919, dsc 0.05081\n",
      "Epoch [102] train done\n",
      "Batch eval [1] loss 0.94111, dsc 0.05889\n",
      "Batch eval [2] loss 0.94798, dsc 0.05202\n",
      "Batch eval [3] loss 0.94161, dsc 0.05839\n",
      "Batch eval [4] loss 0.95369, dsc 0.04631\n",
      "Batch eval [5] loss 0.93979, dsc 0.06021\n",
      "Epoch [102] valid done\n",
      "Epoch [102] T 411.60s, deltaT 4.02s, loss: train 0.94720, valid 0.94484, dsc: train 0.05280, valid 0.05516\n",
      "Batch train [1] loss 0.95154, dsc 0.04846\n",
      "Batch train [2] loss 0.94761, dsc 0.05239\n",
      "Batch train [3] loss 0.95035, dsc 0.04965\n",
      "Batch train [4] loss 0.94287, dsc 0.05713\n",
      "Batch train [5] loss 0.94219, dsc 0.05781\n",
      "Batch train [6] loss 0.94901, dsc 0.05099\n",
      "Batch train [7] loss 0.94630, dsc 0.05370\n",
      "Batch train [8] loss 0.95142, dsc 0.04858\n",
      "Batch train [9] loss 0.94742, dsc 0.05258\n",
      "Batch train [10] loss 0.93950, dsc 0.06050\n",
      "Epoch [103] train done\n",
      "Batch eval [1] loss 0.93987, dsc 0.06013\n",
      "Batch eval [2] loss 0.94709, dsc 0.05291\n",
      "Batch eval [3] loss 0.94065, dsc 0.05935\n",
      "Batch eval [4] loss 0.95276, dsc 0.04724\n",
      "Batch eval [5] loss 0.93870, dsc 0.06130\n",
      "Epoch [103] valid done\n",
      "Epoch [103] T 415.55s, deltaT 3.96s, loss: train 0.94682, valid 0.94381, dsc: train 0.05318, valid 0.05619\n",
      "Batch train [1] loss 0.94803, dsc 0.05197\n",
      "Batch train [2] loss 0.95117, dsc 0.04883\n",
      "Batch train [3] loss 0.94568, dsc 0.05432\n",
      "Batch train [4] loss 0.94664, dsc 0.05336\n",
      "Batch train [5] loss 0.94965, dsc 0.05035\n",
      "Batch train [6] loss 0.93665, dsc 0.06335\n",
      "Batch train [7] loss 0.94653, dsc 0.05347\n",
      "Batch train [8] loss 0.94596, dsc 0.05404\n",
      "Batch train [9] loss 0.95143, dsc 0.04857\n",
      "Batch train [10] loss 0.94265, dsc 0.05735\n",
      "Epoch [104] train done\n",
      "Batch eval [1] loss 0.93955, dsc 0.06045\n",
      "Batch eval [2] loss 0.94727, dsc 0.05273\n",
      "Batch eval [3] loss 0.93998, dsc 0.06002\n",
      "Batch eval [4] loss 0.95292, dsc 0.04708\n",
      "Batch eval [5] loss 0.93895, dsc 0.06105\n",
      "Epoch [104] valid done\n",
      "Epoch [104] T 419.56s, deltaT 4.01s, loss: train 0.94644, valid 0.94373, dsc: train 0.05356, valid 0.05627\n",
      "Batch train [1] loss 0.94974, dsc 0.05026\n",
      "Batch train [2] loss 0.95358, dsc 0.04642\n",
      "Batch train [3] loss 0.94853, dsc 0.05147\n",
      "Batch train [4] loss 0.93598, dsc 0.06402\n",
      "Batch train [5] loss 0.94208, dsc 0.05792\n",
      "Batch train [6] loss 0.94200, dsc 0.05800\n",
      "Batch train [7] loss 0.94715, dsc 0.05285\n",
      "Batch train [8] loss 0.94944, dsc 0.05056\n",
      "Batch train [9] loss 0.94566, dsc 0.05434\n",
      "Batch train [10] loss 0.94622, dsc 0.05378\n",
      "Epoch [105] train done\n",
      "Batch eval [1] loss 0.93938, dsc 0.06062\n",
      "Batch eval [2] loss 0.94606, dsc 0.05394\n",
      "Batch eval [3] loss 0.93933, dsc 0.06067\n",
      "Batch eval [4] loss 0.95192, dsc 0.04808\n",
      "Batch eval [5] loss 0.93780, dsc 0.06220\n",
      "Epoch [105] valid done\n",
      "Epoch [105] T 423.48s, deltaT 3.92s, loss: train 0.94604, valid 0.94290, dsc: train 0.05396, valid 0.05710\n",
      "Batch train [1] loss 0.95148, dsc 0.04852\n",
      "Batch train [2] loss 0.94554, dsc 0.05446\n",
      "Batch train [3] loss 0.93825, dsc 0.06175\n",
      "Batch train [4] loss 0.95172, dsc 0.04828\n",
      "Batch train [5] loss 0.94285, dsc 0.05715\n",
      "Batch train [6] loss 0.93846, dsc 0.06154\n",
      "Batch train [7] loss 0.94765, dsc 0.05235\n",
      "Batch train [8] loss 0.94305, dsc 0.05695\n",
      "Batch train [9] loss 0.95038, dsc 0.04962\n",
      "Batch train [10] loss 0.94685, dsc 0.05315\n",
      "Epoch [106] train done\n",
      "Batch eval [1] loss 0.93936, dsc 0.06064\n",
      "Batch eval [2] loss 0.94626, dsc 0.05374\n",
      "Batch eval [3] loss 0.93952, dsc 0.06048\n",
      "Batch eval [4] loss 0.95186, dsc 0.04814\n",
      "Batch eval [5] loss 0.93777, dsc 0.06223\n",
      "Epoch [106] valid done\n",
      "Epoch [106] T 427.49s, deltaT 4.00s, loss: train 0.94562, valid 0.94295, dsc: train 0.05438, valid 0.05705\n",
      "Batch train [1] loss 0.94141, dsc 0.05859\n",
      "Batch train [2] loss 0.94923, dsc 0.05077\n",
      "Batch train [3] loss 0.94518, dsc 0.05482\n",
      "Batch train [4] loss 0.94816, dsc 0.05184\n",
      "Batch train [5] loss 0.94483, dsc 0.05517\n",
      "Batch train [6] loss 0.95207, dsc 0.04793\n",
      "Batch train [7] loss 0.93848, dsc 0.06152\n",
      "Batch train [8] loss 0.94757, dsc 0.05243\n",
      "Batch train [9] loss 0.94629, dsc 0.05371\n",
      "Batch train [10] loss 0.93906, dsc 0.06094\n",
      "Epoch [107] train done\n",
      "Batch eval [1] loss 0.93789, dsc 0.06211\n",
      "Batch eval [2] loss 0.94573, dsc 0.05427\n",
      "Batch eval [3] loss 0.93846, dsc 0.06154\n",
      "Batch eval [4] loss 0.95133, dsc 0.04867\n",
      "Batch eval [5] loss 0.93701, dsc 0.06299\n",
      "Epoch [107] valid done\n",
      "Epoch [107] T 431.47s, deltaT 3.98s, loss: train 0.94523, valid 0.94208, dsc: train 0.05477, valid 0.05792\n",
      "Batch train [1] loss 0.94467, dsc 0.05533\n",
      "Batch train [2] loss 0.95014, dsc 0.04986\n",
      "Batch train [3] loss 0.94206, dsc 0.05794\n",
      "Batch train [4] loss 0.93702, dsc 0.06298\n",
      "Batch train [5] loss 0.94711, dsc 0.05289\n",
      "Batch train [6] loss 0.94903, dsc 0.05097\n",
      "Batch train [7] loss 0.94660, dsc 0.05340\n",
      "Batch train [8] loss 0.94626, dsc 0.05374\n",
      "Batch train [9] loss 0.94223, dsc 0.05777\n",
      "Batch train [10] loss 0.94309, dsc 0.05691\n",
      "Epoch [108] train done\n",
      "Batch eval [1] loss 0.93778, dsc 0.06222\n",
      "Batch eval [2] loss 0.94521, dsc 0.05479\n",
      "Batch eval [3] loss 0.93794, dsc 0.06206\n",
      "Batch eval [4] loss 0.95080, dsc 0.04920\n",
      "Batch eval [5] loss 0.93707, dsc 0.06293\n",
      "Epoch [108] valid done\n",
      "Epoch [108] T 435.49s, deltaT 4.02s, loss: train 0.94482, valid 0.94176, dsc: train 0.05518, valid 0.05824\n",
      "Batch train [1] loss 0.94642, dsc 0.05358\n",
      "Batch train [2] loss 0.94054, dsc 0.05946\n",
      "Batch train [3] loss 0.93841, dsc 0.06159\n",
      "Batch train [4] loss 0.94922, dsc 0.05078\n",
      "Batch train [5] loss 0.94796, dsc 0.05204\n",
      "Batch train [6] loss 0.94668, dsc 0.05332\n",
      "Batch train [7] loss 0.94420, dsc 0.05580\n",
      "Batch train [8] loss 0.94799, dsc 0.05201\n",
      "Batch train [9] loss 0.94495, dsc 0.05505\n",
      "Batch train [10] loss 0.93737, dsc 0.06263\n",
      "Epoch [109] train done\n",
      "Batch eval [1] loss 0.93781, dsc 0.06219\n",
      "Batch eval [2] loss 0.94506, dsc 0.05494\n",
      "Batch eval [3] loss 0.93773, dsc 0.06227\n",
      "Batch eval [4] loss 0.95126, dsc 0.04874\n",
      "Batch eval [5] loss 0.93616, dsc 0.06384\n",
      "Epoch [109] valid done\n",
      "Epoch [109] T 439.43s, deltaT 3.94s, loss: train 0.94438, valid 0.94160, dsc: train 0.05562, valid 0.05840\n",
      "Batch train [1] loss 0.94217, dsc 0.05783\n",
      "Batch train [2] loss 0.94460, dsc 0.05540\n",
      "Batch train [3] loss 0.94416, dsc 0.05584\n",
      "Batch train [4] loss 0.93952, dsc 0.06048\n",
      "Batch train [5] loss 0.94871, dsc 0.05129\n",
      "Batch train [6] loss 0.94596, dsc 0.05404\n",
      "Batch train [7] loss 0.94007, dsc 0.05993\n",
      "Batch train [8] loss 0.94227, dsc 0.05773\n",
      "Batch train [9] loss 0.94225, dsc 0.05775\n",
      "Batch train [10] loss 0.94964, dsc 0.05036\n",
      "Epoch [110] train done\n",
      "Batch eval [1] loss 0.93787, dsc 0.06213\n",
      "Batch eval [2] loss 0.94532, dsc 0.05468\n",
      "Batch eval [3] loss 0.93794, dsc 0.06206\n",
      "Batch eval [4] loss 0.95100, dsc 0.04900\n",
      "Batch eval [5] loss 0.93654, dsc 0.06346\n",
      "Epoch [110] valid done\n",
      "Epoch [110] T 443.43s, deltaT 4.00s, loss: train 0.94394, valid 0.94173, dsc: train 0.05606, valid 0.05827\n",
      "Batch train [1] loss 0.93874, dsc 0.06126\n",
      "Batch train [2] loss 0.94669, dsc 0.05331\n",
      "Batch train [3] loss 0.95087, dsc 0.04913\n",
      "Batch train [4] loss 0.94950, dsc 0.05050\n",
      "Batch train [5] loss 0.94536, dsc 0.05464\n",
      "Batch train [6] loss 0.94166, dsc 0.05834\n",
      "Batch train [7] loss 0.94400, dsc 0.05600\n",
      "Batch train [8] loss 0.94203, dsc 0.05797\n",
      "Batch train [9] loss 0.94466, dsc 0.05534\n",
      "Batch train [10] loss 0.93175, dsc 0.06825\n",
      "Epoch [111] train done\n",
      "Batch eval [1] loss 0.93585, dsc 0.06415\n",
      "Batch eval [2] loss 0.94420, dsc 0.05580\n",
      "Batch eval [3] loss 0.93707, dsc 0.06293\n",
      "Batch eval [4] loss 0.94956, dsc 0.05044\n",
      "Batch eval [5] loss 0.93488, dsc 0.06512\n",
      "Epoch [111] valid done\n",
      "Epoch [111] T 447.47s, deltaT 4.04s, loss: train 0.94353, valid 0.94031, dsc: train 0.05647, valid 0.05969\n",
      "Batch train [1] loss 0.94955, dsc 0.05045\n",
      "Batch train [2] loss 0.93950, dsc 0.06050\n",
      "Batch train [3] loss 0.94793, dsc 0.05207\n",
      "Batch train [4] loss 0.94161, dsc 0.05839\n",
      "Batch train [5] loss 0.94256, dsc 0.05744\n",
      "Batch train [6] loss 0.94690, dsc 0.05310\n",
      "Batch train [7] loss 0.93556, dsc 0.06444\n",
      "Batch train [8] loss 0.94493, dsc 0.05507\n",
      "Batch train [9] loss 0.94026, dsc 0.05974\n",
      "Batch train [10] loss 0.94200, dsc 0.05800\n",
      "Epoch [112] train done\n",
      "Batch eval [1] loss 0.93631, dsc 0.06369\n",
      "Batch eval [2] loss 0.94419, dsc 0.05581\n",
      "Batch eval [3] loss 0.93671, dsc 0.06329\n",
      "Batch eval [4] loss 0.95018, dsc 0.04982\n",
      "Batch eval [5] loss 0.93502, dsc 0.06498\n",
      "Epoch [112] valid done\n",
      "Epoch [112] T 451.52s, deltaT 4.05s, loss: train 0.94308, valid 0.94048, dsc: train 0.05692, valid 0.05952\n",
      "Batch train [1] loss 0.93680, dsc 0.06320\n",
      "Batch train [2] loss 0.94117, dsc 0.05883\n",
      "Batch train [3] loss 0.95064, dsc 0.04936\n",
      "Batch train [4] loss 0.94035, dsc 0.05965\n",
      "Batch train [5] loss 0.93662, dsc 0.06338\n",
      "Batch train [6] loss 0.94341, dsc 0.05659\n",
      "Batch train [7] loss 0.94729, dsc 0.05271\n",
      "Batch train [8] loss 0.93775, dsc 0.06225\n",
      "Batch train [9] loss 0.94642, dsc 0.05358\n",
      "Batch train [10] loss 0.94586, dsc 0.05414\n",
      "Epoch [113] train done\n",
      "Batch eval [1] loss 0.93590, dsc 0.06410\n",
      "Batch eval [2] loss 0.94334, dsc 0.05666\n",
      "Batch eval [3] loss 0.93625, dsc 0.06375\n",
      "Batch eval [4] loss 0.94956, dsc 0.05044\n",
      "Batch eval [5] loss 0.93453, dsc 0.06547\n",
      "Epoch [113] valid done\n",
      "Epoch [113] T 455.49s, deltaT 3.97s, loss: train 0.94263, valid 0.93992, dsc: train 0.05737, valid 0.06008\n",
      "Batch train [1] loss 0.94245, dsc 0.05755\n",
      "Batch train [2] loss 0.94955, dsc 0.05045\n",
      "Batch train [3] loss 0.94470, dsc 0.05530\n",
      "Batch train [4] loss 0.93427, dsc 0.06573\n",
      "Batch train [5] loss 0.95043, dsc 0.04957\n",
      "Batch train [6] loss 0.93935, dsc 0.06065\n",
      "Batch train [7] loss 0.94560, dsc 0.05440\n",
      "Batch train [8] loss 0.93361, dsc 0.06639\n",
      "Batch train [9] loss 0.93785, dsc 0.06215\n",
      "Batch train [10] loss 0.94411, dsc 0.05589\n",
      "Epoch [114] train done\n",
      "Batch eval [1] loss 0.93483, dsc 0.06517\n",
      "Batch eval [2] loss 0.94315, dsc 0.05685\n",
      "Batch eval [3] loss 0.93597, dsc 0.06403\n",
      "Batch eval [4] loss 0.94903, dsc 0.05097\n",
      "Batch eval [5] loss 0.93451, dsc 0.06549\n",
      "Epoch [114] valid done\n",
      "Epoch [114] T 459.47s, deltaT 3.97s, loss: train 0.94219, valid 0.93950, dsc: train 0.05781, valid 0.06050\n",
      "Batch train [1] loss 0.94218, dsc 0.05782\n",
      "Batch train [2] loss 0.94319, dsc 0.05681\n",
      "Batch train [3] loss 0.94664, dsc 0.05336\n",
      "Batch train [4] loss 0.92884, dsc 0.07116\n",
      "Batch train [5] loss 0.94618, dsc 0.05382\n",
      "Batch train [6] loss 0.94511, dsc 0.05489\n",
      "Batch train [7] loss 0.93870, dsc 0.06130\n",
      "Batch train [8] loss 0.94025, dsc 0.05975\n",
      "Batch train [9] loss 0.94353, dsc 0.05647\n",
      "Batch train [10] loss 0.94272, dsc 0.05728\n",
      "Epoch [115] train done\n",
      "Batch eval [1] loss 0.93475, dsc 0.06525\n",
      "Batch eval [2] loss 0.94272, dsc 0.05728\n",
      "Batch eval [3] loss 0.93562, dsc 0.06438\n",
      "Batch eval [4] loss 0.94932, dsc 0.05068\n",
      "Batch eval [5] loss 0.93400, dsc 0.06600\n",
      "Epoch [115] valid done\n",
      "Epoch [115] T 463.46s, deltaT 3.99s, loss: train 0.94173, valid 0.93928, dsc: train 0.05827, valid 0.06072\n",
      "Batch train [1] loss 0.93983, dsc 0.06017\n",
      "Batch train [2] loss 0.94560, dsc 0.05440\n",
      "Batch train [3] loss 0.94302, dsc 0.05698\n",
      "Batch train [4] loss 0.94493, dsc 0.05507\n",
      "Batch train [5] loss 0.93884, dsc 0.06116\n",
      "Batch train [6] loss 0.94142, dsc 0.05858\n",
      "Batch train [7] loss 0.93770, dsc 0.06230\n",
      "Batch train [8] loss 0.94273, dsc 0.05727\n",
      "Batch train [9] loss 0.94072, dsc 0.05928\n",
      "Batch train [10] loss 0.93747, dsc 0.06253\n",
      "Epoch [116] train done\n",
      "Batch eval [1] loss 0.93345, dsc 0.06655\n",
      "Batch eval [2] loss 0.94165, dsc 0.05835\n",
      "Batch eval [3] loss 0.93375, dsc 0.06625\n",
      "Batch eval [4] loss 0.94798, dsc 0.05202\n",
      "Batch eval [5] loss 0.93236, dsc 0.06764\n",
      "Epoch [116] valid done\n",
      "Epoch [116] T 467.41s, deltaT 3.95s, loss: train 0.94123, valid 0.93784, dsc: train 0.05877, valid 0.06216\n",
      "Batch train [1] loss 0.93135, dsc 0.06865\n",
      "Batch train [2] loss 0.94532, dsc 0.05468\n",
      "Batch train [3] loss 0.94208, dsc 0.05792\n",
      "Batch train [4] loss 0.94412, dsc 0.05588\n",
      "Batch train [5] loss 0.93447, dsc 0.06553\n",
      "Batch train [6] loss 0.93955, dsc 0.06045\n",
      "Batch train [7] loss 0.94063, dsc 0.05937\n",
      "Batch train [8] loss 0.94610, dsc 0.05390\n",
      "Batch train [9] loss 0.94172, dsc 0.05828\n",
      "Batch train [10] loss 0.94230, dsc 0.05770\n",
      "Epoch [117] train done\n",
      "Batch eval [1] loss 0.93340, dsc 0.06660\n",
      "Batch eval [2] loss 0.94172, dsc 0.05828\n",
      "Batch eval [3] loss 0.93446, dsc 0.06554\n",
      "Batch eval [4] loss 0.94797, dsc 0.05203\n",
      "Batch eval [5] loss 0.93257, dsc 0.06743\n",
      "Epoch [117] valid done\n",
      "Epoch [117] T 471.38s, deltaT 3.97s, loss: train 0.94076, valid 0.93802, dsc: train 0.05924, valid 0.06198\n",
      "Batch train [1] loss 0.94298, dsc 0.05702\n",
      "Batch train [2] loss 0.93374, dsc 0.06626\n",
      "Batch train [3] loss 0.95010, dsc 0.04990\n",
      "Batch train [4] loss 0.94052, dsc 0.05948\n",
      "Batch train [5] loss 0.94486, dsc 0.05514\n",
      "Batch train [6] loss 0.93701, dsc 0.06299\n",
      "Batch train [7] loss 0.94404, dsc 0.05596\n",
      "Batch train [8] loss 0.93385, dsc 0.06615\n",
      "Batch train [9] loss 0.93436, dsc 0.06564\n",
      "Batch train [10] loss 0.94192, dsc 0.05808\n",
      "Epoch [118] train done\n",
      "Batch eval [1] loss 0.93220, dsc 0.06780\n",
      "Batch eval [2] loss 0.94041, dsc 0.05959\n",
      "Batch eval [3] loss 0.93417, dsc 0.06583\n",
      "Batch eval [4] loss 0.94726, dsc 0.05274\n",
      "Batch eval [5] loss 0.93140, dsc 0.06860\n",
      "Epoch [118] valid done\n",
      "Epoch [118] T 475.43s, deltaT 4.05s, loss: train 0.94034, valid 0.93709, dsc: train 0.05966, valid 0.06291\n",
      "Batch train [1] loss 0.95222, dsc 0.04778\n",
      "Batch train [2] loss 0.94454, dsc 0.05546\n",
      "Batch train [3] loss 0.93831, dsc 0.06169\n",
      "Batch train [4] loss 0.94742, dsc 0.05258\n",
      "Batch train [5] loss 0.93162, dsc 0.06838\n",
      "Batch train [6] loss 0.93613, dsc 0.06387\n",
      "Batch train [7] loss 0.93644, dsc 0.06355\n",
      "Batch train [8] loss 0.93510, dsc 0.06490\n",
      "Batch train [9] loss 0.93248, dsc 0.06752\n",
      "Batch train [10] loss 0.94448, dsc 0.05552\n",
      "Epoch [119] train done\n",
      "Batch eval [1] loss 0.93395, dsc 0.06605\n",
      "Batch eval [2] loss 0.94139, dsc 0.05861\n",
      "Batch eval [3] loss 0.93467, dsc 0.06533\n",
      "Batch eval [4] loss 0.94751, dsc 0.05249\n",
      "Batch eval [5] loss 0.93244, dsc 0.06756\n",
      "Epoch [119] valid done\n",
      "Epoch [119] T 479.42s, deltaT 3.98s, loss: train 0.93987, valid 0.93799, dsc: train 0.06013, valid 0.06201\n",
      "Batch train [1] loss 0.92490, dsc 0.07510\n",
      "Batch train [2] loss 0.94715, dsc 0.05285\n",
      "Batch train [3] loss 0.93382, dsc 0.06618\n",
      "Batch train [4] loss 0.94929, dsc 0.05071\n",
      "Batch train [5] loss 0.94459, dsc 0.05541\n",
      "Batch train [6] loss 0.93817, dsc 0.06183\n",
      "Batch train [7] loss 0.93739, dsc 0.06261\n",
      "Batch train [8] loss 0.93701, dsc 0.06299\n",
      "Batch train [9] loss 0.94333, dsc 0.05667\n",
      "Batch train [10] loss 0.93780, dsc 0.06220\n",
      "Epoch [120] train done\n",
      "Batch eval [1] loss 0.93186, dsc 0.06814\n",
      "Batch eval [2] loss 0.94010, dsc 0.05990\n",
      "Batch eval [3] loss 0.93283, dsc 0.06717\n",
      "Batch eval [4] loss 0.94676, dsc 0.05324\n",
      "Batch eval [5] loss 0.93095, dsc 0.06905\n",
      "Epoch [120] valid done\n",
      "Epoch [120] T 483.42s, deltaT 4.00s, loss: train 0.93934, valid 0.93650, dsc: train 0.06066, valid 0.06350\n",
      "Batch train [1] loss 0.93878, dsc 0.06122\n",
      "Batch train [2] loss 0.93438, dsc 0.06562\n",
      "Batch train [3] loss 0.93307, dsc 0.06693\n",
      "Batch train [4] loss 0.94261, dsc 0.05739\n",
      "Batch train [5] loss 0.94030, dsc 0.05970\n",
      "Batch train [6] loss 0.94093, dsc 0.05907\n",
      "Batch train [7] loss 0.93512, dsc 0.06488\n",
      "Batch train [8] loss 0.94818, dsc 0.05182\n",
      "Batch train [9] loss 0.93495, dsc 0.06505\n",
      "Batch train [10] loss 0.93956, dsc 0.06044\n",
      "Epoch [121] train done\n",
      "Batch eval [1] loss 0.93044, dsc 0.06956\n",
      "Batch eval [2] loss 0.93874, dsc 0.06126\n",
      "Batch eval [3] loss 0.93037, dsc 0.06963\n",
      "Batch eval [4] loss 0.94573, dsc 0.05427\n",
      "Batch eval [5] loss 0.92925, dsc 0.07075\n",
      "Epoch [121] valid done\n",
      "Epoch [121] T 487.48s, deltaT 4.06s, loss: train 0.93879, valid 0.93491, dsc: train 0.06121, valid 0.06509\n",
      "Batch train [1] loss 0.93188, dsc 0.06812\n",
      "Batch train [2] loss 0.93844, dsc 0.06156\n",
      "Batch train [3] loss 0.93571, dsc 0.06429\n",
      "Batch train [4] loss 0.93938, dsc 0.06062\n",
      "Batch train [5] loss 0.94911, dsc 0.05089\n",
      "Batch train [6] loss 0.94278, dsc 0.05722\n",
      "Batch train [7] loss 0.93241, dsc 0.06759\n",
      "Batch train [8] loss 0.94348, dsc 0.05652\n",
      "Batch train [9] loss 0.92531, dsc 0.07469\n",
      "Batch train [10] loss 0.94386, dsc 0.05614\n",
      "Epoch [122] train done\n",
      "Batch eval [1] loss 0.92998, dsc 0.07002\n",
      "Batch eval [2] loss 0.93813, dsc 0.06187\n",
      "Batch eval [3] loss 0.93089, dsc 0.06911\n",
      "Batch eval [4] loss 0.94457, dsc 0.05543\n",
      "Batch eval [5] loss 0.92890, dsc 0.07110\n",
      "Epoch [122] valid done\n",
      "Epoch [122] T 491.43s, deltaT 3.96s, loss: train 0.93824, valid 0.93449, dsc: train 0.06176, valid 0.06551\n",
      "Batch train [1] loss 0.94492, dsc 0.05508\n",
      "Batch train [2] loss 0.93285, dsc 0.06715\n",
      "Batch train [3] loss 0.94279, dsc 0.05721\n",
      "Batch train [4] loss 0.93809, dsc 0.06191\n",
      "Batch train [5] loss 0.93636, dsc 0.06364\n",
      "Batch train [6] loss 0.94089, dsc 0.05911\n",
      "Batch train [7] loss 0.93400, dsc 0.06600\n",
      "Batch train [8] loss 0.93480, dsc 0.06520\n",
      "Batch train [9] loss 0.93254, dsc 0.06746\n",
      "Batch train [10] loss 0.93907, dsc 0.06093\n",
      "Epoch [123] train done\n",
      "Batch eval [1] loss 0.93014, dsc 0.06986\n",
      "Batch eval [2] loss 0.93912, dsc 0.06088\n",
      "Batch eval [3] loss 0.93042, dsc 0.06958\n",
      "Batch eval [4] loss 0.94536, dsc 0.05464\n",
      "Batch eval [5] loss 0.92883, dsc 0.07117\n",
      "Epoch [123] valid done\n",
      "Epoch [123] T 495.41s, deltaT 3.98s, loss: train 0.93763, valid 0.93477, dsc: train 0.06237, valid 0.06523\n",
      "Batch train [1] loss 0.92837, dsc 0.07163\n",
      "Batch train [2] loss 0.93714, dsc 0.06286\n",
      "Batch train [3] loss 0.94187, dsc 0.05813\n",
      "Batch train [4] loss 0.92929, dsc 0.07071\n",
      "Batch train [5] loss 0.93162, dsc 0.06838\n",
      "Batch train [6] loss 0.93617, dsc 0.06383\n",
      "Batch train [7] loss 0.94880, dsc 0.05120\n",
      "Batch train [8] loss 0.94102, dsc 0.05898\n",
      "Batch train [9] loss 0.93958, dsc 0.06042\n",
      "Batch train [10] loss 0.93789, dsc 0.06211\n",
      "Epoch [124] train done\n",
      "Batch eval [1] loss 0.93108, dsc 0.06892\n",
      "Batch eval [2] loss 0.93765, dsc 0.06235\n",
      "Batch eval [3] loss 0.92972, dsc 0.07028\n",
      "Batch eval [4] loss 0.94341, dsc 0.05659\n",
      "Batch eval [5] loss 0.92814, dsc 0.07186\n",
      "Epoch [124] valid done\n",
      "Epoch [124] T 499.41s, deltaT 4.00s, loss: train 0.93718, valid 0.93400, dsc: train 0.06282, valid 0.06600\n",
      "Batch train [1] loss 0.94425, dsc 0.05575\n",
      "Batch train [2] loss 0.92905, dsc 0.07095\n",
      "Batch train [3] loss 0.93669, dsc 0.06331\n",
      "Batch train [4] loss 0.93500, dsc 0.06500\n",
      "Batch train [5] loss 0.93374, dsc 0.06626\n",
      "Batch train [6] loss 0.94357, dsc 0.05643\n",
      "Batch train [7] loss 0.93816, dsc 0.06184\n",
      "Batch train [8] loss 0.93369, dsc 0.06631\n",
      "Batch train [9] loss 0.93496, dsc 0.06504\n",
      "Batch train [10] loss 0.93657, dsc 0.06343\n",
      "Epoch [125] train done\n",
      "Batch eval [1] loss 0.93080, dsc 0.06920\n",
      "Batch eval [2] loss 0.93848, dsc 0.06152\n",
      "Batch eval [3] loss 0.93121, dsc 0.06879\n",
      "Batch eval [4] loss 0.94464, dsc 0.05536\n",
      "Batch eval [5] loss 0.92861, dsc 0.07139\n",
      "Epoch [125] valid done\n",
      "Epoch [125] T 503.40s, deltaT 3.99s, loss: train 0.93657, valid 0.93475, dsc: train 0.06343, valid 0.06525\n",
      "Batch train [1] loss 0.92978, dsc 0.07022\n",
      "Batch train [2] loss 0.94096, dsc 0.05904\n",
      "Batch train [3] loss 0.94076, dsc 0.05924\n",
      "Batch train [4] loss 0.93104, dsc 0.06896\n",
      "Batch train [5] loss 0.93351, dsc 0.06649\n",
      "Batch train [6] loss 0.93694, dsc 0.06306\n",
      "Batch train [7] loss 0.93481, dsc 0.06519\n",
      "Batch train [8] loss 0.94043, dsc 0.05957\n",
      "Batch train [9] loss 0.93381, dsc 0.06619\n",
      "Batch train [10] loss 0.93772, dsc 0.06228\n",
      "Epoch [126] train done\n",
      "Batch eval [1] loss 0.93019, dsc 0.06981\n",
      "Batch eval [2] loss 0.93730, dsc 0.06270\n",
      "Batch eval [3] loss 0.93022, dsc 0.06978\n",
      "Batch eval [4] loss 0.94354, dsc 0.05646\n",
      "Batch eval [5] loss 0.92770, dsc 0.07230\n",
      "Epoch [126] valid done\n",
      "Epoch [126] T 507.41s, deltaT 4.01s, loss: train 0.93598, valid 0.93379, dsc: train 0.06402, valid 0.06621\n",
      "Batch train [1] loss 0.93698, dsc 0.06302\n",
      "Batch train [2] loss 0.93357, dsc 0.06643\n",
      "Batch train [3] loss 0.92910, dsc 0.07090\n",
      "Batch train [4] loss 0.94140, dsc 0.05860\n",
      "Batch train [5] loss 0.92658, dsc 0.07342\n",
      "Batch train [6] loss 0.94092, dsc 0.05908\n",
      "Batch train [7] loss 0.93913, dsc 0.06087\n",
      "Batch train [8] loss 0.93369, dsc 0.06631\n",
      "Batch train [9] loss 0.93756, dsc 0.06244\n",
      "Batch train [10] loss 0.93460, dsc 0.06540\n",
      "Epoch [127] train done\n",
      "Batch eval [1] loss 0.92956, dsc 0.07044\n",
      "Batch eval [2] loss 0.93746, dsc 0.06254\n",
      "Batch eval [3] loss 0.93028, dsc 0.06972\n",
      "Batch eval [4] loss 0.94331, dsc 0.05669\n",
      "Batch eval [5] loss 0.92854, dsc 0.07146\n",
      "Epoch [127] valid done\n",
      "Epoch [127] T 511.41s, deltaT 4.00s, loss: train 0.93535, valid 0.93383, dsc: train 0.06465, valid 0.06617\n",
      "Batch train [1] loss 0.92525, dsc 0.07475\n",
      "Batch train [2] loss 0.93730, dsc 0.06270\n",
      "Batch train [3] loss 0.93351, dsc 0.06649\n",
      "Batch train [4] loss 0.93930, dsc 0.06070\n",
      "Batch train [5] loss 0.93582, dsc 0.06418\n",
      "Batch train [6] loss 0.93801, dsc 0.06199\n",
      "Batch train [7] loss 0.93448, dsc 0.06552\n",
      "Batch train [8] loss 0.92207, dsc 0.07793\n",
      "Batch train [9] loss 0.94266, dsc 0.05734\n",
      "Batch train [10] loss 0.93964, dsc 0.06036\n",
      "Epoch [128] train done\n",
      "Batch eval [1] loss 0.92744, dsc 0.07256\n",
      "Batch eval [2] loss 0.93648, dsc 0.06352\n",
      "Batch eval [3] loss 0.92810, dsc 0.07190\n",
      "Batch eval [4] loss 0.94327, dsc 0.05673\n",
      "Batch eval [5] loss 0.92622, dsc 0.07378\n",
      "Epoch [128] valid done\n",
      "Epoch [128] T 515.39s, deltaT 3.98s, loss: train 0.93480, valid 0.93230, dsc: train 0.06520, valid 0.06770\n",
      "Batch train [1] loss 0.92691, dsc 0.07309\n",
      "Batch train [2] loss 0.94412, dsc 0.05588\n",
      "Batch train [3] loss 0.94250, dsc 0.05750\n",
      "Batch train [4] loss 0.93493, dsc 0.06507\n",
      "Batch train [5] loss 0.93397, dsc 0.06603\n",
      "Batch train [6] loss 0.93963, dsc 0.06037\n",
      "Batch train [7] loss 0.93027, dsc 0.06973\n",
      "Batch train [8] loss 0.92978, dsc 0.07022\n",
      "Batch train [9] loss 0.93011, dsc 0.06989\n",
      "Batch train [10] loss 0.93041, dsc 0.06959\n",
      "Epoch [129] train done\n",
      "Batch eval [1] loss 0.92682, dsc 0.07318\n",
      "Batch eval [2] loss 0.93561, dsc 0.06439\n",
      "Batch eval [3] loss 0.92784, dsc 0.07216\n",
      "Batch eval [4] loss 0.94205, dsc 0.05795\n",
      "Batch eval [5] loss 0.92562, dsc 0.07438\n",
      "Epoch [129] valid done\n",
      "Epoch [129] T 519.38s, deltaT 3.99s, loss: train 0.93426, valid 0.93159, dsc: train 0.06574, valid 0.06841\n",
      "Batch train [1] loss 0.94207, dsc 0.05793\n",
      "Batch train [2] loss 0.93848, dsc 0.06152\n",
      "Batch train [3] loss 0.93494, dsc 0.06506\n",
      "Batch train [4] loss 0.92763, dsc 0.07237\n",
      "Batch train [5] loss 0.93557, dsc 0.06443\n",
      "Batch train [6] loss 0.93408, dsc 0.06592\n",
      "Batch train [7] loss 0.93555, dsc 0.06445\n",
      "Batch train [8] loss 0.93280, dsc 0.06720\n",
      "Batch train [9] loss 0.92809, dsc 0.07191\n",
      "Batch train [10] loss 0.92656, dsc 0.07344\n",
      "Epoch [130] train done\n",
      "Batch eval [1] loss 0.92470, dsc 0.07530\n",
      "Batch eval [2] loss 0.93383, dsc 0.06617\n",
      "Batch eval [3] loss 0.92486, dsc 0.07514\n",
      "Batch eval [4] loss 0.94054, dsc 0.05946\n",
      "Batch eval [5] loss 0.92354, dsc 0.07646\n",
      "Epoch [130] valid done\n",
      "Epoch [130] T 523.36s, deltaT 3.98s, loss: train 0.93358, valid 0.92949, dsc: train 0.06642, valid 0.07051\n",
      "Batch train [1] loss 0.92418, dsc 0.07582\n",
      "Batch train [2] loss 0.94316, dsc 0.05684\n",
      "Batch train [3] loss 0.94197, dsc 0.05803\n",
      "Batch train [4] loss 0.92490, dsc 0.07510\n",
      "Batch train [5] loss 0.93500, dsc 0.06500\n",
      "Batch train [6] loss 0.93459, dsc 0.06541\n",
      "Batch train [7] loss 0.93316, dsc 0.06684\n",
      "Batch train [8] loss 0.93404, dsc 0.06596\n",
      "Batch train [9] loss 0.93170, dsc 0.06830\n",
      "Batch train [10] loss 0.92615, dsc 0.07385\n",
      "Epoch [131] train done\n",
      "Batch eval [1] loss 0.93129, dsc 0.06871\n",
      "Batch eval [2] loss 0.93578, dsc 0.06422\n",
      "Batch eval [3] loss 0.92879, dsc 0.07121\n",
      "Batch eval [4] loss 0.94295, dsc 0.05705\n",
      "Batch eval [5] loss 0.92657, dsc 0.07343\n",
      "Epoch [131] valid done\n",
      "Epoch [131] T 527.36s, deltaT 4.00s, loss: train 0.93289, valid 0.93307, dsc: train 0.06711, valid 0.06693\n",
      "Batch train [1] loss 0.92847, dsc 0.07153\n",
      "Batch train [2] loss 0.94449, dsc 0.05551\n",
      "Batch train [3] loss 0.93806, dsc 0.06194\n",
      "Batch train [4] loss 0.92481, dsc 0.07519\n",
      "Batch train [5] loss 0.92568, dsc 0.07432\n",
      "Batch train [6] loss 0.92588, dsc 0.07412\n",
      "Batch train [7] loss 0.93478, dsc 0.06522\n",
      "Batch train [8] loss 0.93910, dsc 0.06090\n",
      "Batch train [9] loss 0.93263, dsc 0.06737\n",
      "Batch train [10] loss 0.92871, dsc 0.07129\n",
      "Epoch [132] train done\n",
      "Batch eval [1] loss 0.92426, dsc 0.07574\n",
      "Batch eval [2] loss 0.93370, dsc 0.06630\n",
      "Batch eval [3] loss 0.92500, dsc 0.07500\n",
      "Batch eval [4] loss 0.94017, dsc 0.05983\n",
      "Batch eval [5] loss 0.92362, dsc 0.07638\n",
      "Epoch [132] valid done\n",
      "Epoch [132] T 531.36s, deltaT 4.00s, loss: train 0.93226, valid 0.92935, dsc: train 0.06774, valid 0.07065\n",
      "Batch train [1] loss 0.93844, dsc 0.06156\n",
      "Batch train [2] loss 0.94377, dsc 0.05623\n",
      "Batch train [3] loss 0.92829, dsc 0.07171\n",
      "Batch train [4] loss 0.93326, dsc 0.06674\n",
      "Batch train [5] loss 0.91860, dsc 0.08140\n",
      "Batch train [6] loss 0.92380, dsc 0.07620\n",
      "Batch train [7] loss 0.93794, dsc 0.06206\n",
      "Batch train [8] loss 0.92362, dsc 0.07638\n",
      "Batch train [9] loss 0.93855, dsc 0.06145\n",
      "Batch train [10] loss 0.93042, dsc 0.06958\n",
      "Epoch [133] train done\n",
      "Batch eval [1] loss 0.92571, dsc 0.07429\n",
      "Batch eval [2] loss 0.93409, dsc 0.06591\n",
      "Batch eval [3] loss 0.92610, dsc 0.07390\n",
      "Batch eval [4] loss 0.94050, dsc 0.05950\n",
      "Batch eval [5] loss 0.92343, dsc 0.07657\n",
      "Epoch [133] valid done\n",
      "Epoch [133] T 535.39s, deltaT 4.02s, loss: train 0.93167, valid 0.92996, dsc: train 0.06833, valid 0.07004\n",
      "Batch train [1] loss 0.94396, dsc 0.05604\n",
      "Batch train [2] loss 0.93030, dsc 0.06970\n",
      "Batch train [3] loss 0.92902, dsc 0.07098\n",
      "Batch train [4] loss 0.93372, dsc 0.06628\n",
      "Batch train [5] loss 0.93187, dsc 0.06813\n",
      "Batch train [6] loss 0.92318, dsc 0.07682\n",
      "Batch train [7] loss 0.91893, dsc 0.08107\n",
      "Batch train [8] loss 0.93242, dsc 0.06758\n",
      "Batch train [9] loss 0.93233, dsc 0.06767\n",
      "Batch train [10] loss 0.93427, dsc 0.06573\n",
      "Epoch [134] train done\n",
      "Batch eval [1] loss 0.92389, dsc 0.07611\n",
      "Batch eval [2] loss 0.93262, dsc 0.06738\n",
      "Batch eval [3] loss 0.92439, dsc 0.07561\n",
      "Batch eval [4] loss 0.93839, dsc 0.06161\n",
      "Batch eval [5] loss 0.92263, dsc 0.07737\n",
      "Epoch [134] valid done\n",
      "Epoch [134] T 539.30s, deltaT 3.91s, loss: train 0.93100, valid 0.92838, dsc: train 0.06900, valid 0.07162\n",
      "Batch train [1] loss 0.92806, dsc 0.07194\n",
      "Batch train [2] loss 0.92904, dsc 0.07096\n",
      "Batch train [3] loss 0.93387, dsc 0.06613\n",
      "Batch train [4] loss 0.92544, dsc 0.07456\n",
      "Batch train [5] loss 0.93569, dsc 0.06431\n",
      "Batch train [6] loss 0.93416, dsc 0.06584\n",
      "Batch train [7] loss 0.92450, dsc 0.07550\n",
      "Batch train [8] loss 0.93652, dsc 0.06348\n",
      "Batch train [9] loss 0.92802, dsc 0.07198\n",
      "Batch train [10] loss 0.92832, dsc 0.07168\n",
      "Epoch [135] train done\n",
      "Batch eval [1] loss 0.92142, dsc 0.07858\n",
      "Batch eval [2] loss 0.93133, dsc 0.06867\n",
      "Batch eval [3] loss 0.92239, dsc 0.07761\n",
      "Batch eval [4] loss 0.93753, dsc 0.06247\n",
      "Batch eval [5] loss 0.92018, dsc 0.07982\n",
      "Epoch [135] valid done\n",
      "Epoch [135] T 543.32s, deltaT 4.02s, loss: train 0.93036, valid 0.92657, dsc: train 0.06964, valid 0.07343\n",
      "Batch train [1] loss 0.92418, dsc 0.07582\n",
      "Batch train [2] loss 0.93848, dsc 0.06152\n",
      "Batch train [3] loss 0.92913, dsc 0.07087\n",
      "Batch train [4] loss 0.93009, dsc 0.06991\n",
      "Batch train [5] loss 0.91995, dsc 0.08005\n",
      "Batch train [6] loss 0.92463, dsc 0.07537\n",
      "Batch train [7] loss 0.93422, dsc 0.06578\n",
      "Batch train [8] loss 0.93475, dsc 0.06525\n",
      "Batch train [9] loss 0.92646, dsc 0.07354\n",
      "Batch train [10] loss 0.93539, dsc 0.06461\n",
      "Epoch [136] train done\n",
      "Batch eval [1] loss 0.91962, dsc 0.08038\n",
      "Batch eval [2] loss 0.92928, dsc 0.07072\n",
      "Batch eval [3] loss 0.92067, dsc 0.07933\n",
      "Batch eval [4] loss 0.93643, dsc 0.06357\n",
      "Batch eval [5] loss 0.91870, dsc 0.08130\n",
      "Epoch [136] valid done\n",
      "Epoch [136] T 547.34s, deltaT 4.02s, loss: train 0.92973, valid 0.92494, dsc: train 0.07027, valid 0.07506\n",
      "Batch train [1] loss 0.91970, dsc 0.08030\n",
      "Batch train [2] loss 0.93218, dsc 0.06782\n",
      "Batch train [3] loss 0.92934, dsc 0.07066\n",
      "Batch train [4] loss 0.92594, dsc 0.07406\n",
      "Batch train [5] loss 0.92493, dsc 0.07507\n",
      "Batch train [6] loss 0.92741, dsc 0.07259\n",
      "Batch train [7] loss 0.93733, dsc 0.06267\n",
      "Batch train [8] loss 0.93121, dsc 0.06879\n",
      "Batch train [9] loss 0.93833, dsc 0.06167\n",
      "Batch train [10] loss 0.92390, dsc 0.07610\n",
      "Epoch [137] train done\n",
      "Batch eval [1] loss 0.91852, dsc 0.08148\n",
      "Batch eval [2] loss 0.92859, dsc 0.07141\n",
      "Batch eval [3] loss 0.91901, dsc 0.08099\n",
      "Batch eval [4] loss 0.93482, dsc 0.06518\n",
      "Batch eval [5] loss 0.91688, dsc 0.08312\n",
      "Epoch [137] valid done\n",
      "Epoch [137] T 551.36s, deltaT 4.01s, loss: train 0.92903, valid 0.92356, dsc: train 0.07097, valid 0.07644\n",
      "Batch train [1] loss 0.93433, dsc 0.06567\n",
      "Batch train [2] loss 0.93483, dsc 0.06517\n",
      "Batch train [3] loss 0.92808, dsc 0.07192\n",
      "Batch train [4] loss 0.92397, dsc 0.07603\n",
      "Batch train [5] loss 0.92166, dsc 0.07834\n",
      "Batch train [6] loss 0.93325, dsc 0.06675\n",
      "Batch train [7] loss 0.91631, dsc 0.08369\n",
      "Batch train [8] loss 0.93176, dsc 0.06824\n",
      "Batch train [9] loss 0.93270, dsc 0.06730\n",
      "Batch train [10] loss 0.92654, dsc 0.07346\n",
      "Epoch [138] train done\n",
      "Batch eval [1] loss 0.92085, dsc 0.07915\n",
      "Batch eval [2] loss 0.93019, dsc 0.06981\n",
      "Batch eval [3] loss 0.92216, dsc 0.07784\n",
      "Batch eval [4] loss 0.93599, dsc 0.06401\n",
      "Batch eval [5] loss 0.91918, dsc 0.08082\n",
      "Epoch [138] valid done\n",
      "Epoch [138] T 555.31s, deltaT 3.95s, loss: train 0.92834, valid 0.92567, dsc: train 0.07166, valid 0.07433\n",
      "Batch train [1] loss 0.93607, dsc 0.06393\n",
      "Batch train [2] loss 0.93288, dsc 0.06712\n",
      "Batch train [3] loss 0.92591, dsc 0.07409\n",
      "Batch train [4] loss 0.92377, dsc 0.07623\n",
      "Batch train [5] loss 0.92942, dsc 0.07058\n",
      "Batch train [6] loss 0.91810, dsc 0.08190\n",
      "Batch train [7] loss 0.92492, dsc 0.07508\n",
      "Batch train [8] loss 0.92357, dsc 0.07643\n",
      "Batch train [9] loss 0.92893, dsc 0.07107\n",
      "Batch train [10] loss 0.93209, dsc 0.06791\n",
      "Epoch [139] train done\n",
      "Batch eval [1] loss 0.91582, dsc 0.08418\n",
      "Batch eval [2] loss 0.92667, dsc 0.07333\n",
      "Batch eval [3] loss 0.91732, dsc 0.08268\n",
      "Batch eval [4] loss 0.93399, dsc 0.06601\n",
      "Batch eval [5] loss 0.91502, dsc 0.08498\n",
      "Epoch [139] valid done\n",
      "Epoch [139] T 559.32s, deltaT 4.01s, loss: train 0.92756, valid 0.92176, dsc: train 0.07244, valid 0.07824\n",
      "Batch train [1] loss 0.90885, dsc 0.09115\n",
      "Batch train [2] loss 0.93433, dsc 0.06567\n",
      "Batch train [3] loss 0.92916, dsc 0.07084\n",
      "Batch train [4] loss 0.93410, dsc 0.06590\n",
      "Batch train [5] loss 0.92233, dsc 0.07767\n",
      "Batch train [6] loss 0.92086, dsc 0.07914\n",
      "Batch train [7] loss 0.93562, dsc 0.06438\n",
      "Batch train [8] loss 0.92780, dsc 0.07220\n",
      "Batch train [9] loss 0.92995, dsc 0.07005\n",
      "Batch train [10] loss 0.92537, dsc 0.07463\n",
      "Epoch [140] train done\n",
      "Batch eval [1] loss 0.91678, dsc 0.08322\n",
      "Batch eval [2] loss 0.92758, dsc 0.07242\n",
      "Batch eval [3] loss 0.91850, dsc 0.08150\n",
      "Batch eval [4] loss 0.93442, dsc 0.06558\n",
      "Batch eval [5] loss 0.91649, dsc 0.08351\n",
      "Epoch [140] valid done\n",
      "Epoch [140] T 563.32s, deltaT 4.00s, loss: train 0.92684, valid 0.92275, dsc: train 0.07316, valid 0.07725\n",
      "Batch train [1] loss 0.92254, dsc 0.07746\n",
      "Batch train [2] loss 0.91867, dsc 0.08133\n",
      "Batch train [3] loss 0.92461, dsc 0.07539\n",
      "Batch train [4] loss 0.91983, dsc 0.08017\n",
      "Batch train [5] loss 0.93293, dsc 0.06707\n",
      "Batch train [6] loss 0.92398, dsc 0.07602\n",
      "Batch train [7] loss 0.92200, dsc 0.07800\n",
      "Batch train [8] loss 0.93133, dsc 0.06867\n",
      "Batch train [9] loss 0.93388, dsc 0.06612\n",
      "Batch train [10] loss 0.93098, dsc 0.06902\n",
      "Epoch [141] train done\n",
      "Batch eval [1] loss 0.91759, dsc 0.08241\n",
      "Batch eval [2] loss 0.92717, dsc 0.07283\n",
      "Batch eval [3] loss 0.91882, dsc 0.08118\n",
      "Batch eval [4] loss 0.93318, dsc 0.06682\n",
      "Batch eval [5] loss 0.91606, dsc 0.08394\n",
      "Epoch [141] valid done\n",
      "Epoch [141] T 567.26s, deltaT 3.94s, loss: train 0.92608, valid 0.92256, dsc: train 0.07392, valid 0.07744\n",
      "Batch train [1] loss 0.92906, dsc 0.07094\n",
      "Batch train [2] loss 0.92676, dsc 0.07324\n",
      "Batch train [3] loss 0.92455, dsc 0.07545\n",
      "Batch train [4] loss 0.92858, dsc 0.07142\n",
      "Batch train [5] loss 0.92078, dsc 0.07922\n",
      "Batch train [6] loss 0.93473, dsc 0.06527\n",
      "Batch train [7] loss 0.91341, dsc 0.08659\n",
      "Batch train [8] loss 0.92721, dsc 0.07279\n",
      "Batch train [9] loss 0.92128, dsc 0.07872\n",
      "Batch train [10] loss 0.92627, dsc 0.07373\n",
      "Epoch [142] train done\n",
      "Batch eval [1] loss 0.91530, dsc 0.08470\n",
      "Batch eval [2] loss 0.92552, dsc 0.07448\n",
      "Batch eval [3] loss 0.91613, dsc 0.08387\n",
      "Batch eval [4] loss 0.93263, dsc 0.06737\n",
      "Batch eval [5] loss 0.91332, dsc 0.08668\n",
      "Epoch [142] valid done\n",
      "Epoch [142] T 571.20s, deltaT 3.94s, loss: train 0.92526, valid 0.92058, dsc: train 0.07474, valid 0.07942\n",
      "Batch train [1] loss 0.92318, dsc 0.07682\n",
      "Batch train [2] loss 0.93591, dsc 0.06409\n",
      "Batch train [3] loss 0.91571, dsc 0.08429\n",
      "Batch train [4] loss 0.91776, dsc 0.08224\n",
      "Batch train [5] loss 0.93430, dsc 0.06570\n",
      "Batch train [6] loss 0.92490, dsc 0.07510\n",
      "Batch train [7] loss 0.93309, dsc 0.06691\n",
      "Batch train [8] loss 0.91800, dsc 0.08200\n",
      "Batch train [9] loss 0.91562, dsc 0.08438\n",
      "Batch train [10] loss 0.92687, dsc 0.07313\n",
      "Epoch [143] train done\n",
      "Batch eval [1] loss 0.91810, dsc 0.08190\n",
      "Batch eval [2] loss 0.92595, dsc 0.07405\n",
      "Batch eval [3] loss 0.91866, dsc 0.08134\n",
      "Batch eval [4] loss 0.93208, dsc 0.06792\n",
      "Batch eval [5] loss 0.91374, dsc 0.08626\n",
      "Epoch [143] valid done\n",
      "Epoch [143] T 575.19s, deltaT 3.99s, loss: train 0.92454, valid 0.92171, dsc: train 0.07546, valid 0.07829\n",
      "Batch train [1] loss 0.91609, dsc 0.08391\n",
      "Batch train [2] loss 0.92466, dsc 0.07534\n",
      "Batch train [3] loss 0.92816, dsc 0.07184\n",
      "Batch train [4] loss 0.92201, dsc 0.07799\n",
      "Batch train [5] loss 0.92862, dsc 0.07138\n",
      "Batch train [6] loss 0.92174, dsc 0.07826\n",
      "Batch train [7] loss 0.93346, dsc 0.06654\n",
      "Batch train [8] loss 0.92431, dsc 0.07569\n",
      "Batch train [9] loss 0.91446, dsc 0.08554\n",
      "Batch train [10] loss 0.92426, dsc 0.07574\n",
      "Epoch [144] train done\n",
      "Batch eval [1] loss 0.91244, dsc 0.08756\n",
      "Batch eval [2] loss 0.92343, dsc 0.07657\n",
      "Batch eval [3] loss 0.91388, dsc 0.08612\n",
      "Batch eval [4] loss 0.92991, dsc 0.07009\n",
      "Batch eval [5] loss 0.91064, dsc 0.08936\n",
      "Epoch [144] valid done\n",
      "Epoch [144] T 579.20s, deltaT 4.01s, loss: train 0.92378, valid 0.91806, dsc: train 0.07622, valid 0.08194\n",
      "Batch train [1] loss 0.92646, dsc 0.07354\n",
      "Batch train [2] loss 0.92619, dsc 0.07381\n",
      "Batch train [3] loss 0.92578, dsc 0.07422\n",
      "Batch train [4] loss 0.91635, dsc 0.08365\n",
      "Batch train [5] loss 0.92687, dsc 0.07313\n",
      "Batch train [6] loss 0.92395, dsc 0.07605\n",
      "Batch train [7] loss 0.92791, dsc 0.07209\n",
      "Batch train [8] loss 0.91929, dsc 0.08071\n",
      "Batch train [9] loss 0.91214, dsc 0.08786\n",
      "Batch train [10] loss 0.92499, dsc 0.07501\n",
      "Epoch [145] train done\n",
      "Batch eval [1] loss 0.91199, dsc 0.08801\n",
      "Batch eval [2] loss 0.92358, dsc 0.07642\n",
      "Batch eval [3] loss 0.91343, dsc 0.08657\n",
      "Batch eval [4] loss 0.93039, dsc 0.06961\n",
      "Batch eval [5] loss 0.91117, dsc 0.08883\n",
      "Epoch [145] valid done\n",
      "Epoch [145] T 583.21s, deltaT 4.01s, loss: train 0.92299, valid 0.91811, dsc: train 0.07701, valid 0.08189\n",
      "Batch train [1] loss 0.92446, dsc 0.07554\n",
      "Batch train [2] loss 0.91624, dsc 0.08376\n",
      "Batch train [3] loss 0.92670, dsc 0.07330\n",
      "Batch train [4] loss 0.93170, dsc 0.06830\n",
      "Batch train [5] loss 0.92011, dsc 0.07989\n",
      "Batch train [6] loss 0.93393, dsc 0.06607\n",
      "Batch train [7] loss 0.90911, dsc 0.09089\n",
      "Batch train [8] loss 0.92124, dsc 0.07876\n",
      "Batch train [9] loss 0.91889, dsc 0.08111\n",
      "Batch train [10] loss 0.92016, dsc 0.07984\n",
      "Epoch [146] train done\n",
      "Batch eval [1] loss 0.91061, dsc 0.08939\n",
      "Batch eval [2] loss 0.92252, dsc 0.07748\n",
      "Batch eval [3] loss 0.91272, dsc 0.08728\n",
      "Batch eval [4] loss 0.92875, dsc 0.07125\n",
      "Batch eval [5] loss 0.90985, dsc 0.09015\n",
      "Epoch [146] valid done\n",
      "Epoch [146] T 587.23s, deltaT 4.01s, loss: train 0.92225, valid 0.91689, dsc: train 0.07775, valid 0.08311\n",
      "Batch train [1] loss 0.92990, dsc 0.07010\n",
      "Batch train [2] loss 0.92501, dsc 0.07499\n",
      "Batch train [3] loss 0.92470, dsc 0.07530\n",
      "Batch train [4] loss 0.92100, dsc 0.07900\n",
      "Batch train [5] loss 0.92487, dsc 0.07513\n",
      "Batch train [6] loss 0.91887, dsc 0.08113\n",
      "Batch train [7] loss 0.91806, dsc 0.08194\n",
      "Batch train [8] loss 0.92112, dsc 0.07888\n",
      "Batch train [9] loss 0.91206, dsc 0.08794\n",
      "Batch train [10] loss 0.91818, dsc 0.08182\n",
      "Epoch [147] train done\n",
      "Batch eval [1] loss 0.91070, dsc 0.08930\n",
      "Batch eval [2] loss 0.92214, dsc 0.07786\n",
      "Batch eval [3] loss 0.91340, dsc 0.08660\n",
      "Batch eval [4] loss 0.92785, dsc 0.07215\n",
      "Batch eval [5] loss 0.90923, dsc 0.09077\n",
      "Epoch [147] valid done\n",
      "Epoch [147] T 591.16s, deltaT 3.93s, loss: train 0.92138, valid 0.91666, dsc: train 0.07862, valid 0.08334\n",
      "Batch train [1] loss 0.91928, dsc 0.08072\n",
      "Batch train [2] loss 0.91835, dsc 0.08165\n",
      "Batch train [3] loss 0.92585, dsc 0.07415\n",
      "Batch train [4] loss 0.92369, dsc 0.07631\n",
      "Batch train [5] loss 0.92038, dsc 0.07962\n",
      "Batch train [6] loss 0.92314, dsc 0.07686\n",
      "Batch train [7] loss 0.92150, dsc 0.07850\n",
      "Batch train [8] loss 0.90703, dsc 0.09297\n",
      "Batch train [9] loss 0.91707, dsc 0.08293\n",
      "Batch train [10] loss 0.92953, dsc 0.07047\n",
      "Epoch [148] train done\n",
      "Batch eval [1] loss 0.91035, dsc 0.08965\n",
      "Batch eval [2] loss 0.92122, dsc 0.07878\n",
      "Batch eval [3] loss 0.91313, dsc 0.08687\n",
      "Batch eval [4] loss 0.92802, dsc 0.07198\n",
      "Batch eval [5] loss 0.90892, dsc 0.09108\n",
      "Epoch [148] valid done\n",
      "Epoch [148] T 595.11s, deltaT 3.95s, loss: train 0.92058, valid 0.91633, dsc: train 0.07942, valid 0.08367\n",
      "Batch train [1] loss 0.91767, dsc 0.08233\n",
      "Batch train [2] loss 0.92877, dsc 0.07123\n",
      "Batch train [3] loss 0.93179, dsc 0.06821\n",
      "Batch train [4] loss 0.91293, dsc 0.08707\n",
      "Batch train [5] loss 0.91652, dsc 0.08348\n",
      "Batch train [6] loss 0.92135, dsc 0.07865\n",
      "Batch train [7] loss 0.91161, dsc 0.08839\n",
      "Batch train [8] loss 0.92041, dsc 0.07959\n",
      "Batch train [9] loss 0.91984, dsc 0.08016\n",
      "Batch train [10] loss 0.91648, dsc 0.08352\n",
      "Epoch [149] train done\n",
      "Batch eval [1] loss 0.91041, dsc 0.08959\n",
      "Batch eval [2] loss 0.92125, dsc 0.07875\n",
      "Batch eval [3] loss 0.91154, dsc 0.08846\n",
      "Batch eval [4] loss 0.92747, dsc 0.07253\n",
      "Batch eval [5] loss 0.90788, dsc 0.09212\n",
      "Epoch [149] valid done\n",
      "Epoch [149] T 599.11s, deltaT 4.00s, loss: train 0.91974, valid 0.91571, dsc: train 0.08026, valid 0.08429\n",
      "Batch train [1] loss 0.91985, dsc 0.08015\n",
      "Batch train [2] loss 0.91486, dsc 0.08514\n",
      "Batch train [3] loss 0.91074, dsc 0.08926\n",
      "Batch train [4] loss 0.91033, dsc 0.08967\n",
      "Batch train [5] loss 0.92125, dsc 0.07875\n",
      "Batch train [6] loss 0.93340, dsc 0.06660\n",
      "Batch train [7] loss 0.92597, dsc 0.07403\n",
      "Batch train [8] loss 0.91817, dsc 0.08183\n",
      "Batch train [9] loss 0.91729, dsc 0.08271\n",
      "Batch train [10] loss 0.91688, dsc 0.08312\n",
      "Epoch [150] train done\n",
      "Batch eval [1] loss 0.91171, dsc 0.08829\n",
      "Batch eval [2] loss 0.92059, dsc 0.07941\n",
      "Batch eval [3] loss 0.91136, dsc 0.08864\n",
      "Batch eval [4] loss 0.92744, dsc 0.07256\n",
      "Batch eval [5] loss 0.90817, dsc 0.09183\n",
      "Epoch [150] valid done\n",
      "Epoch [150] T 603.13s, deltaT 4.02s, loss: train 0.91887, valid 0.91585, dsc: train 0.08113, valid 0.08415\n",
      "Batch train [1] loss 0.92033, dsc 0.07967\n",
      "Batch train [2] loss 0.91845, dsc 0.08155\n",
      "Batch train [3] loss 0.91116, dsc 0.08884\n",
      "Batch train [4] loss 0.92337, dsc 0.07663\n",
      "Batch train [5] loss 0.91956, dsc 0.08044\n",
      "Batch train [6] loss 0.91830, dsc 0.08170\n",
      "Batch train [7] loss 0.91434, dsc 0.08566\n",
      "Batch train [8] loss 0.91948, dsc 0.08052\n",
      "Batch train [9] loss 0.91937, dsc 0.08063\n",
      "Batch train [10] loss 0.91520, dsc 0.08480\n",
      "Epoch [151] train done\n",
      "Batch eval [1] loss 0.90957, dsc 0.09043\n",
      "Batch eval [2] loss 0.91995, dsc 0.08005\n",
      "Batch eval [3] loss 0.91034, dsc 0.08966\n",
      "Batch eval [4] loss 0.92673, dsc 0.07327\n",
      "Batch eval [5] loss 0.90812, dsc 0.09188\n",
      "Epoch [151] valid done\n",
      "Epoch [151] T 607.15s, deltaT 4.02s, loss: train 0.91796, valid 0.91494, dsc: train 0.08204, valid 0.08506\n",
      "Batch train [1] loss 0.92505, dsc 0.07495\n",
      "Batch train [2] loss 0.91387, dsc 0.08613\n",
      "Batch train [3] loss 0.91969, dsc 0.08031\n",
      "Batch train [4] loss 0.91064, dsc 0.08936\n",
      "Batch train [5] loss 0.92097, dsc 0.07903\n",
      "Batch train [6] loss 0.91704, dsc 0.08296\n",
      "Batch train [7] loss 0.91555, dsc 0.08445\n",
      "Batch train [8] loss 0.92035, dsc 0.07965\n",
      "Batch train [9] loss 0.90922, dsc 0.09078\n",
      "Batch train [10] loss 0.91892, dsc 0.08108\n",
      "Epoch [152] train done\n",
      "Batch eval [1] loss 0.90636, dsc 0.09364\n",
      "Batch eval [2] loss 0.91801, dsc 0.08199\n",
      "Batch eval [3] loss 0.90748, dsc 0.09252\n",
      "Batch eval [4] loss 0.92552, dsc 0.07448\n",
      "Batch eval [5] loss 0.90513, dsc 0.09487\n",
      "Epoch [152] valid done\n",
      "Epoch [152] T 611.16s, deltaT 4.01s, loss: train 0.91713, valid 0.91250, dsc: train 0.08287, valid 0.08750\n",
      "Batch train [1] loss 0.91541, dsc 0.08459\n",
      "Batch train [2] loss 0.92722, dsc 0.07278\n",
      "Batch train [3] loss 0.91197, dsc 0.08803\n",
      "Batch train [4] loss 0.90668, dsc 0.09332\n",
      "Batch train [5] loss 0.91719, dsc 0.08281\n",
      "Batch train [6] loss 0.91226, dsc 0.08774\n",
      "Batch train [7] loss 0.91377, dsc 0.08623\n",
      "Batch train [8] loss 0.91241, dsc 0.08759\n",
      "Batch train [9] loss 0.91917, dsc 0.08083\n",
      "Batch train [10] loss 0.92699, dsc 0.07301\n",
      "Epoch [153] train done\n",
      "Batch eval [1] loss 0.90741, dsc 0.09259\n",
      "Batch eval [2] loss 0.91749, dsc 0.08251\n",
      "Batch eval [3] loss 0.90673, dsc 0.09327\n",
      "Batch eval [4] loss 0.92319, dsc 0.07681\n",
      "Batch eval [5] loss 0.90372, dsc 0.09628\n",
      "Epoch [153] valid done\n",
      "Epoch [153] T 615.16s, deltaT 4.00s, loss: train 0.91631, valid 0.91171, dsc: train 0.08369, valid 0.08829\n",
      "Batch train [1] loss 0.91264, dsc 0.08736\n",
      "Batch train [2] loss 0.92933, dsc 0.07067\n",
      "Batch train [3] loss 0.91907, dsc 0.08093\n",
      "Batch train [4] loss 0.91809, dsc 0.08191\n",
      "Batch train [5] loss 0.90885, dsc 0.09115\n",
      "Batch train [6] loss 0.92022, dsc 0.07978\n",
      "Batch train [7] loss 0.91994, dsc 0.08006\n",
      "Batch train [8] loss 0.90372, dsc 0.09628\n",
      "Batch train [9] loss 0.90845, dsc 0.09155\n",
      "Batch train [10] loss 0.91275, dsc 0.08725\n",
      "Epoch [154] train done\n",
      "Batch eval [1] loss 0.90420, dsc 0.09580\n",
      "Batch eval [2] loss 0.91628, dsc 0.08372\n",
      "Batch eval [3] loss 0.90669, dsc 0.09331\n",
      "Batch eval [4] loss 0.92373, dsc 0.07627\n",
      "Batch eval [5] loss 0.90274, dsc 0.09726\n",
      "Epoch [154] valid done\n",
      "Epoch [154] T 619.16s, deltaT 4.00s, loss: train 0.91530, valid 0.91073, dsc: train 0.08470, valid 0.08927\n",
      "Batch train [1] loss 0.91291, dsc 0.08709\n",
      "Batch train [2] loss 0.91700, dsc 0.08300\n",
      "Batch train [3] loss 0.91972, dsc 0.08028\n",
      "Batch train [4] loss 0.92222, dsc 0.07778\n",
      "Batch train [5] loss 0.89880, dsc 0.10120\n",
      "Batch train [6] loss 0.91264, dsc 0.08736\n",
      "Batch train [7] loss 0.92148, dsc 0.07852\n",
      "Batch train [8] loss 0.90980, dsc 0.09020\n",
      "Batch train [9] loss 0.91659, dsc 0.08341\n",
      "Batch train [10] loss 0.91297, dsc 0.08703\n",
      "Epoch [155] train done\n",
      "Batch eval [1] loss 0.90694, dsc 0.09306\n",
      "Batch eval [2] loss 0.91684, dsc 0.08316\n",
      "Batch eval [3] loss 0.90702, dsc 0.09298\n",
      "Batch eval [4] loss 0.92298, dsc 0.07702\n",
      "Batch eval [5] loss 0.90398, dsc 0.09602\n",
      "Epoch [155] valid done\n",
      "Epoch [155] T 623.18s, deltaT 4.02s, loss: train 0.91441, valid 0.91155, dsc: train 0.08559, valid 0.08845\n",
      "Batch train [1] loss 0.92435, dsc 0.07565\n",
      "Batch train [2] loss 0.91957, dsc 0.08043\n",
      "Batch train [3] loss 0.91622, dsc 0.08378\n",
      "Batch train [4] loss 0.89557, dsc 0.10443\n",
      "Batch train [5] loss 0.91670, dsc 0.08330\n",
      "Batch train [6] loss 0.91194, dsc 0.08806\n",
      "Batch train [7] loss 0.91753, dsc 0.08247\n",
      "Batch train [8] loss 0.91341, dsc 0.08659\n",
      "Batch train [9] loss 0.91177, dsc 0.08823\n",
      "Batch train [10] loss 0.90740, dsc 0.09260\n",
      "Epoch [156] train done\n",
      "Batch eval [1] loss 0.90670, dsc 0.09330\n",
      "Batch eval [2] loss 0.91624, dsc 0.08376\n",
      "Batch eval [3] loss 0.90605, dsc 0.09395\n",
      "Batch eval [4] loss 0.92229, dsc 0.07771\n",
      "Batch eval [5] loss 0.90265, dsc 0.09735\n",
      "Epoch [156] valid done\n",
      "Epoch [156] T 627.19s, deltaT 4.00s, loss: train 0.91345, valid 0.91079, dsc: train 0.08655, valid 0.08921\n",
      "Batch train [1] loss 0.90284, dsc 0.09716\n",
      "Batch train [2] loss 0.92523, dsc 0.07477\n",
      "Batch train [3] loss 0.90818, dsc 0.09182\n",
      "Batch train [4] loss 0.91306, dsc 0.08694\n",
      "Batch train [5] loss 0.92168, dsc 0.07832\n",
      "Batch train [6] loss 0.90739, dsc 0.09261\n",
      "Batch train [7] loss 0.90440, dsc 0.09560\n",
      "Batch train [8] loss 0.90520, dsc 0.09480\n",
      "Batch train [9] loss 0.91163, dsc 0.08837\n",
      "Batch train [10] loss 0.92584, dsc 0.07416\n",
      "Epoch [157] train done\n",
      "Batch eval [1] loss 0.90277, dsc 0.09723\n",
      "Batch eval [2] loss 0.91434, dsc 0.08566\n",
      "Batch eval [3] loss 0.90332, dsc 0.09668\n",
      "Batch eval [4] loss 0.92105, dsc 0.07895\n",
      "Batch eval [5] loss 0.90088, dsc 0.09912\n",
      "Epoch [157] valid done\n",
      "Epoch [157] T 631.17s, deltaT 3.98s, loss: train 0.91254, valid 0.90847, dsc: train 0.08746, valid 0.09153\n",
      "Batch train [1] loss 0.90699, dsc 0.09301\n",
      "Batch train [2] loss 0.89653, dsc 0.10347\n",
      "Batch train [3] loss 0.91701, dsc 0.08299\n",
      "Batch train [4] loss 0.91286, dsc 0.08714\n",
      "Batch train [5] loss 0.91475, dsc 0.08525\n",
      "Batch train [6] loss 0.91396, dsc 0.08604\n",
      "Batch train [7] loss 0.91559, dsc 0.08441\n",
      "Batch train [8] loss 0.91231, dsc 0.08769\n",
      "Batch train [9] loss 0.91174, dsc 0.08826\n",
      "Batch train [10] loss 0.91378, dsc 0.08622\n",
      "Epoch [158] train done\n",
      "Batch eval [1] loss 0.89903, dsc 0.10097\n",
      "Batch eval [2] loss 0.91189, dsc 0.08811\n",
      "Batch eval [3] loss 0.90191, dsc 0.09809\n",
      "Batch eval [4] loss 0.91949, dsc 0.08051\n",
      "Batch eval [5] loss 0.89738, dsc 0.10262\n",
      "Epoch [158] valid done\n",
      "Epoch [158] T 635.19s, deltaT 4.02s, loss: train 0.91155, valid 0.90594, dsc: train 0.08845, valid 0.09406\n",
      "Batch train [1] loss 0.90287, dsc 0.09713\n",
      "Batch train [2] loss 0.90727, dsc 0.09273\n",
      "Batch train [3] loss 0.90149, dsc 0.09851\n",
      "Batch train [4] loss 0.91555, dsc 0.08445\n",
      "Batch train [5] loss 0.92147, dsc 0.07853\n",
      "Batch train [6] loss 0.92045, dsc 0.07955\n",
      "Batch train [7] loss 0.90537, dsc 0.09463\n",
      "Batch train [8] loss 0.90959, dsc 0.09041\n",
      "Batch train [9] loss 0.90948, dsc 0.09052\n",
      "Batch train [10] loss 0.91228, dsc 0.08772\n",
      "Epoch [159] train done\n",
      "Batch eval [1] loss 0.90197, dsc 0.09803\n",
      "Batch eval [2] loss 0.91112, dsc 0.08888\n",
      "Batch eval [3] loss 0.90119, dsc 0.09881\n",
      "Batch eval [4] loss 0.91876, dsc 0.08124\n",
      "Batch eval [5] loss 0.89680, dsc 0.10320\n",
      "Epoch [159] valid done\n",
      "Epoch [159] T 639.19s, deltaT 3.99s, loss: train 0.91058, valid 0.90597, dsc: train 0.08942, valid 0.09403\n",
      "Batch train [1] loss 0.90949, dsc 0.09051\n",
      "Batch train [2] loss 0.90907, dsc 0.09093\n",
      "Batch train [3] loss 0.90203, dsc 0.09797\n",
      "Batch train [4] loss 0.92162, dsc 0.07838\n",
      "Batch train [5] loss 0.90905, dsc 0.09095\n",
      "Batch train [6] loss 0.90475, dsc 0.09525\n",
      "Batch train [7] loss 0.91643, dsc 0.08357\n",
      "Batch train [8] loss 0.89821, dsc 0.10179\n",
      "Batch train [9] loss 0.90827, dsc 0.09173\n",
      "Batch train [10] loss 0.91722, dsc 0.08278\n",
      "Epoch [160] train done\n",
      "Batch eval [1] loss 0.89049, dsc 0.10951\n",
      "Batch eval [2] loss 0.90379, dsc 0.09621\n",
      "Batch eval [3] loss 0.89222, dsc 0.10778\n",
      "Batch eval [4] loss 0.91185, dsc 0.08815\n",
      "Batch eval [5] loss 0.88974, dsc 0.11026\n",
      "Epoch [160] valid done\n",
      "Epoch [160] T 643.28s, deltaT 4.09s, loss: train 0.90961, valid 0.89762, dsc: train 0.09039, valid 0.10238\n",
      "Batch train [1] loss 0.90037, dsc 0.09963\n",
      "Batch train [2] loss 0.90919, dsc 0.09081\n",
      "Batch train [3] loss 0.90175, dsc 0.09825\n",
      "Batch train [4] loss 0.91400, dsc 0.08600\n",
      "Batch train [5] loss 0.91428, dsc 0.08572\n",
      "Batch train [6] loss 0.90865, dsc 0.09135\n",
      "Batch train [7] loss 0.92415, dsc 0.07585\n",
      "Batch train [8] loss 0.90957, dsc 0.09043\n",
      "Batch train [9] loss 0.90158, dsc 0.09842\n",
      "Batch train [10] loss 0.90335, dsc 0.09665\n",
      "Epoch [161] train done\n",
      "Batch eval [1] loss 0.89580, dsc 0.10420\n",
      "Batch eval [2] loss 0.90773, dsc 0.09227\n",
      "Batch eval [3] loss 0.89629, dsc 0.10371\n",
      "Batch eval [4] loss 0.91597, dsc 0.08403\n",
      "Batch eval [5] loss 0.89278, dsc 0.10722\n",
      "Epoch [161] valid done\n",
      "Epoch [161] T 647.21s, deltaT 3.94s, loss: train 0.90869, valid 0.90171, dsc: train 0.09131, valid 0.09829\n",
      "Batch train [1] loss 0.91235, dsc 0.08765\n",
      "Batch train [2] loss 0.90169, dsc 0.09831\n",
      "Batch train [3] loss 0.89086, dsc 0.10914\n",
      "Batch train [4] loss 0.90857, dsc 0.09143\n",
      "Batch train [5] loss 0.90834, dsc 0.09166\n",
      "Batch train [6] loss 0.91172, dsc 0.08828\n",
      "Batch train [7] loss 0.90049, dsc 0.09951\n",
      "Batch train [8] loss 0.91881, dsc 0.08119\n",
      "Batch train [9] loss 0.91508, dsc 0.08492\n",
      "Batch train [10] loss 0.90855, dsc 0.09145\n",
      "Epoch [162] train done\n",
      "Batch eval [1] loss 0.89751, dsc 0.10249\n",
      "Batch eval [2] loss 0.90917, dsc 0.09083\n",
      "Batch eval [3] loss 0.89903, dsc 0.10097\n",
      "Batch eval [4] loss 0.91606, dsc 0.08394\n",
      "Batch eval [5] loss 0.89479, dsc 0.10521\n",
      "Epoch [162] valid done\n",
      "Epoch [162] T 651.23s, deltaT 4.01s, loss: train 0.90765, valid 0.90331, dsc: train 0.09235, valid 0.09669\n",
      "Batch train [1] loss 0.89918, dsc 0.10082\n",
      "Batch train [2] loss 0.90814, dsc 0.09186\n",
      "Batch train [3] loss 0.91505, dsc 0.08495\n",
      "Batch train [4] loss 0.88918, dsc 0.11082\n",
      "Batch train [5] loss 0.90721, dsc 0.09279\n",
      "Batch train [6] loss 0.91858, dsc 0.08142\n",
      "Batch train [7] loss 0.89864, dsc 0.10136\n",
      "Batch train [8] loss 0.90076, dsc 0.09924\n",
      "Batch train [9] loss 0.91707, dsc 0.08293\n",
      "Batch train [10] loss 0.91115, dsc 0.08884\n",
      "Epoch [163] train done\n",
      "Batch eval [1] loss 0.89541, dsc 0.10459\n",
      "Batch eval [2] loss 0.90673, dsc 0.09327\n",
      "Batch eval [3] loss 0.89478, dsc 0.10522\n",
      "Batch eval [4] loss 0.91223, dsc 0.08777\n",
      "Batch eval [5] loss 0.89018, dsc 0.10982\n",
      "Epoch [163] valid done\n",
      "Epoch [163] T 655.25s, deltaT 4.03s, loss: train 0.90650, valid 0.89987, dsc: train 0.09350, valid 0.10013\n",
      "Batch train [1] loss 0.89925, dsc 0.10075\n",
      "Batch train [2] loss 0.90619, dsc 0.09381\n",
      "Batch train [3] loss 0.91424, dsc 0.08576\n",
      "Batch train [4] loss 0.90515, dsc 0.09485\n",
      "Batch train [5] loss 0.89802, dsc 0.10198\n",
      "Batch train [6] loss 0.90591, dsc 0.09409\n",
      "Batch train [7] loss 0.90217, dsc 0.09783\n",
      "Batch train [8] loss 0.91261, dsc 0.08739\n",
      "Batch train [9] loss 0.90759, dsc 0.09241\n",
      "Batch train [10] loss 0.90287, dsc 0.09713\n",
      "Epoch [164] train done\n",
      "Batch eval [1] loss 0.89983, dsc 0.10017\n",
      "Batch eval [2] loss 0.90987, dsc 0.09013\n",
      "Batch eval [3] loss 0.89965, dsc 0.10035\n",
      "Batch eval [4] loss 0.91735, dsc 0.08265\n",
      "Batch eval [5] loss 0.89484, dsc 0.10516\n",
      "Epoch [164] valid done\n",
      "Epoch [164] T 659.27s, deltaT 4.01s, loss: train 0.90540, valid 0.90431, dsc: train 0.09460, valid 0.09569\n",
      "Batch train [1] loss 0.90883, dsc 0.09117\n",
      "Batch train [2] loss 0.89657, dsc 0.10343\n",
      "Batch train [3] loss 0.89964, dsc 0.10036\n",
      "Batch train [4] loss 0.90542, dsc 0.09458\n",
      "Batch train [5] loss 0.91103, dsc 0.08897\n",
      "Batch train [6] loss 0.91883, dsc 0.08117\n",
      "Batch train [7] loss 0.88441, dsc 0.11559\n",
      "Batch train [8] loss 0.90655, dsc 0.09345\n",
      "Batch train [9] loss 0.90640, dsc 0.09360\n",
      "Batch train [10] loss 0.90570, dsc 0.09430\n",
      "Epoch [165] train done\n",
      "Batch eval [1] loss 0.89408, dsc 0.10592\n",
      "Batch eval [2] loss 0.90752, dsc 0.09248\n",
      "Batch eval [3] loss 0.89632, dsc 0.10368\n",
      "Batch eval [4] loss 0.91395, dsc 0.08605\n",
      "Batch eval [5] loss 0.89249, dsc 0.10751\n",
      "Epoch [165] valid done\n",
      "Epoch [165] T 663.20s, deltaT 3.93s, loss: train 0.90434, valid 0.90087, dsc: train 0.09566, valid 0.09913\n",
      "Batch train [1] loss 0.90652, dsc 0.09348\n",
      "Batch train [2] loss 0.91370, dsc 0.08630\n",
      "Batch train [3] loss 0.89484, dsc 0.10516\n",
      "Batch train [4] loss 0.88546, dsc 0.11454\n",
      "Batch train [5] loss 0.90926, dsc 0.09074\n",
      "Batch train [6] loss 0.89946, dsc 0.10054\n",
      "Batch train [7] loss 0.91595, dsc 0.08405\n",
      "Batch train [8] loss 0.89492, dsc 0.10508\n",
      "Batch train [9] loss 0.90647, dsc 0.09353\n",
      "Batch train [10] loss 0.90744, dsc 0.09256\n",
      "Epoch [166] train done\n",
      "Batch eval [1] loss 0.89078, dsc 0.10922\n",
      "Batch eval [2] loss 0.90445, dsc 0.09555\n",
      "Batch eval [3] loss 0.89181, dsc 0.10819\n",
      "Batch eval [4] loss 0.91290, dsc 0.08710\n",
      "Batch eval [5] loss 0.88873, dsc 0.11127\n",
      "Epoch [166] valid done\n",
      "Epoch [166] T 667.25s, deltaT 4.05s, loss: train 0.90340, valid 0.89773, dsc: train 0.09660, valid 0.10227\n",
      "Batch train [1] loss 0.89499, dsc 0.10501\n",
      "Batch train [2] loss 0.90624, dsc 0.09376\n",
      "Batch train [3] loss 0.89683, dsc 0.10317\n",
      "Batch train [4] loss 0.90660, dsc 0.09340\n",
      "Batch train [5] loss 0.89847, dsc 0.10153\n",
      "Batch train [6] loss 0.89439, dsc 0.10561\n",
      "Batch train [7] loss 0.91067, dsc 0.08933\n",
      "Batch train [8] loss 0.89925, dsc 0.10075\n",
      "Batch train [9] loss 0.91831, dsc 0.08169\n",
      "Batch train [10] loss 0.89680, dsc 0.10320\n",
      "Epoch [167] train done\n",
      "Batch eval [1] loss 0.89000, dsc 0.11000\n",
      "Batch eval [2] loss 0.90390, dsc 0.09610\n",
      "Batch eval [3] loss 0.89038, dsc 0.10962\n",
      "Batch eval [4] loss 0.91044, dsc 0.08956\n",
      "Batch eval [5] loss 0.88647, dsc 0.11353\n",
      "Epoch [167] valid done\n",
      "Epoch [167] T 671.23s, deltaT 3.98s, loss: train 0.90225, valid 0.89624, dsc: train 0.09775, valid 0.10376\n",
      "Batch train [1] loss 0.90430, dsc 0.09570\n",
      "Batch train [2] loss 0.90469, dsc 0.09531\n",
      "Batch train [3] loss 0.89614, dsc 0.10386\n",
      "Batch train [4] loss 0.90299, dsc 0.09701\n",
      "Batch train [5] loss 0.90377, dsc 0.09623\n",
      "Batch train [6] loss 0.89915, dsc 0.10085\n",
      "Batch train [7] loss 0.89001, dsc 0.10999\n",
      "Batch train [8] loss 0.90730, dsc 0.09270\n",
      "Batch train [9] loss 0.88850, dsc 0.11150\n",
      "Batch train [10] loss 0.91349, dsc 0.08651\n",
      "Epoch [168] train done\n",
      "Batch eval [1] loss 0.89039, dsc 0.10961\n",
      "Batch eval [2] loss 0.90287, dsc 0.09713\n",
      "Batch eval [3] loss 0.89030, dsc 0.10970\n",
      "Batch eval [4] loss 0.91049, dsc 0.08951\n",
      "Batch eval [5] loss 0.88538, dsc 0.11462\n",
      "Epoch [168] valid done\n",
      "Epoch [168] T 675.22s, deltaT 3.99s, loss: train 0.90103, valid 0.89589, dsc: train 0.09897, valid 0.10411\n",
      "Batch train [1] loss 0.90924, dsc 0.09076\n",
      "Batch train [2] loss 0.89702, dsc 0.10298\n",
      "Batch train [3] loss 0.90571, dsc 0.09429\n",
      "Batch train [4] loss 0.89974, dsc 0.10026\n",
      "Batch train [5] loss 0.89327, dsc 0.10673\n",
      "Batch train [6] loss 0.90896, dsc 0.09104\n",
      "Batch train [7] loss 0.90003, dsc 0.09997\n",
      "Batch train [8] loss 0.88839, dsc 0.11161\n",
      "Batch train [9] loss 0.89495, dsc 0.10505\n",
      "Batch train [10] loss 0.90142, dsc 0.09858\n",
      "Epoch [169] train done\n",
      "Batch eval [1] loss 0.88920, dsc 0.11080\n",
      "Batch eval [2] loss 0.90306, dsc 0.09694\n",
      "Batch eval [3] loss 0.89046, dsc 0.10954\n",
      "Batch eval [4] loss 0.90980, dsc 0.09020\n",
      "Batch eval [5] loss 0.88657, dsc 0.11343\n",
      "Epoch [169] valid done\n",
      "Epoch [169] T 679.20s, deltaT 3.97s, loss: train 0.89987, valid 0.89582, dsc: train 0.10013, valid 0.10418\n",
      "Batch train [1] loss 0.91182, dsc 0.08818\n",
      "Batch train [2] loss 0.89138, dsc 0.10862\n",
      "Batch train [3] loss 0.90502, dsc 0.09498\n",
      "Batch train [4] loss 0.90171, dsc 0.09829\n",
      "Batch train [5] loss 0.88666, dsc 0.11334\n",
      "Batch train [6] loss 0.90824, dsc 0.09176\n",
      "Batch train [7] loss 0.88999, dsc 0.11001\n",
      "Batch train [8] loss 0.89497, dsc 0.10503\n",
      "Batch train [9] loss 0.88516, dsc 0.11484\n",
      "Batch train [10] loss 0.91289, dsc 0.08711\n",
      "Epoch [170] train done\n",
      "Batch eval [1] loss 0.88418, dsc 0.11582\n",
      "Batch eval [2] loss 0.89742, dsc 0.10258\n",
      "Batch eval [3] loss 0.88632, dsc 0.11368\n",
      "Batch eval [4] loss 0.90680, dsc 0.09320\n",
      "Batch eval [5] loss 0.88224, dsc 0.11776\n",
      "Epoch [170] valid done\n",
      "Epoch [170] T 683.19s, deltaT 3.99s, loss: train 0.89878, valid 0.89139, dsc: train 0.10122, valid 0.10861\n",
      "Batch train [1] loss 0.89870, dsc 0.10130\n",
      "Batch train [2] loss 0.90275, dsc 0.09725\n",
      "Batch train [3] loss 0.88923, dsc 0.11077\n",
      "Batch train [4] loss 0.89703, dsc 0.10297\n",
      "Batch train [5] loss 0.89355, dsc 0.10645\n",
      "Batch train [6] loss 0.89106, dsc 0.10894\n",
      "Batch train [7] loss 0.90652, dsc 0.09348\n",
      "Batch train [8] loss 0.89620, dsc 0.10380\n",
      "Batch train [9] loss 0.90054, dsc 0.09946\n",
      "Batch train [10] loss 0.90064, dsc 0.09936\n",
      "Epoch [171] train done\n",
      "Batch eval [1] loss 0.88085, dsc 0.11915\n",
      "Batch eval [2] loss 0.89585, dsc 0.10415\n",
      "Batch eval [3] loss 0.88193, dsc 0.11807\n",
      "Batch eval [4] loss 0.90239, dsc 0.09761\n",
      "Batch eval [5] loss 0.87795, dsc 0.12205\n",
      "Epoch [171] valid done\n",
      "Epoch [171] T 687.21s, deltaT 4.01s, loss: train 0.89762, valid 0.88779, dsc: train 0.10238, valid 0.11221\n",
      "Batch train [1] loss 0.90412, dsc 0.09588\n",
      "Batch train [2] loss 0.91365, dsc 0.08635\n",
      "Batch train [3] loss 0.88261, dsc 0.11739\n",
      "Batch train [4] loss 0.90290, dsc 0.09710\n",
      "Batch train [5] loss 0.88933, dsc 0.11067\n",
      "Batch train [6] loss 0.89511, dsc 0.10489\n",
      "Batch train [7] loss 0.89083, dsc 0.10917\n",
      "Batch train [8] loss 0.89426, dsc 0.10574\n",
      "Batch train [9] loss 0.88791, dsc 0.11209\n",
      "Batch train [10] loss 0.90346, dsc 0.09654\n",
      "Epoch [172] train done\n",
      "Batch eval [1] loss 0.88554, dsc 0.11446\n",
      "Batch eval [2] loss 0.89797, dsc 0.10203\n",
      "Batch eval [3] loss 0.88759, dsc 0.11241\n",
      "Batch eval [4] loss 0.90646, dsc 0.09354\n",
      "Batch eval [5] loss 0.88197, dsc 0.11803\n",
      "Epoch [172] valid done\n",
      "Epoch [172] T 691.18s, deltaT 3.98s, loss: train 0.89642, valid 0.89191, dsc: train 0.10358, valid 0.10809\n",
      "Batch train [1] loss 0.89441, dsc 0.10559\n",
      "Batch train [2] loss 0.89320, dsc 0.10680\n",
      "Batch train [3] loss 0.88230, dsc 0.11770\n",
      "Batch train [4] loss 0.89123, dsc 0.10877\n",
      "Batch train [5] loss 0.88874, dsc 0.11126\n",
      "Batch train [6] loss 0.90434, dsc 0.09566\n",
      "Batch train [7] loss 0.89795, dsc 0.10205\n",
      "Batch train [8] loss 0.90510, dsc 0.09490\n",
      "Batch train [9] loss 0.90003, dsc 0.09997\n",
      "Batch train [10] loss 0.89503, dsc 0.10497\n",
      "Epoch [173] train done\n",
      "Batch eval [1] loss 0.88412, dsc 0.11588\n",
      "Batch eval [2] loss 0.89908, dsc 0.10092\n",
      "Batch eval [3] loss 0.88493, dsc 0.11507\n",
      "Batch eval [4] loss 0.90647, dsc 0.09353\n",
      "Batch eval [5] loss 0.88145, dsc 0.11855\n",
      "Epoch [173] valid done\n",
      "Epoch [173] T 695.18s, deltaT 3.99s, loss: train 0.89523, valid 0.89121, dsc: train 0.10477, valid 0.10879\n",
      "Batch train [1] loss 0.88611, dsc 0.11389\n",
      "Batch train [2] loss 0.89481, dsc 0.10519\n",
      "Batch train [3] loss 0.89238, dsc 0.10762\n",
      "Batch train [4] loss 0.90516, dsc 0.09484\n",
      "Batch train [5] loss 0.90316, dsc 0.09684\n",
      "Batch train [6] loss 0.88648, dsc 0.11352\n",
      "Batch train [7] loss 0.88266, dsc 0.11734\n",
      "Batch train [8] loss 0.90541, dsc 0.09459\n",
      "Batch train [9] loss 0.89416, dsc 0.10584\n",
      "Batch train [10] loss 0.88996, dsc 0.11004\n",
      "Epoch [174] train done\n",
      "Batch eval [1] loss 0.88105, dsc 0.11895\n",
      "Batch eval [2] loss 0.89658, dsc 0.10342\n",
      "Batch eval [3] loss 0.88281, dsc 0.11719\n",
      "Batch eval [4] loss 0.90536, dsc 0.09464\n",
      "Batch eval [5] loss 0.87844, dsc 0.12156\n",
      "Epoch [174] valid done\n",
      "Epoch [174] T 699.16s, deltaT 3.98s, loss: train 0.89403, valid 0.88885, dsc: train 0.10597, valid 0.11115\n",
      "Batch train [1] loss 0.90509, dsc 0.09491\n",
      "Batch train [2] loss 0.88830, dsc 0.11170\n",
      "Batch train [3] loss 0.89011, dsc 0.10989\n",
      "Batch train [4] loss 0.89532, dsc 0.10468\n",
      "Batch train [5] loss 0.87503, dsc 0.12497\n",
      "Batch train [6] loss 0.89587, dsc 0.10413\n",
      "Batch train [7] loss 0.88355, dsc 0.11645\n",
      "Batch train [8] loss 0.88446, dsc 0.11554\n",
      "Batch train [9] loss 0.90891, dsc 0.09109\n",
      "Batch train [10] loss 0.90200, dsc 0.09800\n",
      "Epoch [175] train done\n",
      "Batch eval [1] loss 0.87456, dsc 0.12544\n",
      "Batch eval [2] loss 0.89199, dsc 0.10801\n",
      "Batch eval [3] loss 0.87706, dsc 0.12294\n",
      "Batch eval [4] loss 0.89847, dsc 0.10153\n",
      "Batch eval [5] loss 0.87240, dsc 0.12760\n",
      "Epoch [175] valid done\n",
      "Epoch [175] T 703.12s, deltaT 3.95s, loss: train 0.89286, valid 0.88290, dsc: train 0.10714, valid 0.11710\n",
      "Batch train [1] loss 0.88577, dsc 0.11423\n",
      "Batch train [2] loss 0.89086, dsc 0.10914\n",
      "Batch train [3] loss 0.88603, dsc 0.11397\n",
      "Batch train [4] loss 0.89284, dsc 0.10716\n",
      "Batch train [5] loss 0.88899, dsc 0.11101\n",
      "Batch train [6] loss 0.89245, dsc 0.10755\n",
      "Batch train [7] loss 0.88665, dsc 0.11335\n",
      "Batch train [8] loss 0.89270, dsc 0.10730\n",
      "Batch train [9] loss 0.89478, dsc 0.10522\n",
      "Batch train [10] loss 0.90469, dsc 0.09531\n",
      "Epoch [176] train done\n",
      "Batch eval [1] loss 0.87361, dsc 0.12639\n",
      "Batch eval [2] loss 0.89047, dsc 0.10953\n",
      "Batch eval [3] loss 0.87655, dsc 0.12345\n",
      "Batch eval [4] loss 0.89771, dsc 0.10229\n",
      "Batch eval [5] loss 0.87270, dsc 0.12730\n",
      "Epoch [176] valid done\n",
      "Epoch [176] T 707.07s, deltaT 3.95s, loss: train 0.89158, valid 0.88221, dsc: train 0.10842, valid 0.11779\n",
      "Batch train [1] loss 0.88630, dsc 0.11370\n",
      "Batch train [2] loss 0.89233, dsc 0.10767\n",
      "Batch train [3] loss 0.88570, dsc 0.11430\n",
      "Batch train [4] loss 0.89788, dsc 0.10212\n",
      "Batch train [5] loss 0.89158, dsc 0.10842\n",
      "Batch train [6] loss 0.89291, dsc 0.10709\n",
      "Batch train [7] loss 0.88331, dsc 0.11669\n",
      "Batch train [8] loss 0.89398, dsc 0.10602\n",
      "Batch train [9] loss 0.88035, dsc 0.11965\n",
      "Batch train [10] loss 0.89914, dsc 0.10086\n",
      "Epoch [177] train done\n",
      "Batch eval [1] loss 0.87674, dsc 0.12326\n",
      "Batch eval [2] loss 0.89232, dsc 0.10768\n",
      "Batch eval [3] loss 0.87971, dsc 0.12029\n",
      "Batch eval [4] loss 0.90027, dsc 0.09973\n",
      "Batch eval [5] loss 0.87430, dsc 0.12570\n",
      "Epoch [177] valid done\n",
      "Epoch [177] T 711.08s, deltaT 4.01s, loss: train 0.89035, valid 0.88467, dsc: train 0.10965, valid 0.11533\n",
      "Batch train [1] loss 0.88997, dsc 0.11003\n",
      "Batch train [2] loss 0.90038, dsc 0.09962\n",
      "Batch train [3] loss 0.89506, dsc 0.10494\n",
      "Batch train [4] loss 0.88514, dsc 0.11486\n",
      "Batch train [5] loss 0.89124, dsc 0.10876\n",
      "Batch train [6] loss 0.88530, dsc 0.11470\n",
      "Batch train [7] loss 0.87249, dsc 0.12751\n",
      "Batch train [8] loss 0.90093, dsc 0.09907\n",
      "Batch train [9] loss 0.87801, dsc 0.12199\n",
      "Batch train [10] loss 0.89292, dsc 0.10708\n",
      "Epoch [178] train done\n",
      "Batch eval [1] loss 0.87750, dsc 0.12250\n",
      "Batch eval [2] loss 0.89159, dsc 0.10841\n",
      "Batch eval [3] loss 0.88001, dsc 0.11999\n",
      "Batch eval [4] loss 0.89938, dsc 0.10062\n",
      "Batch eval [5] loss 0.87242, dsc 0.12758\n",
      "Epoch [178] valid done\n",
      "Epoch [178] T 715.09s, deltaT 4.00s, loss: train 0.88914, valid 0.88418, dsc: train 0.11086, valid 0.11582\n",
      "Batch train [1] loss 0.87854, dsc 0.12146\n",
      "Batch train [2] loss 0.87841, dsc 0.12159\n",
      "Batch train [3] loss 0.88832, dsc 0.11168\n",
      "Batch train [4] loss 0.88921, dsc 0.11078\n",
      "Batch train [5] loss 0.89145, dsc 0.10855\n",
      "Batch train [6] loss 0.89721, dsc 0.10279\n",
      "Batch train [7] loss 0.87584, dsc 0.12416\n",
      "Batch train [8] loss 0.89520, dsc 0.10480\n",
      "Batch train [9] loss 0.88342, dsc 0.11658\n",
      "Batch train [10] loss 0.89888, dsc 0.10112\n",
      "Epoch [179] train done\n",
      "Batch eval [1] loss 0.87515, dsc 0.12485\n",
      "Batch eval [2] loss 0.88989, dsc 0.11011\n",
      "Batch eval [3] loss 0.87735, dsc 0.12265\n",
      "Batch eval [4] loss 0.89770, dsc 0.10230\n",
      "Batch eval [5] loss 0.87020, dsc 0.12980\n",
      "Epoch [179] valid done\n",
      "Epoch [179] T 719.08s, deltaT 3.99s, loss: train 0.88765, valid 0.88206, dsc: train 0.11235, valid 0.11794\n",
      "Batch train [1] loss 0.89720, dsc 0.10280\n",
      "Batch train [2] loss 0.88068, dsc 0.11932\n",
      "Batch train [3] loss 0.88193, dsc 0.11807\n",
      "Batch train [4] loss 0.88296, dsc 0.11704\n",
      "Batch train [5] loss 0.89350, dsc 0.10650\n",
      "Batch train [6] loss 0.88746, dsc 0.11254\n",
      "Batch train [7] loss 0.87213, dsc 0.12787\n",
      "Batch train [8] loss 0.90321, dsc 0.09679\n",
      "Batch train [9] loss 0.88390, dsc 0.11610\n",
      "Batch train [10] loss 0.88095, dsc 0.11905\n",
      "Epoch [180] train done\n",
      "Batch eval [1] loss 0.87466, dsc 0.12534\n",
      "Batch eval [2] loss 0.89008, dsc 0.10992\n",
      "Batch eval [3] loss 0.87788, dsc 0.12212\n",
      "Batch eval [4] loss 0.89827, dsc 0.10173\n",
      "Batch eval [5] loss 0.87375, dsc 0.12625\n",
      "Epoch [180] valid done\n",
      "Epoch [180] T 723.06s, deltaT 3.98s, loss: train 0.88639, valid 0.88293, dsc: train 0.11361, valid 0.11707\n",
      "Batch train [1] loss 0.87583, dsc 0.12417\n",
      "Batch train [2] loss 0.88898, dsc 0.11102\n",
      "Batch train [3] loss 0.89661, dsc 0.10339\n",
      "Batch train [4] loss 0.88255, dsc 0.11745\n",
      "Batch train [5] loss 0.88831, dsc 0.11169\n",
      "Batch train [6] loss 0.88797, dsc 0.11203\n",
      "Batch train [7] loss 0.87548, dsc 0.12452\n",
      "Batch train [8] loss 0.88930, dsc 0.11070\n",
      "Batch train [9] loss 0.87200, dsc 0.12800\n",
      "Batch train [10] loss 0.89339, dsc 0.10661\n",
      "Epoch [181] train done\n",
      "Batch eval [1] loss 0.87270, dsc 0.12730\n",
      "Batch eval [2] loss 0.88872, dsc 0.11128\n",
      "Batch eval [3] loss 0.87416, dsc 0.12584\n",
      "Batch eval [4] loss 0.89724, dsc 0.10276\n",
      "Batch eval [5] loss 0.87079, dsc 0.12921\n",
      "Epoch [181] valid done\n",
      "Epoch [181] T 727.07s, deltaT 4.00s, loss: train 0.88504, valid 0.88072, dsc: train 0.11496, valid 0.11928\n",
      "Batch train [1] loss 0.88234, dsc 0.11766\n",
      "Batch train [2] loss 0.88219, dsc 0.11781\n",
      "Batch train [3] loss 0.88386, dsc 0.11614\n",
      "Batch train [4] loss 0.89458, dsc 0.10542\n",
      "Batch train [5] loss 0.87802, dsc 0.12198\n",
      "Batch train [6] loss 0.88815, dsc 0.11185\n",
      "Batch train [7] loss 0.85872, dsc 0.14128\n",
      "Batch train [8] loss 0.88851, dsc 0.11149\n",
      "Batch train [9] loss 0.88632, dsc 0.11368\n",
      "Batch train [10] loss 0.89504, dsc 0.10496\n",
      "Epoch [182] train done\n",
      "Batch eval [1] loss 0.86459, dsc 0.13541\n",
      "Batch eval [2] loss 0.88140, dsc 0.11860\n",
      "Batch eval [3] loss 0.86721, dsc 0.13279\n",
      "Batch eval [4] loss 0.89034, dsc 0.10966\n",
      "Batch eval [5] loss 0.86143, dsc 0.13857\n",
      "Epoch [182] valid done\n",
      "Epoch [182] T 731.07s, deltaT 4.01s, loss: train 0.88377, valid 0.87300, dsc: train 0.11623, valid 0.12700\n",
      "Batch train [1] loss 0.88609, dsc 0.11391\n",
      "Batch train [2] loss 0.88691, dsc 0.11309\n",
      "Batch train [3] loss 0.87547, dsc 0.12453\n",
      "Batch train [4] loss 0.87784, dsc 0.12216\n",
      "Batch train [5] loss 0.88512, dsc 0.11488\n",
      "Batch train [6] loss 0.89149, dsc 0.10851\n",
      "Batch train [7] loss 0.87679, dsc 0.12321\n",
      "Batch train [8] loss 0.87578, dsc 0.12422\n",
      "Batch train [9] loss 0.87419, dsc 0.12581\n",
      "Batch train [10] loss 0.89372, dsc 0.10628\n",
      "Epoch [183] train done\n",
      "Batch eval [1] loss 0.86623, dsc 0.13377\n",
      "Batch eval [2] loss 0.88307, dsc 0.11693\n",
      "Batch eval [3] loss 0.86896, dsc 0.13104\n",
      "Batch eval [4] loss 0.89178, dsc 0.10822\n",
      "Batch eval [5] loss 0.86289, dsc 0.13711\n",
      "Epoch [183] valid done\n",
      "Epoch [183] T 735.11s, deltaT 4.03s, loss: train 0.88234, valid 0.87459, dsc: train 0.11766, valid 0.12541\n",
      "Batch train [1] loss 0.87927, dsc 0.12073\n",
      "Batch train [2] loss 0.87208, dsc 0.12792\n",
      "Batch train [3] loss 0.87268, dsc 0.12732\n",
      "Batch train [4] loss 0.89088, dsc 0.10912\n",
      "Batch train [5] loss 0.88823, dsc 0.11177\n",
      "Batch train [6] loss 0.88662, dsc 0.11338\n",
      "Batch train [7] loss 0.87875, dsc 0.12125\n",
      "Batch train [8] loss 0.86921, dsc 0.13079\n",
      "Batch train [9] loss 0.89468, dsc 0.10532\n",
      "Batch train [10] loss 0.87611, dsc 0.12389\n",
      "Epoch [184] train done\n",
      "Batch eval [1] loss 0.86396, dsc 0.13604\n",
      "Batch eval [2] loss 0.88222, dsc 0.11778\n",
      "Batch eval [3] loss 0.87023, dsc 0.12977\n",
      "Batch eval [4] loss 0.89073, dsc 0.10927\n",
      "Batch eval [5] loss 0.86227, dsc 0.13773\n",
      "Epoch [184] valid done\n",
      "Epoch [184] T 739.07s, deltaT 3.97s, loss: train 0.88085, valid 0.87388, dsc: train 0.11915, valid 0.12612\n",
      "Batch train [1] loss 0.88991, dsc 0.11009\n",
      "Batch train [2] loss 0.87541, dsc 0.12459\n",
      "Batch train [3] loss 0.87347, dsc 0.12653\n",
      "Batch train [4] loss 0.88626, dsc 0.11374\n",
      "Batch train [5] loss 0.88337, dsc 0.11663\n",
      "Batch train [6] loss 0.86505, dsc 0.13495\n",
      "Batch train [7] loss 0.86344, dsc 0.13656\n",
      "Batch train [8] loss 0.88494, dsc 0.11506\n",
      "Batch train [9] loss 0.88891, dsc 0.11109\n",
      "Batch train [10] loss 0.88307, dsc 0.11693\n",
      "Epoch [185] train done\n",
      "Batch eval [1] loss 0.86437, dsc 0.13563\n",
      "Batch eval [2] loss 0.88213, dsc 0.11787\n",
      "Batch eval [3] loss 0.86764, dsc 0.13236\n",
      "Batch eval [4] loss 0.89035, dsc 0.10965\n",
      "Batch eval [5] loss 0.86223, dsc 0.13777\n",
      "Epoch [185] valid done\n",
      "Epoch [185] T 743.09s, deltaT 4.01s, loss: train 0.87938, valid 0.87335, dsc: train 0.12062, valid 0.12665\n",
      "Batch train [1] loss 0.88020, dsc 0.11980\n",
      "Batch train [2] loss 0.87096, dsc 0.12904\n",
      "Batch train [3] loss 0.88145, dsc 0.11855\n",
      "Batch train [4] loss 0.89252, dsc 0.10748\n",
      "Batch train [5] loss 0.87088, dsc 0.12912\n",
      "Batch train [6] loss 0.89349, dsc 0.10651\n",
      "Batch train [7] loss 0.88382, dsc 0.11618\n",
      "Batch train [8] loss 0.87048, dsc 0.12952\n",
      "Batch train [9] loss 0.87085, dsc 0.12915\n",
      "Batch train [10] loss 0.86598, dsc 0.13402\n",
      "Epoch [186] train done\n",
      "Batch eval [1] loss 0.85813, dsc 0.14187\n",
      "Batch eval [2] loss 0.87616, dsc 0.12384\n",
      "Batch eval [3] loss 0.86126, dsc 0.13874\n",
      "Batch eval [4] loss 0.88434, dsc 0.11566\n",
      "Batch eval [5] loss 0.85544, dsc 0.14456\n",
      "Epoch [186] valid done\n",
      "Epoch [186] T 747.08s, deltaT 3.99s, loss: train 0.87806, valid 0.86707, dsc: train 0.12194, valid 0.13293\n",
      "Batch train [1] loss 0.87570, dsc 0.12430\n",
      "Batch train [2] loss 0.86729, dsc 0.13271\n",
      "Batch train [3] loss 0.88687, dsc 0.11313\n",
      "Batch train [4] loss 0.87907, dsc 0.12093\n",
      "Batch train [5] loss 0.86771, dsc 0.13229\n",
      "Batch train [6] loss 0.86507, dsc 0.13493\n",
      "Batch train [7] loss 0.87227, dsc 0.12773\n",
      "Batch train [8] loss 0.89578, dsc 0.10422\n",
      "Batch train [9] loss 0.87350, dsc 0.12650\n",
      "Batch train [10] loss 0.88238, dsc 0.11762\n",
      "Epoch [187] train done\n",
      "Batch eval [1] loss 0.86289, dsc 0.13711\n",
      "Batch eval [2] loss 0.87892, dsc 0.12108\n",
      "Batch eval [3] loss 0.86512, dsc 0.13488\n",
      "Batch eval [4] loss 0.88731, dsc 0.11269\n",
      "Batch eval [5] loss 0.85901, dsc 0.14099\n",
      "Epoch [187] valid done\n",
      "Epoch [187] T 751.06s, deltaT 3.98s, loss: train 0.87657, valid 0.87065, dsc: train 0.12343, valid 0.12935\n",
      "Batch train [1] loss 0.87087, dsc 0.12913\n",
      "Batch train [2] loss 0.87736, dsc 0.12264\n",
      "Batch train [3] loss 0.87002, dsc 0.12998\n",
      "Batch train [4] loss 0.88339, dsc 0.11661\n",
      "Batch train [5] loss 0.87299, dsc 0.12701\n",
      "Batch train [6] loss 0.87997, dsc 0.12003\n",
      "Batch train [7] loss 0.88932, dsc 0.11068\n",
      "Batch train [8] loss 0.87523, dsc 0.12477\n",
      "Batch train [9] loss 0.87892, dsc 0.12108\n",
      "Batch train [10] loss 0.85173, dsc 0.14827\n",
      "Epoch [188] train done\n",
      "Batch eval [1] loss 0.85832, dsc 0.14168\n",
      "Batch eval [2] loss 0.87421, dsc 0.12579\n",
      "Batch eval [3] loss 0.86078, dsc 0.13922\n",
      "Batch eval [4] loss 0.88484, dsc 0.11516\n",
      "Batch eval [5] loss 0.85422, dsc 0.14578\n",
      "Epoch [188] valid done\n",
      "Epoch [188] T 755.08s, deltaT 4.02s, loss: train 0.87498, valid 0.86647, dsc: train 0.12502, valid 0.13353\n",
      "Batch train [1] loss 0.85152, dsc 0.14848\n",
      "Batch train [2] loss 0.87640, dsc 0.12360\n",
      "Batch train [3] loss 0.89149, dsc 0.10851\n",
      "Batch train [4] loss 0.88053, dsc 0.11947\n",
      "Batch train [5] loss 0.86527, dsc 0.13473\n",
      "Batch train [6] loss 0.87601, dsc 0.12399\n",
      "Batch train [7] loss 0.87611, dsc 0.12389\n",
      "Batch train [8] loss 0.88068, dsc 0.11932\n",
      "Batch train [9] loss 0.87567, dsc 0.12433\n",
      "Batch train [10] loss 0.86113, dsc 0.13887\n",
      "Epoch [189] train done\n",
      "Batch eval [1] loss 0.86040, dsc 0.13960\n",
      "Batch eval [2] loss 0.87621, dsc 0.12379\n",
      "Batch eval [3] loss 0.86056, dsc 0.13944\n",
      "Batch eval [4] loss 0.88359, dsc 0.11641\n",
      "Batch eval [5] loss 0.85615, dsc 0.14385\n",
      "Epoch [189] valid done\n",
      "Epoch [189] T 759.09s, deltaT 4.01s, loss: train 0.87348, valid 0.86738, dsc: train 0.12652, valid 0.13262\n",
      "Batch train [1] loss 0.87944, dsc 0.12056\n",
      "Batch train [2] loss 0.85852, dsc 0.14148\n",
      "Batch train [3] loss 0.88221, dsc 0.11779\n",
      "Batch train [4] loss 0.85756, dsc 0.14244\n",
      "Batch train [5] loss 0.87557, dsc 0.12443\n",
      "Batch train [6] loss 0.87473, dsc 0.12527\n",
      "Batch train [7] loss 0.87643, dsc 0.12357\n",
      "Batch train [8] loss 0.87003, dsc 0.12997\n",
      "Batch train [9] loss 0.87233, dsc 0.12767\n",
      "Batch train [10] loss 0.87304, dsc 0.12696\n",
      "Epoch [190] train done\n",
      "Batch eval [1] loss 0.86086, dsc 0.13914\n",
      "Batch eval [2] loss 0.87542, dsc 0.12458\n",
      "Batch eval [3] loss 0.86159, dsc 0.13841\n",
      "Batch eval [4] loss 0.88364, dsc 0.11636\n",
      "Batch eval [5] loss 0.85659, dsc 0.14341\n",
      "Epoch [190] valid done\n",
      "Epoch [190] T 763.08s, deltaT 3.99s, loss: train 0.87198, valid 0.86762, dsc: train 0.12802, valid 0.13238\n",
      "Batch train [1] loss 0.87145, dsc 0.12855\n",
      "Batch train [2] loss 0.88769, dsc 0.11231\n",
      "Batch train [3] loss 0.86510, dsc 0.13490\n",
      "Batch train [4] loss 0.86262, dsc 0.13738\n",
      "Batch train [5] loss 0.88697, dsc 0.11303\n",
      "Batch train [6] loss 0.86693, dsc 0.13307\n",
      "Batch train [7] loss 0.86902, dsc 0.13098\n",
      "Batch train [8] loss 0.85744, dsc 0.14256\n",
      "Batch train [9] loss 0.87420, dsc 0.12580\n",
      "Batch train [10] loss 0.86465, dsc 0.13535\n",
      "Epoch [191] train done\n",
      "Batch eval [1] loss 0.85599, dsc 0.14401\n",
      "Batch eval [2] loss 0.87192, dsc 0.12808\n",
      "Batch eval [3] loss 0.85786, dsc 0.14214\n",
      "Batch eval [4] loss 0.88213, dsc 0.11787\n",
      "Batch eval [5] loss 0.85282, dsc 0.14718\n",
      "Epoch [191] valid done\n",
      "Epoch [191] T 767.13s, deltaT 4.05s, loss: train 0.87061, valid 0.86415, dsc: train 0.12939, valid 0.13585\n",
      "Batch train [1] loss 0.87962, dsc 0.12038\n",
      "Batch train [2] loss 0.85596, dsc 0.14404\n",
      "Batch train [3] loss 0.85366, dsc 0.14634\n",
      "Batch train [4] loss 0.87273, dsc 0.12727\n",
      "Batch train [5] loss 0.86676, dsc 0.13324\n",
      "Batch train [6] loss 0.87203, dsc 0.12797\n",
      "Batch train [7] loss 0.87060, dsc 0.12940\n",
      "Batch train [8] loss 0.86790, dsc 0.13210\n",
      "Batch train [9] loss 0.87986, dsc 0.12014\n",
      "Batch train [10] loss 0.87067, dsc 0.12933\n",
      "Epoch [192] train done\n",
      "Batch eval [1] loss 0.85624, dsc 0.14376\n",
      "Batch eval [2] loss 0.87309, dsc 0.12691\n",
      "Batch eval [3] loss 0.85951, dsc 0.14049\n",
      "Batch eval [4] loss 0.87976, dsc 0.12024\n",
      "Batch eval [5] loss 0.85165, dsc 0.14835\n",
      "Epoch [192] valid done\n",
      "Epoch [192] T 771.13s, deltaT 4.00s, loss: train 0.86898, valid 0.86405, dsc: train 0.13102, valid 0.13595\n",
      "Batch train [1] loss 0.86162, dsc 0.13838\n",
      "Batch train [2] loss 0.86131, dsc 0.13869\n",
      "Batch train [3] loss 0.87715, dsc 0.12285\n",
      "Batch train [4] loss 0.86225, dsc 0.13775\n",
      "Batch train [5] loss 0.84770, dsc 0.15230\n",
      "Batch train [6] loss 0.88139, dsc 0.11861\n",
      "Batch train [7] loss 0.87427, dsc 0.12573\n",
      "Batch train [8] loss 0.88091, dsc 0.11909\n",
      "Batch train [9] loss 0.86046, dsc 0.13954\n",
      "Batch train [10] loss 0.86636, dsc 0.13364\n",
      "Epoch [193] train done\n",
      "Batch eval [1] loss 0.85474, dsc 0.14526\n",
      "Batch eval [2] loss 0.87057, dsc 0.12943\n",
      "Batch eval [3] loss 0.85665, dsc 0.14335\n",
      "Batch eval [4] loss 0.87907, dsc 0.12093\n",
      "Batch eval [5] loss 0.84967, dsc 0.15033\n",
      "Epoch [193] valid done\n",
      "Epoch [193] T 775.14s, deltaT 4.01s, loss: train 0.86734, valid 0.86214, dsc: train 0.13266, valid 0.13786\n",
      "Batch train [1] loss 0.87654, dsc 0.12346\n",
      "Batch train [2] loss 0.87816, dsc 0.12184\n",
      "Batch train [3] loss 0.85296, dsc 0.14704\n",
      "Batch train [4] loss 0.84081, dsc 0.15919\n",
      "Batch train [5] loss 0.86570, dsc 0.13430\n",
      "Batch train [6] loss 0.86679, dsc 0.13321\n",
      "Batch train [7] loss 0.86938, dsc 0.13062\n",
      "Batch train [8] loss 0.88204, dsc 0.11796\n",
      "Batch train [9] loss 0.85330, dsc 0.14670\n",
      "Batch train [10] loss 0.87280, dsc 0.12720\n",
      "Epoch [194] train done\n",
      "Batch eval [1] loss 0.85108, dsc 0.14892\n",
      "Batch eval [2] loss 0.87024, dsc 0.12976\n",
      "Batch eval [3] loss 0.85455, dsc 0.14545\n",
      "Batch eval [4] loss 0.87727, dsc 0.12273\n",
      "Batch eval [5] loss 0.84697, dsc 0.15303\n",
      "Epoch [194] valid done\n",
      "Epoch [194] T 779.18s, deltaT 4.05s, loss: train 0.86585, valid 0.86002, dsc: train 0.13415, valid 0.13998\n",
      "Batch train [1] loss 0.85392, dsc 0.14608\n",
      "Batch train [2] loss 0.87977, dsc 0.12023\n",
      "Batch train [3] loss 0.86948, dsc 0.13052\n",
      "Batch train [4] loss 0.88198, dsc 0.11802\n",
      "Batch train [5] loss 0.87383, dsc 0.12617\n",
      "Batch train [6] loss 0.85472, dsc 0.14528\n",
      "Batch train [7] loss 0.85168, dsc 0.14832\n",
      "Batch train [8] loss 0.84242, dsc 0.15758\n",
      "Batch train [9] loss 0.87193, dsc 0.12807\n",
      "Batch train [10] loss 0.86218, dsc 0.13782\n",
      "Epoch [195] train done\n",
      "Batch eval [1] loss 0.85046, dsc 0.14954\n",
      "Batch eval [2] loss 0.87016, dsc 0.12984\n",
      "Batch eval [3] loss 0.85358, dsc 0.14642\n",
      "Batch eval [4] loss 0.87912, dsc 0.12088\n",
      "Batch eval [5] loss 0.84565, dsc 0.15435\n",
      "Epoch [195] valid done\n",
      "Epoch [195] T 783.11s, deltaT 3.93s, loss: train 0.86419, valid 0.85979, dsc: train 0.13581, valid 0.14021\n",
      "Batch train [1] loss 0.85878, dsc 0.14122\n",
      "Batch train [2] loss 0.86735, dsc 0.13265\n",
      "Batch train [3] loss 0.85961, dsc 0.14039\n",
      "Batch train [4] loss 0.87898, dsc 0.12102\n",
      "Batch train [5] loss 0.85736, dsc 0.14264\n",
      "Batch train [6] loss 0.85882, dsc 0.14118\n",
      "Batch train [7] loss 0.85880, dsc 0.14120\n",
      "Batch train [8] loss 0.85802, dsc 0.14198\n",
      "Batch train [9] loss 0.86720, dsc 0.13280\n",
      "Batch train [10] loss 0.85972, dsc 0.14028\n",
      "Epoch [196] train done\n",
      "Batch eval [1] loss 0.85726, dsc 0.14274\n",
      "Batch eval [2] loss 0.87212, dsc 0.12788\n",
      "Batch eval [3] loss 0.85733, dsc 0.14267\n",
      "Batch eval [4] loss 0.87991, dsc 0.12009\n",
      "Batch eval [5] loss 0.85035, dsc 0.14965\n",
      "Epoch [196] valid done\n",
      "Epoch [196] T 787.15s, deltaT 4.03s, loss: train 0.86246, valid 0.86339, dsc: train 0.13754, valid 0.13661\n",
      "Batch train [1] loss 0.84463, dsc 0.15537\n",
      "Batch train [2] loss 0.87778, dsc 0.12222\n",
      "Batch train [3] loss 0.85402, dsc 0.14598\n",
      "Batch train [4] loss 0.87045, dsc 0.12955\n",
      "Batch train [5] loss 0.87293, dsc 0.12707\n",
      "Batch train [6] loss 0.85331, dsc 0.14669\n",
      "Batch train [7] loss 0.86863, dsc 0.13137\n",
      "Batch train [8] loss 0.84350, dsc 0.15650\n",
      "Batch train [9] loss 0.86313, dsc 0.13687\n",
      "Batch train [10] loss 0.86129, dsc 0.13871\n",
      "Epoch [197] train done\n",
      "Batch eval [1] loss 0.84232, dsc 0.15768\n",
      "Batch eval [2] loss 0.86274, dsc 0.13726\n",
      "Batch eval [3] loss 0.84526, dsc 0.15474\n",
      "Batch eval [4] loss 0.87220, dsc 0.12780\n",
      "Batch eval [5] loss 0.83909, dsc 0.16091\n",
      "Epoch [197] valid done\n",
      "Epoch [197] T 791.16s, deltaT 4.01s, loss: train 0.86097, valid 0.85232, dsc: train 0.13903, valid 0.14768\n",
      "Batch train [1] loss 0.86307, dsc 0.13693\n",
      "Batch train [2] loss 0.85311, dsc 0.14689\n",
      "Batch train [3] loss 0.86067, dsc 0.13933\n",
      "Batch train [4] loss 0.86579, dsc 0.13421\n",
      "Batch train [5] loss 0.86041, dsc 0.13959\n",
      "Batch train [6] loss 0.86034, dsc 0.13966\n",
      "Batch train [7] loss 0.83910, dsc 0.16090\n",
      "Batch train [8] loss 0.85107, dsc 0.14893\n",
      "Batch train [9] loss 0.87531, dsc 0.12469\n",
      "Batch train [10] loss 0.86464, dsc 0.13536\n",
      "Epoch [198] train done\n",
      "Batch eval [1] loss 0.84181, dsc 0.15819\n",
      "Batch eval [2] loss 0.86177, dsc 0.13823\n",
      "Batch eval [3] loss 0.84238, dsc 0.15762\n",
      "Batch eval [4] loss 0.86932, dsc 0.13068\n",
      "Batch eval [5] loss 0.83693, dsc 0.16307\n",
      "Epoch [198] valid done\n",
      "Epoch [198] T 795.19s, deltaT 4.02s, loss: train 0.85935, valid 0.85044, dsc: train 0.14065, valid 0.14956\n",
      "Batch train [1] loss 0.84960, dsc 0.15040\n",
      "Batch train [2] loss 0.87146, dsc 0.12854\n",
      "Batch train [3] loss 0.85420, dsc 0.14580\n",
      "Batch train [4] loss 0.86069, dsc 0.13931\n",
      "Batch train [5] loss 0.86437, dsc 0.13563\n",
      "Batch train [6] loss 0.84803, dsc 0.15197\n",
      "Batch train [7] loss 0.87503, dsc 0.12497\n",
      "Batch train [8] loss 0.85838, dsc 0.14162\n",
      "Batch train [9] loss 0.85313, dsc 0.14687\n",
      "Batch train [10] loss 0.84124, dsc 0.15876\n",
      "Epoch [199] train done\n",
      "Batch eval [1] loss 0.84401, dsc 0.15599\n",
      "Batch eval [2] loss 0.86447, dsc 0.13553\n",
      "Batch eval [3] loss 0.84581, dsc 0.15419\n",
      "Batch eval [4] loss 0.87226, dsc 0.12774\n",
      "Batch eval [5] loss 0.84201, dsc 0.15799\n",
      "Epoch [199] valid done\n",
      "Epoch [199] T 799.21s, deltaT 4.02s, loss: train 0.85761, valid 0.85371, dsc: train 0.14239, valid 0.14629\n",
      "Batch train [1] loss 0.86563, dsc 0.13437\n",
      "Batch train [2] loss 0.86354, dsc 0.13646\n",
      "Batch train [3] loss 0.85808, dsc 0.14192\n",
      "Batch train [4] loss 0.85796, dsc 0.14204\n",
      "Batch train [5] loss 0.84615, dsc 0.15385\n",
      "Batch train [6] loss 0.86710, dsc 0.13290\n",
      "Batch train [7] loss 0.84943, dsc 0.15057\n",
      "Batch train [8] loss 0.84716, dsc 0.15284\n",
      "Batch train [9] loss 0.85897, dsc 0.14103\n",
      "Batch train [10] loss 0.84401, dsc 0.15599\n",
      "Epoch [200] train done\n",
      "Batch eval [1] loss 0.84600, dsc 0.15400\n",
      "Batch eval [2] loss 0.86304, dsc 0.13696\n",
      "Batch eval [3] loss 0.84688, dsc 0.15312\n",
      "Batch eval [4] loss 0.87220, dsc 0.12780\n",
      "Batch eval [5] loss 0.84334, dsc 0.15666\n",
      "Epoch [200] valid done\n",
      "Epoch [200] T 803.23s, deltaT 4.02s, loss: train 0.85580, valid 0.85429, dsc: train 0.14420, valid 0.14571\n",
      "Batch train [1] loss 0.86292, dsc 0.13708\n",
      "Batch train [2] loss 0.85865, dsc 0.14135\n",
      "Batch train [3] loss 0.86246, dsc 0.13754\n",
      "Batch train [4] loss 0.86398, dsc 0.13602\n",
      "Batch train [5] loss 0.84551, dsc 0.15449\n",
      "Batch train [6] loss 0.84169, dsc 0.15831\n",
      "Batch train [7] loss 0.84092, dsc 0.15908\n",
      "Batch train [8] loss 0.87095, dsc 0.12905\n",
      "Batch train [9] loss 0.85906, dsc 0.14094\n",
      "Batch train [10] loss 0.83739, dsc 0.16261\n",
      "Epoch [201] train done\n",
      "Batch eval [1] loss 0.84240, dsc 0.15760\n",
      "Batch eval [2] loss 0.85836, dsc 0.14164\n",
      "Batch eval [3] loss 0.84326, dsc 0.15674\n",
      "Batch eval [4] loss 0.86871, dsc 0.13129\n",
      "Batch eval [5] loss 0.83770, dsc 0.16230\n",
      "Epoch [201] valid done\n",
      "Epoch [201] T 807.21s, deltaT 3.97s, loss: train 0.85435, valid 0.85009, dsc: train 0.14565, valid 0.14991\n",
      "Batch train [1] loss 0.86161, dsc 0.13839\n",
      "Batch train [2] loss 0.86358, dsc 0.13642\n",
      "Batch train [3] loss 0.83828, dsc 0.16172\n",
      "Batch train [4] loss 0.85213, dsc 0.14787\n",
      "Batch train [5] loss 0.85548, dsc 0.14452\n",
      "Batch train [6] loss 0.84777, dsc 0.15223\n",
      "Batch train [7] loss 0.84412, dsc 0.15588\n",
      "Batch train [8] loss 0.84569, dsc 0.15431\n",
      "Batch train [9] loss 0.85870, dsc 0.14130\n",
      "Batch train [10] loss 0.85781, dsc 0.14219\n",
      "Epoch [202] train done\n",
      "Batch eval [1] loss 0.84366, dsc 0.15634\n",
      "Batch eval [2] loss 0.86052, dsc 0.13948\n",
      "Batch eval [3] loss 0.84569, dsc 0.15431\n",
      "Batch eval [4] loss 0.86993, dsc 0.13007\n",
      "Batch eval [5] loss 0.84113, dsc 0.15887\n",
      "Epoch [202] valid done\n",
      "Epoch [202] T 811.21s, deltaT 4.01s, loss: train 0.85252, valid 0.85219, dsc: train 0.14748, valid 0.14781\n",
      "Batch train [1] loss 0.84696, dsc 0.15304\n",
      "Batch train [2] loss 0.84512, dsc 0.15488\n",
      "Batch train [3] loss 0.85974, dsc 0.14026\n",
      "Batch train [4] loss 0.86959, dsc 0.13041\n",
      "Batch train [5] loss 0.86021, dsc 0.13979\n",
      "Batch train [6] loss 0.85861, dsc 0.14139\n",
      "Batch train [7] loss 0.84676, dsc 0.15324\n",
      "Batch train [8] loss 0.84659, dsc 0.15341\n",
      "Batch train [9] loss 0.83500, dsc 0.16500\n",
      "Batch train [10] loss 0.84075, dsc 0.15925\n",
      "Epoch [203] train done\n",
      "Batch eval [1] loss 0.83234, dsc 0.16766\n",
      "Batch eval [2] loss 0.85195, dsc 0.14805\n",
      "Batch eval [3] loss 0.83417, dsc 0.16583\n",
      "Batch eval [4] loss 0.86249, dsc 0.13751\n",
      "Batch eval [5] loss 0.82832, dsc 0.17168\n",
      "Epoch [203] valid done\n",
      "Epoch [203] T 815.28s, deltaT 4.06s, loss: train 0.85093, valid 0.84185, dsc: train 0.14907, valid 0.15815\n",
      "Batch train [1] loss 0.86751, dsc 0.13249\n",
      "Batch train [2] loss 0.85146, dsc 0.14854\n",
      "Batch train [3] loss 0.84300, dsc 0.15700\n",
      "Batch train [4] loss 0.84204, dsc 0.15796\n",
      "Batch train [5] loss 0.83815, dsc 0.16185\n",
      "Batch train [6] loss 0.83181, dsc 0.16819\n",
      "Batch train [7] loss 0.84277, dsc 0.15723\n",
      "Batch train [8] loss 0.84661, dsc 0.15339\n",
      "Batch train [9] loss 0.86359, dsc 0.13641\n",
      "Batch train [10] loss 0.86060, dsc 0.13940\n",
      "Epoch [204] train done\n",
      "Batch eval [1] loss 0.83664, dsc 0.16336\n",
      "Batch eval [2] loss 0.85477, dsc 0.14523\n",
      "Batch eval [3] loss 0.83861, dsc 0.16139\n",
      "Batch eval [4] loss 0.86357, dsc 0.13643\n",
      "Batch eval [5] loss 0.83470, dsc 0.16530\n",
      "Epoch [204] valid done\n",
      "Epoch [204] T 819.30s, deltaT 4.03s, loss: train 0.84876, valid 0.84566, dsc: train 0.15124, valid 0.15434\n",
      "Batch train [1] loss 0.84325, dsc 0.15675\n",
      "Batch train [2] loss 0.83931, dsc 0.16069\n",
      "Batch train [3] loss 0.84091, dsc 0.15909\n",
      "Batch train [4] loss 0.85712, dsc 0.14288\n",
      "Batch train [5] loss 0.86026, dsc 0.13974\n",
      "Batch train [6] loss 0.85646, dsc 0.14354\n",
      "Batch train [7] loss 0.84642, dsc 0.15358\n",
      "Batch train [8] loss 0.86145, dsc 0.13855\n",
      "Batch train [9] loss 0.83844, dsc 0.16156\n",
      "Batch train [10] loss 0.81980, dsc 0.18020\n",
      "Epoch [205] train done\n",
      "Batch eval [1] loss 0.82196, dsc 0.17804\n",
      "Batch eval [2] loss 0.84135, dsc 0.15865\n",
      "Batch eval [3] loss 0.82501, dsc 0.17499\n",
      "Batch eval [4] loss 0.85609, dsc 0.14391\n",
      "Batch eval [5] loss 0.82371, dsc 0.17629\n",
      "Epoch [205] valid done\n",
      "Epoch [205] T 823.29s, deltaT 3.98s, loss: train 0.84634, valid 0.83362, dsc: train 0.15366, valid 0.16638\n",
      "Batch train [1] loss 0.83546, dsc 0.16454\n",
      "Batch train [2] loss 0.85272, dsc 0.14728\n",
      "Batch train [3] loss 0.84860, dsc 0.15140\n",
      "Batch train [4] loss 0.84200, dsc 0.15800\n",
      "Batch train [5] loss 0.83740, dsc 0.16260\n",
      "Batch train [6] loss 0.84724, dsc 0.15276\n",
      "Batch train [7] loss 0.84293, dsc 0.15707\n",
      "Batch train [8] loss 0.86313, dsc 0.13687\n",
      "Batch train [9] loss 0.84942, dsc 0.15058\n",
      "Batch train [10] loss 0.83996, dsc 0.16004\n",
      "Epoch [206] train done\n",
      "Batch eval [1] loss 0.82519, dsc 0.17481\n",
      "Batch eval [2] loss 0.84523, dsc 0.15477\n",
      "Batch eval [3] loss 0.83178, dsc 0.16822\n",
      "Batch eval [4] loss 0.85944, dsc 0.14056\n",
      "Batch eval [5] loss 0.82516, dsc 0.17484\n",
      "Epoch [206] valid done\n",
      "Epoch [206] T 827.28s, deltaT 3.99s, loss: train 0.84588, valid 0.83736, dsc: train 0.15412, valid 0.16264\n",
      "Batch train [1] loss 0.84638, dsc 0.15362\n",
      "Batch train [2] loss 0.83129, dsc 0.16871\n",
      "Batch train [3] loss 0.84543, dsc 0.15457\n",
      "Batch train [4] loss 0.82190, dsc 0.17810\n",
      "Batch train [5] loss 0.85196, dsc 0.14804\n",
      "Batch train [6] loss 0.85548, dsc 0.14452\n",
      "Batch train [7] loss 0.85573, dsc 0.14427\n",
      "Batch train [8] loss 0.84097, dsc 0.15903\n",
      "Batch train [9] loss 0.83201, dsc 0.16799\n",
      "Batch train [10] loss 0.84600, dsc 0.15400\n",
      "Epoch [207] train done\n",
      "Batch eval [1] loss 0.82187, dsc 0.17813\n",
      "Batch eval [2] loss 0.83780, dsc 0.16220\n",
      "Batch eval [3] loss 0.82539, dsc 0.17461\n",
      "Batch eval [4] loss 0.85261, dsc 0.14739\n",
      "Batch eval [5] loss 0.82070, dsc 0.17930\n",
      "Epoch [207] valid done\n",
      "Epoch [207] T 831.31s, deltaT 4.03s, loss: train 0.84271, valid 0.83167, dsc: train 0.15729, valid 0.16833\n",
      "Batch train [1] loss 0.84763, dsc 0.15237\n",
      "Batch train [2] loss 0.83997, dsc 0.16003\n",
      "Batch train [3] loss 0.85559, dsc 0.14441\n",
      "Batch train [4] loss 0.82277, dsc 0.17723\n",
      "Batch train [5] loss 0.83102, dsc 0.16898\n",
      "Batch train [6] loss 0.82756, dsc 0.17244\n",
      "Batch train [7] loss 0.84754, dsc 0.15246\n",
      "Batch train [8] loss 0.83813, dsc 0.16187\n",
      "Batch train [9] loss 0.84252, dsc 0.15748\n",
      "Batch train [10] loss 0.84992, dsc 0.15008\n",
      "Epoch [208] train done\n",
      "Batch eval [1] loss 0.82527, dsc 0.17473\n",
      "Batch eval [2] loss 0.84549, dsc 0.15451\n",
      "Batch eval [3] loss 0.82882, dsc 0.17118\n",
      "Batch eval [4] loss 0.85936, dsc 0.14064\n",
      "Batch eval [5] loss 0.82468, dsc 0.17532\n",
      "Epoch [208] valid done\n",
      "Epoch [208] T 835.29s, deltaT 3.98s, loss: train 0.84027, valid 0.83673, dsc: train 0.15973, valid 0.16327\n",
      "Batch train [1] loss 0.83050, dsc 0.16950\n",
      "Batch train [2] loss 0.83786, dsc 0.16214\n",
      "Batch train [3] loss 0.82097, dsc 0.17903\n",
      "Batch train [4] loss 0.84960, dsc 0.15040\n",
      "Batch train [5] loss 0.82792, dsc 0.17208\n",
      "Batch train [6] loss 0.82673, dsc 0.17327\n",
      "Batch train [7] loss 0.83788, dsc 0.16212\n",
      "Batch train [8] loss 0.86653, dsc 0.13347\n",
      "Batch train [9] loss 0.84353, dsc 0.15647\n",
      "Batch train [10] loss 0.83705, dsc 0.16295\n",
      "Epoch [209] train done\n",
      "Batch eval [1] loss 0.82500, dsc 0.17500\n",
      "Batch eval [2] loss 0.84343, dsc 0.15657\n",
      "Batch eval [3] loss 0.82839, dsc 0.17161\n",
      "Batch eval [4] loss 0.85996, dsc 0.14004\n",
      "Batch eval [5] loss 0.82773, dsc 0.17227\n",
      "Epoch [209] valid done\n",
      "Epoch [209] T 839.25s, deltaT 3.95s, loss: train 0.83786, valid 0.83690, dsc: train 0.16214, valid 0.16310\n",
      "Batch train [1] loss 0.85393, dsc 0.14607\n",
      "Batch train [2] loss 0.83378, dsc 0.16622\n",
      "Batch train [3] loss 0.84338, dsc 0.15662\n",
      "Batch train [4] loss 0.85116, dsc 0.14884\n",
      "Batch train [5] loss 0.83117, dsc 0.16883\n",
      "Batch train [6] loss 0.83810, dsc 0.16190\n",
      "Batch train [7] loss 0.82566, dsc 0.17434\n",
      "Batch train [8] loss 0.81601, dsc 0.18399\n",
      "Batch train [9] loss 0.82837, dsc 0.17163\n",
      "Batch train [10] loss 0.83656, dsc 0.16344\n",
      "Epoch [210] train done\n",
      "Batch eval [1] loss 0.82746, dsc 0.17254\n",
      "Batch eval [2] loss 0.84384, dsc 0.15616\n",
      "Batch eval [3] loss 0.83232, dsc 0.16768\n",
      "Batch eval [4] loss 0.85724, dsc 0.14276\n",
      "Batch eval [5] loss 0.82510, dsc 0.17490\n",
      "Epoch [210] valid done\n",
      "Epoch [210] T 843.25s, deltaT 4.00s, loss: train 0.83581, valid 0.83719, dsc: train 0.16419, valid 0.16281\n",
      "Batch train [1] loss 0.85063, dsc 0.14937\n",
      "Batch train [2] loss 0.84806, dsc 0.15194\n",
      "Batch train [3] loss 0.84970, dsc 0.15030\n",
      "Batch train [4] loss 0.82846, dsc 0.17154\n",
      "Batch train [5] loss 0.83168, dsc 0.16832\n",
      "Batch train [6] loss 0.84103, dsc 0.15897\n",
      "Batch train [7] loss 0.82859, dsc 0.17141\n",
      "Batch train [8] loss 0.81366, dsc 0.18634\n",
      "Batch train [9] loss 0.80327, dsc 0.19673\n",
      "Batch train [10] loss 0.84368, dsc 0.15632\n",
      "Epoch [211] train done\n",
      "Batch eval [1] loss 0.81901, dsc 0.18099\n",
      "Batch eval [2] loss 0.83789, dsc 0.16211\n",
      "Batch eval [3] loss 0.82419, dsc 0.17581\n",
      "Batch eval [4] loss 0.85189, dsc 0.14811\n",
      "Batch eval [5] loss 0.82009, dsc 0.17991\n",
      "Epoch [211] valid done\n",
      "Epoch [211] T 847.18s, deltaT 3.92s, loss: train 0.83388, valid 0.83061, dsc: train 0.16612, valid 0.16939\n",
      "Batch train [1] loss 0.84027, dsc 0.15973\n",
      "Batch train [2] loss 0.84696, dsc 0.15304\n",
      "Batch train [3] loss 0.83239, dsc 0.16761\n",
      "Batch train [4] loss 0.83177, dsc 0.16823\n",
      "Batch train [5] loss 0.84142, dsc 0.15858\n",
      "Batch train [6] loss 0.81516, dsc 0.18484\n",
      "Batch train [7] loss 0.83763, dsc 0.16237\n",
      "Batch train [8] loss 0.81399, dsc 0.18601\n",
      "Batch train [9] loss 0.82195, dsc 0.17805\n",
      "Batch train [10] loss 0.83375, dsc 0.16625\n",
      "Epoch [212] train done\n",
      "Batch eval [1] loss 0.82012, dsc 0.17988\n",
      "Batch eval [2] loss 0.83862, dsc 0.16138\n",
      "Batch eval [3] loss 0.82313, dsc 0.17687\n",
      "Batch eval [4] loss 0.85279, dsc 0.14721\n",
      "Batch eval [5] loss 0.82276, dsc 0.17724\n",
      "Epoch [212] valid done\n",
      "Epoch [212] T 851.20s, deltaT 4.02s, loss: train 0.83153, valid 0.83148, dsc: train 0.16847, valid 0.16852\n",
      "Batch train [1] loss 0.82873, dsc 0.17127\n",
      "Batch train [2] loss 0.83091, dsc 0.16909\n",
      "Batch train [3] loss 0.81683, dsc 0.18317\n",
      "Batch train [4] loss 0.83317, dsc 0.16683\n",
      "Batch train [5] loss 0.83284, dsc 0.16716\n",
      "Batch train [6] loss 0.82247, dsc 0.17753\n",
      "Batch train [7] loss 0.83046, dsc 0.16954\n",
      "Batch train [8] loss 0.83979, dsc 0.16021\n",
      "Batch train [9] loss 0.83462, dsc 0.16538\n",
      "Batch train [10] loss 0.82101, dsc 0.17899\n",
      "Epoch [213] train done\n",
      "Batch eval [1] loss 0.81593, dsc 0.18408\n",
      "Batch eval [2] loss 0.83482, dsc 0.16518\n",
      "Batch eval [3] loss 0.81767, dsc 0.18233\n",
      "Batch eval [4] loss 0.85091, dsc 0.14909\n",
      "Batch eval [5] loss 0.81833, dsc 0.18167\n",
      "Epoch [213] valid done\n",
      "Epoch [213] T 855.21s, deltaT 4.01s, loss: train 0.82908, valid 0.82753, dsc: train 0.17092, valid 0.17247\n",
      "Batch train [1] loss 0.82987, dsc 0.17013\n",
      "Batch train [2] loss 0.83756, dsc 0.16244\n",
      "Batch train [3] loss 0.84510, dsc 0.15490\n",
      "Batch train [4] loss 0.83015, dsc 0.16985\n",
      "Batch train [5] loss 0.83134, dsc 0.16866\n",
      "Batch train [6] loss 0.81917, dsc 0.18083\n",
      "Batch train [7] loss 0.80872, dsc 0.19128\n",
      "Batch train [8] loss 0.85120, dsc 0.14880\n",
      "Batch train [9] loss 0.81563, dsc 0.18437\n",
      "Batch train [10] loss 0.80271, dsc 0.19729\n",
      "Epoch [214] train done\n",
      "Batch eval [1] loss 0.81587, dsc 0.18413\n",
      "Batch eval [2] loss 0.83532, dsc 0.16468\n",
      "Batch eval [3] loss 0.81886, dsc 0.18114\n",
      "Batch eval [4] loss 0.85044, dsc 0.14956\n",
      "Batch eval [5] loss 0.81816, dsc 0.18184\n",
      "Epoch [214] valid done\n",
      "Epoch [214] T 859.14s, deltaT 3.93s, loss: train 0.82714, valid 0.82773, dsc: train 0.17286, valid 0.17227\n",
      "Batch train [1] loss 0.83717, dsc 0.16283\n",
      "Batch train [2] loss 0.82604, dsc 0.17396\n",
      "Batch train [3] loss 0.83901, dsc 0.16099\n",
      "Batch train [4] loss 0.80276, dsc 0.19724\n",
      "Batch train [5] loss 0.82307, dsc 0.17693\n",
      "Batch train [6] loss 0.84134, dsc 0.15866\n",
      "Batch train [7] loss 0.82277, dsc 0.17723\n",
      "Batch train [8] loss 0.80868, dsc 0.19132\n",
      "Batch train [9] loss 0.82523, dsc 0.17477\n",
      "Batch train [10] loss 0.82060, dsc 0.17940\n",
      "Epoch [215] train done\n",
      "Batch eval [1] loss 0.81669, dsc 0.18331\n",
      "Batch eval [2] loss 0.83461, dsc 0.16539\n",
      "Batch eval [3] loss 0.81993, dsc 0.18007\n",
      "Batch eval [4] loss 0.84858, dsc 0.15142\n",
      "Batch eval [5] loss 0.81720, dsc 0.18280\n",
      "Epoch [215] valid done\n",
      "Epoch [215] T 863.13s, deltaT 3.99s, loss: train 0.82467, valid 0.82740, dsc: train 0.17533, valid 0.17260\n",
      "Batch train [1] loss 0.82153, dsc 0.17847\n",
      "Batch train [2] loss 0.82895, dsc 0.17105\n",
      "Batch train [3] loss 0.82178, dsc 0.17822\n",
      "Batch train [4] loss 0.80883, dsc 0.19117\n",
      "Batch train [5] loss 0.82255, dsc 0.17745\n",
      "Batch train [6] loss 0.83454, dsc 0.16546\n",
      "Batch train [7] loss 0.81682, dsc 0.18318\n",
      "Batch train [8] loss 0.84372, dsc 0.15628\n",
      "Batch train [9] loss 0.80395, dsc 0.19605\n",
      "Batch train [10] loss 0.82429, dsc 0.17571\n",
      "Epoch [216] train done\n",
      "Batch eval [1] loss 0.80890, dsc 0.19110\n",
      "Batch eval [2] loss 0.82953, dsc 0.17047\n",
      "Batch eval [3] loss 0.81138, dsc 0.18862\n",
      "Batch eval [4] loss 0.84624, dsc 0.15376\n",
      "Batch eval [5] loss 0.81097, dsc 0.18903\n",
      "Epoch [216] valid done\n",
      "Epoch [216] T 867.18s, deltaT 4.04s, loss: train 0.82270, valid 0.82140, dsc: train 0.17730, valid 0.17860\n",
      "Batch train [1] loss 0.80564, dsc 0.19436\n",
      "Batch train [2] loss 0.84098, dsc 0.15902\n",
      "Batch train [3] loss 0.81745, dsc 0.18255\n",
      "Batch train [4] loss 0.84013, dsc 0.15987\n",
      "Batch train [5] loss 0.81692, dsc 0.18308\n",
      "Batch train [6] loss 0.80381, dsc 0.19619\n",
      "Batch train [7] loss 0.79840, dsc 0.20160\n",
      "Batch train [8] loss 0.83099, dsc 0.16901\n",
      "Batch train [9] loss 0.83176, dsc 0.16824\n",
      "Batch train [10] loss 0.82157, dsc 0.17843\n",
      "Epoch [217] train done\n",
      "Batch eval [1] loss 0.81164, dsc 0.18836\n",
      "Batch eval [2] loss 0.83083, dsc 0.16917\n",
      "Batch eval [3] loss 0.81643, dsc 0.18357\n",
      "Batch eval [4] loss 0.84487, dsc 0.15513\n",
      "Batch eval [5] loss 0.81382, dsc 0.18618\n",
      "Epoch [217] valid done\n",
      "Epoch [217] T 871.20s, deltaT 4.02s, loss: train 0.82076, valid 0.82352, dsc: train 0.17924, valid 0.17648\n",
      "Batch train [1] loss 0.80183, dsc 0.19817\n",
      "Batch train [2] loss 0.83816, dsc 0.16184\n",
      "Batch train [3] loss 0.84088, dsc 0.15912\n",
      "Batch train [4] loss 0.81683, dsc 0.18317\n",
      "Batch train [5] loss 0.83079, dsc 0.16921\n",
      "Batch train [6] loss 0.81001, dsc 0.18999\n",
      "Batch train [7] loss 0.80468, dsc 0.19532\n",
      "Batch train [8] loss 0.79815, dsc 0.20185\n",
      "Batch train [9] loss 0.82223, dsc 0.17777\n",
      "Batch train [10] loss 0.82461, dsc 0.17539\n",
      "Epoch [218] train done\n",
      "Batch eval [1] loss 0.80471, dsc 0.19529\n",
      "Batch eval [2] loss 0.82512, dsc 0.17488\n",
      "Batch eval [3] loss 0.80899, dsc 0.19101\n",
      "Batch eval [4] loss 0.84229, dsc 0.15771\n",
      "Batch eval [5] loss 0.80617, dsc 0.19383\n",
      "Epoch [218] valid done\n",
      "Epoch [218] T 875.20s, deltaT 4.00s, loss: train 0.81882, valid 0.81746, dsc: train 0.18118, valid 0.18254\n",
      "Batch train [1] loss 0.83349, dsc 0.16651\n",
      "Batch train [2] loss 0.80213, dsc 0.19786\n",
      "Batch train [3] loss 0.82277, dsc 0.17723\n",
      "Batch train [4] loss 0.79223, dsc 0.20777\n",
      "Batch train [5] loss 0.81338, dsc 0.18662\n",
      "Batch train [6] loss 0.81590, dsc 0.18410\n",
      "Batch train [7] loss 0.80573, dsc 0.19427\n",
      "Batch train [8] loss 0.82840, dsc 0.17160\n",
      "Batch train [9] loss 0.81197, dsc 0.18803\n",
      "Batch train [10] loss 0.83930, dsc 0.16070\n",
      "Epoch [219] train done\n",
      "Batch eval [1] loss 0.79839, dsc 0.20161\n",
      "Batch eval [2] loss 0.81742, dsc 0.18258\n",
      "Batch eval [3] loss 0.80444, dsc 0.19556\n",
      "Batch eval [4] loss 0.83668, dsc 0.16332\n",
      "Batch eval [5] loss 0.79713, dsc 0.20287\n",
      "Epoch [219] valid done\n",
      "Epoch [219] T 879.16s, deltaT 3.95s, loss: train 0.81653, valid 0.81081, dsc: train 0.18347, valid 0.18919\n",
      "Batch train [1] loss 0.81054, dsc 0.18946\n",
      "Batch train [2] loss 0.81855, dsc 0.18145\n",
      "Batch train [3] loss 0.80249, dsc 0.19751\n",
      "Batch train [4] loss 0.79694, dsc 0.20306\n",
      "Batch train [5] loss 0.81211, dsc 0.18789\n",
      "Batch train [6] loss 0.84048, dsc 0.15952\n",
      "Batch train [7] loss 0.79594, dsc 0.20406\n",
      "Batch train [8] loss 0.82378, dsc 0.17622\n",
      "Batch train [9] loss 0.83620, dsc 0.16380\n",
      "Batch train [10] loss 0.80420, dsc 0.19580\n",
      "Epoch [220] train done\n",
      "Batch eval [1] loss 0.79973, dsc 0.20027\n",
      "Batch eval [2] loss 0.82245, dsc 0.17755\n",
      "Batch eval [3] loss 0.80180, dsc 0.19820\n",
      "Batch eval [4] loss 0.83536, dsc 0.16464\n",
      "Batch eval [5] loss 0.80099, dsc 0.19901\n",
      "Epoch [220] valid done\n",
      "Epoch [220] T 883.15s, deltaT 3.99s, loss: train 0.81412, valid 0.81206, dsc: train 0.18588, valid 0.18794\n",
      "Batch train [1] loss 0.82281, dsc 0.17719\n",
      "Batch train [2] loss 0.80473, dsc 0.19527\n",
      "Batch train [3] loss 0.80001, dsc 0.19999\n",
      "Batch train [4] loss 0.80106, dsc 0.19894\n",
      "Batch train [5] loss 0.79904, dsc 0.20096\n",
      "Batch train [6] loss 0.81948, dsc 0.18052\n",
      "Batch train [7] loss 0.81620, dsc 0.18380\n",
      "Batch train [8] loss 0.80551, dsc 0.19449\n",
      "Batch train [9] loss 0.84021, dsc 0.15979\n",
      "Batch train [10] loss 0.80468, dsc 0.19532\n",
      "Epoch [221] train done\n",
      "Batch eval [1] loss 0.80049, dsc 0.19951\n",
      "Batch eval [2] loss 0.81979, dsc 0.18021\n",
      "Batch eval [3] loss 0.80150, dsc 0.19850\n",
      "Batch eval [4] loss 0.83649, dsc 0.16351\n",
      "Batch eval [5] loss 0.80243, dsc 0.19757\n",
      "Epoch [221] valid done\n",
      "Epoch [221] T 887.16s, deltaT 4.02s, loss: train 0.81137, valid 0.81214, dsc: train 0.18863, valid 0.18786\n",
      "Batch train [1] loss 0.80959, dsc 0.19041\n",
      "Batch train [2] loss 0.82056, dsc 0.17944\n",
      "Batch train [3] loss 0.83145, dsc 0.16855\n",
      "Batch train [4] loss 0.80604, dsc 0.19396\n",
      "Batch train [5] loss 0.79582, dsc 0.20418\n",
      "Batch train [6] loss 0.81813, dsc 0.18187\n",
      "Batch train [7] loss 0.78693, dsc 0.21307\n",
      "Batch train [8] loss 0.81359, dsc 0.18641\n",
      "Batch train [9] loss 0.80663, dsc 0.19337\n",
      "Batch train [10] loss 0.80661, dsc 0.19339\n",
      "Epoch [222] train done\n",
      "Batch eval [1] loss 0.79181, dsc 0.20819\n",
      "Batch eval [2] loss 0.81529, dsc 0.18471\n",
      "Batch eval [3] loss 0.79636, dsc 0.20364\n",
      "Batch eval [4] loss 0.82927, dsc 0.17073\n",
      "Batch eval [5] loss 0.79517, dsc 0.20483\n",
      "Epoch [222] valid done\n",
      "Epoch [222] T 891.18s, deltaT 4.01s, loss: train 0.80953, valid 0.80558, dsc: train 0.19047, valid 0.19442\n",
      "Batch train [1] loss 0.80678, dsc 0.19322\n",
      "Batch train [2] loss 0.82498, dsc 0.17502\n",
      "Batch train [3] loss 0.79154, dsc 0.20846\n",
      "Batch train [4] loss 0.79136, dsc 0.20864\n",
      "Batch train [5] loss 0.81353, dsc 0.18647\n",
      "Batch train [6] loss 0.79075, dsc 0.20925\n",
      "Batch train [7] loss 0.82203, dsc 0.17797\n",
      "Batch train [8] loss 0.79464, dsc 0.20536\n",
      "Batch train [9] loss 0.81775, dsc 0.18225\n",
      "Batch train [10] loss 0.81731, dsc 0.18269\n",
      "Epoch [223] train done\n",
      "Batch eval [1] loss 0.79527, dsc 0.20473\n",
      "Batch eval [2] loss 0.81613, dsc 0.18387\n",
      "Batch eval [3] loss 0.80049, dsc 0.19951\n",
      "Batch eval [4] loss 0.83294, dsc 0.16706\n",
      "Batch eval [5] loss 0.79668, dsc 0.20332\n",
      "Epoch [223] valid done\n",
      "Epoch [223] T 895.29s, deltaT 4.11s, loss: train 0.80707, valid 0.80830, dsc: train 0.19293, valid 0.19170\n",
      "Batch train [1] loss 0.79274, dsc 0.20726\n",
      "Batch train [2] loss 0.80383, dsc 0.19617\n",
      "Batch train [3] loss 0.80629, dsc 0.19371\n",
      "Batch train [4] loss 0.80503, dsc 0.19497\n",
      "Batch train [5] loss 0.82058, dsc 0.17942\n",
      "Batch train [6] loss 0.81600, dsc 0.18400\n",
      "Batch train [7] loss 0.80291, dsc 0.19709\n",
      "Batch train [8] loss 0.80189, dsc 0.19811\n",
      "Batch train [9] loss 0.78725, dsc 0.21275\n",
      "Batch train [10] loss 0.81051, dsc 0.18949\n",
      "Epoch [224] train done\n",
      "Batch eval [1] loss 0.79380, dsc 0.20620\n",
      "Batch eval [2] loss 0.81742, dsc 0.18258\n",
      "Batch eval [3] loss 0.80186, dsc 0.19814\n",
      "Batch eval [4] loss 0.83030, dsc 0.16970\n",
      "Batch eval [5] loss 0.79387, dsc 0.20613\n",
      "Epoch [224] valid done\n",
      "Epoch [224] T 899.29s, deltaT 4.00s, loss: train 0.80470, valid 0.80745, dsc: train 0.19530, valid 0.19255\n",
      "Batch train [1] loss 0.81172, dsc 0.18828\n",
      "Batch train [2] loss 0.82462, dsc 0.17538\n",
      "Batch train [3] loss 0.80443, dsc 0.19557\n",
      "Batch train [4] loss 0.79938, dsc 0.20062\n",
      "Batch train [5] loss 0.78180, dsc 0.21820\n",
      "Batch train [6] loss 0.80999, dsc 0.19001\n",
      "Batch train [7] loss 0.81307, dsc 0.18693\n",
      "Batch train [8] loss 0.80280, dsc 0.19720\n",
      "Batch train [9] loss 0.80274, dsc 0.19726\n",
      "Batch train [10] loss 0.77776, dsc 0.22224\n",
      "Epoch [225] train done\n",
      "Batch eval [1] loss 0.79066, dsc 0.20934\n",
      "Batch eval [2] loss 0.81021, dsc 0.18979\n",
      "Batch eval [3] loss 0.79388, dsc 0.20612\n",
      "Batch eval [4] loss 0.82915, dsc 0.17085\n",
      "Batch eval [5] loss 0.79247, dsc 0.20753\n",
      "Epoch [225] valid done\n",
      "Epoch [225] T 903.28s, deltaT 3.99s, loss: train 0.80283, valid 0.80328, dsc: train 0.19717, valid 0.19672\n",
      "Batch train [1] loss 0.80006, dsc 0.19994\n",
      "Batch train [2] loss 0.80857, dsc 0.19143\n",
      "Batch train [3] loss 0.79129, dsc 0.20871\n",
      "Batch train [4] loss 0.81360, dsc 0.18640\n",
      "Batch train [5] loss 0.79850, dsc 0.20150\n",
      "Batch train [6] loss 0.81141, dsc 0.18859\n",
      "Batch train [7] loss 0.78376, dsc 0.21624\n",
      "Batch train [8] loss 0.79777, dsc 0.20223\n",
      "Batch train [9] loss 0.78564, dsc 0.21436\n",
      "Batch train [10] loss 0.81673, dsc 0.18327\n",
      "Epoch [226] train done\n",
      "Batch eval [1] loss 0.78136, dsc 0.21864\n",
      "Batch eval [2] loss 0.80724, dsc 0.19276\n",
      "Batch eval [3] loss 0.78555, dsc 0.21445\n",
      "Batch eval [4] loss 0.82243, dsc 0.17757\n",
      "Batch eval [5] loss 0.78495, dsc 0.21505\n",
      "Epoch [226] valid done\n",
      "Epoch [226] T 907.27s, deltaT 3.99s, loss: train 0.80073, valid 0.79631, dsc: train 0.19927, valid 0.20369\n",
      "Batch train [1] loss 0.79717, dsc 0.20283\n",
      "Batch train [2] loss 0.80982, dsc 0.19018\n",
      "Batch train [3] loss 0.78430, dsc 0.21570\n",
      "Batch train [4] loss 0.79622, dsc 0.20378\n",
      "Batch train [5] loss 0.79178, dsc 0.20822\n",
      "Batch train [6] loss 0.79975, dsc 0.20025\n",
      "Batch train [7] loss 0.80806, dsc 0.19194\n",
      "Batch train [8] loss 0.77059, dsc 0.22941\n",
      "Batch train [9] loss 0.80919, dsc 0.19081\n",
      "Batch train [10] loss 0.81752, dsc 0.18248\n",
      "Epoch [227] train done\n",
      "Batch eval [1] loss 0.78661, dsc 0.21339\n",
      "Batch eval [2] loss 0.80662, dsc 0.19338\n",
      "Batch eval [3] loss 0.79086, dsc 0.20914\n",
      "Batch eval [4] loss 0.82211, dsc 0.17789\n",
      "Batch eval [5] loss 0.78638, dsc 0.21362\n",
      "Epoch [227] valid done\n",
      "Epoch [227] T 911.44s, deltaT 4.17s, loss: train 0.79844, valid 0.79852, dsc: train 0.20156, valid 0.20148\n",
      "Batch train [1] loss 0.80868, dsc 0.19132\n",
      "Batch train [2] loss 0.81335, dsc 0.18665\n",
      "Batch train [3] loss 0.78946, dsc 0.21054\n",
      "Batch train [4] loss 0.78408, dsc 0.21592\n",
      "Batch train [5] loss 0.80118, dsc 0.19882\n",
      "Batch train [6] loss 0.78808, dsc 0.21192\n",
      "Batch train [7] loss 0.82305, dsc 0.17695\n",
      "Batch train [8] loss 0.77440, dsc 0.22560\n",
      "Batch train [9] loss 0.79601, dsc 0.20399\n",
      "Batch train [10] loss 0.77928, dsc 0.22072\n",
      "Epoch [228] train done\n",
      "Batch eval [1] loss 0.77571, dsc 0.22429\n",
      "Batch eval [2] loss 0.80238, dsc 0.19762\n",
      "Batch eval [3] loss 0.78289, dsc 0.21711\n",
      "Batch eval [4] loss 0.81874, dsc 0.18126\n",
      "Batch eval [5] loss 0.78236, dsc 0.21764\n",
      "Epoch [228] valid done\n",
      "Epoch [228] T 915.66s, deltaT 4.22s, loss: train 0.79576, valid 0.79242, dsc: train 0.20424, valid 0.20758\n",
      "Batch train [1] loss 0.79373, dsc 0.20627\n",
      "Batch train [2] loss 0.79915, dsc 0.20085\n",
      "Batch train [3] loss 0.78414, dsc 0.21586\n",
      "Batch train [4] loss 0.82021, dsc 0.17979\n",
      "Batch train [5] loss 0.77739, dsc 0.22261\n",
      "Batch train [6] loss 0.81955, dsc 0.18045\n",
      "Batch train [7] loss 0.77363, dsc 0.22637\n",
      "Batch train [8] loss 0.79563, dsc 0.20437\n",
      "Batch train [9] loss 0.77249, dsc 0.22751\n",
      "Batch train [10] loss 0.79868, dsc 0.20132\n",
      "Epoch [229] train done\n",
      "Batch eval [1] loss 0.77991, dsc 0.22009\n",
      "Batch eval [2] loss 0.80453, dsc 0.19547\n",
      "Batch eval [3] loss 0.78663, dsc 0.21337\n",
      "Batch eval [4] loss 0.81865, dsc 0.18135\n",
      "Batch eval [5] loss 0.78295, dsc 0.21705\n",
      "Epoch [229] valid done\n",
      "Epoch [229] T 919.84s, deltaT 4.18s, loss: train 0.79346, valid 0.79454, dsc: train 0.20654, valid 0.20546\n",
      "Batch train [1] loss 0.76221, dsc 0.23779\n",
      "Batch train [2] loss 0.78246, dsc 0.21754\n",
      "Batch train [3] loss 0.79929, dsc 0.20071\n",
      "Batch train [4] loss 0.78017, dsc 0.21983\n",
      "Batch train [5] loss 0.79813, dsc 0.20187\n",
      "Batch train [6] loss 0.81696, dsc 0.18304\n",
      "Batch train [7] loss 0.79383, dsc 0.20617\n",
      "Batch train [8] loss 0.78868, dsc 0.21132\n",
      "Batch train [9] loss 0.79875, dsc 0.20125\n",
      "Batch train [10] loss 0.79004, dsc 0.20996\n",
      "Epoch [230] train done\n",
      "Batch eval [1] loss 0.78152, dsc 0.21848\n",
      "Batch eval [2] loss 0.80588, dsc 0.19412\n",
      "Batch eval [3] loss 0.78679, dsc 0.21321\n",
      "Batch eval [4] loss 0.81854, dsc 0.18146\n",
      "Batch eval [5] loss 0.78677, dsc 0.21323\n",
      "Epoch [230] valid done\n",
      "Epoch [230] T 924.06s, deltaT 4.21s, loss: train 0.79105, valid 0.79590, dsc: train 0.20895, valid 0.20410\n",
      "Batch train [1] loss 0.78565, dsc 0.21435\n",
      "Batch train [2] loss 0.79520, dsc 0.20480\n",
      "Batch train [3] loss 0.78512, dsc 0.21488\n",
      "Batch train [4] loss 0.80562, dsc 0.19438\n",
      "Batch train [5] loss 0.76052, dsc 0.23948\n",
      "Batch train [6] loss 0.76959, dsc 0.23041\n",
      "Batch train [7] loss 0.80919, dsc 0.19081\n",
      "Batch train [8] loss 0.78294, dsc 0.21706\n",
      "Batch train [9] loss 0.80847, dsc 0.19153\n",
      "Batch train [10] loss 0.78706, dsc 0.21294\n",
      "Epoch [231] train done\n",
      "Batch eval [1] loss 0.77009, dsc 0.22991\n",
      "Batch eval [2] loss 0.79667, dsc 0.20333\n",
      "Batch eval [3] loss 0.77572, dsc 0.22428\n",
      "Batch eval [4] loss 0.81317, dsc 0.18683\n",
      "Batch eval [5] loss 0.77338, dsc 0.22662\n",
      "Epoch [231] valid done\n",
      "Epoch [231] T 928.02s, deltaT 3.96s, loss: train 0.78894, valid 0.78581, dsc: train 0.21106, valid 0.21419\n",
      "Batch train [1] loss 0.81332, dsc 0.18668\n",
      "Batch train [2] loss 0.79240, dsc 0.20760\n",
      "Batch train [3] loss 0.78486, dsc 0.21514\n",
      "Batch train [4] loss 0.81844, dsc 0.18156\n",
      "Batch train [5] loss 0.75268, dsc 0.24732\n",
      "Batch train [6] loss 0.77845, dsc 0.22155\n",
      "Batch train [7] loss 0.79711, dsc 0.20289\n",
      "Batch train [8] loss 0.75544, dsc 0.24456\n",
      "Batch train [9] loss 0.79041, dsc 0.20959\n",
      "Batch train [10] loss 0.79010, dsc 0.20990\n",
      "Epoch [232] train done\n",
      "Batch eval [1] loss 0.76790, dsc 0.23210\n",
      "Batch eval [2] loss 0.79425, dsc 0.20575\n",
      "Batch eval [3] loss 0.77530, dsc 0.22470\n",
      "Batch eval [4] loss 0.80961, dsc 0.19039\n",
      "Batch eval [5] loss 0.76882, dsc 0.23118\n",
      "Epoch [232] valid done\n",
      "Epoch [232] T 932.05s, deltaT 4.03s, loss: train 0.78732, valid 0.78318, dsc: train 0.21268, valid 0.21682\n",
      "Batch train [1] loss 0.79739, dsc 0.20261\n",
      "Batch train [2] loss 0.77731, dsc 0.22269\n",
      "Batch train [3] loss 0.79562, dsc 0.20438\n",
      "Batch train [4] loss 0.78228, dsc 0.21772\n",
      "Batch train [5] loss 0.77864, dsc 0.22136\n",
      "Batch train [6] loss 0.78408, dsc 0.21592\n",
      "Batch train [7] loss 0.81541, dsc 0.18459\n",
      "Batch train [8] loss 0.77644, dsc 0.22356\n",
      "Batch train [9] loss 0.78054, dsc 0.21946\n",
      "Batch train [10] loss 0.75842, dsc 0.24158\n",
      "Epoch [233] train done\n",
      "Batch eval [1] loss 0.76331, dsc 0.23669\n",
      "Batch eval [2] loss 0.78767, dsc 0.21233\n",
      "Batch eval [3] loss 0.77065, dsc 0.22935\n",
      "Batch eval [4] loss 0.80652, dsc 0.19348\n",
      "Batch eval [5] loss 0.76602, dsc 0.23398\n",
      "Epoch [233] valid done\n",
      "Epoch [233] T 936.02s, deltaT 3.97s, loss: train 0.78461, valid 0.77884, dsc: train 0.21539, valid 0.22116\n",
      "Batch train [1] loss 0.81790, dsc 0.18210\n",
      "Batch train [2] loss 0.75785, dsc 0.24215\n",
      "Batch train [3] loss 0.78371, dsc 0.21629\n",
      "Batch train [4] loss 0.78201, dsc 0.21799\n",
      "Batch train [5] loss 0.76172, dsc 0.23828\n",
      "Batch train [6] loss 0.77431, dsc 0.22569\n",
      "Batch train [7] loss 0.78043, dsc 0.21957\n",
      "Batch train [8] loss 0.79554, dsc 0.20446\n",
      "Batch train [9] loss 0.79344, dsc 0.20656\n",
      "Batch train [10] loss 0.77726, dsc 0.22274\n",
      "Epoch [234] train done\n",
      "Batch eval [1] loss 0.76997, dsc 0.23003\n",
      "Batch eval [2] loss 0.79221, dsc 0.20779\n",
      "Batch eval [3] loss 0.77534, dsc 0.22466\n",
      "Batch eval [4] loss 0.80614, dsc 0.19386\n",
      "Batch eval [5] loss 0.77302, dsc 0.22698\n",
      "Epoch [234] valid done\n",
      "Epoch [234] T 940.05s, deltaT 4.03s, loss: train 0.78242, valid 0.78334, dsc: train 0.21758, valid 0.21666\n",
      "Batch train [1] loss 0.79341, dsc 0.20659\n",
      "Batch train [2] loss 0.80603, dsc 0.19397\n",
      "Batch train [3] loss 0.76534, dsc 0.23466\n",
      "Batch train [4] loss 0.78329, dsc 0.21671\n",
      "Batch train [5] loss 0.76204, dsc 0.23796\n",
      "Batch train [6] loss 0.78804, dsc 0.21196\n",
      "Batch train [7] loss 0.78683, dsc 0.21317\n",
      "Batch train [8] loss 0.77473, dsc 0.22527\n",
      "Batch train [9] loss 0.76813, dsc 0.23187\n",
      "Batch train [10] loss 0.77272, dsc 0.22728\n",
      "Epoch [235] train done\n",
      "Batch eval [1] loss 0.76797, dsc 0.23203\n",
      "Batch eval [2] loss 0.79047, dsc 0.20953\n",
      "Batch eval [3] loss 0.77090, dsc 0.22910\n",
      "Batch eval [4] loss 0.80790, dsc 0.19210\n",
      "Batch eval [5] loss 0.76785, dsc 0.23215\n",
      "Epoch [235] valid done\n",
      "Epoch [235] T 944.04s, deltaT 3.99s, loss: train 0.78006, valid 0.78102, dsc: train 0.21994, valid 0.21898\n",
      "Batch train [1] loss 0.77682, dsc 0.22318\n",
      "Batch train [2] loss 0.77237, dsc 0.22763\n",
      "Batch train [3] loss 0.77144, dsc 0.22856\n",
      "Batch train [4] loss 0.77440, dsc 0.22560\n",
      "Batch train [5] loss 0.78811, dsc 0.21189\n",
      "Batch train [6] loss 0.75863, dsc 0.24137\n",
      "Batch train [7] loss 0.79846, dsc 0.20154\n",
      "Batch train [8] loss 0.80152, dsc 0.19848\n",
      "Batch train [9] loss 0.75736, dsc 0.24264\n",
      "Batch train [10] loss 0.77797, dsc 0.22203\n",
      "Epoch [236] train done\n",
      "Batch eval [1] loss 0.76841, dsc 0.23159\n",
      "Batch eval [2] loss 0.79247, dsc 0.20753\n",
      "Batch eval [3] loss 0.77217, dsc 0.22783\n",
      "Batch eval [4] loss 0.80549, dsc 0.19451\n",
      "Batch eval [5] loss 0.77299, dsc 0.22701\n",
      "Epoch [236] valid done\n",
      "Epoch [236] T 948.05s, deltaT 4.01s, loss: train 0.77771, valid 0.78230, dsc: train 0.22229, valid 0.21770\n",
      "Batch train [1] loss 0.79065, dsc 0.20935\n",
      "Batch train [2] loss 0.76931, dsc 0.23069\n",
      "Batch train [3] loss 0.74868, dsc 0.25132\n",
      "Batch train [4] loss 0.77348, dsc 0.22652\n",
      "Batch train [5] loss 0.78919, dsc 0.21081\n",
      "Batch train [6] loss 0.75222, dsc 0.24778\n",
      "Batch train [7] loss 0.76501, dsc 0.23499\n",
      "Batch train [8] loss 0.76596, dsc 0.23404\n",
      "Batch train [9] loss 0.79806, dsc 0.20194\n",
      "Batch train [10] loss 0.80472, dsc 0.19528\n",
      "Epoch [237] train done\n",
      "Batch eval [1] loss 0.76536, dsc 0.23464\n",
      "Batch eval [2] loss 0.79043, dsc 0.20957\n",
      "Batch eval [3] loss 0.77188, dsc 0.22812\n",
      "Batch eval [4] loss 0.80562, dsc 0.19438\n",
      "Batch eval [5] loss 0.76737, dsc 0.23263\n",
      "Epoch [237] valid done\n",
      "Epoch [237] T 952.04s, deltaT 3.99s, loss: train 0.77573, valid 0.78013, dsc: train 0.22427, valid 0.21987\n",
      "Batch train [1] loss 0.77757, dsc 0.22243\n",
      "Batch train [2] loss 0.77874, dsc 0.22126\n",
      "Batch train [3] loss 0.77849, dsc 0.22151\n",
      "Batch train [4] loss 0.79028, dsc 0.20972\n",
      "Batch train [5] loss 0.75803, dsc 0.24197\n",
      "Batch train [6] loss 0.77784, dsc 0.22216\n",
      "Batch train [7] loss 0.77137, dsc 0.22863\n",
      "Batch train [8] loss 0.75285, dsc 0.24715\n",
      "Batch train [9] loss 0.77440, dsc 0.22560\n",
      "Batch train [10] loss 0.76791, dsc 0.23209\n",
      "Epoch [238] train done\n",
      "Batch eval [1] loss 0.75788, dsc 0.24212\n",
      "Batch eval [2] loss 0.78774, dsc 0.21226\n",
      "Batch eval [3] loss 0.77859, dsc 0.22141\n",
      "Batch eval [4] loss 0.80156, dsc 0.19844\n",
      "Batch eval [5] loss 0.76427, dsc 0.23573\n",
      "Epoch [238] valid done\n",
      "Epoch [238] T 956.04s, deltaT 4.00s, loss: train 0.77275, valid 0.77801, dsc: train 0.22725, valid 0.22199\n",
      "Batch train [1] loss 0.75747, dsc 0.24253\n",
      "Batch train [2] loss 0.74420, dsc 0.25580\n",
      "Batch train [3] loss 0.76323, dsc 0.23677\n",
      "Batch train [4] loss 0.78571, dsc 0.21429\n",
      "Batch train [5] loss 0.78345, dsc 0.21655\n",
      "Batch train [6] loss 0.76558, dsc 0.23442\n",
      "Batch train [7] loss 0.73574, dsc 0.26426\n",
      "Batch train [8] loss 0.78290, dsc 0.21710\n",
      "Batch train [9] loss 0.78996, dsc 0.21004\n",
      "Batch train [10] loss 0.79948, dsc 0.20052\n",
      "Epoch [239] train done\n",
      "Batch eval [1] loss 0.76105, dsc 0.23895\n",
      "Batch eval [2] loss 0.78815, dsc 0.21185\n",
      "Batch eval [3] loss 0.77188, dsc 0.22812\n",
      "Batch eval [4] loss 0.80235, dsc 0.19765\n",
      "Batch eval [5] loss 0.76737, dsc 0.23263\n",
      "Epoch [239] valid done\n",
      "Epoch [239] T 960.04s, deltaT 4.00s, loss: train 0.77077, valid 0.77816, dsc: train 0.22923, valid 0.22184\n",
      "Batch train [1] loss 0.76970, dsc 0.23030\n",
      "Batch train [2] loss 0.74944, dsc 0.25056\n",
      "Batch train [3] loss 0.74215, dsc 0.25785\n",
      "Batch train [4] loss 0.76069, dsc 0.23931\n",
      "Batch train [5] loss 0.78083, dsc 0.21917\n",
      "Batch train [6] loss 0.76716, dsc 0.23284\n",
      "Batch train [7] loss 0.75928, dsc 0.24072\n",
      "Batch train [8] loss 0.77790, dsc 0.22210\n",
      "Batch train [9] loss 0.78927, dsc 0.21073\n",
      "Batch train [10] loss 0.78422, dsc 0.21578\n",
      "Epoch [240] train done\n",
      "Batch eval [1] loss 0.75923, dsc 0.24077\n",
      "Batch eval [2] loss 0.78620, dsc 0.21380\n",
      "Batch eval [3] loss 0.76824, dsc 0.23176\n",
      "Batch eval [4] loss 0.80129, dsc 0.19871\n",
      "Batch eval [5] loss 0.76382, dsc 0.23618\n",
      "Epoch [240] valid done\n",
      "Epoch [240] T 964.05s, deltaT 4.01s, loss: train 0.76806, valid 0.77576, dsc: train 0.23194, valid 0.22424\n",
      "Batch train [1] loss 0.78762, dsc 0.21238\n",
      "Batch train [2] loss 0.75241, dsc 0.24759\n",
      "Batch train [3] loss 0.76218, dsc 0.23782\n",
      "Batch train [4] loss 0.76044, dsc 0.23956\n",
      "Batch train [5] loss 0.76474, dsc 0.23526\n",
      "Batch train [6] loss 0.78971, dsc 0.21029\n",
      "Batch train [7] loss 0.75871, dsc 0.24129\n",
      "Batch train [8] loss 0.74830, dsc 0.25170\n",
      "Batch train [9] loss 0.78969, dsc 0.21031\n",
      "Batch train [10] loss 0.74615, dsc 0.25385\n",
      "Epoch [241] train done\n",
      "Batch eval [1] loss 0.75530, dsc 0.24470\n",
      "Batch eval [2] loss 0.78438, dsc 0.21563\n",
      "Batch eval [3] loss 0.76549, dsc 0.23451\n",
      "Batch eval [4] loss 0.79870, dsc 0.20130\n",
      "Batch eval [5] loss 0.75873, dsc 0.24127\n",
      "Epoch [241] valid done\n",
      "Epoch [241] T 968.09s, deltaT 4.03s, loss: train 0.76599, valid 0.77252, dsc: train 0.23401, valid 0.22748\n",
      "Batch train [1] loss 0.75574, dsc 0.24426\n",
      "Batch train [2] loss 0.78819, dsc 0.21181\n",
      "Batch train [3] loss 0.76719, dsc 0.23281\n",
      "Batch train [4] loss 0.73690, dsc 0.26310\n",
      "Batch train [5] loss 0.75887, dsc 0.24113\n",
      "Batch train [6] loss 0.74690, dsc 0.25310\n",
      "Batch train [7] loss 0.77548, dsc 0.22452\n",
      "Batch train [8] loss 0.76621, dsc 0.23379\n",
      "Batch train [9] loss 0.76694, dsc 0.23306\n",
      "Batch train [10] loss 0.77246, dsc 0.22754\n",
      "Epoch [242] train done\n",
      "Batch eval [1] loss 0.75463, dsc 0.24537\n",
      "Batch eval [2] loss 0.78301, dsc 0.21699\n",
      "Batch eval [3] loss 0.76225, dsc 0.23775\n",
      "Batch eval [4] loss 0.79863, dsc 0.20137\n",
      "Batch eval [5] loss 0.76233, dsc 0.23767\n",
      "Epoch [242] valid done\n",
      "Epoch [242] T 972.16s, deltaT 4.07s, loss: train 0.76349, valid 0.77217, dsc: train 0.23651, valid 0.22783\n",
      "Batch train [1] loss 0.77761, dsc 0.22239\n",
      "Batch train [2] loss 0.74743, dsc 0.25257\n",
      "Batch train [3] loss 0.78634, dsc 0.21366\n",
      "Batch train [4] loss 0.74741, dsc 0.25259\n",
      "Batch train [5] loss 0.75928, dsc 0.24072\n",
      "Batch train [6] loss 0.77823, dsc 0.22177\n",
      "Batch train [7] loss 0.77404, dsc 0.22596\n",
      "Batch train [8] loss 0.75888, dsc 0.24112\n",
      "Batch train [9] loss 0.73764, dsc 0.26236\n",
      "Batch train [10] loss 0.74432, dsc 0.25568\n",
      "Epoch [243] train done\n",
      "Batch eval [1] loss 0.75776, dsc 0.24224\n",
      "Batch eval [2] loss 0.78306, dsc 0.21694\n",
      "Batch eval [3] loss 0.76190, dsc 0.23810\n",
      "Batch eval [4] loss 0.79788, dsc 0.20212\n",
      "Batch eval [5] loss 0.76271, dsc 0.23729\n",
      "Epoch [243] valid done\n",
      "Epoch [243] T 976.16s, deltaT 4.00s, loss: train 0.76112, valid 0.77266, dsc: train 0.23888, valid 0.22734\n",
      "Batch train [1] loss 0.76713, dsc 0.23287\n",
      "Batch train [2] loss 0.75832, dsc 0.24168\n",
      "Batch train [3] loss 0.73156, dsc 0.26844\n",
      "Batch train [4] loss 0.77237, dsc 0.22763\n",
      "Batch train [5] loss 0.75399, dsc 0.24601\n",
      "Batch train [6] loss 0.78964, dsc 0.21036\n",
      "Batch train [7] loss 0.73878, dsc 0.26122\n",
      "Batch train [8] loss 0.74717, dsc 0.25283\n",
      "Batch train [9] loss 0.76378, dsc 0.23622\n",
      "Batch train [10] loss 0.76705, dsc 0.23295\n",
      "Epoch [244] train done\n",
      "Batch eval [1] loss 0.75705, dsc 0.24295\n",
      "Batch eval [2] loss 0.77775, dsc 0.22225\n",
      "Batch eval [3] loss 0.75777, dsc 0.24223\n",
      "Batch eval [4] loss 0.79240, dsc 0.20760\n",
      "Batch eval [5] loss 0.75560, dsc 0.24440\n",
      "Epoch [244] valid done\n",
      "Epoch [244] T 980.23s, deltaT 4.06s, loss: train 0.75898, valid 0.76811, dsc: train 0.24102, valid 0.23189\n",
      "Batch train [1] loss 0.75011, dsc 0.24989\n",
      "Batch train [2] loss 0.74330, dsc 0.25670\n",
      "Batch train [3] loss 0.76203, dsc 0.23797\n",
      "Batch train [4] loss 0.74029, dsc 0.25971\n",
      "Batch train [5] loss 0.76984, dsc 0.23016\n",
      "Batch train [6] loss 0.77289, dsc 0.22711\n",
      "Batch train [7] loss 0.73955, dsc 0.26045\n",
      "Batch train [8] loss 0.76888, dsc 0.23112\n",
      "Batch train [9] loss 0.75541, dsc 0.24459\n",
      "Batch train [10] loss 0.76452, dsc 0.23548\n",
      "Epoch [245] train done\n",
      "Batch eval [1] loss 0.74921, dsc 0.25079\n",
      "Batch eval [2] loss 0.77467, dsc 0.22533\n",
      "Batch eval [3] loss 0.75408, dsc 0.24592\n",
      "Batch eval [4] loss 0.78562, dsc 0.21438\n",
      "Batch eval [5] loss 0.74977, dsc 0.25023\n",
      "Epoch [245] valid done\n",
      "Epoch [245] T 984.26s, deltaT 4.03s, loss: train 0.75668, valid 0.76267, dsc: train 0.24332, valid 0.23733\n",
      "Batch train [1] loss 0.76332, dsc 0.23668\n",
      "Batch train [2] loss 0.75680, dsc 0.24320\n",
      "Batch train [3] loss 0.75734, dsc 0.24266\n",
      "Batch train [4] loss 0.76171, dsc 0.23829\n",
      "Batch train [5] loss 0.73494, dsc 0.26506\n",
      "Batch train [6] loss 0.76727, dsc 0.23273\n",
      "Batch train [7] loss 0.74130, dsc 0.25870\n",
      "Batch train [8] loss 0.75421, dsc 0.24579\n",
      "Batch train [9] loss 0.72676, dsc 0.27324\n",
      "Batch train [10] loss 0.78278, dsc 0.21722\n",
      "Epoch [246] train done\n",
      "Batch eval [1] loss 0.74887, dsc 0.25113\n",
      "Batch eval [2] loss 0.77347, dsc 0.22653\n",
      "Batch eval [3] loss 0.75519, dsc 0.24481\n",
      "Batch eval [4] loss 0.78900, dsc 0.21100\n",
      "Batch eval [5] loss 0.75200, dsc 0.24800\n",
      "Epoch [246] valid done\n",
      "Epoch [246] T 988.33s, deltaT 4.07s, loss: train 0.75464, valid 0.76370, dsc: train 0.24536, valid 0.23630\n",
      "Batch train [1] loss 0.76559, dsc 0.23441\n",
      "Batch train [2] loss 0.76700, dsc 0.23300\n",
      "Batch train [3] loss 0.75228, dsc 0.24772\n",
      "Batch train [4] loss 0.74386, dsc 0.25614\n",
      "Batch train [5] loss 0.75551, dsc 0.24449\n",
      "Batch train [6] loss 0.73546, dsc 0.26454\n",
      "Batch train [7] loss 0.74526, dsc 0.25474\n",
      "Batch train [8] loss 0.76257, dsc 0.23743\n",
      "Batch train [9] loss 0.75098, dsc 0.24902\n",
      "Batch train [10] loss 0.74028, dsc 0.25972\n",
      "Epoch [247] train done\n",
      "Batch eval [1] loss 0.74616, dsc 0.25384\n",
      "Batch eval [2] loss 0.77512, dsc 0.22488\n",
      "Batch eval [3] loss 0.75170, dsc 0.24830\n",
      "Batch eval [4] loss 0.78772, dsc 0.21228\n",
      "Batch eval [5] loss 0.74801, dsc 0.25199\n",
      "Epoch [247] valid done\n",
      "Epoch [247] T 992.43s, deltaT 4.10s, loss: train 0.75188, valid 0.76174, dsc: train 0.24812, valid 0.23826\n",
      "Batch train [1] loss 0.73547, dsc 0.26453\n",
      "Batch train [2] loss 0.75612, dsc 0.24388\n",
      "Batch train [3] loss 0.75712, dsc 0.24288\n",
      "Batch train [4] loss 0.72587, dsc 0.27413\n",
      "Batch train [5] loss 0.73119, dsc 0.26881\n",
      "Batch train [6] loss 0.73192, dsc 0.26808\n",
      "Batch train [7] loss 0.77545, dsc 0.22455\n",
      "Batch train [8] loss 0.76188, dsc 0.23812\n",
      "Batch train [9] loss 0.75867, dsc 0.24133\n",
      "Batch train [10] loss 0.76427, dsc 0.23573\n",
      "Epoch [248] train done\n",
      "Batch eval [1] loss 0.74388, dsc 0.25612\n",
      "Batch eval [2] loss 0.76875, dsc 0.23125\n",
      "Batch eval [3] loss 0.75230, dsc 0.24770\n",
      "Batch eval [4] loss 0.78566, dsc 0.21434\n",
      "Batch eval [5] loss 0.75005, dsc 0.24995\n",
      "Epoch [248] valid done\n",
      "Epoch [248] T 996.41s, deltaT 3.98s, loss: train 0.74979, valid 0.76012, dsc: train 0.25021, valid 0.23988\n",
      "Batch train [1] loss 0.77395, dsc 0.22605\n",
      "Batch train [2] loss 0.72595, dsc 0.27405\n",
      "Batch train [3] loss 0.74631, dsc 0.25369\n",
      "Batch train [4] loss 0.73896, dsc 0.26104\n",
      "Batch train [5] loss 0.72347, dsc 0.27653\n",
      "Batch train [6] loss 0.77726, dsc 0.22274\n",
      "Batch train [7] loss 0.72588, dsc 0.27412\n",
      "Batch train [8] loss 0.76549, dsc 0.23451\n",
      "Batch train [9] loss 0.75180, dsc 0.24820\n",
      "Batch train [10] loss 0.75152, dsc 0.24848\n",
      "Epoch [249] train done\n",
      "Batch eval [1] loss 0.73583, dsc 0.26417\n",
      "Batch eval [2] loss 0.76554, dsc 0.23446\n",
      "Batch eval [3] loss 0.74280, dsc 0.25720\n",
      "Batch eval [4] loss 0.78214, dsc 0.21786\n",
      "Batch eval [5] loss 0.74257, dsc 0.25743\n",
      "Epoch [249] valid done\n",
      "Epoch [249] T 1000.36s, deltaT 3.95s, loss: train 0.74806, valid 0.75377, dsc: train 0.25194, valid 0.24623\n",
      "Batch train [1] loss 0.72033, dsc 0.27967\n",
      "Batch train [2] loss 0.76464, dsc 0.23536\n",
      "Batch train [3] loss 0.72025, dsc 0.27975\n",
      "Batch train [4] loss 0.73780, dsc 0.26220\n",
      "Batch train [5] loss 0.75711, dsc 0.24289\n",
      "Batch train [6] loss 0.75191, dsc 0.24809\n",
      "Batch train [7] loss 0.74889, dsc 0.25111\n",
      "Batch train [8] loss 0.74139, dsc 0.25861\n",
      "Batch train [9] loss 0.75933, dsc 0.24067\n",
      "Batch train [10] loss 0.75117, dsc 0.24883\n",
      "Epoch [250] train done\n",
      "Batch eval [1] loss 0.73855, dsc 0.26145\n",
      "Batch eval [2] loss 0.76470, dsc 0.23530\n",
      "Batch eval [3] loss 0.74620, dsc 0.25380\n",
      "Batch eval [4] loss 0.78212, dsc 0.21788\n",
      "Batch eval [5] loss 0.74186, dsc 0.25814\n",
      "Epoch [250] valid done\n",
      "Epoch [250] T 1004.42s, deltaT 4.05s, loss: train 0.74528, valid 0.75469, dsc: train 0.25472, valid 0.24531\n",
      "Batch train [1] loss 0.72693, dsc 0.27307\n",
      "Batch train [2] loss 0.75755, dsc 0.24245\n",
      "Batch train [3] loss 0.74283, dsc 0.25717\n",
      "Batch train [4] loss 0.76106, dsc 0.23894\n",
      "Batch train [5] loss 0.76780, dsc 0.23220\n",
      "Batch train [6] loss 0.75561, dsc 0.24439\n",
      "Batch train [7] loss 0.71952, dsc 0.28048\n",
      "Batch train [8] loss 0.72372, dsc 0.27628\n",
      "Batch train [9] loss 0.73461, dsc 0.26539\n",
      "Batch train [10] loss 0.74016, dsc 0.25984\n",
      "Epoch [251] train done\n",
      "Batch eval [1] loss 0.73493, dsc 0.26507\n",
      "Batch eval [2] loss 0.76048, dsc 0.23952\n",
      "Batch eval [3] loss 0.73924, dsc 0.26076\n",
      "Batch eval [4] loss 0.77382, dsc 0.22618\n",
      "Batch eval [5] loss 0.73937, dsc 0.26063\n",
      "Epoch [251] valid done\n",
      "Epoch [251] T 1008.35s, deltaT 3.93s, loss: train 0.74298, valid 0.74957, dsc: train 0.25702, valid 0.25043\n",
      "Batch train [1] loss 0.74366, dsc 0.25634\n",
      "Batch train [2] loss 0.74339, dsc 0.25661\n",
      "Batch train [3] loss 0.74468, dsc 0.25532\n",
      "Batch train [4] loss 0.73531, dsc 0.26469\n",
      "Batch train [5] loss 0.74783, dsc 0.25217\n",
      "Batch train [6] loss 0.76756, dsc 0.23244\n",
      "Batch train [7] loss 0.71545, dsc 0.28455\n",
      "Batch train [8] loss 0.73027, dsc 0.26973\n",
      "Batch train [9] loss 0.72906, dsc 0.27094\n",
      "Batch train [10] loss 0.75086, dsc 0.24914\n",
      "Epoch [252] train done\n",
      "Batch eval [1] loss 0.73502, dsc 0.26498\n",
      "Batch eval [2] loss 0.76009, dsc 0.23991\n",
      "Batch eval [3] loss 0.73689, dsc 0.26311\n",
      "Batch eval [4] loss 0.77605, dsc 0.22395\n",
      "Batch eval [5] loss 0.74111, dsc 0.25889\n",
      "Epoch [252] valid done\n",
      "Epoch [252] T 1012.37s, deltaT 4.02s, loss: train 0.74081, valid 0.74983, dsc: train 0.25919, valid 0.25017\n",
      "Batch train [1] loss 0.76104, dsc 0.23896\n",
      "Batch train [2] loss 0.72357, dsc 0.27643\n",
      "Batch train [3] loss 0.77052, dsc 0.22948\n",
      "Batch train [4] loss 0.71558, dsc 0.28442\n",
      "Batch train [5] loss 0.71738, dsc 0.28262\n",
      "Batch train [6] loss 0.71663, dsc 0.28337\n",
      "Batch train [7] loss 0.73796, dsc 0.26204\n",
      "Batch train [8] loss 0.74650, dsc 0.25350\n",
      "Batch train [9] loss 0.74120, dsc 0.25880\n",
      "Batch train [10] loss 0.75626, dsc 0.24374\n",
      "Epoch [253] train done\n",
      "Batch eval [1] loss 0.73979, dsc 0.26021\n",
      "Batch eval [2] loss 0.76739, dsc 0.23261\n",
      "Batch eval [3] loss 0.73973, dsc 0.26027\n",
      "Batch eval [4] loss 0.77931, dsc 0.22069\n",
      "Batch eval [5] loss 0.74288, dsc 0.25712\n",
      "Epoch [253] valid done\n",
      "Epoch [253] T 1016.28s, deltaT 3.91s, loss: train 0.73866, valid 0.75382, dsc: train 0.26134, valid 0.24618\n",
      "Batch train [1] loss 0.74188, dsc 0.25812\n",
      "Batch train [2] loss 0.75772, dsc 0.24228\n",
      "Batch train [3] loss 0.73924, dsc 0.26076\n",
      "Batch train [4] loss 0.71431, dsc 0.28569\n",
      "Batch train [5] loss 0.73502, dsc 0.26498\n",
      "Batch train [6] loss 0.72725, dsc 0.27275\n",
      "Batch train [7] loss 0.73977, dsc 0.26023\n",
      "Batch train [8] loss 0.72344, dsc 0.27656\n",
      "Batch train [9] loss 0.74843, dsc 0.25157\n",
      "Batch train [10] loss 0.73253, dsc 0.26747\n",
      "Epoch [254] train done\n",
      "Batch eval [1] loss 0.72993, dsc 0.27007\n",
      "Batch eval [2] loss 0.75755, dsc 0.24245\n",
      "Batch eval [3] loss 0.73346, dsc 0.26654\n",
      "Batch eval [4] loss 0.77513, dsc 0.22487\n",
      "Batch eval [5] loss 0.73485, dsc 0.26515\n",
      "Epoch [254] valid done\n",
      "Epoch [254] T 1020.29s, deltaT 4.01s, loss: train 0.73596, valid 0.74618, dsc: train 0.26404, valid 0.25382\n",
      "Batch train [1] loss 0.74923, dsc 0.25077\n",
      "Batch train [2] loss 0.71264, dsc 0.28736\n",
      "Batch train [3] loss 0.74718, dsc 0.25282\n",
      "Batch train [4] loss 0.70872, dsc 0.29128\n",
      "Batch train [5] loss 0.73541, dsc 0.26459\n",
      "Batch train [6] loss 0.73006, dsc 0.26994\n",
      "Batch train [7] loss 0.73662, dsc 0.26338\n",
      "Batch train [8] loss 0.73093, dsc 0.26907\n",
      "Batch train [9] loss 0.74586, dsc 0.25414\n",
      "Batch train [10] loss 0.74428, dsc 0.25572\n",
      "Epoch [255] train done\n",
      "Batch eval [1] loss 0.72202, dsc 0.27798\n",
      "Batch eval [2] loss 0.75243, dsc 0.24757\n",
      "Batch eval [3] loss 0.72919, dsc 0.27081\n",
      "Batch eval [4] loss 0.77031, dsc 0.22969\n",
      "Batch eval [5] loss 0.72461, dsc 0.27539\n",
      "Epoch [255] valid done\n",
      "Epoch [255] T 1024.35s, deltaT 4.05s, loss: train 0.73409, valid 0.73971, dsc: train 0.26591, valid 0.26029\n",
      "Batch train [1] loss 0.75481, dsc 0.24519\n",
      "Batch train [2] loss 0.75186, dsc 0.24814\n",
      "Batch train [3] loss 0.72225, dsc 0.27775\n",
      "Batch train [4] loss 0.73453, dsc 0.26547\n",
      "Batch train [5] loss 0.71628, dsc 0.28372\n",
      "Batch train [6] loss 0.74110, dsc 0.25890\n",
      "Batch train [7] loss 0.75601, dsc 0.24399\n",
      "Batch train [8] loss 0.71449, dsc 0.28551\n",
      "Batch train [9] loss 0.71043, dsc 0.28957\n",
      "Batch train [10] loss 0.71448, dsc 0.28552\n",
      "Epoch [256] train done\n",
      "Batch eval [1] loss 0.72037, dsc 0.27963\n",
      "Batch eval [2] loss 0.74829, dsc 0.25171\n",
      "Batch eval [3] loss 0.72603, dsc 0.27397\n",
      "Batch eval [4] loss 0.76949, dsc 0.23051\n",
      "Batch eval [5] loss 0.72389, dsc 0.27611\n",
      "Epoch [256] valid done\n",
      "Epoch [256] T 1028.37s, deltaT 4.02s, loss: train 0.73163, valid 0.73761, dsc: train 0.26837, valid 0.26239\n",
      "Batch train [1] loss 0.71261, dsc 0.28739\n",
      "Batch train [2] loss 0.73205, dsc 0.26795\n",
      "Batch train [3] loss 0.72235, dsc 0.27765\n",
      "Batch train [4] loss 0.75852, dsc 0.24148\n",
      "Batch train [5] loss 0.76612, dsc 0.23388\n",
      "Batch train [6] loss 0.71457, dsc 0.28543\n",
      "Batch train [7] loss 0.71773, dsc 0.28227\n",
      "Batch train [8] loss 0.69924, dsc 0.30076\n",
      "Batch train [9] loss 0.73730, dsc 0.26270\n",
      "Batch train [10] loss 0.74027, dsc 0.25973\n",
      "Epoch [257] train done\n",
      "Batch eval [1] loss 0.72058, dsc 0.27942\n",
      "Batch eval [2] loss 0.75063, dsc 0.24937\n",
      "Batch eval [3] loss 0.72693, dsc 0.27307\n",
      "Batch eval [4] loss 0.76438, dsc 0.23562\n",
      "Batch eval [5] loss 0.72777, dsc 0.27223\n",
      "Epoch [257] valid done\n",
      "Epoch [257] T 1032.40s, deltaT 4.03s, loss: train 0.73008, valid 0.73806, dsc: train 0.26992, valid 0.26194\n",
      "Batch train [1] loss 0.71062, dsc 0.28938\n",
      "Batch train [2] loss 0.72472, dsc 0.27528\n",
      "Batch train [3] loss 0.73647, dsc 0.26353\n",
      "Batch train [4] loss 0.73884, dsc 0.26116\n",
      "Batch train [5] loss 0.73860, dsc 0.26140\n",
      "Batch train [6] loss 0.75717, dsc 0.24283\n",
      "Batch train [7] loss 0.72444, dsc 0.27556\n",
      "Batch train [8] loss 0.72118, dsc 0.27882\n",
      "Batch train [9] loss 0.70830, dsc 0.29170\n",
      "Batch train [10] loss 0.71648, dsc 0.28352\n",
      "Epoch [258] train done\n",
      "Batch eval [1] loss 0.71931, dsc 0.28069\n",
      "Batch eval [2] loss 0.74492, dsc 0.25508\n",
      "Batch eval [3] loss 0.72629, dsc 0.27371\n",
      "Batch eval [4] loss 0.76324, dsc 0.23676\n",
      "Batch eval [5] loss 0.72306, dsc 0.27694\n",
      "Epoch [258] valid done\n",
      "Epoch [258] T 1036.43s, deltaT 4.03s, loss: train 0.72768, valid 0.73536, dsc: train 0.27232, valid 0.26464\n",
      "Batch train [1] loss 0.70730, dsc 0.29270\n",
      "Batch train [2] loss 0.72536, dsc 0.27464\n",
      "Batch train [3] loss 0.71887, dsc 0.28113\n",
      "Batch train [4] loss 0.72190, dsc 0.27810\n",
      "Batch train [5] loss 0.74918, dsc 0.25082\n",
      "Batch train [6] loss 0.72552, dsc 0.27448\n",
      "Batch train [7] loss 0.72239, dsc 0.27761\n",
      "Batch train [8] loss 0.74224, dsc 0.25776\n",
      "Batch train [9] loss 0.71645, dsc 0.28355\n",
      "Batch train [10] loss 0.72409, dsc 0.27591\n",
      "Epoch [259] train done\n",
      "Batch eval [1] loss 0.71480, dsc 0.28520\n",
      "Batch eval [2] loss 0.74631, dsc 0.25369\n",
      "Batch eval [3] loss 0.72178, dsc 0.27822\n",
      "Batch eval [4] loss 0.76131, dsc 0.23869\n",
      "Batch eval [5] loss 0.72547, dsc 0.27453\n",
      "Epoch [259] valid done\n",
      "Epoch [259] T 1040.41s, deltaT 3.98s, loss: train 0.72533, valid 0.73393, dsc: train 0.27467, valid 0.26607\n",
      "Batch train [1] loss 0.72796, dsc 0.27204\n",
      "Batch train [2] loss 0.74384, dsc 0.25616\n",
      "Batch train [3] loss 0.73120, dsc 0.26880\n",
      "Batch train [4] loss 0.74111, dsc 0.25889\n",
      "Batch train [5] loss 0.69629, dsc 0.30371\n",
      "Batch train [6] loss 0.74769, dsc 0.25231\n",
      "Batch train [7] loss 0.70074, dsc 0.29926\n",
      "Batch train [8] loss 0.70304, dsc 0.29696\n",
      "Batch train [9] loss 0.71957, dsc 0.28043\n",
      "Batch train [10] loss 0.72172, dsc 0.27828\n",
      "Epoch [260] train done\n",
      "Batch eval [1] loss 0.72262, dsc 0.27738\n",
      "Batch eval [2] loss 0.74966, dsc 0.25034\n",
      "Batch eval [3] loss 0.73152, dsc 0.26848\n",
      "Batch eval [4] loss 0.76592, dsc 0.23408\n",
      "Batch eval [5] loss 0.72863, dsc 0.27137\n",
      "Epoch [260] valid done\n",
      "Epoch [260] T 1044.41s, deltaT 4.00s, loss: train 0.72332, valid 0.73967, dsc: train 0.27668, valid 0.26033\n",
      "Batch train [1] loss 0.73687, dsc 0.26313\n",
      "Batch train [2] loss 0.72228, dsc 0.27772\n",
      "Batch train [3] loss 0.71988, dsc 0.28012\n",
      "Batch train [4] loss 0.71972, dsc 0.28028\n",
      "Batch train [5] loss 0.71303, dsc 0.28697\n",
      "Batch train [6] loss 0.70609, dsc 0.29391\n",
      "Batch train [7] loss 0.72116, dsc 0.27884\n",
      "Batch train [8] loss 0.74694, dsc 0.25306\n",
      "Batch train [9] loss 0.71103, dsc 0.28897\n",
      "Batch train [10] loss 0.70780, dsc 0.29220\n",
      "Epoch [261] train done\n",
      "Batch eval [1] loss 0.71642, dsc 0.28358\n",
      "Batch eval [2] loss 0.74304, dsc 0.25696\n",
      "Batch eval [3] loss 0.71721, dsc 0.28279\n",
      "Batch eval [4] loss 0.75987, dsc 0.24013\n",
      "Batch eval [5] loss 0.71868, dsc 0.28132\n",
      "Epoch [261] valid done\n",
      "Epoch [261] T 1048.40s, deltaT 3.99s, loss: train 0.72048, valid 0.73105, dsc: train 0.27952, valid 0.26895\n",
      "Batch train [1] loss 0.75115, dsc 0.24885\n",
      "Batch train [2] loss 0.68662, dsc 0.31338\n",
      "Batch train [3] loss 0.69718, dsc 0.30282\n",
      "Batch train [4] loss 0.73266, dsc 0.26734\n",
      "Batch train [5] loss 0.71732, dsc 0.28268\n",
      "Batch train [6] loss 0.71931, dsc 0.28069\n",
      "Batch train [7] loss 0.74187, dsc 0.25813\n",
      "Batch train [8] loss 0.71577, dsc 0.28423\n",
      "Batch train [9] loss 0.70098, dsc 0.29902\n",
      "Batch train [10] loss 0.72775, dsc 0.27225\n",
      "Epoch [262] train done\n",
      "Batch eval [1] loss 0.71956, dsc 0.28044\n",
      "Batch eval [2] loss 0.74180, dsc 0.25820\n",
      "Batch eval [3] loss 0.72003, dsc 0.27997\n",
      "Batch eval [4] loss 0.75907, dsc 0.24093\n",
      "Batch eval [5] loss 0.72114, dsc 0.27886\n",
      "Epoch [262] valid done\n",
      "Epoch [262] T 1052.42s, deltaT 4.02s, loss: train 0.71906, valid 0.73232, dsc: train 0.28094, valid 0.26768\n",
      "Batch train [1] loss 0.69925, dsc 0.30075\n",
      "Batch train [2] loss 0.73959, dsc 0.26041\n",
      "Batch train [3] loss 0.70943, dsc 0.29057\n",
      "Batch train [4] loss 0.73028, dsc 0.26972\n",
      "Batch train [5] loss 0.70401, dsc 0.29599\n",
      "Batch train [6] loss 0.73896, dsc 0.26104\n",
      "Batch train [7] loss 0.72290, dsc 0.27710\n",
      "Batch train [8] loss 0.69728, dsc 0.30272\n",
      "Batch train [9] loss 0.71879, dsc 0.28121\n",
      "Batch train [10] loss 0.70425, dsc 0.29575\n",
      "Epoch [263] train done\n",
      "Batch eval [1] loss 0.71430, dsc 0.28570\n",
      "Batch eval [2] loss 0.74226, dsc 0.25774\n",
      "Batch eval [3] loss 0.71786, dsc 0.28214\n",
      "Batch eval [4] loss 0.75823, dsc 0.24177\n",
      "Batch eval [5] loss 0.71864, dsc 0.28136\n",
      "Epoch [263] valid done\n",
      "Epoch [263] T 1056.54s, deltaT 4.12s, loss: train 0.71647, valid 0.73026, dsc: train 0.28353, valid 0.26974\n",
      "Batch train [1] loss 0.72112, dsc 0.27888\n",
      "Batch train [2] loss 0.70077, dsc 0.29923\n",
      "Batch train [3] loss 0.73361, dsc 0.26639\n",
      "Batch train [4] loss 0.71478, dsc 0.28522\n",
      "Batch train [5] loss 0.73236, dsc 0.26764\n",
      "Batch train [6] loss 0.71588, dsc 0.28412\n",
      "Batch train [7] loss 0.74732, dsc 0.25268\n",
      "Batch train [8] loss 0.69735, dsc 0.30265\n",
      "Batch train [9] loss 0.67799, dsc 0.32201\n",
      "Batch train [10] loss 0.70669, dsc 0.29331\n",
      "Epoch [264] train done\n",
      "Batch eval [1] loss 0.70617, dsc 0.29383\n",
      "Batch eval [2] loss 0.73379, dsc 0.26621\n",
      "Batch eval [3] loss 0.70757, dsc 0.29243\n",
      "Batch eval [4] loss 0.75580, dsc 0.24420\n",
      "Batch eval [5] loss 0.71111, dsc 0.28889\n",
      "Epoch [264] valid done\n",
      "Epoch [264] T 1060.62s, deltaT 4.07s, loss: train 0.71479, valid 0.72289, dsc: train 0.28521, valid 0.27711\n",
      "Batch train [1] loss 0.69686, dsc 0.30314\n",
      "Batch train [2] loss 0.73470, dsc 0.26530\n",
      "Batch train [3] loss 0.70195, dsc 0.29805\n",
      "Batch train [4] loss 0.71970, dsc 0.28030\n",
      "Batch train [5] loss 0.72886, dsc 0.27114\n",
      "Batch train [6] loss 0.70932, dsc 0.29068\n",
      "Batch train [7] loss 0.70454, dsc 0.29546\n",
      "Batch train [8] loss 0.69443, dsc 0.30557\n",
      "Batch train [9] loss 0.71130, dsc 0.28870\n",
      "Batch train [10] loss 0.72875, dsc 0.27125\n",
      "Epoch [265] train done\n",
      "Batch eval [1] loss 0.70049, dsc 0.29951\n",
      "Batch eval [2] loss 0.72771, dsc 0.27229\n",
      "Batch eval [3] loss 0.71079, dsc 0.28921\n",
      "Batch eval [4] loss 0.75001, dsc 0.24999\n",
      "Batch eval [5] loss 0.70262, dsc 0.29738\n",
      "Epoch [265] valid done\n",
      "Epoch [265] T 1064.68s, deltaT 4.07s, loss: train 0.71304, valid 0.71833, dsc: train 0.28696, valid 0.28167\n",
      "Batch train [1] loss 0.69210, dsc 0.30790\n",
      "Batch train [2] loss 0.68817, dsc 0.31183\n",
      "Batch train [3] loss 0.70932, dsc 0.29068\n",
      "Batch train [4] loss 0.68233, dsc 0.31767\n",
      "Batch train [5] loss 0.72176, dsc 0.27824\n",
      "Batch train [6] loss 0.73458, dsc 0.26542\n",
      "Batch train [7] loss 0.71095, dsc 0.28905\n",
      "Batch train [8] loss 0.71139, dsc 0.28861\n",
      "Batch train [9] loss 0.75066, dsc 0.24934\n",
      "Batch train [10] loss 0.70728, dsc 0.29272\n",
      "Epoch [266] train done\n",
      "Batch eval [1] loss 0.69873, dsc 0.30127\n",
      "Batch eval [2] loss 0.73083, dsc 0.26917\n",
      "Batch eval [3] loss 0.70790, dsc 0.29210\n",
      "Batch eval [4] loss 0.75216, dsc 0.24784\n",
      "Batch eval [5] loss 0.70567, dsc 0.29433\n",
      "Epoch [266] valid done\n",
      "Epoch [266] T 1068.81s, deltaT 4.13s, loss: train 0.71086, valid 0.71906, dsc: train 0.28914, valid 0.28094\n",
      "Batch train [1] loss 0.74809, dsc 0.25191\n",
      "Batch train [2] loss 0.68143, dsc 0.31857\n",
      "Batch train [3] loss 0.70742, dsc 0.29258\n",
      "Batch train [4] loss 0.69961, dsc 0.30039\n",
      "Batch train [5] loss 0.70797, dsc 0.29203\n",
      "Batch train [6] loss 0.73419, dsc 0.26581\n",
      "Batch train [7] loss 0.67251, dsc 0.32749\n",
      "Batch train [8] loss 0.71211, dsc 0.28789\n",
      "Batch train [9] loss 0.69714, dsc 0.30286\n",
      "Batch train [10] loss 0.72969, dsc 0.27031\n",
      "Epoch [267] train done\n",
      "Batch eval [1] loss 0.69710, dsc 0.30290\n",
      "Batch eval [2] loss 0.72894, dsc 0.27106\n",
      "Batch eval [3] loss 0.70381, dsc 0.29619\n",
      "Batch eval [4] loss 0.74737, dsc 0.25263\n",
      "Batch eval [5] loss 0.70379, dsc 0.29621\n",
      "Epoch [267] valid done\n",
      "Epoch [267] T 1072.82s, deltaT 4.01s, loss: train 0.70902, valid 0.71620, dsc: train 0.29098, valid 0.28380\n",
      "Batch train [1] loss 0.67109, dsc 0.32891\n",
      "Batch train [2] loss 0.70279, dsc 0.29721\n",
      "Batch train [3] loss 0.72566, dsc 0.27434\n",
      "Batch train [4] loss 0.71911, dsc 0.28089\n",
      "Batch train [5] loss 0.72121, dsc 0.27879\n",
      "Batch train [6] loss 0.72012, dsc 0.27988\n",
      "Batch train [7] loss 0.68661, dsc 0.31339\n",
      "Batch train [8] loss 0.67580, dsc 0.32420\n",
      "Batch train [9] loss 0.70999, dsc 0.29001\n",
      "Batch train [10] loss 0.72971, dsc 0.27029\n",
      "Epoch [268] train done\n",
      "Batch eval [1] loss 0.70436, dsc 0.29564\n",
      "Batch eval [2] loss 0.73020, dsc 0.26980\n",
      "Batch eval [3] loss 0.70986, dsc 0.29014\n",
      "Batch eval [4] loss 0.74901, dsc 0.25099\n",
      "Batch eval [5] loss 0.70564, dsc 0.29436\n",
      "Epoch [268] valid done\n",
      "Epoch [268] T 1076.82s, deltaT 4.00s, loss: train 0.70621, valid 0.71981, dsc: train 0.29379, valid 0.28019\n",
      "Batch train [1] loss 0.70893, dsc 0.29107\n",
      "Batch train [2] loss 0.72008, dsc 0.27992\n",
      "Batch train [3] loss 0.68268, dsc 0.31732\n",
      "Batch train [4] loss 0.71648, dsc 0.28352\n",
      "Batch train [5] loss 0.68642, dsc 0.31358\n",
      "Batch train [6] loss 0.70095, dsc 0.29905\n",
      "Batch train [7] loss 0.70578, dsc 0.29422\n",
      "Batch train [8] loss 0.73874, dsc 0.26126\n",
      "Batch train [9] loss 0.67512, dsc 0.32488\n",
      "Batch train [10] loss 0.70137, dsc 0.29863\n",
      "Epoch [269] train done\n",
      "Batch eval [1] loss 0.70018, dsc 0.29982\n",
      "Batch eval [2] loss 0.72892, dsc 0.27108\n",
      "Batch eval [3] loss 0.70734, dsc 0.29266\n",
      "Batch eval [4] loss 0.74557, dsc 0.25443\n",
      "Batch eval [5] loss 0.70593, dsc 0.29407\n",
      "Epoch [269] valid done\n",
      "Epoch [269] T 1080.81s, deltaT 3.99s, loss: train 0.70366, valid 0.71759, dsc: train 0.29634, valid 0.28241\n",
      "Batch train [1] loss 0.73375, dsc 0.26625\n",
      "Batch train [2] loss 0.69825, dsc 0.30175\n",
      "Batch train [3] loss 0.71292, dsc 0.28708\n",
      "Batch train [4] loss 0.69390, dsc 0.30610\n",
      "Batch train [5] loss 0.71596, dsc 0.28404\n",
      "Batch train [6] loss 0.68096, dsc 0.31904\n",
      "Batch train [7] loss 0.67812, dsc 0.32188\n",
      "Batch train [8] loss 0.70156, dsc 0.29844\n",
      "Batch train [9] loss 0.67368, dsc 0.32632\n",
      "Batch train [10] loss 0.72615, dsc 0.27385\n",
      "Epoch [270] train done\n",
      "Batch eval [1] loss 0.70484, dsc 0.29516\n",
      "Batch eval [2] loss 0.73277, dsc 0.26723\n",
      "Batch eval [3] loss 0.70683, dsc 0.29317\n",
      "Batch eval [4] loss 0.75076, dsc 0.24924\n",
      "Batch eval [5] loss 0.70915, dsc 0.29085\n",
      "Epoch [270] valid done\n",
      "Epoch [270] T 1084.80s, deltaT 3.99s, loss: train 0.70153, valid 0.72087, dsc: train 0.29847, valid 0.27913\n",
      "Batch train [1] loss 0.72626, dsc 0.27374\n",
      "Batch train [2] loss 0.70597, dsc 0.29403\n",
      "Batch train [3] loss 0.71555, dsc 0.28445\n",
      "Batch train [4] loss 0.72306, dsc 0.27694\n",
      "Batch train [5] loss 0.68128, dsc 0.31872\n",
      "Batch train [6] loss 0.69350, dsc 0.30650\n",
      "Batch train [7] loss 0.68375, dsc 0.31625\n",
      "Batch train [8] loss 0.69433, dsc 0.30567\n",
      "Batch train [9] loss 0.69783, dsc 0.30217\n",
      "Batch train [10] loss 0.67447, dsc 0.32553\n",
      "Epoch [271] train done\n",
      "Batch eval [1] loss 0.70399, dsc 0.29601\n",
      "Batch eval [2] loss 0.72951, dsc 0.27049\n",
      "Batch eval [3] loss 0.70572, dsc 0.29428\n",
      "Batch eval [4] loss 0.74371, dsc 0.25629\n",
      "Batch eval [5] loss 0.70196, dsc 0.29804\n",
      "Epoch [271] valid done\n",
      "Epoch [271] T 1088.82s, deltaT 4.02s, loss: train 0.69960, valid 0.71698, dsc: train 0.30040, valid 0.28302\n",
      "Batch train [1] loss 0.67973, dsc 0.32027\n",
      "Batch train [2] loss 0.73656, dsc 0.26344\n",
      "Batch train [3] loss 0.68493, dsc 0.31507\n",
      "Batch train [4] loss 0.67507, dsc 0.32493\n",
      "Batch train [5] loss 0.69199, dsc 0.30801\n",
      "Batch train [6] loss 0.70323, dsc 0.29677\n",
      "Batch train [7] loss 0.70826, dsc 0.29174\n",
      "Batch train [8] loss 0.68531, dsc 0.31469\n",
      "Batch train [9] loss 0.70547, dsc 0.29453\n",
      "Batch train [10] loss 0.70526, dsc 0.29474\n",
      "Epoch [272] train done\n",
      "Batch eval [1] loss 0.70752, dsc 0.29248\n",
      "Batch eval [2] loss 0.72894, dsc 0.27106\n",
      "Batch eval [3] loss 0.70129, dsc 0.29871\n",
      "Batch eval [4] loss 0.74615, dsc 0.25385\n",
      "Batch eval [5] loss 0.70084, dsc 0.29916\n",
      "Epoch [272] valid done\n",
      "Epoch [272] T 1092.85s, deltaT 4.03s, loss: train 0.69758, valid 0.71695, dsc: train 0.30242, valid 0.28305\n",
      "Batch train [1] loss 0.70918, dsc 0.29082\n",
      "Batch train [2] loss 0.71204, dsc 0.28796\n",
      "Batch train [3] loss 0.69666, dsc 0.30334\n",
      "Batch train [4] loss 0.67527, dsc 0.32473\n",
      "Batch train [5] loss 0.68721, dsc 0.31279\n",
      "Batch train [6] loss 0.67669, dsc 0.32331\n",
      "Batch train [7] loss 0.70463, dsc 0.29537\n",
      "Batch train [8] loss 0.69675, dsc 0.30325\n",
      "Batch train [9] loss 0.70433, dsc 0.29567\n",
      "Batch train [10] loss 0.69307, dsc 0.30693\n",
      "Epoch [273] train done\n",
      "Batch eval [1] loss 0.70375, dsc 0.29625\n",
      "Batch eval [2] loss 0.72601, dsc 0.27399\n",
      "Batch eval [3] loss 0.70197, dsc 0.29803\n",
      "Batch eval [4] loss 0.74297, dsc 0.25703\n",
      "Batch eval [5] loss 0.69872, dsc 0.30128\n",
      "Epoch [273] valid done\n",
      "Epoch [273] T 1096.83s, deltaT 3.98s, loss: train 0.69558, valid 0.71468, dsc: train 0.30442, valid 0.28532\n",
      "Batch train [1] loss 0.68761, dsc 0.31239\n",
      "Batch train [2] loss 0.71670, dsc 0.28330\n",
      "Batch train [3] loss 0.67425, dsc 0.32575\n",
      "Batch train [4] loss 0.69311, dsc 0.30689\n",
      "Batch train [5] loss 0.69870, dsc 0.30130\n",
      "Batch train [6] loss 0.66846, dsc 0.33154\n",
      "Batch train [7] loss 0.73105, dsc 0.26895\n",
      "Batch train [8] loss 0.67888, dsc 0.32112\n",
      "Batch train [9] loss 0.70174, dsc 0.29826\n",
      "Batch train [10] loss 0.68508, dsc 0.31492\n",
      "Epoch [274] train done\n",
      "Batch eval [1] loss 0.69573, dsc 0.30427\n",
      "Batch eval [2] loss 0.72312, dsc 0.27688\n",
      "Batch eval [3] loss 0.70010, dsc 0.29990\n",
      "Batch eval [4] loss 0.73996, dsc 0.26004\n",
      "Batch eval [5] loss 0.69737, dsc 0.30263\n",
      "Epoch [274] valid done\n",
      "Epoch [274] T 1100.88s, deltaT 4.05s, loss: train 0.69356, valid 0.71126, dsc: train 0.30644, valid 0.28874\n",
      "Batch train [1] loss 0.71612, dsc 0.28388\n",
      "Batch train [2] loss 0.70277, dsc 0.29723\n",
      "Batch train [3] loss 0.69624, dsc 0.30376\n",
      "Batch train [4] loss 0.67855, dsc 0.32145\n",
      "Batch train [5] loss 0.66117, dsc 0.33883\n",
      "Batch train [6] loss 0.68147, dsc 0.31853\n",
      "Batch train [7] loss 0.70823, dsc 0.29177\n",
      "Batch train [8] loss 0.69173, dsc 0.30827\n",
      "Batch train [9] loss 0.68224, dsc 0.31776\n",
      "Batch train [10] loss 0.69555, dsc 0.30445\n",
      "Epoch [275] train done\n",
      "Batch eval [1] loss 0.68967, dsc 0.31033\n",
      "Batch eval [2] loss 0.71912, dsc 0.28088\n",
      "Batch eval [3] loss 0.69641, dsc 0.30359\n",
      "Batch eval [4] loss 0.73811, dsc 0.26189\n",
      "Batch eval [5] loss 0.69313, dsc 0.30687\n",
      "Epoch [275] valid done\n",
      "Epoch [275] T 1104.81s, deltaT 3.93s, loss: train 0.69141, valid 0.70729, dsc: train 0.30859, valid 0.29271\n",
      "Batch train [1] loss 0.69080, dsc 0.30920\n",
      "Batch train [2] loss 0.70791, dsc 0.29209\n",
      "Batch train [3] loss 0.65325, dsc 0.34675\n",
      "Batch train [4] loss 0.71344, dsc 0.28656\n",
      "Batch train [5] loss 0.69446, dsc 0.30554\n",
      "Batch train [6] loss 0.66323, dsc 0.33677\n",
      "Batch train [7] loss 0.68875, dsc 0.31125\n",
      "Batch train [8] loss 0.70936, dsc 0.29064\n",
      "Batch train [9] loss 0.68873, dsc 0.31127\n",
      "Batch train [10] loss 0.68818, dsc 0.31182\n",
      "Epoch [276] train done\n",
      "Batch eval [1] loss 0.70252, dsc 0.29748\n",
      "Batch eval [2] loss 0.72358, dsc 0.27642\n",
      "Batch eval [3] loss 0.70547, dsc 0.29453\n",
      "Batch eval [4] loss 0.74077, dsc 0.25923\n",
      "Batch eval [5] loss 0.70330, dsc 0.29670\n",
      "Epoch [276] valid done\n",
      "Epoch [276] T 1108.86s, deltaT 4.05s, loss: train 0.68981, valid 0.71513, dsc: train 0.31019, valid 0.28487\n",
      "Batch train [1] loss 0.67574, dsc 0.32426\n",
      "Batch train [2] loss 0.70314, dsc 0.29686\n",
      "Batch train [3] loss 0.68349, dsc 0.31651\n",
      "Batch train [4] loss 0.67720, dsc 0.32280\n",
      "Batch train [5] loss 0.66370, dsc 0.33630\n",
      "Batch train [6] loss 0.70750, dsc 0.29250\n",
      "Batch train [7] loss 0.66836, dsc 0.33164\n",
      "Batch train [8] loss 0.68468, dsc 0.31532\n",
      "Batch train [9] loss 0.70302, dsc 0.29698\n",
      "Batch train [10] loss 0.70789, dsc 0.29211\n",
      "Epoch [277] train done\n",
      "Batch eval [1] loss 0.69109, dsc 0.30891\n",
      "Batch eval [2] loss 0.71705, dsc 0.28295\n",
      "Batch eval [3] loss 0.69474, dsc 0.30526\n",
      "Batch eval [4] loss 0.73355, dsc 0.26645\n",
      "Batch eval [5] loss 0.69581, dsc 0.30419\n",
      "Epoch [277] valid done\n",
      "Epoch [277] T 1112.88s, deltaT 4.02s, loss: train 0.68747, valid 0.70645, dsc: train 0.31253, valid 0.29355\n",
      "Batch train [1] loss 0.66916, dsc 0.33084\n",
      "Batch train [2] loss 0.67159, dsc 0.32841\n",
      "Batch train [3] loss 0.70644, dsc 0.29356\n",
      "Batch train [4] loss 0.66742, dsc 0.33258\n",
      "Batch train [5] loss 0.68415, dsc 0.31585\n",
      "Batch train [6] loss 0.70936, dsc 0.29064\n",
      "Batch train [7] loss 0.69473, dsc 0.30527\n",
      "Batch train [8] loss 0.66573, dsc 0.33427\n",
      "Batch train [9] loss 0.68287, dsc 0.31713\n",
      "Batch train [10] loss 0.70927, dsc 0.29073\n",
      "Epoch [278] train done\n",
      "Batch eval [1] loss 0.68919, dsc 0.31081\n",
      "Batch eval [2] loss 0.71606, dsc 0.28394\n",
      "Batch eval [3] loss 0.69790, dsc 0.30210\n",
      "Batch eval [4] loss 0.73283, dsc 0.26717\n",
      "Batch eval [5] loss 0.69167, dsc 0.30833\n",
      "Epoch [278] valid done\n",
      "Epoch [278] T 1116.90s, deltaT 4.02s, loss: train 0.68607, valid 0.70553, dsc: train 0.31393, valid 0.29447\n",
      "Batch train [1] loss 0.67370, dsc 0.32630\n",
      "Batch train [2] loss 0.66247, dsc 0.33753\n",
      "Batch train [3] loss 0.70888, dsc 0.29112\n",
      "Batch train [4] loss 0.69918, dsc 0.30082\n",
      "Batch train [5] loss 0.72909, dsc 0.27091\n",
      "Batch train [6] loss 0.65715, dsc 0.34285\n",
      "Batch train [7] loss 0.67400, dsc 0.32600\n",
      "Batch train [8] loss 0.67955, dsc 0.32045\n",
      "Batch train [9] loss 0.66974, dsc 0.33026\n",
      "Batch train [10] loss 0.68558, dsc 0.31442\n",
      "Epoch [279] train done\n",
      "Batch eval [1] loss 0.68421, dsc 0.31579\n",
      "Batch eval [2] loss 0.71343, dsc 0.28657\n",
      "Batch eval [3] loss 0.69027, dsc 0.30973\n",
      "Batch eval [4] loss 0.73118, dsc 0.26882\n",
      "Batch eval [5] loss 0.68778, dsc 0.31222\n",
      "Epoch [279] valid done\n",
      "Epoch [279] T 1120.93s, deltaT 4.03s, loss: train 0.68393, valid 0.70137, dsc: train 0.31607, valid 0.29863\n",
      "Batch train [1] loss 0.66994, dsc 0.33006\n",
      "Batch train [2] loss 0.68519, dsc 0.31481\n",
      "Batch train [3] loss 0.66004, dsc 0.33996\n",
      "Batch train [4] loss 0.69439, dsc 0.30561\n",
      "Batch train [5] loss 0.67432, dsc 0.32568\n",
      "Batch train [6] loss 0.67495, dsc 0.32505\n",
      "Batch train [7] loss 0.67225, dsc 0.32775\n",
      "Batch train [8] loss 0.70658, dsc 0.29342\n",
      "Batch train [9] loss 0.70960, dsc 0.29040\n",
      "Batch train [10] loss 0.67048, dsc 0.32952\n",
      "Epoch [280] train done\n",
      "Batch eval [1] loss 0.68434, dsc 0.31566\n",
      "Batch eval [2] loss 0.71662, dsc 0.28338\n",
      "Batch eval [3] loss 0.68781, dsc 0.31219\n",
      "Batch eval [4] loss 0.73602, dsc 0.26398\n",
      "Batch eval [5] loss 0.69514, dsc 0.30486\n",
      "Epoch [280] valid done\n",
      "Epoch [280] T 1124.94s, deltaT 4.01s, loss: train 0.68177, valid 0.70399, dsc: train 0.31823, valid 0.29601\n",
      "Batch train [1] loss 0.68885, dsc 0.31115\n",
      "Batch train [2] loss 0.67301, dsc 0.32699\n",
      "Batch train [3] loss 0.67137, dsc 0.32863\n",
      "Batch train [4] loss 0.67609, dsc 0.32391\n",
      "Batch train [5] loss 0.68072, dsc 0.31928\n",
      "Batch train [6] loss 0.67875, dsc 0.32125\n",
      "Batch train [7] loss 0.68902, dsc 0.31098\n",
      "Batch train [8] loss 0.67070, dsc 0.32930\n",
      "Batch train [9] loss 0.68795, dsc 0.31205\n",
      "Batch train [10] loss 0.67941, dsc 0.32059\n",
      "Epoch [281] train done\n",
      "Batch eval [1] loss 0.68148, dsc 0.31852\n",
      "Batch eval [2] loss 0.71113, dsc 0.28887\n",
      "Batch eval [3] loss 0.68806, dsc 0.31194\n",
      "Batch eval [4] loss 0.72914, dsc 0.27086\n",
      "Batch eval [5] loss 0.68459, dsc 0.31541\n",
      "Epoch [281] valid done\n",
      "Epoch [281] T 1128.96s, deltaT 4.01s, loss: train 0.67959, valid 0.69888, dsc: train 0.32041, valid 0.30112\n",
      "Batch train [1] loss 0.65234, dsc 0.34766\n",
      "Batch train [2] loss 0.68957, dsc 0.31043\n",
      "Batch train [3] loss 0.67655, dsc 0.32345\n",
      "Batch train [4] loss 0.68263, dsc 0.31737\n",
      "Batch train [5] loss 0.64717, dsc 0.35283\n",
      "Batch train [6] loss 0.67004, dsc 0.32996\n",
      "Batch train [7] loss 0.66217, dsc 0.33783\n",
      "Batch train [8] loss 0.69777, dsc 0.30223\n",
      "Batch train [9] loss 0.72350, dsc 0.27650\n",
      "Batch train [10] loss 0.67500, dsc 0.32500\n",
      "Epoch [282] train done\n",
      "Batch eval [1] loss 0.68531, dsc 0.31469\n",
      "Batch eval [2] loss 0.70886, dsc 0.29114\n",
      "Batch eval [3] loss 0.68718, dsc 0.31282\n",
      "Batch eval [4] loss 0.72781, dsc 0.27219\n",
      "Batch eval [5] loss 0.68780, dsc 0.31220\n",
      "Epoch [282] valid done\n",
      "Epoch [282] T 1132.95s, deltaT 4.00s, loss: train 0.67767, valid 0.69939, dsc: train 0.32233, valid 0.30061\n",
      "Batch train [1] loss 0.68095, dsc 0.31905\n",
      "Batch train [2] loss 0.67780, dsc 0.32220\n",
      "Batch train [3] loss 0.68359, dsc 0.31641\n",
      "Batch train [4] loss 0.66597, dsc 0.33403\n",
      "Batch train [5] loss 0.70733, dsc 0.29267\n",
      "Batch train [6] loss 0.68746, dsc 0.31254\n",
      "Batch train [7] loss 0.66382, dsc 0.33618\n",
      "Batch train [8] loss 0.65691, dsc 0.34309\n",
      "Batch train [9] loss 0.64274, dsc 0.35726\n",
      "Batch train [10] loss 0.68906, dsc 0.31094\n",
      "Epoch [283] train done\n",
      "Batch eval [1] loss 0.68441, dsc 0.31559\n",
      "Batch eval [2] loss 0.70528, dsc 0.29472\n",
      "Batch eval [3] loss 0.68726, dsc 0.31274\n",
      "Batch eval [4] loss 0.72797, dsc 0.27203\n",
      "Batch eval [5] loss 0.68305, dsc 0.31695\n",
      "Epoch [283] valid done\n",
      "Epoch [283] T 1136.98s, deltaT 4.03s, loss: train 0.67556, valid 0.69759, dsc: train 0.32444, valid 0.30241\n",
      "Batch train [1] loss 0.69658, dsc 0.30342\n",
      "Batch train [2] loss 0.67112, dsc 0.32888\n",
      "Batch train [3] loss 0.66487, dsc 0.33513\n",
      "Batch train [4] loss 0.66749, dsc 0.33251\n",
      "Batch train [5] loss 0.66890, dsc 0.33110\n",
      "Batch train [6] loss 0.65452, dsc 0.34548\n",
      "Batch train [7] loss 0.64585, dsc 0.35415\n",
      "Batch train [8] loss 0.69839, dsc 0.30161\n",
      "Batch train [9] loss 0.66181, dsc 0.33819\n",
      "Batch train [10] loss 0.71006, dsc 0.28994\n",
      "Epoch [284] train done\n",
      "Batch eval [1] loss 0.69103, dsc 0.30897\n",
      "Batch eval [2] loss 0.71223, dsc 0.28777\n",
      "Batch eval [3] loss 0.69032, dsc 0.30968\n",
      "Batch eval [4] loss 0.73175, dsc 0.26825\n",
      "Batch eval [5] loss 0.69238, dsc 0.30762\n",
      "Epoch [284] valid done\n",
      "Epoch [284] T 1141.04s, deltaT 4.05s, loss: train 0.67396, valid 0.70354, dsc: train 0.32604, valid 0.29646\n",
      "Batch train [1] loss 0.70572, dsc 0.29428\n",
      "Batch train [2] loss 0.68274, dsc 0.31726\n",
      "Batch train [3] loss 0.63466, dsc 0.36534\n",
      "Batch train [4] loss 0.65646, dsc 0.34354\n",
      "Batch train [5] loss 0.64900, dsc 0.35100\n",
      "Batch train [6] loss 0.67484, dsc 0.32516\n",
      "Batch train [7] loss 0.70003, dsc 0.29997\n",
      "Batch train [8] loss 0.65522, dsc 0.34478\n",
      "Batch train [9] loss 0.68429, dsc 0.31571\n",
      "Batch train [10] loss 0.67732, dsc 0.32268\n",
      "Epoch [285] train done\n",
      "Batch eval [1] loss 0.68366, dsc 0.31634\n",
      "Batch eval [2] loss 0.71027, dsc 0.28973\n",
      "Batch eval [3] loss 0.68727, dsc 0.31273\n",
      "Batch eval [4] loss 0.72499, dsc 0.27501\n",
      "Batch eval [5] loss 0.68643, dsc 0.31357\n",
      "Epoch [285] valid done\n",
      "Epoch [285] T 1145.07s, deltaT 4.03s, loss: train 0.67203, valid 0.69852, dsc: train 0.32797, valid 0.30148\n",
      "Batch train [1] loss 0.67597, dsc 0.32403\n",
      "Batch train [2] loss 0.65849, dsc 0.34151\n",
      "Batch train [3] loss 0.65391, dsc 0.34609\n",
      "Batch train [4] loss 0.67836, dsc 0.32164\n",
      "Batch train [5] loss 0.66982, dsc 0.33018\n",
      "Batch train [6] loss 0.69885, dsc 0.30115\n",
      "Batch train [7] loss 0.65570, dsc 0.34430\n",
      "Batch train [8] loss 0.66900, dsc 0.33100\n",
      "Batch train [9] loss 0.68630, dsc 0.31370\n",
      "Batch train [10] loss 0.65817, dsc 0.34183\n",
      "Epoch [286] train done\n",
      "Batch eval [1] loss 0.67461, dsc 0.32539\n",
      "Batch eval [2] loss 0.70543, dsc 0.29457\n",
      "Batch eval [3] loss 0.67746, dsc 0.32254\n",
      "Batch eval [4] loss 0.72256, dsc 0.27744\n",
      "Batch eval [5] loss 0.67741, dsc 0.32259\n",
      "Epoch [286] valid done\n",
      "Epoch [286] T 1149.06s, deltaT 3.99s, loss: train 0.67046, valid 0.69150, dsc: train 0.32954, valid 0.30850\n",
      "Batch train [1] loss 0.62782, dsc 0.37218\n",
      "Batch train [2] loss 0.67567, dsc 0.32433\n",
      "Batch train [3] loss 0.66006, dsc 0.33994\n",
      "Batch train [4] loss 0.68032, dsc 0.31968\n",
      "Batch train [5] loss 0.66114, dsc 0.33886\n",
      "Batch train [6] loss 0.65665, dsc 0.34335\n",
      "Batch train [7] loss 0.69798, dsc 0.30202\n",
      "Batch train [8] loss 0.67691, dsc 0.32309\n",
      "Batch train [9] loss 0.69276, dsc 0.30724\n",
      "Batch train [10] loss 0.66048, dsc 0.33952\n",
      "Epoch [287] train done\n",
      "Batch eval [1] loss 0.67315, dsc 0.32685\n",
      "Batch eval [2] loss 0.70240, dsc 0.29760\n",
      "Batch eval [3] loss 0.67030, dsc 0.32970\n",
      "Batch eval [4] loss 0.72301, dsc 0.27699\n",
      "Batch eval [5] loss 0.67287, dsc 0.32713\n",
      "Epoch [287] valid done\n",
      "Epoch [287] T 1153.14s, deltaT 4.07s, loss: train 0.66898, valid 0.68835, dsc: train 0.33102, valid 0.31165\n",
      "Batch train [1] loss 0.68312, dsc 0.31688\n",
      "Batch train [2] loss 0.66892, dsc 0.33108\n",
      "Batch train [3] loss 0.64309, dsc 0.35691\n",
      "Batch train [4] loss 0.68118, dsc 0.31882\n",
      "Batch train [5] loss 0.67222, dsc 0.32778\n",
      "Batch train [6] loss 0.64487, dsc 0.35513\n",
      "Batch train [7] loss 0.65731, dsc 0.34269\n",
      "Batch train [8] loss 0.69876, dsc 0.30124\n",
      "Batch train [9] loss 0.64349, dsc 0.35651\n",
      "Batch train [10] loss 0.67558, dsc 0.32442\n",
      "Epoch [288] train done\n",
      "Batch eval [1] loss 0.66338, dsc 0.33662\n",
      "Batch eval [2] loss 0.69585, dsc 0.30415\n",
      "Batch eval [3] loss 0.66649, dsc 0.33351\n",
      "Batch eval [4] loss 0.71639, dsc 0.28361\n",
      "Batch eval [5] loss 0.66704, dsc 0.33296\n",
      "Epoch [288] valid done\n",
      "Epoch [288] T 1157.20s, deltaT 4.06s, loss: train 0.66685, valid 0.68183, dsc: train 0.33315, valid 0.31817\n",
      "Batch train [1] loss 0.66700, dsc 0.33300\n",
      "Batch train [2] loss 0.68445, dsc 0.31555\n",
      "Batch train [3] loss 0.67612, dsc 0.32388\n",
      "Batch train [4] loss 0.67357, dsc 0.32643\n",
      "Batch train [5] loss 0.67856, dsc 0.32144\n",
      "Batch train [6] loss 0.67297, dsc 0.32703\n",
      "Batch train [7] loss 0.64629, dsc 0.35371\n",
      "Batch train [8] loss 0.64850, dsc 0.35150\n",
      "Batch train [9] loss 0.63492, dsc 0.36508\n",
      "Batch train [10] loss 0.66695, dsc 0.33305\n",
      "Epoch [289] train done\n",
      "Batch eval [1] loss 0.66902, dsc 0.33098\n",
      "Batch eval [2] loss 0.69914, dsc 0.30086\n",
      "Batch eval [3] loss 0.67046, dsc 0.32954\n",
      "Batch eval [4] loss 0.71872, dsc 0.28128\n",
      "Batch eval [5] loss 0.67548, dsc 0.32452\n",
      "Epoch [289] valid done\n",
      "Epoch [289] T 1161.19s, deltaT 3.99s, loss: train 0.66493, valid 0.68657, dsc: train 0.33507, valid 0.31343\n",
      "Batch train [1] loss 0.68674, dsc 0.31326\n",
      "Batch train [2] loss 0.67022, dsc 0.32978\n",
      "Batch train [3] loss 0.66855, dsc 0.33145\n",
      "Batch train [4] loss 0.67716, dsc 0.32284\n",
      "Batch train [5] loss 0.64402, dsc 0.35598\n",
      "Batch train [6] loss 0.64092, dsc 0.35908\n",
      "Batch train [7] loss 0.64992, dsc 0.35008\n",
      "Batch train [8] loss 0.67586, dsc 0.32414\n",
      "Batch train [9] loss 0.66005, dsc 0.33995\n",
      "Batch train [10] loss 0.65544, dsc 0.34456\n",
      "Epoch [290] train done\n",
      "Batch eval [1] loss 0.66851, dsc 0.33149\n",
      "Batch eval [2] loss 0.69531, dsc 0.30469\n",
      "Batch eval [3] loss 0.66806, dsc 0.33194\n",
      "Batch eval [4] loss 0.71637, dsc 0.28363\n",
      "Batch eval [5] loss 0.66288, dsc 0.33712\n",
      "Epoch [290] valid done\n",
      "Epoch [290] T 1165.25s, deltaT 4.06s, loss: train 0.66289, valid 0.68223, dsc: train 0.33711, valid 0.31777\n",
      "Batch train [1] loss 0.66049, dsc 0.33951\n",
      "Batch train [2] loss 0.64028, dsc 0.35972\n",
      "Batch train [3] loss 0.69570, dsc 0.30430\n",
      "Batch train [4] loss 0.65128, dsc 0.34872\n",
      "Batch train [5] loss 0.68230, dsc 0.31770\n",
      "Batch train [6] loss 0.67276, dsc 0.32724\n",
      "Batch train [7] loss 0.64158, dsc 0.35842\n",
      "Batch train [8] loss 0.66409, dsc 0.33591\n",
      "Batch train [9] loss 0.64490, dsc 0.35510\n",
      "Batch train [10] loss 0.65585, dsc 0.34415\n",
      "Epoch [291] train done\n",
      "Batch eval [1] loss 0.66975, dsc 0.33025\n",
      "Batch eval [2] loss 0.69503, dsc 0.30497\n",
      "Batch eval [3] loss 0.67029, dsc 0.32971\n",
      "Batch eval [4] loss 0.71289, dsc 0.28711\n",
      "Batch eval [5] loss 0.67451, dsc 0.32549\n",
      "Epoch [291] valid done\n",
      "Epoch [291] T 1169.43s, deltaT 4.18s, loss: train 0.66092, valid 0.68449, dsc: train 0.33908, valid 0.31551\n",
      "Batch train [1] loss 0.68403, dsc 0.31597\n",
      "Batch train [2] loss 0.68018, dsc 0.31982\n",
      "Batch train [3] loss 0.66590, dsc 0.33410\n",
      "Batch train [4] loss 0.65007, dsc 0.34993\n",
      "Batch train [5] loss 0.67622, dsc 0.32378\n",
      "Batch train [6] loss 0.65328, dsc 0.34672\n",
      "Batch train [7] loss 0.65951, dsc 0.34049\n",
      "Batch train [8] loss 0.65692, dsc 0.34308\n",
      "Batch train [9] loss 0.61729, dsc 0.38271\n",
      "Batch train [10] loss 0.64760, dsc 0.35240\n",
      "Epoch [292] train done\n",
      "Batch eval [1] loss 0.66364, dsc 0.33636\n",
      "Batch eval [2] loss 0.68731, dsc 0.31269\n",
      "Batch eval [3] loss 0.66247, dsc 0.33753\n",
      "Batch eval [4] loss 0.70976, dsc 0.29024\n",
      "Batch eval [5] loss 0.66430, dsc 0.33570\n",
      "Epoch [292] valid done\n",
      "Epoch [292] T 1173.68s, deltaT 4.25s, loss: train 0.65910, valid 0.67749, dsc: train 0.34090, valid 0.32251\n",
      "Batch train [1] loss 0.65470, dsc 0.34530\n",
      "Batch train [2] loss 0.63220, dsc 0.36780\n",
      "Batch train [3] loss 0.65620, dsc 0.34380\n",
      "Batch train [4] loss 0.63848, dsc 0.36152\n",
      "Batch train [5] loss 0.67387, dsc 0.32613\n",
      "Batch train [6] loss 0.65361, dsc 0.34639\n",
      "Batch train [7] loss 0.67439, dsc 0.32561\n",
      "Batch train [8] loss 0.66191, dsc 0.33809\n",
      "Batch train [9] loss 0.68343, dsc 0.31657\n",
      "Batch train [10] loss 0.64048, dsc 0.35952\n",
      "Epoch [293] train done\n",
      "Batch eval [1] loss 0.66978, dsc 0.33022\n",
      "Batch eval [2] loss 0.69525, dsc 0.30475\n",
      "Batch eval [3] loss 0.67350, dsc 0.32650\n",
      "Batch eval [4] loss 0.71864, dsc 0.28136\n",
      "Batch eval [5] loss 0.67577, dsc 0.32423\n",
      "Epoch [293] valid done\n",
      "Epoch [293] T 1177.75s, deltaT 4.07s, loss: train 0.65693, valid 0.68659, dsc: train 0.34307, valid 0.31341\n",
      "Batch train [1] loss 0.65404, dsc 0.34596\n",
      "Batch train [2] loss 0.67617, dsc 0.32383\n",
      "Batch train [3] loss 0.66023, dsc 0.33977\n",
      "Batch train [4] loss 0.67012, dsc 0.32988\n",
      "Batch train [5] loss 0.65412, dsc 0.34588\n",
      "Batch train [6] loss 0.62704, dsc 0.37296\n",
      "Batch train [7] loss 0.65254, dsc 0.34746\n",
      "Batch train [8] loss 0.67392, dsc 0.32608\n",
      "Batch train [9] loss 0.62713, dsc 0.37287\n",
      "Batch train [10] loss 0.65475, dsc 0.34525\n",
      "Epoch [294] train done\n",
      "Batch eval [1] loss 0.66119, dsc 0.33881\n",
      "Batch eval [2] loss 0.68634, dsc 0.31366\n",
      "Batch eval [3] loss 0.66489, dsc 0.33511\n",
      "Batch eval [4] loss 0.70933, dsc 0.29067\n",
      "Batch eval [5] loss 0.66271, dsc 0.33729\n",
      "Epoch [294] valid done\n",
      "Epoch [294] T 1181.72s, deltaT 3.96s, loss: train 0.65501, valid 0.67689, dsc: train 0.34499, valid 0.32311\n",
      "Batch train [1] loss 0.65197, dsc 0.34803\n",
      "Batch train [2] loss 0.65116, dsc 0.34884\n",
      "Batch train [3] loss 0.63222, dsc 0.36778\n",
      "Batch train [4] loss 0.65908, dsc 0.34092\n",
      "Batch train [5] loss 0.66577, dsc 0.33423\n",
      "Batch train [6] loss 0.67675, dsc 0.32325\n",
      "Batch train [7] loss 0.63977, dsc 0.36023\n",
      "Batch train [8] loss 0.66520, dsc 0.33480\n",
      "Batch train [9] loss 0.63784, dsc 0.36216\n",
      "Batch train [10] loss 0.65363, dsc 0.34637\n",
      "Epoch [295] train done\n",
      "Batch eval [1] loss 0.66092, dsc 0.33908\n",
      "Batch eval [2] loss 0.69396, dsc 0.30604\n",
      "Batch eval [3] loss 0.66209, dsc 0.33791\n",
      "Batch eval [4] loss 0.71247, dsc 0.28753\n",
      "Batch eval [5] loss 0.66862, dsc 0.33138\n",
      "Epoch [295] valid done\n",
      "Epoch [295] T 1185.62s, deltaT 3.90s, loss: train 0.65334, valid 0.67961, dsc: train 0.34666, valid 0.32039\n",
      "Batch train [1] loss 0.62574, dsc 0.37426\n",
      "Batch train [2] loss 0.62499, dsc 0.37501\n",
      "Batch train [3] loss 0.64559, dsc 0.35441\n",
      "Batch train [4] loss 0.68812, dsc 0.31188\n",
      "Batch train [5] loss 0.64293, dsc 0.35707\n",
      "Batch train [6] loss 0.66380, dsc 0.33620\n",
      "Batch train [7] loss 0.64755, dsc 0.35245\n",
      "Batch train [8] loss 0.65584, dsc 0.34416\n",
      "Batch train [9] loss 0.68555, dsc 0.31445\n",
      "Batch train [10] loss 0.63410, dsc 0.36590\n",
      "Epoch [296] train done\n",
      "Batch eval [1] loss 0.66848, dsc 0.33152\n",
      "Batch eval [2] loss 0.68969, dsc 0.31031\n",
      "Batch eval [3] loss 0.66311, dsc 0.33689\n",
      "Batch eval [4] loss 0.71009, dsc 0.28991\n",
      "Batch eval [5] loss 0.66198, dsc 0.33802\n",
      "Epoch [296] valid done\n",
      "Epoch [296] T 1189.61s, deltaT 3.98s, loss: train 0.65142, valid 0.67867, dsc: train 0.34858, valid 0.32133\n",
      "Batch train [1] loss 0.65834, dsc 0.34166\n",
      "Batch train [2] loss 0.64434, dsc 0.35566\n",
      "Batch train [3] loss 0.66728, dsc 0.33272\n",
      "Batch train [4] loss 0.62412, dsc 0.37588\n",
      "Batch train [5] loss 0.68281, dsc 0.31719\n",
      "Batch train [6] loss 0.66038, dsc 0.33962\n",
      "Batch train [7] loss 0.63541, dsc 0.36459\n",
      "Batch train [8] loss 0.62505, dsc 0.37495\n",
      "Batch train [9] loss 0.65097, dsc 0.34903\n",
      "Batch train [10] loss 0.64365, dsc 0.35635\n",
      "Epoch [297] train done\n",
      "Batch eval [1] loss 0.66070, dsc 0.33930\n",
      "Batch eval [2] loss 0.68712, dsc 0.31288\n",
      "Batch eval [3] loss 0.66481, dsc 0.33519\n",
      "Batch eval [4] loss 0.70720, dsc 0.29280\n",
      "Batch eval [5] loss 0.66272, dsc 0.33728\n",
      "Epoch [297] valid done\n",
      "Epoch [297] T 1193.51s, deltaT 3.91s, loss: train 0.64924, valid 0.67651, dsc: train 0.35076, valid 0.32349\n",
      "Batch train [1] loss 0.62838, dsc 0.37162\n",
      "Batch train [2] loss 0.66276, dsc 0.33724\n",
      "Batch train [3] loss 0.67824, dsc 0.32176\n",
      "Batch train [4] loss 0.67456, dsc 0.32544\n",
      "Batch train [5] loss 0.63707, dsc 0.36293\n",
      "Batch train [6] loss 0.63565, dsc 0.36435\n",
      "Batch train [7] loss 0.64396, dsc 0.35604\n",
      "Batch train [8] loss 0.62366, dsc 0.37634\n",
      "Batch train [9] loss 0.63555, dsc 0.36445\n",
      "Batch train [10] loss 0.65570, dsc 0.34430\n",
      "Epoch [298] train done\n",
      "Batch eval [1] loss 0.66544, dsc 0.33456\n",
      "Batch eval [2] loss 0.68837, dsc 0.31163\n",
      "Batch eval [3] loss 0.66181, dsc 0.33819\n",
      "Batch eval [4] loss 0.70465, dsc 0.29535\n",
      "Batch eval [5] loss 0.66282, dsc 0.33718\n",
      "Epoch [298] valid done\n",
      "Epoch [298] T 1197.62s, deltaT 4.10s, loss: train 0.64755, valid 0.67662, dsc: train 0.35245, valid 0.32338\n",
      "Batch train [1] loss 0.65014, dsc 0.34986\n",
      "Batch train [2] loss 0.68145, dsc 0.31855\n",
      "Batch train [3] loss 0.59979, dsc 0.40021\n",
      "Batch train [4] loss 0.62934, dsc 0.37066\n",
      "Batch train [5] loss 0.65155, dsc 0.34845\n",
      "Batch train [6] loss 0.62177, dsc 0.37823\n",
      "Batch train [7] loss 0.65000, dsc 0.35000\n",
      "Batch train [8] loss 0.67025, dsc 0.32975\n",
      "Batch train [9] loss 0.64638, dsc 0.35362\n",
      "Batch train [10] loss 0.66264, dsc 0.33736\n",
      "Epoch [299] train done\n",
      "Batch eval [1] loss 0.66008, dsc 0.33992\n",
      "Batch eval [2] loss 0.68279, dsc 0.31721\n",
      "Batch eval [3] loss 0.65876, dsc 0.34124\n",
      "Batch eval [4] loss 0.70631, dsc 0.29369\n",
      "Batch eval [5] loss 0.66413, dsc 0.33587\n",
      "Epoch [299] valid done\n",
      "Epoch [299] T 1201.84s, deltaT 4.23s, loss: train 0.64633, valid 0.67441, dsc: train 0.35367, valid 0.32559\n",
      "Batch train [1] loss 0.62547, dsc 0.37453\n",
      "Batch train [2] loss 0.66291, dsc 0.33709\n",
      "Batch train [3] loss 0.64932, dsc 0.35068\n",
      "Batch train [4] loss 0.66470, dsc 0.33530\n",
      "Batch train [5] loss 0.64785, dsc 0.35215\n",
      "Batch train [6] loss 0.63320, dsc 0.36680\n",
      "Batch train [7] loss 0.63843, dsc 0.36157\n",
      "Batch train [8] loss 0.63750, dsc 0.36250\n",
      "Batch train [9] loss 0.64573, dsc 0.35427\n",
      "Batch train [10] loss 0.63519, dsc 0.36481\n",
      "Epoch [300] train done\n",
      "Batch eval [1] loss 0.65190, dsc 0.34810\n",
      "Batch eval [2] loss 0.68363, dsc 0.31637\n",
      "Batch eval [3] loss 0.65698, dsc 0.34302\n",
      "Batch eval [4] loss 0.70478, dsc 0.29522\n",
      "Batch eval [5] loss 0.66519, dsc 0.33481\n",
      "Epoch [300] valid done\n",
      "Epoch [300] T 1206.07s, deltaT 4.22s, loss: train 0.64403, valid 0.67250, dsc: train 0.35597, valid 0.32750\n",
      "Batch train [1] loss 0.68385, dsc 0.31615\n",
      "Batch train [2] loss 0.65335, dsc 0.34665\n",
      "Batch train [3] loss 0.62152, dsc 0.37848\n",
      "Batch train [4] loss 0.61714, dsc 0.38286\n",
      "Batch train [5] loss 0.64895, dsc 0.35105\n",
      "Batch train [6] loss 0.65199, dsc 0.34801\n",
      "Batch train [7] loss 0.63621, dsc 0.36379\n",
      "Batch train [8] loss 0.66098, dsc 0.33902\n",
      "Batch train [9] loss 0.62363, dsc 0.37637\n",
      "Batch train [10] loss 0.62695, dsc 0.37305\n",
      "Epoch [301] train done\n",
      "Batch eval [1] loss 0.65351, dsc 0.34649\n",
      "Batch eval [2] loss 0.67828, dsc 0.32172\n",
      "Batch eval [3] loss 0.65531, dsc 0.34469\n",
      "Batch eval [4] loss 0.70058, dsc 0.29942\n",
      "Batch eval [5] loss 0.65630, dsc 0.34370\n",
      "Epoch [301] valid done\n",
      "Epoch [301] T 1210.29s, deltaT 4.22s, loss: train 0.64246, valid 0.66880, dsc: train 0.35754, valid 0.33120\n",
      "Batch train [1] loss 0.61591, dsc 0.38409\n",
      "Batch train [2] loss 0.64078, dsc 0.35922\n",
      "Batch train [3] loss 0.68085, dsc 0.31915\n",
      "Batch train [4] loss 0.63087, dsc 0.36913\n",
      "Batch train [5] loss 0.61203, dsc 0.38797\n",
      "Batch train [6] loss 0.62666, dsc 0.37334\n",
      "Batch train [7] loss 0.66059, dsc 0.33941\n",
      "Batch train [8] loss 0.63704, dsc 0.36296\n",
      "Batch train [9] loss 0.66534, dsc 0.33466\n",
      "Batch train [10] loss 0.64059, dsc 0.35941\n",
      "Epoch [302] train done\n",
      "Batch eval [1] loss 0.65430, dsc 0.34570\n",
      "Batch eval [2] loss 0.68281, dsc 0.31719\n",
      "Batch eval [3] loss 0.65377, dsc 0.34623\n",
      "Batch eval [4] loss 0.70284, dsc 0.29716\n",
      "Batch eval [5] loss 0.65905, dsc 0.34095\n",
      "Epoch [302] valid done\n",
      "Epoch [302] T 1214.49s, deltaT 4.20s, loss: train 0.64107, valid 0.67055, dsc: train 0.35893, valid 0.32945\n",
      "Batch train [1] loss 0.66597, dsc 0.33403\n",
      "Batch train [2] loss 0.63771, dsc 0.36229\n",
      "Batch train [3] loss 0.59639, dsc 0.40361\n",
      "Batch train [4] loss 0.63571, dsc 0.36429\n",
      "Batch train [5] loss 0.66074, dsc 0.33926\n",
      "Batch train [6] loss 0.66036, dsc 0.33964\n",
      "Batch train [7] loss 0.63292, dsc 0.36708\n",
      "Batch train [8] loss 0.61547, dsc 0.38453\n",
      "Batch train [9] loss 0.63928, dsc 0.36072\n",
      "Batch train [10] loss 0.64951, dsc 0.35049\n",
      "Epoch [303] train done\n",
      "Batch eval [1] loss 0.64855, dsc 0.35145\n",
      "Batch eval [2] loss 0.67644, dsc 0.32356\n",
      "Batch eval [3] loss 0.64857, dsc 0.35143\n",
      "Batch eval [4] loss 0.69742, dsc 0.30258\n",
      "Batch eval [5] loss 0.65604, dsc 0.34396\n",
      "Epoch [303] valid done\n",
      "Epoch [303] T 1218.72s, deltaT 4.23s, loss: train 0.63941, valid 0.66540, dsc: train 0.36059, valid 0.33460\n",
      "Batch train [1] loss 0.62582, dsc 0.37418\n",
      "Batch train [2] loss 0.64135, dsc 0.35865\n",
      "Batch train [3] loss 0.63732, dsc 0.36268\n",
      "Batch train [4] loss 0.66856, dsc 0.33144\n",
      "Batch train [5] loss 0.63135, dsc 0.36865\n",
      "Batch train [6] loss 0.66483, dsc 0.33517\n",
      "Batch train [7] loss 0.60261, dsc 0.39739\n",
      "Batch train [8] loss 0.62923, dsc 0.37077\n",
      "Batch train [9] loss 0.64200, dsc 0.35800\n",
      "Batch train [10] loss 0.63026, dsc 0.36974\n",
      "Epoch [304] train done\n",
      "Batch eval [1] loss 0.65434, dsc 0.34566\n",
      "Batch eval [2] loss 0.67991, dsc 0.32009\n",
      "Batch eval [3] loss 0.65588, dsc 0.34412\n",
      "Batch eval [4] loss 0.69959, dsc 0.30041\n",
      "Batch eval [5] loss 0.65837, dsc 0.34163\n",
      "Epoch [304] valid done\n",
      "Epoch [304] T 1222.70s, deltaT 3.98s, loss: train 0.63733, valid 0.66961, dsc: train 0.36267, valid 0.33039\n",
      "Batch train [1] loss 0.67596, dsc 0.32404\n",
      "Batch train [2] loss 0.63390, dsc 0.36610\n",
      "Batch train [3] loss 0.64037, dsc 0.35963\n",
      "Batch train [4] loss 0.63126, dsc 0.36874\n",
      "Batch train [5] loss 0.62120, dsc 0.37880\n",
      "Batch train [6] loss 0.63087, dsc 0.36913\n",
      "Batch train [7] loss 0.64885, dsc 0.35115\n",
      "Batch train [8] loss 0.60741, dsc 0.39259\n",
      "Batch train [9] loss 0.62505, dsc 0.37495\n",
      "Batch train [10] loss 0.64064, dsc 0.35936\n",
      "Epoch [305] train done\n",
      "Batch eval [1] loss 0.65735, dsc 0.34265\n",
      "Batch eval [2] loss 0.68438, dsc 0.31562\n",
      "Batch eval [3] loss 0.65563, dsc 0.34437\n",
      "Batch eval [4] loss 0.70153, dsc 0.29847\n",
      "Batch eval [5] loss 0.66087, dsc 0.33913\n",
      "Epoch [305] valid done\n",
      "Epoch [305] T 1226.79s, deltaT 4.09s, loss: train 0.63555, valid 0.67195, dsc: train 0.36445, valid 0.32805\n",
      "Batch train [1] loss 0.63935, dsc 0.36065\n",
      "Batch train [2] loss 0.61972, dsc 0.38028\n",
      "Batch train [3] loss 0.64690, dsc 0.35310\n",
      "Batch train [4] loss 0.65979, dsc 0.34021\n",
      "Batch train [5] loss 0.61182, dsc 0.38818\n",
      "Batch train [6] loss 0.62911, dsc 0.37089\n",
      "Batch train [7] loss 0.62787, dsc 0.37213\n",
      "Batch train [8] loss 0.63526, dsc 0.36474\n",
      "Batch train [9] loss 0.64139, dsc 0.35861\n",
      "Batch train [10] loss 0.62461, dsc 0.37539\n",
      "Epoch [306] train done\n",
      "Batch eval [1] loss 0.65011, dsc 0.34989\n",
      "Batch eval [2] loss 0.66994, dsc 0.33006\n",
      "Batch eval [3] loss 0.64699, dsc 0.35301\n",
      "Batch eval [4] loss 0.69305, dsc 0.30695\n",
      "Batch eval [5] loss 0.65163, dsc 0.34837\n",
      "Epoch [306] valid done\n",
      "Epoch [306] T 1230.82s, deltaT 4.03s, loss: train 0.63358, valid 0.66234, dsc: train 0.36642, valid 0.33766\n",
      "Batch train [1] loss 0.64182, dsc 0.35818\n",
      "Batch train [2] loss 0.64720, dsc 0.35280\n",
      "Batch train [3] loss 0.62327, dsc 0.37673\n",
      "Batch train [4] loss 0.65737, dsc 0.34263\n",
      "Batch train [5] loss 0.64558, dsc 0.35442\n",
      "Batch train [6] loss 0.60099, dsc 0.39901\n",
      "Batch train [7] loss 0.62804, dsc 0.37196\n",
      "Batch train [8] loss 0.67289, dsc 0.32711\n",
      "Batch train [9] loss 0.59612, dsc 0.40388\n",
      "Batch train [10] loss 0.61568, dsc 0.38432\n",
      "Epoch [307] train done\n",
      "Batch eval [1] loss 0.64261, dsc 0.35739\n",
      "Batch eval [2] loss 0.66794, dsc 0.33206\n",
      "Batch eval [3] loss 0.64361, dsc 0.35639\n",
      "Batch eval [4] loss 0.69008, dsc 0.30992\n",
      "Batch eval [5] loss 0.64423, dsc 0.35577\n",
      "Epoch [307] valid done\n",
      "Epoch [307] T 1234.77s, deltaT 3.95s, loss: train 0.63290, valid 0.65769, dsc: train 0.36710, valid 0.34231\n",
      "Batch train [1] loss 0.59402, dsc 0.40598\n",
      "Batch train [2] loss 0.61458, dsc 0.38542\n",
      "Batch train [3] loss 0.61039, dsc 0.38961\n",
      "Batch train [4] loss 0.64740, dsc 0.35260\n",
      "Batch train [5] loss 0.63832, dsc 0.36168\n",
      "Batch train [6] loss 0.64181, dsc 0.35819\n",
      "Batch train [7] loss 0.62433, dsc 0.37567\n",
      "Batch train [8] loss 0.67196, dsc 0.32804\n",
      "Batch train [9] loss 0.62884, dsc 0.37116\n",
      "Batch train [10] loss 0.64374, dsc 0.35626\n",
      "Epoch [308] train done\n",
      "Batch eval [1] loss 0.64060, dsc 0.35940\n",
      "Batch eval [2] loss 0.67644, dsc 0.32356\n",
      "Batch eval [3] loss 0.63662, dsc 0.36338\n",
      "Batch eval [4] loss 0.69298, dsc 0.30702\n",
      "Batch eval [5] loss 0.65047, dsc 0.34953\n",
      "Epoch [308] valid done\n",
      "Epoch [308] T 1238.77s, deltaT 4.00s, loss: train 0.63154, valid 0.65942, dsc: train 0.36846, valid 0.34058\n",
      "Batch train [1] loss 0.62602, dsc 0.37398\n",
      "Batch train [2] loss 0.63952, dsc 0.36048\n",
      "Batch train [3] loss 0.63109, dsc 0.36891\n",
      "Batch train [4] loss 0.60815, dsc 0.39185\n",
      "Batch train [5] loss 0.63679, dsc 0.36321\n",
      "Batch train [6] loss 0.61920, dsc 0.38080\n",
      "Batch train [7] loss 0.62704, dsc 0.37296\n",
      "Batch train [8] loss 0.64912, dsc 0.35088\n",
      "Batch train [9] loss 0.63419, dsc 0.36581\n",
      "Batch train [10] loss 0.62665, dsc 0.37335\n",
      "Epoch [309] train done\n",
      "Batch eval [1] loss 0.64988, dsc 0.35012\n",
      "Batch eval [2] loss 0.67368, dsc 0.32632\n",
      "Batch eval [3] loss 0.65599, dsc 0.34401\n",
      "Batch eval [4] loss 0.68968, dsc 0.31032\n",
      "Batch eval [5] loss 0.65026, dsc 0.34974\n",
      "Epoch [309] valid done\n",
      "Epoch [309] T 1242.78s, deltaT 4.00s, loss: train 0.62978, valid 0.66390, dsc: train 0.37022, valid 0.33610\n",
      "Batch train [1] loss 0.63501, dsc 0.36499\n",
      "Batch train [2] loss 0.62778, dsc 0.37222\n",
      "Batch train [3] loss 0.63345, dsc 0.36655\n",
      "Batch train [4] loss 0.63024, dsc 0.36976\n",
      "Batch train [5] loss 0.63586, dsc 0.36414\n",
      "Batch train [6] loss 0.63694, dsc 0.36306\n",
      "Batch train [7] loss 0.65650, dsc 0.34350\n",
      "Batch train [8] loss 0.58947, dsc 0.41053\n",
      "Batch train [9] loss 0.62665, dsc 0.37335\n",
      "Batch train [10] loss 0.61359, dsc 0.38641\n",
      "Epoch [310] train done\n",
      "Batch eval [1] loss 0.63724, dsc 0.36276\n",
      "Batch eval [2] loss 0.66784, dsc 0.33216\n",
      "Batch eval [3] loss 0.64023, dsc 0.35977\n",
      "Batch eval [4] loss 0.68141, dsc 0.31859\n",
      "Batch eval [5] loss 0.64371, dsc 0.35629\n",
      "Epoch [310] valid done\n",
      "Epoch [310] T 1246.85s, deltaT 4.07s, loss: train 0.62855, valid 0.65409, dsc: train 0.37145, valid 0.34591\n",
      "Batch train [1] loss 0.65314, dsc 0.34686\n",
      "Batch train [2] loss 0.62012, dsc 0.37988\n",
      "Batch train [3] loss 0.65272, dsc 0.34728\n",
      "Batch train [4] loss 0.64575, dsc 0.35425\n",
      "Batch train [5] loss 0.57936, dsc 0.42064\n",
      "Batch train [6] loss 0.63326, dsc 0.36674\n",
      "Batch train [7] loss 0.61331, dsc 0.38669\n",
      "Batch train [8] loss 0.61434, dsc 0.38566\n",
      "Batch train [9] loss 0.59945, dsc 0.40055\n",
      "Batch train [10] loss 0.66125, dsc 0.33875\n",
      "Epoch [311] train done\n",
      "Batch eval [1] loss 0.63373, dsc 0.36627\n",
      "Batch eval [2] loss 0.66472, dsc 0.33528\n",
      "Batch eval [3] loss 0.63612, dsc 0.36388\n",
      "Batch eval [4] loss 0.68367, dsc 0.31633\n",
      "Batch eval [5] loss 0.64067, dsc 0.35933\n",
      "Epoch [311] valid done\n",
      "Epoch [311] T 1250.81s, deltaT 3.96s, loss: train 0.62727, valid 0.65178, dsc: train 0.37273, valid 0.34822\n",
      "Batch train [1] loss 0.62826, dsc 0.37174\n",
      "Batch train [2] loss 0.60426, dsc 0.39574\n",
      "Batch train [3] loss 0.66933, dsc 0.33067\n",
      "Batch train [4] loss 0.58606, dsc 0.41394\n",
      "Batch train [5] loss 0.65678, dsc 0.34322\n",
      "Batch train [6] loss 0.61407, dsc 0.38593\n",
      "Batch train [7] loss 0.64360, dsc 0.35640\n",
      "Batch train [8] loss 0.63026, dsc 0.36974\n",
      "Batch train [9] loss 0.59991, dsc 0.40009\n",
      "Batch train [10] loss 0.62075, dsc 0.37925\n",
      "Epoch [312] train done\n",
      "Batch eval [1] loss 0.63194, dsc 0.36806\n",
      "Batch eval [2] loss 0.66593, dsc 0.33407\n",
      "Batch eval [3] loss 0.63847, dsc 0.36153\n",
      "Batch eval [4] loss 0.68365, dsc 0.31635\n",
      "Batch eval [5] loss 0.64258, dsc 0.35742\n",
      "Epoch [312] valid done\n",
      "Epoch [312] T 1254.84s, deltaT 4.03s, loss: train 0.62533, valid 0.65251, dsc: train 0.37467, valid 0.34749\n",
      "Batch train [1] loss 0.65425, dsc 0.34575\n",
      "Batch train [2] loss 0.65079, dsc 0.34921\n",
      "Batch train [3] loss 0.61008, dsc 0.38992\n",
      "Batch train [4] loss 0.59070, dsc 0.40930\n",
      "Batch train [5] loss 0.64681, dsc 0.35319\n",
      "Batch train [6] loss 0.60727, dsc 0.39273\n",
      "Batch train [7] loss 0.64111, dsc 0.35889\n",
      "Batch train [8] loss 0.59939, dsc 0.40061\n",
      "Batch train [9] loss 0.61784, dsc 0.38216\n",
      "Batch train [10] loss 0.61541, dsc 0.38459\n",
      "Epoch [313] train done\n",
      "Batch eval [1] loss 0.64742, dsc 0.35258\n",
      "Batch eval [2] loss 0.67502, dsc 0.32498\n",
      "Batch eval [3] loss 0.64069, dsc 0.35931\n",
      "Batch eval [4] loss 0.68758, dsc 0.31242\n",
      "Batch eval [5] loss 0.64646, dsc 0.35354\n",
      "Epoch [313] valid done\n",
      "Epoch [313] T 1258.86s, deltaT 4.02s, loss: train 0.62337, valid 0.65943, dsc: train 0.37663, valid 0.34057\n",
      "Batch train [1] loss 0.60700, dsc 0.39300\n",
      "Batch train [2] loss 0.62394, dsc 0.37606\n",
      "Batch train [3] loss 0.64694, dsc 0.35306\n",
      "Batch train [4] loss 0.61682, dsc 0.38318\n",
      "Batch train [5] loss 0.62862, dsc 0.37138\n",
      "Batch train [6] loss 0.61683, dsc 0.38317\n",
      "Batch train [7] loss 0.61410, dsc 0.38590\n",
      "Batch train [8] loss 0.63212, dsc 0.36788\n",
      "Batch train [9] loss 0.62204, dsc 0.37796\n",
      "Batch train [10] loss 0.59889, dsc 0.40111\n",
      "Epoch [314] train done\n",
      "Batch eval [1] loss 0.64132, dsc 0.35868\n",
      "Batch eval [2] loss 0.66554, dsc 0.33446\n",
      "Batch eval [3] loss 0.64383, dsc 0.35617\n",
      "Batch eval [4] loss 0.68285, dsc 0.31715\n",
      "Batch eval [5] loss 0.64312, dsc 0.35688\n",
      "Epoch [314] valid done\n",
      "Epoch [314] T 1262.86s, deltaT 4.00s, loss: train 0.62073, valid 0.65533, dsc: train 0.37927, valid 0.34467\n",
      "Batch train [1] loss 0.61412, dsc 0.38588\n",
      "Batch train [2] loss 0.59164, dsc 0.40836\n",
      "Batch train [3] loss 0.61477, dsc 0.38523\n",
      "Batch train [4] loss 0.59569, dsc 0.40431\n",
      "Batch train [5] loss 0.65502, dsc 0.34498\n",
      "Batch train [6] loss 0.60718, dsc 0.39282\n",
      "Batch train [7] loss 0.63167, dsc 0.36833\n",
      "Batch train [8] loss 0.63239, dsc 0.36761\n",
      "Batch train [9] loss 0.61864, dsc 0.38136\n",
      "Batch train [10] loss 0.63758, dsc 0.36242\n",
      "Epoch [315] train done\n",
      "Batch eval [1] loss 0.63674, dsc 0.36326\n",
      "Batch eval [2] loss 0.67020, dsc 0.32980\n",
      "Batch eval [3] loss 0.64638, dsc 0.35362\n",
      "Batch eval [4] loss 0.68452, dsc 0.31548\n",
      "Batch eval [5] loss 0.64736, dsc 0.35264\n",
      "Epoch [315] valid done\n",
      "Epoch [315] T 1266.89s, deltaT 4.03s, loss: train 0.61987, valid 0.65704, dsc: train 0.38013, valid 0.34296\n",
      "Batch train [1] loss 0.63341, dsc 0.36659\n",
      "Batch train [2] loss 0.61173, dsc 0.38827\n",
      "Batch train [3] loss 0.64963, dsc 0.35037\n",
      "Batch train [4] loss 0.61290, dsc 0.38710\n",
      "Batch train [5] loss 0.64688, dsc 0.35312\n",
      "Batch train [6] loss 0.60190, dsc 0.39810\n",
      "Batch train [7] loss 0.57481, dsc 0.42519\n",
      "Batch train [8] loss 0.63047, dsc 0.36953\n",
      "Batch train [9] loss 0.58545, dsc 0.41455\n",
      "Batch train [10] loss 0.64306, dsc 0.35694\n",
      "Epoch [316] train done\n",
      "Batch eval [1] loss 0.62803, dsc 0.37197\n",
      "Batch eval [2] loss 0.66428, dsc 0.33572\n",
      "Batch eval [3] loss 0.64242, dsc 0.35758\n",
      "Batch eval [4] loss 0.68025, dsc 0.31975\n",
      "Batch eval [5] loss 0.63569, dsc 0.36431\n",
      "Epoch [316] valid done\n",
      "Epoch [316] T 1270.94s, deltaT 4.04s, loss: train 0.61902, valid 0.65013, dsc: train 0.38098, valid 0.34987\n",
      "Batch train [1] loss 0.59448, dsc 0.40552\n",
      "Batch train [2] loss 0.61737, dsc 0.38263\n",
      "Batch train [3] loss 0.62383, dsc 0.37617\n",
      "Batch train [4] loss 0.61685, dsc 0.38315\n",
      "Batch train [5] loss 0.62610, dsc 0.37390\n",
      "Batch train [6] loss 0.61755, dsc 0.38245\n",
      "Batch train [7] loss 0.63270, dsc 0.36730\n",
      "Batch train [8] loss 0.64242, dsc 0.35758\n",
      "Batch train [9] loss 0.58263, dsc 0.41737\n",
      "Batch train [10] loss 0.61152, dsc 0.38848\n",
      "Epoch [317] train done\n",
      "Batch eval [1] loss 0.62886, dsc 0.37114\n",
      "Batch eval [2] loss 0.65899, dsc 0.34101\n",
      "Batch eval [3] loss 0.63778, dsc 0.36222\n",
      "Batch eval [4] loss 0.68121, dsc 0.31879\n",
      "Batch eval [5] loss 0.63809, dsc 0.36191\n",
      "Epoch [317] valid done\n",
      "Epoch [317] T 1274.93s, deltaT 4.00s, loss: train 0.61654, valid 0.64899, dsc: train 0.38346, valid 0.35101\n",
      "Batch train [1] loss 0.61414, dsc 0.38586\n",
      "Batch train [2] loss 0.61095, dsc 0.38905\n",
      "Batch train [3] loss 0.62342, dsc 0.37658\n",
      "Batch train [4] loss 0.61881, dsc 0.38119\n",
      "Batch train [5] loss 0.62545, dsc 0.37455\n",
      "Batch train [6] loss 0.58907, dsc 0.41093\n",
      "Batch train [7] loss 0.60912, dsc 0.39088\n",
      "Batch train [8] loss 0.61308, dsc 0.38692\n",
      "Batch train [9] loss 0.63981, dsc 0.36019\n",
      "Batch train [10] loss 0.60797, dsc 0.39203\n",
      "Epoch [318] train done\n",
      "Batch eval [1] loss 0.62914, dsc 0.37086\n",
      "Batch eval [2] loss 0.66087, dsc 0.33913\n",
      "Batch eval [3] loss 0.63577, dsc 0.36423\n",
      "Batch eval [4] loss 0.67638, dsc 0.32362\n",
      "Batch eval [5] loss 0.63319, dsc 0.36681\n",
      "Epoch [318] valid done\n",
      "Epoch [318] T 1278.89s, deltaT 3.96s, loss: train 0.61518, valid 0.64707, dsc: train 0.38482, valid 0.35293\n",
      "Batch train [1] loss 0.61278, dsc 0.38722\n",
      "Batch train [2] loss 0.63081, dsc 0.36919\n",
      "Batch train [3] loss 0.60129, dsc 0.39871\n",
      "Batch train [4] loss 0.64172, dsc 0.35828\n",
      "Batch train [5] loss 0.60260, dsc 0.39740\n",
      "Batch train [6] loss 0.60462, dsc 0.39538\n",
      "Batch train [7] loss 0.60806, dsc 0.39194\n",
      "Batch train [8] loss 0.61775, dsc 0.38225\n",
      "Batch train [9] loss 0.60503, dsc 0.39497\n",
      "Batch train [10] loss 0.60878, dsc 0.39122\n",
      "Epoch [319] train done\n",
      "Batch eval [1] loss 0.63952, dsc 0.36048\n",
      "Batch eval [2] loss 0.66597, dsc 0.33403\n",
      "Batch eval [3] loss 0.64246, dsc 0.35754\n",
      "Batch eval [4] loss 0.68440, dsc 0.31560\n",
      "Batch eval [5] loss 0.64086, dsc 0.35914\n",
      "Epoch [319] valid done\n",
      "Epoch [319] T 1282.83s, deltaT 3.94s, loss: train 0.61334, valid 0.65464, dsc: train 0.38666, valid 0.34536\n",
      "Batch train [1] loss 0.61492, dsc 0.38508\n",
      "Batch train [2] loss 0.61205, dsc 0.38795\n",
      "Batch train [3] loss 0.61490, dsc 0.38510\n",
      "Batch train [4] loss 0.62895, dsc 0.37105\n",
      "Batch train [5] loss 0.59785, dsc 0.40215\n",
      "Batch train [6] loss 0.61663, dsc 0.38337\n",
      "Batch train [7] loss 0.60137, dsc 0.39863\n",
      "Batch train [8] loss 0.58712, dsc 0.41288\n",
      "Batch train [9] loss 0.63458, dsc 0.36542\n",
      "Batch train [10] loss 0.61443, dsc 0.38557\n",
      "Epoch [320] train done\n",
      "Batch eval [1] loss 0.62579, dsc 0.37421\n",
      "Batch eval [2] loss 0.66194, dsc 0.33806\n",
      "Batch eval [3] loss 0.63848, dsc 0.36152\n",
      "Batch eval [4] loss 0.67754, dsc 0.32246\n",
      "Batch eval [5] loss 0.63556, dsc 0.36444\n",
      "Epoch [320] valid done\n",
      "Epoch [320] T 1286.75s, deltaT 3.92s, loss: train 0.61228, valid 0.64786, dsc: train 0.38772, valid 0.35214\n",
      "Batch train [1] loss 0.62388, dsc 0.37612\n",
      "Batch train [2] loss 0.62864, dsc 0.37136\n",
      "Batch train [3] loss 0.59292, dsc 0.40708\n",
      "Batch train [4] loss 0.57693, dsc 0.42307\n",
      "Batch train [5] loss 0.60789, dsc 0.39211\n",
      "Batch train [6] loss 0.61699, dsc 0.38301\n",
      "Batch train [7] loss 0.60047, dsc 0.39953\n",
      "Batch train [8] loss 0.61472, dsc 0.38528\n",
      "Batch train [9] loss 0.61839, dsc 0.38161\n",
      "Batch train [10] loss 0.62304, dsc 0.37696\n",
      "Epoch [321] train done\n",
      "Batch eval [1] loss 0.63980, dsc 0.36020\n",
      "Batch eval [2] loss 0.65931, dsc 0.34069\n",
      "Batch eval [3] loss 0.64076, dsc 0.35924\n",
      "Batch eval [4] loss 0.68036, dsc 0.31964\n",
      "Batch eval [5] loss 0.64384, dsc 0.35616\n",
      "Epoch [321] valid done\n",
      "Epoch [321] T 1290.81s, deltaT 4.06s, loss: train 0.61039, valid 0.65282, dsc: train 0.38961, valid 0.34718\n",
      "Batch train [1] loss 0.58954, dsc 0.41046\n",
      "Batch train [2] loss 0.62361, dsc 0.37639\n",
      "Batch train [3] loss 0.59892, dsc 0.40108\n",
      "Batch train [4] loss 0.61697, dsc 0.38303\n",
      "Batch train [5] loss 0.59856, dsc 0.40144\n",
      "Batch train [6] loss 0.59212, dsc 0.40788\n",
      "Batch train [7] loss 0.61331, dsc 0.38669\n",
      "Batch train [8] loss 0.63297, dsc 0.36703\n",
      "Batch train [9] loss 0.57315, dsc 0.42685\n",
      "Batch train [10] loss 0.65858, dsc 0.34142\n",
      "Epoch [322] train done\n",
      "Batch eval [1] loss 0.62165, dsc 0.37835\n",
      "Batch eval [2] loss 0.65461, dsc 0.34539\n",
      "Batch eval [3] loss 0.62909, dsc 0.37091\n",
      "Batch eval [4] loss 0.67604, dsc 0.32396\n",
      "Batch eval [5] loss 0.62809, dsc 0.37191\n",
      "Epoch [322] valid done\n",
      "Epoch [322] T 1294.92s, deltaT 4.10s, loss: train 0.60977, valid 0.64189, dsc: train 0.39023, valid 0.35811\n",
      "Batch train [1] loss 0.57422, dsc 0.42578\n",
      "Batch train [2] loss 0.59946, dsc 0.40054\n",
      "Batch train [3] loss 0.62136, dsc 0.37864\n",
      "Batch train [4] loss 0.59483, dsc 0.40517\n",
      "Batch train [5] loss 0.65393, dsc 0.34607\n",
      "Batch train [6] loss 0.61158, dsc 0.38842\n",
      "Batch train [7] loss 0.61071, dsc 0.38929\n",
      "Batch train [8] loss 0.57841, dsc 0.42159\n",
      "Batch train [9] loss 0.59598, dsc 0.40402\n",
      "Batch train [10] loss 0.64298, dsc 0.35702\n",
      "Epoch [323] train done\n",
      "Batch eval [1] loss 0.61945, dsc 0.38055\n",
      "Batch eval [2] loss 0.65290, dsc 0.34710\n",
      "Batch eval [3] loss 0.61993, dsc 0.38007\n",
      "Batch eval [4] loss 0.67279, dsc 0.32721\n",
      "Batch eval [5] loss 0.63378, dsc 0.36622\n",
      "Epoch [323] valid done\n",
      "Epoch [323] T 1299.08s, deltaT 4.16s, loss: train 0.60835, valid 0.63977, dsc: train 0.39165, valid 0.36023\n",
      "Batch train [1] loss 0.60209, dsc 0.39791\n",
      "Batch train [2] loss 0.61992, dsc 0.38008\n",
      "Batch train [3] loss 0.58707, dsc 0.41293\n",
      "Batch train [4] loss 0.61174, dsc 0.38826\n",
      "Batch train [5] loss 0.63291, dsc 0.36709\n",
      "Batch train [6] loss 0.63462, dsc 0.36538\n",
      "Batch train [7] loss 0.60711, dsc 0.39289\n",
      "Batch train [8] loss 0.58061, dsc 0.41939\n",
      "Batch train [9] loss 0.56542, dsc 0.43458\n",
      "Batch train [10] loss 0.62279, dsc 0.37721\n",
      "Epoch [324] train done\n",
      "Batch eval [1] loss 0.62838, dsc 0.37162\n",
      "Batch eval [2] loss 0.65484, dsc 0.34516\n",
      "Batch eval [3] loss 0.63654, dsc 0.36346\n",
      "Batch eval [4] loss 0.67692, dsc 0.32308\n",
      "Batch eval [5] loss 0.63575, dsc 0.36425\n",
      "Epoch [324] valid done\n",
      "Epoch [324] T 1303.36s, deltaT 4.27s, loss: train 0.60643, valid 0.64649, dsc: train 0.39357, valid 0.35351\n",
      "Batch train [1] loss 0.63442, dsc 0.36558\n",
      "Batch train [2] loss 0.64223, dsc 0.35777\n",
      "Batch train [3] loss 0.58484, dsc 0.41516\n",
      "Batch train [4] loss 0.59452, dsc 0.40548\n",
      "Batch train [5] loss 0.60238, dsc 0.39762\n",
      "Batch train [6] loss 0.59081, dsc 0.40919\n",
      "Batch train [7] loss 0.61416, dsc 0.38584\n",
      "Batch train [8] loss 0.61078, dsc 0.38922\n",
      "Batch train [9] loss 0.61260, dsc 0.38740\n",
      "Batch train [10] loss 0.56337, dsc 0.43663\n",
      "Epoch [325] train done\n",
      "Batch eval [1] loss 0.63210, dsc 0.36790\n",
      "Batch eval [2] loss 0.65623, dsc 0.34377\n",
      "Batch eval [3] loss 0.63126, dsc 0.36874\n",
      "Batch eval [4] loss 0.66935, dsc 0.33065\n",
      "Batch eval [5] loss 0.63122, dsc 0.36878\n",
      "Epoch [325] valid done\n",
      "Epoch [325] T 1307.44s, deltaT 4.08s, loss: train 0.60501, valid 0.64403, dsc: train 0.39499, valid 0.35597\n",
      "Batch train [1] loss 0.60238, dsc 0.39762\n",
      "Batch train [2] loss 0.62594, dsc 0.37406\n",
      "Batch train [3] loss 0.60834, dsc 0.39166\n",
      "Batch train [4] loss 0.62220, dsc 0.37780\n",
      "Batch train [5] loss 0.59551, dsc 0.40449\n",
      "Batch train [6] loss 0.56262, dsc 0.43738\n",
      "Batch train [7] loss 0.60316, dsc 0.39684\n",
      "Batch train [8] loss 0.62114, dsc 0.37886\n",
      "Batch train [9] loss 0.59396, dsc 0.40604\n",
      "Batch train [10] loss 0.59450, dsc 0.40550\n",
      "Epoch [326] train done\n",
      "Batch eval [1] loss 0.62831, dsc 0.37169\n",
      "Batch eval [2] loss 0.65168, dsc 0.34832\n",
      "Batch eval [3] loss 0.63034, dsc 0.36966\n",
      "Batch eval [4] loss 0.67212, dsc 0.32788\n",
      "Batch eval [5] loss 0.63010, dsc 0.36990\n",
      "Epoch [326] valid done\n",
      "Epoch [326] T 1311.46s, deltaT 4.02s, loss: train 0.60298, valid 0.64251, dsc: train 0.39702, valid 0.35749\n",
      "Batch train [1] loss 0.59560, dsc 0.40440\n",
      "Batch train [2] loss 0.60822, dsc 0.39178\n",
      "Batch train [3] loss 0.60145, dsc 0.39855\n",
      "Batch train [4] loss 0.61754, dsc 0.38246\n",
      "Batch train [5] loss 0.57811, dsc 0.42189\n",
      "Batch train [6] loss 0.58368, dsc 0.41632\n",
      "Batch train [7] loss 0.59632, dsc 0.40368\n",
      "Batch train [8] loss 0.61514, dsc 0.38486\n",
      "Batch train [9] loss 0.62582, dsc 0.37418\n",
      "Batch train [10] loss 0.58971, dsc 0.41029\n",
      "Epoch [327] train done\n",
      "Batch eval [1] loss 0.62466, dsc 0.37534\n",
      "Batch eval [2] loss 0.65073, dsc 0.34927\n",
      "Batch eval [3] loss 0.62116, dsc 0.37884\n",
      "Batch eval [4] loss 0.66591, dsc 0.33409\n",
      "Batch eval [5] loss 0.62258, dsc 0.37742\n",
      "Epoch [327] valid done\n",
      "Epoch [327] T 1315.53s, deltaT 4.07s, loss: train 0.60116, valid 0.63701, dsc: train 0.39884, valid 0.36299\n",
      "Batch train [1] loss 0.57043, dsc 0.42957\n",
      "Batch train [2] loss 0.62528, dsc 0.37472\n",
      "Batch train [3] loss 0.60897, dsc 0.39103\n",
      "Batch train [4] loss 0.59789, dsc 0.40211\n",
      "Batch train [5] loss 0.64283, dsc 0.35717\n",
      "Batch train [6] loss 0.59702, dsc 0.40298\n",
      "Batch train [7] loss 0.59851, dsc 0.40149\n",
      "Batch train [8] loss 0.61014, dsc 0.38986\n",
      "Batch train [9] loss 0.58030, dsc 0.41970\n",
      "Batch train [10] loss 0.57135, dsc 0.42865\n",
      "Epoch [328] train done\n",
      "Batch eval [1] loss 0.61338, dsc 0.38662\n",
      "Batch eval [2] loss 0.64921, dsc 0.35079\n",
      "Batch eval [3] loss 0.61354, dsc 0.38646\n",
      "Batch eval [4] loss 0.66153, dsc 0.33847\n",
      "Batch eval [5] loss 0.61814, dsc 0.38186\n",
      "Epoch [328] valid done\n",
      "Epoch [328] T 1319.60s, deltaT 4.06s, loss: train 0.60027, valid 0.63116, dsc: train 0.39973, valid 0.36884\n",
      "Batch train [1] loss 0.61464, dsc 0.38536\n",
      "Batch train [2] loss 0.61226, dsc 0.38774\n",
      "Batch train [3] loss 0.62134, dsc 0.37866\n",
      "Batch train [4] loss 0.60474, dsc 0.39526\n",
      "Batch train [5] loss 0.58205, dsc 0.41795\n",
      "Batch train [6] loss 0.61495, dsc 0.38505\n",
      "Batch train [7] loss 0.60790, dsc 0.39210\n",
      "Batch train [8] loss 0.57544, dsc 0.42456\n",
      "Batch train [9] loss 0.58408, dsc 0.41592\n",
      "Batch train [10] loss 0.57016, dsc 0.42984\n",
      "Epoch [329] train done\n",
      "Batch eval [1] loss 0.61426, dsc 0.38574\n",
      "Batch eval [2] loss 0.64506, dsc 0.35494\n",
      "Batch eval [3] loss 0.61615, dsc 0.38385\n",
      "Batch eval [4] loss 0.66442, dsc 0.33558\n",
      "Batch eval [5] loss 0.61904, dsc 0.38096\n",
      "Epoch [329] valid done\n",
      "Epoch [329] T 1323.62s, deltaT 4.02s, loss: train 0.59875, valid 0.63179, dsc: train 0.40125, valid 0.36821\n",
      "Batch train [1] loss 0.63147, dsc 0.36853\n",
      "Batch train [2] loss 0.59181, dsc 0.40819\n",
      "Batch train [3] loss 0.58794, dsc 0.41206\n",
      "Batch train [4] loss 0.61495, dsc 0.38505\n",
      "Batch train [5] loss 0.58939, dsc 0.41061\n",
      "Batch train [6] loss 0.61639, dsc 0.38361\n",
      "Batch train [7] loss 0.59338, dsc 0.40662\n",
      "Batch train [8] loss 0.60222, dsc 0.39778\n",
      "Batch train [9] loss 0.55744, dsc 0.44256\n",
      "Batch train [10] loss 0.58695, dsc 0.41305\n",
      "Epoch [330] train done\n",
      "Batch eval [1] loss 0.61641, dsc 0.38359\n",
      "Batch eval [2] loss 0.64314, dsc 0.35686\n",
      "Batch eval [3] loss 0.61467, dsc 0.38533\n",
      "Batch eval [4] loss 0.66428, dsc 0.33572\n",
      "Batch eval [5] loss 0.61635, dsc 0.38365\n",
      "Epoch [330] valid done\n",
      "Epoch [330] T 1327.70s, deltaT 4.08s, loss: train 0.59719, valid 0.63097, dsc: train 0.40281, valid 0.36903\n",
      "Batch train [1] loss 0.60253, dsc 0.39747\n",
      "Batch train [2] loss 0.60065, dsc 0.39935\n",
      "Batch train [3] loss 0.61325, dsc 0.38675\n",
      "Batch train [4] loss 0.57498, dsc 0.42502\n",
      "Batch train [5] loss 0.57662, dsc 0.42338\n",
      "Batch train [6] loss 0.58370, dsc 0.41630\n",
      "Batch train [7] loss 0.59021, dsc 0.40979\n",
      "Batch train [8] loss 0.62919, dsc 0.37081\n",
      "Batch train [9] loss 0.61219, dsc 0.38781\n",
      "Batch train [10] loss 0.57633, dsc 0.42367\n",
      "Epoch [331] train done\n",
      "Batch eval [1] loss 0.62519, dsc 0.37481\n",
      "Batch eval [2] loss 0.65103, dsc 0.34897\n",
      "Batch eval [3] loss 0.62930, dsc 0.37070\n",
      "Batch eval [4] loss 0.66723, dsc 0.33277\n",
      "Batch eval [5] loss 0.62046, dsc 0.37954\n",
      "Epoch [331] valid done\n",
      "Epoch [331] T 1331.73s, deltaT 4.03s, loss: train 0.59596, valid 0.63864, dsc: train 0.40404, valid 0.36136\n",
      "Batch train [1] loss 0.58474, dsc 0.41526\n",
      "Batch train [2] loss 0.62740, dsc 0.37260\n",
      "Batch train [3] loss 0.57614, dsc 0.42386\n",
      "Batch train [4] loss 0.57561, dsc 0.42439\n",
      "Batch train [5] loss 0.58055, dsc 0.41945\n",
      "Batch train [6] loss 0.62341, dsc 0.37659\n",
      "Batch train [7] loss 0.62228, dsc 0.37772\n",
      "Batch train [8] loss 0.60672, dsc 0.39328\n",
      "Batch train [9] loss 0.59303, dsc 0.40697\n",
      "Batch train [10] loss 0.56331, dsc 0.43669\n",
      "Epoch [332] train done\n",
      "Batch eval [1] loss 0.62336, dsc 0.37664\n",
      "Batch eval [2] loss 0.65794, dsc 0.34206\n",
      "Batch eval [3] loss 0.62073, dsc 0.37927\n",
      "Batch eval [4] loss 0.66815, dsc 0.33185\n",
      "Batch eval [5] loss 0.62885, dsc 0.37115\n",
      "Epoch [332] valid done\n",
      "Epoch [332] T 1335.77s, deltaT 4.04s, loss: train 0.59532, valid 0.63981, dsc: train 0.40468, valid 0.36019\n",
      "Batch train [1] loss 0.58660, dsc 0.41340\n",
      "Batch train [2] loss 0.58138, dsc 0.41862\n",
      "Batch train [3] loss 0.59661, dsc 0.40339\n",
      "Batch train [4] loss 0.57130, dsc 0.42870\n",
      "Batch train [5] loss 0.60368, dsc 0.39632\n",
      "Batch train [6] loss 0.58021, dsc 0.41979\n",
      "Batch train [7] loss 0.60103, dsc 0.39897\n",
      "Batch train [8] loss 0.59837, dsc 0.40163\n",
      "Batch train [9] loss 0.58933, dsc 0.41067\n",
      "Batch train [10] loss 0.62925, dsc 0.37075\n",
      "Epoch [333] train done\n",
      "Batch eval [1] loss 0.61793, dsc 0.38207\n",
      "Batch eval [2] loss 0.64715, dsc 0.35285\n",
      "Batch eval [3] loss 0.62041, dsc 0.37959\n",
      "Batch eval [4] loss 0.66827, dsc 0.33173\n",
      "Batch eval [5] loss 0.61830, dsc 0.38170\n",
      "Epoch [333] valid done\n",
      "Epoch [333] T 1339.83s, deltaT 4.06s, loss: train 0.59378, valid 0.63441, dsc: train 0.40622, valid 0.36559\n",
      "Batch train [1] loss 0.58848, dsc 0.41152\n",
      "Batch train [2] loss 0.58345, dsc 0.41655\n",
      "Batch train [3] loss 0.57379, dsc 0.42621\n",
      "Batch train [4] loss 0.58929, dsc 0.41071\n",
      "Batch train [5] loss 0.62750, dsc 0.37250\n",
      "Batch train [6] loss 0.59143, dsc 0.40857\n",
      "Batch train [7] loss 0.57958, dsc 0.42042\n",
      "Batch train [8] loss 0.60733, dsc 0.39267\n",
      "Batch train [9] loss 0.60465, dsc 0.39535\n",
      "Batch train [10] loss 0.57742, dsc 0.42258\n",
      "Epoch [334] train done\n",
      "Batch eval [1] loss 0.60711, dsc 0.39289\n",
      "Batch eval [2] loss 0.64506, dsc 0.35494\n",
      "Batch eval [3] loss 0.61592, dsc 0.38408\n",
      "Batch eval [4] loss 0.66090, dsc 0.33910\n",
      "Batch eval [5] loss 0.61689, dsc 0.38311\n",
      "Epoch [334] valid done\n",
      "Epoch [334] T 1343.95s, deltaT 4.12s, loss: train 0.59229, valid 0.62918, dsc: train 0.40771, valid 0.37082\n",
      "Batch train [1] loss 0.58452, dsc 0.41548\n",
      "Batch train [2] loss 0.58884, dsc 0.41116\n",
      "Batch train [3] loss 0.57765, dsc 0.42235\n",
      "Batch train [4] loss 0.60446, dsc 0.39554\n",
      "Batch train [5] loss 0.56275, dsc 0.43725\n",
      "Batch train [6] loss 0.64250, dsc 0.35750\n",
      "Batch train [7] loss 0.58247, dsc 0.41753\n",
      "Batch train [8] loss 0.58800, dsc 0.41200\n",
      "Batch train [9] loss 0.58058, dsc 0.41942\n",
      "Batch train [10] loss 0.59690, dsc 0.40310\n",
      "Epoch [335] train done\n",
      "Batch eval [1] loss 0.61094, dsc 0.38906\n",
      "Batch eval [2] loss 0.63881, dsc 0.36119\n",
      "Batch eval [3] loss 0.61430, dsc 0.38570\n",
      "Batch eval [4] loss 0.65664, dsc 0.34336\n",
      "Batch eval [5] loss 0.61875, dsc 0.38125\n",
      "Epoch [335] valid done\n",
      "Epoch [335] T 1348.01s, deltaT 4.06s, loss: train 0.59087, valid 0.62789, dsc: train 0.40913, valid 0.37211\n",
      "Batch train [1] loss 0.59254, dsc 0.40746\n",
      "Batch train [2] loss 0.59446, dsc 0.40554\n",
      "Batch train [3] loss 0.56665, dsc 0.43335\n",
      "Batch train [4] loss 0.61960, dsc 0.38040\n",
      "Batch train [5] loss 0.56056, dsc 0.43944\n",
      "Batch train [6] loss 0.60161, dsc 0.39839\n",
      "Batch train [7] loss 0.56967, dsc 0.43033\n",
      "Batch train [8] loss 0.58759, dsc 0.41241\n",
      "Batch train [9] loss 0.61244, dsc 0.38756\n",
      "Batch train [10] loss 0.58653, dsc 0.41347\n",
      "Epoch [336] train done\n",
      "Batch eval [1] loss 0.61724, dsc 0.38276\n",
      "Batch eval [2] loss 0.64180, dsc 0.35820\n",
      "Batch eval [3] loss 0.61356, dsc 0.38644\n",
      "Batch eval [4] loss 0.65816, dsc 0.34184\n",
      "Batch eval [5] loss 0.61946, dsc 0.38054\n",
      "Epoch [336] valid done\n",
      "Epoch [336] T 1352.13s, deltaT 4.11s, loss: train 0.58916, valid 0.63004, dsc: train 0.41084, valid 0.36996\n",
      "Batch train [1] loss 0.59471, dsc 0.40529\n",
      "Batch train [2] loss 0.59113, dsc 0.40887\n",
      "Batch train [3] loss 0.58115, dsc 0.41885\n",
      "Batch train [4] loss 0.58669, dsc 0.41331\n",
      "Batch train [5] loss 0.60803, dsc 0.39197\n",
      "Batch train [6] loss 0.56487, dsc 0.43513\n",
      "Batch train [7] loss 0.60266, dsc 0.39734\n",
      "Batch train [8] loss 0.58383, dsc 0.41617\n",
      "Batch train [9] loss 0.57834, dsc 0.42166\n",
      "Batch train [10] loss 0.57714, dsc 0.42286\n",
      "Epoch [337] train done\n",
      "Batch eval [1] loss 0.61569, dsc 0.38431\n",
      "Batch eval [2] loss 0.64391, dsc 0.35609\n",
      "Batch eval [3] loss 0.62038, dsc 0.37962\n",
      "Batch eval [4] loss 0.65811, dsc 0.34189\n",
      "Batch eval [5] loss 0.62184, dsc 0.37816\n",
      "Epoch [337] valid done\n",
      "Epoch [337] T 1356.20s, deltaT 4.07s, loss: train 0.58686, valid 0.63199, dsc: train 0.41314, valid 0.36801\n",
      "Batch train [1] loss 0.61313, dsc 0.38687\n",
      "Batch train [2] loss 0.60346, dsc 0.39654\n",
      "Batch train [3] loss 0.58205, dsc 0.41795\n",
      "Batch train [4] loss 0.61299, dsc 0.38701\n",
      "Batch train [5] loss 0.55684, dsc 0.44316\n",
      "Batch train [6] loss 0.59775, dsc 0.40225\n",
      "Batch train [7] loss 0.56045, dsc 0.43955\n",
      "Batch train [8] loss 0.57872, dsc 0.42128\n",
      "Batch train [9] loss 0.55279, dsc 0.44721\n",
      "Batch train [10] loss 0.60203, dsc 0.39797\n",
      "Epoch [338] train done\n",
      "Batch eval [1] loss 0.61199, dsc 0.38801\n",
      "Batch eval [2] loss 0.63498, dsc 0.36502\n",
      "Batch eval [3] loss 0.60784, dsc 0.39216\n",
      "Batch eval [4] loss 0.65412, dsc 0.34588\n",
      "Batch eval [5] loss 0.61661, dsc 0.38339\n",
      "Epoch [338] valid done\n",
      "Epoch [338] T 1360.29s, deltaT 4.09s, loss: train 0.58602, valid 0.62511, dsc: train 0.41398, valid 0.37489\n",
      "Batch train [1] loss 0.54429, dsc 0.45571\n",
      "Batch train [2] loss 0.54996, dsc 0.45004\n",
      "Batch train [3] loss 0.56735, dsc 0.43265\n",
      "Batch train [4] loss 0.56977, dsc 0.43023\n",
      "Batch train [5] loss 0.60185, dsc 0.39815\n",
      "Batch train [6] loss 0.59712, dsc 0.40288\n",
      "Batch train [7] loss 0.60981, dsc 0.39019\n",
      "Batch train [8] loss 0.60405, dsc 0.39595\n",
      "Batch train [9] loss 0.59708, dsc 0.40292\n",
      "Batch train [10] loss 0.60791, dsc 0.39209\n",
      "Epoch [339] train done\n",
      "Batch eval [1] loss 0.60361, dsc 0.39639\n",
      "Batch eval [2] loss 0.63575, dsc 0.36425\n",
      "Batch eval [3] loss 0.61629, dsc 0.38371\n",
      "Batch eval [4] loss 0.65683, dsc 0.34317\n",
      "Batch eval [5] loss 0.61263, dsc 0.38737\n",
      "Epoch [339] valid done\n",
      "Epoch [339] T 1364.52s, deltaT 4.23s, loss: train 0.58492, valid 0.62502, dsc: train 0.41508, valid 0.37498\n",
      "Batch train [1] loss 0.56319, dsc 0.43681\n",
      "Batch train [2] loss 0.57508, dsc 0.42492\n",
      "Batch train [3] loss 0.56195, dsc 0.43805\n",
      "Batch train [4] loss 0.61207, dsc 0.38793\n",
      "Batch train [5] loss 0.55445, dsc 0.44555\n",
      "Batch train [6] loss 0.59651, dsc 0.40349\n",
      "Batch train [7] loss 0.59779, dsc 0.40221\n",
      "Batch train [8] loss 0.58282, dsc 0.41718\n",
      "Batch train [9] loss 0.60677, dsc 0.39323\n",
      "Batch train [10] loss 0.58600, dsc 0.41400\n",
      "Epoch [340] train done\n",
      "Batch eval [1] loss 0.61800, dsc 0.38200\n",
      "Batch eval [2] loss 0.64940, dsc 0.35060\n",
      "Batch eval [3] loss 0.61410, dsc 0.38590\n",
      "Batch eval [4] loss 0.65646, dsc 0.34354\n",
      "Batch eval [5] loss 0.62520, dsc 0.37480\n",
      "Epoch [340] valid done\n",
      "Epoch [340] T 1368.58s, deltaT 4.06s, loss: train 0.58366, valid 0.63263, dsc: train 0.41634, valid 0.36737\n",
      "Batch train [1] loss 0.58217, dsc 0.41783\n",
      "Batch train [2] loss 0.58832, dsc 0.41168\n",
      "Batch train [3] loss 0.57038, dsc 0.42962\n",
      "Batch train [4] loss 0.57572, dsc 0.42428\n",
      "Batch train [5] loss 0.57993, dsc 0.42007\n",
      "Batch train [6] loss 0.59743, dsc 0.40257\n",
      "Batch train [7] loss 0.56946, dsc 0.43054\n",
      "Batch train [8] loss 0.56241, dsc 0.43759\n",
      "Batch train [9] loss 0.59579, dsc 0.40421\n",
      "Batch train [10] loss 0.60213, dsc 0.39787\n",
      "Epoch [341] train done\n",
      "Batch eval [1] loss 0.61928, dsc 0.38072\n",
      "Batch eval [2] loss 0.64044, dsc 0.35956\n",
      "Batch eval [3] loss 0.61384, dsc 0.38616\n",
      "Batch eval [4] loss 0.65380, dsc 0.34620\n",
      "Batch eval [5] loss 0.61333, dsc 0.38667\n",
      "Epoch [341] valid done\n",
      "Epoch [341] T 1372.59s, deltaT 4.00s, loss: train 0.58237, valid 0.62814, dsc: train 0.41763, valid 0.37186\n",
      "Batch train [1] loss 0.59271, dsc 0.40729\n",
      "Batch train [2] loss 0.58255, dsc 0.41745\n",
      "Batch train [3] loss 0.57973, dsc 0.42027\n",
      "Batch train [4] loss 0.59376, dsc 0.40624\n",
      "Batch train [5] loss 0.57077, dsc 0.42923\n",
      "Batch train [6] loss 0.61680, dsc 0.38320\n",
      "Batch train [7] loss 0.57514, dsc 0.42486\n",
      "Batch train [8] loss 0.58806, dsc 0.41194\n",
      "Batch train [9] loss 0.58373, dsc 0.41627\n",
      "Batch train [10] loss 0.52951, dsc 0.47049\n",
      "Epoch [342] train done\n",
      "Batch eval [1] loss 0.61304, dsc 0.38696\n",
      "Batch eval [2] loss 0.63871, dsc 0.36129\n",
      "Batch eval [3] loss 0.61597, dsc 0.38403\n",
      "Batch eval [4] loss 0.65688, dsc 0.34312\n",
      "Batch eval [5] loss 0.61583, dsc 0.38417\n",
      "Epoch [342] valid done\n",
      "Epoch [342] T 1376.58s, deltaT 3.99s, loss: train 0.58127, valid 0.62808, dsc: train 0.41873, valid 0.37192\n",
      "Batch train [1] loss 0.56805, dsc 0.43195\n",
      "Batch train [2] loss 0.57593, dsc 0.42407\n",
      "Batch train [3] loss 0.59050, dsc 0.40950\n",
      "Batch train [4] loss 0.59752, dsc 0.40248\n",
      "Batch train [5] loss 0.57838, dsc 0.42162\n",
      "Batch train [6] loss 0.54193, dsc 0.45807\n",
      "Batch train [7] loss 0.57671, dsc 0.42329\n",
      "Batch train [8] loss 0.61586, dsc 0.38414\n",
      "Batch train [9] loss 0.55756, dsc 0.44244\n",
      "Batch train [10] loss 0.59646, dsc 0.40354\n",
      "Epoch [343] train done\n",
      "Batch eval [1] loss 0.60703, dsc 0.39297\n",
      "Batch eval [2] loss 0.63843, dsc 0.36157\n",
      "Batch eval [3] loss 0.61245, dsc 0.38755\n",
      "Batch eval [4] loss 0.65077, dsc 0.34923\n",
      "Batch eval [5] loss 0.60577, dsc 0.39423\n",
      "Epoch [343] valid done\n",
      "Epoch [343] T 1380.68s, deltaT 4.09s, loss: train 0.57989, valid 0.62289, dsc: train 0.42011, valid 0.37711\n",
      "Batch train [1] loss 0.57252, dsc 0.42748\n",
      "Batch train [2] loss 0.60085, dsc 0.39914\n",
      "Batch train [3] loss 0.57325, dsc 0.42675\n",
      "Batch train [4] loss 0.61416, dsc 0.38584\n",
      "Batch train [5] loss 0.54758, dsc 0.45242\n",
      "Batch train [6] loss 0.56689, dsc 0.43311\n",
      "Batch train [7] loss 0.55585, dsc 0.44415\n",
      "Batch train [8] loss 0.56593, dsc 0.43407\n",
      "Batch train [9] loss 0.58912, dsc 0.41088\n",
      "Batch train [10] loss 0.60075, dsc 0.39925\n",
      "Epoch [344] train done\n",
      "Batch eval [1] loss 0.60714, dsc 0.39286\n",
      "Batch eval [2] loss 0.63039, dsc 0.36961\n",
      "Batch eval [3] loss 0.61034, dsc 0.38966\n",
      "Batch eval [4] loss 0.65003, dsc 0.34997\n",
      "Batch eval [5] loss 0.60812, dsc 0.39188\n",
      "Epoch [344] valid done\n",
      "Epoch [344] T 1384.70s, deltaT 4.02s, loss: train 0.57869, valid 0.62120, dsc: train 0.42131, valid 0.37880\n",
      "Batch train [1] loss 0.60161, dsc 0.39839\n",
      "Batch train [2] loss 0.61753, dsc 0.38247\n",
      "Batch train [3] loss 0.55200, dsc 0.44800\n",
      "Batch train [4] loss 0.58656, dsc 0.41344\n",
      "Batch train [5] loss 0.57023, dsc 0.42977\n",
      "Batch train [6] loss 0.56166, dsc 0.43834\n",
      "Batch train [7] loss 0.58576, dsc 0.41424\n",
      "Batch train [8] loss 0.56434, dsc 0.43566\n",
      "Batch train [9] loss 0.57266, dsc 0.42734\n",
      "Batch train [10] loss 0.56086, dsc 0.43914\n",
      "Epoch [345] train done\n",
      "Batch eval [1] loss 0.59690, dsc 0.40310\n",
      "Batch eval [2] loss 0.63420, dsc 0.36580\n",
      "Batch eval [3] loss 0.60757, dsc 0.39243\n",
      "Batch eval [4] loss 0.65886, dsc 0.34114\n",
      "Batch eval [5] loss 0.61366, dsc 0.38634\n",
      "Epoch [345] valid done\n",
      "Epoch [345] T 1388.69s, deltaT 4.00s, loss: train 0.57732, valid 0.62224, dsc: train 0.42268, valid 0.37776\n",
      "Batch train [1] loss 0.59586, dsc 0.40414\n",
      "Batch train [2] loss 0.56661, dsc 0.43339\n",
      "Batch train [3] loss 0.57792, dsc 0.42208\n",
      "Batch train [4] loss 0.58975, dsc 0.41025\n",
      "Batch train [5] loss 0.55047, dsc 0.44953\n",
      "Batch train [6] loss 0.53948, dsc 0.46052\n",
      "Batch train [7] loss 0.60342, dsc 0.39658\n",
      "Batch train [8] loss 0.59437, dsc 0.40563\n",
      "Batch train [9] loss 0.58644, dsc 0.41356\n",
      "Batch train [10] loss 0.55663, dsc 0.44337\n",
      "Epoch [346] train done\n",
      "Batch eval [1] loss 0.60760, dsc 0.39240\n",
      "Batch eval [2] loss 0.64105, dsc 0.35895\n",
      "Batch eval [3] loss 0.60380, dsc 0.39620\n",
      "Batch eval [4] loss 0.64531, dsc 0.35469\n",
      "Batch eval [5] loss 0.60898, dsc 0.39102\n",
      "Epoch [346] valid done\n",
      "Epoch [346] T 1392.71s, deltaT 4.01s, loss: train 0.57609, valid 0.62135, dsc: train 0.42391, valid 0.37865\n",
      "Batch train [1] loss 0.57216, dsc 0.42784\n",
      "Batch train [2] loss 0.55186, dsc 0.44814\n",
      "Batch train [3] loss 0.55142, dsc 0.44858\n",
      "Batch train [4] loss 0.55401, dsc 0.44599\n",
      "Batch train [5] loss 0.59676, dsc 0.40324\n",
      "Batch train [6] loss 0.57762, dsc 0.42238\n",
      "Batch train [7] loss 0.60901, dsc 0.39099\n",
      "Batch train [8] loss 0.58799, dsc 0.41201\n",
      "Batch train [9] loss 0.57654, dsc 0.42346\n",
      "Batch train [10] loss 0.57396, dsc 0.42604\n",
      "Epoch [347] train done\n",
      "Batch eval [1] loss 0.60949, dsc 0.39051\n",
      "Batch eval [2] loss 0.63818, dsc 0.36182\n",
      "Batch eval [3] loss 0.60789, dsc 0.39211\n",
      "Batch eval [4] loss 0.65628, dsc 0.34372\n",
      "Batch eval [5] loss 0.61565, dsc 0.38435\n",
      "Epoch [347] valid done\n",
      "Epoch [347] T 1396.68s, deltaT 3.97s, loss: train 0.57513, valid 0.62550, dsc: train 0.42487, valid 0.37450\n",
      "Batch train [1] loss 0.60125, dsc 0.39875\n",
      "Batch train [2] loss 0.57115, dsc 0.42885\n",
      "Batch train [3] loss 0.56904, dsc 0.43096\n",
      "Batch train [4] loss 0.57228, dsc 0.42772\n",
      "Batch train [5] loss 0.56238, dsc 0.43762\n",
      "Batch train [6] loss 0.57474, dsc 0.42526\n",
      "Batch train [7] loss 0.58111, dsc 0.41889\n",
      "Batch train [8] loss 0.59449, dsc 0.40551\n",
      "Batch train [9] loss 0.57447, dsc 0.42553\n",
      "Batch train [10] loss 0.53049, dsc 0.46951\n",
      "Epoch [348] train done\n",
      "Batch eval [1] loss 0.59906, dsc 0.40094\n",
      "Batch eval [2] loss 0.62884, dsc 0.37116\n",
      "Batch eval [3] loss 0.60167, dsc 0.39833\n",
      "Batch eval [4] loss 0.64668, dsc 0.35332\n",
      "Batch eval [5] loss 0.60529, dsc 0.39471\n",
      "Epoch [348] valid done\n",
      "Epoch [348] T 1400.67s, deltaT 3.99s, loss: train 0.57314, valid 0.61631, dsc: train 0.42686, valid 0.38369\n",
      "Batch train [1] loss 0.54963, dsc 0.45037\n",
      "Batch train [2] loss 0.59701, dsc 0.40299\n",
      "Batch train [3] loss 0.59093, dsc 0.40907\n",
      "Batch train [4] loss 0.54969, dsc 0.45031\n",
      "Batch train [5] loss 0.57764, dsc 0.42236\n",
      "Batch train [6] loss 0.56545, dsc 0.43455\n",
      "Batch train [7] loss 0.60550, dsc 0.39450\n",
      "Batch train [8] loss 0.58102, dsc 0.41898\n",
      "Batch train [9] loss 0.56413, dsc 0.43587\n",
      "Batch train [10] loss 0.53802, dsc 0.46198\n",
      "Epoch [349] train done\n",
      "Batch eval [1] loss 0.59216, dsc 0.40784\n",
      "Batch eval [2] loss 0.62830, dsc 0.37170\n",
      "Batch eval [3] loss 0.59272, dsc 0.40728\n",
      "Batch eval [4] loss 0.64398, dsc 0.35602\n",
      "Batch eval [5] loss 0.60345, dsc 0.39655\n",
      "Epoch [349] valid done\n",
      "Epoch [349] T 1404.68s, deltaT 4.00s, loss: train 0.57190, valid 0.61212, dsc: train 0.42810, valid 0.38788\n",
      "Batch train [1] loss 0.53129, dsc 0.46871\n",
      "Batch train [2] loss 0.56293, dsc 0.43707\n",
      "Batch train [3] loss 0.55317, dsc 0.44683\n",
      "Batch train [4] loss 0.56845, dsc 0.43155\n",
      "Batch train [5] loss 0.59537, dsc 0.40463\n",
      "Batch train [6] loss 0.54745, dsc 0.45255\n",
      "Batch train [7] loss 0.58444, dsc 0.41556\n",
      "Batch train [8] loss 0.57116, dsc 0.42884\n",
      "Batch train [9] loss 0.57987, dsc 0.42013\n",
      "Batch train [10] loss 0.60858, dsc 0.39142\n",
      "Epoch [350] train done\n",
      "Batch eval [1] loss 0.58834, dsc 0.41166\n",
      "Batch eval [2] loss 0.62522, dsc 0.37478\n",
      "Batch eval [3] loss 0.59785, dsc 0.40215\n",
      "Batch eval [4] loss 0.64399, dsc 0.35601\n",
      "Batch eval [5] loss 0.60183, dsc 0.39817\n",
      "Epoch [350] valid done\n",
      "Epoch [350] T 1408.69s, deltaT 4.01s, loss: train 0.57027, valid 0.61145, dsc: train 0.42973, valid 0.38855\n",
      "Batch train [1] loss 0.56813, dsc 0.43187\n",
      "Batch train [2] loss 0.54961, dsc 0.45039\n",
      "Batch train [3] loss 0.59905, dsc 0.40095\n",
      "Batch train [4] loss 0.55697, dsc 0.44303\n",
      "Batch train [5] loss 0.57462, dsc 0.42538\n",
      "Batch train [6] loss 0.60748, dsc 0.39252\n",
      "Batch train [7] loss 0.54782, dsc 0.45218\n",
      "Batch train [8] loss 0.54688, dsc 0.45312\n",
      "Batch train [9] loss 0.56046, dsc 0.43954\n",
      "Batch train [10] loss 0.57573, dsc 0.42427\n",
      "Epoch [351] train done\n",
      "Batch eval [1] loss 0.59583, dsc 0.40417\n",
      "Batch eval [2] loss 0.63209, dsc 0.36791\n",
      "Batch eval [3] loss 0.59893, dsc 0.40107\n",
      "Batch eval [4] loss 0.64614, dsc 0.35386\n",
      "Batch eval [5] loss 0.60647, dsc 0.39353\n",
      "Epoch [351] valid done\n",
      "Epoch [351] T 1412.62s, deltaT 3.92s, loss: train 0.56868, valid 0.61589, dsc: train 0.43132, valid 0.38411\n",
      "Batch train [1] loss 0.58823, dsc 0.41177\n",
      "Batch train [2] loss 0.55674, dsc 0.44326\n",
      "Batch train [3] loss 0.54415, dsc 0.45585\n",
      "Batch train [4] loss 0.57091, dsc 0.42909\n",
      "Batch train [5] loss 0.58822, dsc 0.41178\n",
      "Batch train [6] loss 0.55810, dsc 0.44190\n",
      "Batch train [7] loss 0.57512, dsc 0.42488\n",
      "Batch train [8] loss 0.55573, dsc 0.44427\n",
      "Batch train [9] loss 0.54986, dsc 0.45014\n",
      "Batch train [10] loss 0.58792, dsc 0.41208\n",
      "Epoch [352] train done\n",
      "Batch eval [1] loss 0.59413, dsc 0.40587\n",
      "Batch eval [2] loss 0.63115, dsc 0.36885\n",
      "Batch eval [3] loss 0.60017, dsc 0.39983\n",
      "Batch eval [4] loss 0.63982, dsc 0.36018\n",
      "Batch eval [5] loss 0.60259, dsc 0.39741\n",
      "Epoch [352] valid done\n",
      "Epoch [352] T 1416.62s, deltaT 4.01s, loss: train 0.56750, valid 0.61357, dsc: train 0.43250, valid 0.38643\n",
      "Batch train [1] loss 0.56670, dsc 0.43330\n",
      "Batch train [2] loss 0.55262, dsc 0.44738\n",
      "Batch train [3] loss 0.58315, dsc 0.41685\n",
      "Batch train [4] loss 0.56363, dsc 0.43637\n",
      "Batch train [5] loss 0.57912, dsc 0.42088\n",
      "Batch train [6] loss 0.57705, dsc 0.42295\n",
      "Batch train [7] loss 0.56954, dsc 0.43046\n",
      "Batch train [8] loss 0.56911, dsc 0.43089\n",
      "Batch train [9] loss 0.54249, dsc 0.45751\n",
      "Batch train [10] loss 0.55865, dsc 0.44135\n",
      "Epoch [353] train done\n",
      "Batch eval [1] loss 0.58572, dsc 0.41428\n",
      "Batch eval [2] loss 0.62607, dsc 0.37393\n",
      "Batch eval [3] loss 0.59551, dsc 0.40449\n",
      "Batch eval [4] loss 0.64080, dsc 0.35920\n",
      "Batch eval [5] loss 0.59810, dsc 0.40190\n",
      "Epoch [353] valid done\n",
      "Epoch [353] T 1420.62s, deltaT 4.00s, loss: train 0.56621, valid 0.60924, dsc: train 0.43379, valid 0.39076\n",
      "Batch train [1] loss 0.59814, dsc 0.40186\n",
      "Batch train [2] loss 0.53182, dsc 0.46818\n",
      "Batch train [3] loss 0.56036, dsc 0.43964\n",
      "Batch train [4] loss 0.55755, dsc 0.44245\n",
      "Batch train [5] loss 0.56725, dsc 0.43275\n",
      "Batch train [6] loss 0.57367, dsc 0.42633\n",
      "Batch train [7] loss 0.58606, dsc 0.41394\n",
      "Batch train [8] loss 0.55074, dsc 0.44926\n",
      "Batch train [9] loss 0.54663, dsc 0.45337\n",
      "Batch train [10] loss 0.58214, dsc 0.41786\n",
      "Epoch [354] train done\n",
      "Batch eval [1] loss 0.59419, dsc 0.40581\n",
      "Batch eval [2] loss 0.61885, dsc 0.38115\n",
      "Batch eval [3] loss 0.59682, dsc 0.40318\n",
      "Batch eval [4] loss 0.63907, dsc 0.36093\n",
      "Batch eval [5] loss 0.59564, dsc 0.40436\n",
      "Epoch [354] valid done\n",
      "Epoch [354] T 1424.65s, deltaT 4.03s, loss: train 0.56544, valid 0.60891, dsc: train 0.43456, valid 0.39109\n",
      "Batch train [1] loss 0.57072, dsc 0.42928\n",
      "Batch train [2] loss 0.57706, dsc 0.42294\n",
      "Batch train [3] loss 0.58701, dsc 0.41299\n",
      "Batch train [4] loss 0.57151, dsc 0.42849\n",
      "Batch train [5] loss 0.54888, dsc 0.45112\n",
      "Batch train [6] loss 0.56276, dsc 0.43724\n",
      "Batch train [7] loss 0.52775, dsc 0.47225\n",
      "Batch train [8] loss 0.57022, dsc 0.42978\n",
      "Batch train [9] loss 0.55915, dsc 0.44085\n",
      "Batch train [10] loss 0.56775, dsc 0.43225\n",
      "Epoch [355] train done\n",
      "Batch eval [1] loss 0.58801, dsc 0.41199\n",
      "Batch eval [2] loss 0.62405, dsc 0.37595\n",
      "Batch eval [3] loss 0.58791, dsc 0.41209\n",
      "Batch eval [4] loss 0.63828, dsc 0.36172\n",
      "Batch eval [5] loss 0.59919, dsc 0.40081\n",
      "Epoch [355] valid done\n",
      "Epoch [355] T 1428.68s, deltaT 4.03s, loss: train 0.56428, valid 0.60749, dsc: train 0.43572, valid 0.39251\n",
      "Batch train [1] loss 0.58036, dsc 0.41964\n",
      "Batch train [2] loss 0.57401, dsc 0.42599\n",
      "Batch train [3] loss 0.54665, dsc 0.45335\n",
      "Batch train [4] loss 0.56592, dsc 0.43408\n",
      "Batch train [5] loss 0.57405, dsc 0.42595\n",
      "Batch train [6] loss 0.56161, dsc 0.43839\n",
      "Batch train [7] loss 0.55016, dsc 0.44984\n",
      "Batch train [8] loss 0.56002, dsc 0.43998\n",
      "Batch train [9] loss 0.57825, dsc 0.42175\n",
      "Batch train [10] loss 0.53978, dsc 0.46022\n",
      "Epoch [356] train done\n",
      "Batch eval [1] loss 0.58975, dsc 0.41025\n",
      "Batch eval [2] loss 0.61968, dsc 0.38032\n",
      "Batch eval [3] loss 0.59229, dsc 0.40771\n",
      "Batch eval [4] loss 0.64157, dsc 0.35843\n",
      "Batch eval [5] loss 0.59834, dsc 0.40166\n",
      "Epoch [356] valid done\n",
      "Epoch [356] T 1432.67s, deltaT 3.99s, loss: train 0.56308, valid 0.60833, dsc: train 0.43692, valid 0.39167\n",
      "Batch train [1] loss 0.57601, dsc 0.42399\n",
      "Batch train [2] loss 0.54602, dsc 0.45398\n",
      "Batch train [3] loss 0.55823, dsc 0.44177\n",
      "Batch train [4] loss 0.58396, dsc 0.41604\n",
      "Batch train [5] loss 0.55395, dsc 0.44605\n",
      "Batch train [6] loss 0.58048, dsc 0.41952\n",
      "Batch train [7] loss 0.56065, dsc 0.43935\n",
      "Batch train [8] loss 0.56933, dsc 0.43067\n",
      "Batch train [9] loss 0.55068, dsc 0.44932\n",
      "Batch train [10] loss 0.54286, dsc 0.45714\n",
      "Epoch [357] train done\n",
      "Batch eval [1] loss 0.58510, dsc 0.41490\n",
      "Batch eval [2] loss 0.61837, dsc 0.38163\n",
      "Batch eval [3] loss 0.59414, dsc 0.40586\n",
      "Batch eval [4] loss 0.63997, dsc 0.36003\n",
      "Batch eval [5] loss 0.59827, dsc 0.40173\n",
      "Epoch [357] valid done\n",
      "Epoch [357] T 1436.68s, deltaT 4.01s, loss: train 0.56222, valid 0.60717, dsc: train 0.43778, valid 0.39283\n",
      "Batch train [1] loss 0.54904, dsc 0.45096\n",
      "Batch train [2] loss 0.56041, dsc 0.43959\n",
      "Batch train [3] loss 0.59029, dsc 0.40971\n",
      "Batch train [4] loss 0.55085, dsc 0.44915\n",
      "Batch train [5] loss 0.56508, dsc 0.43492\n",
      "Batch train [6] loss 0.53298, dsc 0.46702\n",
      "Batch train [7] loss 0.53352, dsc 0.46648\n",
      "Batch train [8] loss 0.59641, dsc 0.40359\n",
      "Batch train [9] loss 0.59753, dsc 0.40247\n",
      "Batch train [10] loss 0.53439, dsc 0.46561\n",
      "Epoch [358] train done\n",
      "Batch eval [1] loss 0.57637, dsc 0.42363\n",
      "Batch eval [2] loss 0.61698, dsc 0.38302\n",
      "Batch eval [3] loss 0.58951, dsc 0.41049\n",
      "Batch eval [4] loss 0.63738, dsc 0.36262\n",
      "Batch eval [5] loss 0.59365, dsc 0.40635\n",
      "Epoch [358] valid done\n",
      "Epoch [358] T 1440.70s, deltaT 4.02s, loss: train 0.56105, valid 0.60278, dsc: train 0.43895, valid 0.39722\n",
      "Batch train [1] loss 0.56057, dsc 0.43943\n",
      "Batch train [2] loss 0.54463, dsc 0.45537\n",
      "Batch train [3] loss 0.53830, dsc 0.46170\n",
      "Batch train [4] loss 0.57541, dsc 0.42459\n",
      "Batch train [5] loss 0.54912, dsc 0.45088\n",
      "Batch train [6] loss 0.56974, dsc 0.43026\n",
      "Batch train [7] loss 0.52259, dsc 0.47741\n",
      "Batch train [8] loss 0.55799, dsc 0.44201\n",
      "Batch train [9] loss 0.58788, dsc 0.41212\n",
      "Batch train [10] loss 0.59062, dsc 0.40938\n",
      "Epoch [359] train done\n",
      "Batch eval [1] loss 0.58426, dsc 0.41574\n",
      "Batch eval [2] loss 0.61874, dsc 0.38126\n",
      "Batch eval [3] loss 0.60016, dsc 0.39984\n",
      "Batch eval [4] loss 0.63644, dsc 0.36356\n",
      "Batch eval [5] loss 0.59131, dsc 0.40869\n",
      "Epoch [359] valid done\n",
      "Epoch [359] T 1444.67s, deltaT 3.97s, loss: train 0.55968, valid 0.60618, dsc: train 0.44032, valid 0.39382\n",
      "Batch train [1] loss 0.55300, dsc 0.44700\n",
      "Batch train [2] loss 0.55689, dsc 0.44311\n",
      "Batch train [3] loss 0.54479, dsc 0.45521\n",
      "Batch train [4] loss 0.54869, dsc 0.45131\n",
      "Batch train [5] loss 0.55148, dsc 0.44852\n",
      "Batch train [6] loss 0.52579, dsc 0.47421\n",
      "Batch train [7] loss 0.57108, dsc 0.42892\n",
      "Batch train [8] loss 0.59448, dsc 0.40552\n",
      "Batch train [9] loss 0.56081, dsc 0.43919\n",
      "Batch train [10] loss 0.57287, dsc 0.42713\n",
      "Epoch [360] train done\n",
      "Batch eval [1] loss 0.58362, dsc 0.41638\n",
      "Batch eval [2] loss 0.62590, dsc 0.37410\n",
      "Batch eval [3] loss 0.58518, dsc 0.41482\n",
      "Batch eval [4] loss 0.63587, dsc 0.36413\n",
      "Batch eval [5] loss 0.59537, dsc 0.40463\n",
      "Epoch [360] valid done\n",
      "Epoch [360] T 1448.70s, deltaT 4.02s, loss: train 0.55799, valid 0.60519, dsc: train 0.44201, valid 0.39481\n",
      "Batch train [1] loss 0.52926, dsc 0.47074\n",
      "Batch train [2] loss 0.58180, dsc 0.41820\n",
      "Batch train [3] loss 0.55493, dsc 0.44507\n",
      "Batch train [4] loss 0.56746, dsc 0.43254\n",
      "Batch train [5] loss 0.55661, dsc 0.44339\n",
      "Batch train [6] loss 0.52494, dsc 0.47506\n",
      "Batch train [7] loss 0.56279, dsc 0.43721\n",
      "Batch train [8] loss 0.58854, dsc 0.41146\n",
      "Batch train [9] loss 0.53875, dsc 0.46125\n",
      "Batch train [10] loss 0.56879, dsc 0.43121\n",
      "Epoch [361] train done\n",
      "Batch eval [1] loss 0.58232, dsc 0.41768\n",
      "Batch eval [2] loss 0.62393, dsc 0.37607\n",
      "Batch eval [3] loss 0.58858, dsc 0.41142\n",
      "Batch eval [4] loss 0.64094, dsc 0.35906\n",
      "Batch eval [5] loss 0.59848, dsc 0.40152\n",
      "Epoch [361] valid done\n",
      "Epoch [361] T 1452.74s, deltaT 4.05s, loss: train 0.55739, valid 0.60685, dsc: train 0.44261, valid 0.39315\n",
      "Batch train [1] loss 0.55078, dsc 0.44922\n",
      "Batch train [2] loss 0.56388, dsc 0.43611\n",
      "Batch train [3] loss 0.56717, dsc 0.43283\n",
      "Batch train [4] loss 0.60335, dsc 0.39665\n",
      "Batch train [5] loss 0.57767, dsc 0.42233\n",
      "Batch train [6] loss 0.54422, dsc 0.45578\n",
      "Batch train [7] loss 0.51891, dsc 0.48109\n",
      "Batch train [8] loss 0.53060, dsc 0.46940\n",
      "Batch train [9] loss 0.54107, dsc 0.45893\n",
      "Batch train [10] loss 0.57132, dsc 0.42868\n",
      "Epoch [362] train done\n",
      "Batch eval [1] loss 0.58071, dsc 0.41929\n",
      "Batch eval [2] loss 0.60931, dsc 0.39069\n",
      "Batch eval [3] loss 0.58341, dsc 0.41659\n",
      "Batch eval [4] loss 0.62919, dsc 0.37081\n",
      "Batch eval [5] loss 0.58669, dsc 0.41331\n",
      "Epoch [362] valid done\n",
      "Epoch [362] T 1456.75s, deltaT 4.00s, loss: train 0.55690, valid 0.59786, dsc: train 0.44310, valid 0.40214\n",
      "Batch train [1] loss 0.56992, dsc 0.43008\n",
      "Batch train [2] loss 0.56053, dsc 0.43947\n",
      "Batch train [3] loss 0.56718, dsc 0.43282\n",
      "Batch train [4] loss 0.55369, dsc 0.44631\n",
      "Batch train [5] loss 0.57594, dsc 0.42406\n",
      "Batch train [6] loss 0.55295, dsc 0.44705\n",
      "Batch train [7] loss 0.52473, dsc 0.47527\n",
      "Batch train [8] loss 0.55501, dsc 0.44499\n",
      "Batch train [9] loss 0.58672, dsc 0.41328\n",
      "Batch train [10] loss 0.50358, dsc 0.49642\n",
      "Epoch [363] train done\n",
      "Batch eval [1] loss 0.57917, dsc 0.42083\n",
      "Batch eval [2] loss 0.61048, dsc 0.38952\n",
      "Batch eval [3] loss 0.58243, dsc 0.41757\n",
      "Batch eval [4] loss 0.62970, dsc 0.37030\n",
      "Batch eval [5] loss 0.58863, dsc 0.41137\n",
      "Epoch [363] valid done\n",
      "Epoch [363] T 1460.78s, deltaT 4.03s, loss: train 0.55503, valid 0.59808, dsc: train 0.44497, valid 0.40192\n",
      "Batch train [1] loss 0.56179, dsc 0.43821\n",
      "Batch train [2] loss 0.53824, dsc 0.46176\n",
      "Batch train [3] loss 0.55243, dsc 0.44757\n",
      "Batch train [4] loss 0.56662, dsc 0.43338\n",
      "Batch train [5] loss 0.55039, dsc 0.44961\n",
      "Batch train [6] loss 0.54562, dsc 0.45438\n",
      "Batch train [7] loss 0.57982, dsc 0.42018\n",
      "Batch train [8] loss 0.52264, dsc 0.47736\n",
      "Batch train [9] loss 0.59594, dsc 0.40406\n",
      "Batch train [10] loss 0.52811, dsc 0.47189\n",
      "Epoch [364] train done\n",
      "Batch eval [1] loss 0.58375, dsc 0.41625\n",
      "Batch eval [2] loss 0.61132, dsc 0.38868\n",
      "Batch eval [3] loss 0.58945, dsc 0.41055\n",
      "Batch eval [4] loss 0.62982, dsc 0.37018\n",
      "Batch eval [5] loss 0.59501, dsc 0.40499\n",
      "Epoch [364] valid done\n",
      "Epoch [364] T 1464.74s, deltaT 3.96s, loss: train 0.55416, valid 0.60187, dsc: train 0.44584, valid 0.39813\n",
      "Batch train [1] loss 0.52645, dsc 0.47355\n",
      "Batch train [2] loss 0.54268, dsc 0.45732\n",
      "Batch train [3] loss 0.55488, dsc 0.44512\n",
      "Batch train [4] loss 0.56200, dsc 0.43800\n",
      "Batch train [5] loss 0.54627, dsc 0.45373\n",
      "Batch train [6] loss 0.54283, dsc 0.45717\n",
      "Batch train [7] loss 0.59379, dsc 0.40621\n",
      "Batch train [8] loss 0.52680, dsc 0.47320\n",
      "Batch train [9] loss 0.54236, dsc 0.45764\n",
      "Batch train [10] loss 0.59722, dsc 0.40278\n",
      "Epoch [365] train done\n",
      "Batch eval [1] loss 0.57864, dsc 0.42136\n",
      "Batch eval [2] loss 0.61736, dsc 0.38264\n",
      "Batch eval [3] loss 0.59097, dsc 0.40903\n",
      "Batch eval [4] loss 0.63130, dsc 0.36870\n",
      "Batch eval [5] loss 0.59681, dsc 0.40319\n",
      "Epoch [365] valid done\n",
      "Epoch [365] T 1468.81s, deltaT 4.07s, loss: train 0.55353, valid 0.60301, dsc: train 0.44647, valid 0.39699\n",
      "Batch train [1] loss 0.54087, dsc 0.45913\n",
      "Batch train [2] loss 0.54026, dsc 0.45974\n",
      "Batch train [3] loss 0.58153, dsc 0.41847\n",
      "Batch train [4] loss 0.54943, dsc 0.45057\n",
      "Batch train [5] loss 0.52216, dsc 0.47784\n",
      "Batch train [6] loss 0.52857, dsc 0.47143\n",
      "Batch train [7] loss 0.53155, dsc 0.46845\n",
      "Batch train [8] loss 0.55911, dsc 0.44089\n",
      "Batch train [9] loss 0.57959, dsc 0.42041\n",
      "Batch train [10] loss 0.58461, dsc 0.41539\n",
      "Epoch [366] train done\n",
      "Batch eval [1] loss 0.58092, dsc 0.41908\n",
      "Batch eval [2] loss 0.61096, dsc 0.38904\n",
      "Batch eval [3] loss 0.58661, dsc 0.41339\n",
      "Batch eval [4] loss 0.63184, dsc 0.36816\n",
      "Batch eval [5] loss 0.59198, dsc 0.40802\n",
      "Epoch [366] valid done\n",
      "Epoch [366] T 1472.86s, deltaT 4.05s, loss: train 0.55177, valid 0.60046, dsc: train 0.44823, valid 0.39954\n",
      "Batch train [1] loss 0.53824, dsc 0.46176\n",
      "Batch train [2] loss 0.51708, dsc 0.48292\n",
      "Batch train [3] loss 0.54080, dsc 0.45920\n",
      "Batch train [4] loss 0.56789, dsc 0.43211\n",
      "Batch train [5] loss 0.57053, dsc 0.42947\n",
      "Batch train [6] loss 0.53958, dsc 0.46042\n",
      "Batch train [7] loss 0.55079, dsc 0.44921\n",
      "Batch train [8] loss 0.56146, dsc 0.43854\n",
      "Batch train [9] loss 0.54073, dsc 0.45927\n",
      "Batch train [10] loss 0.56735, dsc 0.43265\n",
      "Epoch [367] train done\n",
      "Batch eval [1] loss 0.57784, dsc 0.42216\n",
      "Batch eval [2] loss 0.60649, dsc 0.39351\n",
      "Batch eval [3] loss 0.58452, dsc 0.41548\n",
      "Batch eval [4] loss 0.62698, dsc 0.37302\n",
      "Batch eval [5] loss 0.58448, dsc 0.41552\n",
      "Epoch [367] valid done\n",
      "Epoch [367] T 1476.89s, deltaT 4.02s, loss: train 0.54944, valid 0.59606, dsc: train 0.45056, valid 0.40394\n",
      "Batch train [1] loss 0.52941, dsc 0.47059\n",
      "Batch train [2] loss 0.55978, dsc 0.44022\n",
      "Batch train [3] loss 0.55871, dsc 0.44129\n",
      "Batch train [4] loss 0.55498, dsc 0.44502\n",
      "Batch train [5] loss 0.52962, dsc 0.47038\n",
      "Batch train [6] loss 0.57515, dsc 0.42485\n",
      "Batch train [7] loss 0.54089, dsc 0.45911\n",
      "Batch train [8] loss 0.55598, dsc 0.44402\n",
      "Batch train [9] loss 0.53373, dsc 0.46627\n",
      "Batch train [10] loss 0.54963, dsc 0.45037\n",
      "Epoch [368] train done\n",
      "Batch eval [1] loss 0.57622, dsc 0.42378\n",
      "Batch eval [2] loss 0.60268, dsc 0.39732\n",
      "Batch eval [3] loss 0.58018, dsc 0.41982\n",
      "Batch eval [4] loss 0.62363, dsc 0.37637\n",
      "Batch eval [5] loss 0.58332, dsc 0.41668\n",
      "Epoch [368] valid done\n",
      "Epoch [368] T 1480.91s, deltaT 4.02s, loss: train 0.54879, valid 0.59321, dsc: train 0.45121, valid 0.40679\n",
      "Batch train [1] loss 0.53427, dsc 0.46573\n",
      "Batch train [2] loss 0.53356, dsc 0.46644\n",
      "Batch train [3] loss 0.50825, dsc 0.49175\n",
      "Batch train [4] loss 0.55460, dsc 0.44540\n",
      "Batch train [5] loss 0.53915, dsc 0.46085\n",
      "Batch train [6] loss 0.53958, dsc 0.46042\n",
      "Batch train [7] loss 0.58187, dsc 0.41813\n",
      "Batch train [8] loss 0.54955, dsc 0.45045\n",
      "Batch train [9] loss 0.54294, dsc 0.45706\n",
      "Batch train [10] loss 0.59590, dsc 0.40410\n",
      "Epoch [369] train done\n",
      "Batch eval [1] loss 0.57866, dsc 0.42134\n",
      "Batch eval [2] loss 0.60611, dsc 0.39389\n",
      "Batch eval [3] loss 0.58392, dsc 0.41608\n",
      "Batch eval [4] loss 0.62919, dsc 0.37081\n",
      "Batch eval [5] loss 0.58480, dsc 0.41520\n",
      "Epoch [369] valid done\n",
      "Epoch [369] T 1484.91s, deltaT 4.00s, loss: train 0.54797, valid 0.59653, dsc: train 0.45203, valid 0.40347\n",
      "Batch train [1] loss 0.54517, dsc 0.45483\n",
      "Batch train [2] loss 0.50551, dsc 0.49449\n",
      "Batch train [3] loss 0.52204, dsc 0.47796\n",
      "Batch train [4] loss 0.54802, dsc 0.45198\n",
      "Batch train [5] loss 0.58752, dsc 0.41248\n",
      "Batch train [6] loss 0.55341, dsc 0.44659\n",
      "Batch train [7] loss 0.54956, dsc 0.45044\n",
      "Batch train [8] loss 0.53819, dsc 0.46181\n",
      "Batch train [9] loss 0.56309, dsc 0.43691\n",
      "Batch train [10] loss 0.55521, dsc 0.44479\n",
      "Epoch [370] train done\n",
      "Batch eval [1] loss 0.57181, dsc 0.42819\n",
      "Batch eval [2] loss 0.60318, dsc 0.39682\n",
      "Batch eval [3] loss 0.58374, dsc 0.41626\n",
      "Batch eval [4] loss 0.62386, dsc 0.37614\n",
      "Batch eval [5] loss 0.58347, dsc 0.41653\n",
      "Epoch [370] valid done\n",
      "Epoch [370] T 1488.86s, deltaT 3.95s, loss: train 0.54677, valid 0.59321, dsc: train 0.45323, valid 0.40679\n",
      "Batch train [1] loss 0.50111, dsc 0.49889\n",
      "Batch train [2] loss 0.55042, dsc 0.44958\n",
      "Batch train [3] loss 0.53985, dsc 0.46015\n",
      "Batch train [4] loss 0.59440, dsc 0.40560\n",
      "Batch train [5] loss 0.55048, dsc 0.44952\n",
      "Batch train [6] loss 0.51207, dsc 0.48793\n",
      "Batch train [7] loss 0.53470, dsc 0.46530\n",
      "Batch train [8] loss 0.56600, dsc 0.43400\n",
      "Batch train [9] loss 0.54525, dsc 0.45475\n",
      "Batch train [10] loss 0.56206, dsc 0.43794\n",
      "Epoch [371] train done\n",
      "Batch eval [1] loss 0.57825, dsc 0.42175\n",
      "Batch eval [2] loss 0.61642, dsc 0.38358\n",
      "Batch eval [3] loss 0.58315, dsc 0.41685\n",
      "Batch eval [4] loss 0.62774, dsc 0.37226\n",
      "Batch eval [5] loss 0.59276, dsc 0.40724\n",
      "Epoch [371] valid done\n",
      "Epoch [371] T 1492.89s, deltaT 4.03s, loss: train 0.54563, valid 0.59966, dsc: train 0.45437, valid 0.40034\n",
      "Batch train [1] loss 0.54822, dsc 0.45178\n",
      "Batch train [2] loss 0.54856, dsc 0.45144\n",
      "Batch train [3] loss 0.51557, dsc 0.48443\n",
      "Batch train [4] loss 0.55382, dsc 0.44618\n",
      "Batch train [5] loss 0.52261, dsc 0.47739\n",
      "Batch train [6] loss 0.51775, dsc 0.48225\n",
      "Batch train [7] loss 0.53608, dsc 0.46392\n",
      "Batch train [8] loss 0.57938, dsc 0.42062\n",
      "Batch train [9] loss 0.56010, dsc 0.43990\n",
      "Batch train [10] loss 0.55803, dsc 0.44197\n",
      "Epoch [372] train done\n",
      "Batch eval [1] loss 0.57324, dsc 0.42676\n",
      "Batch eval [2] loss 0.61324, dsc 0.38676\n",
      "Batch eval [3] loss 0.58168, dsc 0.41832\n",
      "Batch eval [4] loss 0.63106, dsc 0.36894\n",
      "Batch eval [5] loss 0.59075, dsc 0.40925\n",
      "Epoch [372] valid done\n",
      "Epoch [372] T 1496.91s, deltaT 4.02s, loss: train 0.54401, valid 0.59800, dsc: train 0.45599, valid 0.40200\n",
      "Batch train [1] loss 0.51847, dsc 0.48153\n",
      "Batch train [2] loss 0.55189, dsc 0.44811\n",
      "Batch train [3] loss 0.53166, dsc 0.46834\n",
      "Batch train [4] loss 0.52985, dsc 0.47015\n",
      "Batch train [5] loss 0.56093, dsc 0.43907\n",
      "Batch train [6] loss 0.57094, dsc 0.42906\n",
      "Batch train [7] loss 0.53145, dsc 0.46855\n",
      "Batch train [8] loss 0.53875, dsc 0.46125\n",
      "Batch train [9] loss 0.56323, dsc 0.43677\n",
      "Batch train [10] loss 0.53252, dsc 0.46748\n",
      "Epoch [373] train done\n",
      "Batch eval [1] loss 0.57487, dsc 0.42513\n",
      "Batch eval [2] loss 0.60864, dsc 0.39136\n",
      "Batch eval [3] loss 0.57851, dsc 0.42149\n",
      "Batch eval [4] loss 0.62790, dsc 0.37210\n",
      "Batch eval [5] loss 0.59513, dsc 0.40487\n",
      "Epoch [373] valid done\n",
      "Epoch [373] T 1500.93s, deltaT 4.02s, loss: train 0.54297, valid 0.59701, dsc: train 0.45703, valid 0.40299\n",
      "Batch train [1] loss 0.55311, dsc 0.44689\n",
      "Batch train [2] loss 0.53877, dsc 0.46123\n",
      "Batch train [3] loss 0.55313, dsc 0.44687\n",
      "Batch train [4] loss 0.57780, dsc 0.42220\n",
      "Batch train [5] loss 0.55399, dsc 0.44601\n",
      "Batch train [6] loss 0.53691, dsc 0.46309\n",
      "Batch train [7] loss 0.53111, dsc 0.46889\n",
      "Batch train [8] loss 0.49433, dsc 0.50567\n",
      "Batch train [9] loss 0.57666, dsc 0.42334\n",
      "Batch train [10] loss 0.51325, dsc 0.48675\n",
      "Epoch [374] train done\n",
      "Batch eval [1] loss 0.57896, dsc 0.42104\n",
      "Batch eval [2] loss 0.60684, dsc 0.39316\n",
      "Batch eval [3] loss 0.58385, dsc 0.41615\n",
      "Batch eval [4] loss 0.62893, dsc 0.37107\n",
      "Batch eval [5] loss 0.58694, dsc 0.41306\n",
      "Epoch [374] valid done\n",
      "Epoch [374] T 1504.93s, deltaT 3.99s, loss: train 0.54291, valid 0.59710, dsc: train 0.45709, valid 0.40290\n",
      "Batch train [1] loss 0.56264, dsc 0.43736\n",
      "Batch train [2] loss 0.55842, dsc 0.44158\n",
      "Batch train [3] loss 0.52585, dsc 0.47415\n",
      "Batch train [4] loss 0.54063, dsc 0.45937\n",
      "Batch train [5] loss 0.53572, dsc 0.46428\n",
      "Batch train [6] loss 0.52623, dsc 0.47377\n",
      "Batch train [7] loss 0.53066, dsc 0.46934\n",
      "Batch train [8] loss 0.56538, dsc 0.43462\n",
      "Batch train [9] loss 0.54362, dsc 0.45638\n",
      "Batch train [10] loss 0.51717, dsc 0.48283\n",
      "Epoch [375] train done\n",
      "Batch eval [1] loss 0.56986, dsc 0.43014\n",
      "Batch eval [2] loss 0.60297, dsc 0.39703\n",
      "Batch eval [3] loss 0.57195, dsc 0.42805\n",
      "Batch eval [4] loss 0.62342, dsc 0.37658\n",
      "Batch eval [5] loss 0.57815, dsc 0.42185\n",
      "Epoch [375] valid done\n",
      "Epoch [375] T 1508.92s, deltaT 3.99s, loss: train 0.54063, valid 0.58927, dsc: train 0.45937, valid 0.41073\n",
      "Batch train [1] loss 0.49915, dsc 0.50085\n",
      "Batch train [2] loss 0.52934, dsc 0.47066\n",
      "Batch train [3] loss 0.55115, dsc 0.44885\n",
      "Batch train [4] loss 0.53703, dsc 0.46297\n",
      "Batch train [5] loss 0.54465, dsc 0.45535\n",
      "Batch train [6] loss 0.51917, dsc 0.48083\n",
      "Batch train [7] loss 0.56083, dsc 0.43917\n",
      "Batch train [8] loss 0.57427, dsc 0.42573\n",
      "Batch train [9] loss 0.52008, dsc 0.47992\n",
      "Batch train [10] loss 0.56759, dsc 0.43241\n",
      "Epoch [376] train done\n",
      "Batch eval [1] loss 0.56926, dsc 0.43074\n",
      "Batch eval [2] loss 0.60411, dsc 0.39589\n",
      "Batch eval [3] loss 0.56863, dsc 0.43137\n",
      "Batch eval [4] loss 0.62299, dsc 0.37701\n",
      "Batch eval [5] loss 0.58568, dsc 0.41432\n",
      "Epoch [376] valid done\n",
      "Epoch [376] T 1512.94s, deltaT 4.01s, loss: train 0.54033, valid 0.59013, dsc: train 0.45967, valid 0.40987\n",
      "Batch train [1] loss 0.50545, dsc 0.49455\n",
      "Batch train [2] loss 0.54465, dsc 0.45535\n",
      "Batch train [3] loss 0.54274, dsc 0.45726\n",
      "Batch train [4] loss 0.54501, dsc 0.45499\n",
      "Batch train [5] loss 0.53338, dsc 0.46662\n",
      "Batch train [6] loss 0.54648, dsc 0.45352\n",
      "Batch train [7] loss 0.52785, dsc 0.47215\n",
      "Batch train [8] loss 0.56590, dsc 0.43409\n",
      "Batch train [9] loss 0.52448, dsc 0.47552\n",
      "Batch train [10] loss 0.54711, dsc 0.45289\n",
      "Epoch [377] train done\n",
      "Batch eval [1] loss 0.57373, dsc 0.42627\n",
      "Batch eval [2] loss 0.60168, dsc 0.39832\n",
      "Batch eval [3] loss 0.56715, dsc 0.43285\n",
      "Batch eval [4] loss 0.62643, dsc 0.37357\n",
      "Batch eval [5] loss 0.58609, dsc 0.41391\n",
      "Epoch [377] valid done\n",
      "Epoch [377] T 1516.94s, deltaT 4.00s, loss: train 0.53831, valid 0.59102, dsc: train 0.46169, valid 0.40898\n",
      "Batch train [1] loss 0.54728, dsc 0.45272\n",
      "Batch train [2] loss 0.52449, dsc 0.47551\n",
      "Batch train [3] loss 0.52089, dsc 0.47911\n",
      "Batch train [4] loss 0.57199, dsc 0.42801\n",
      "Batch train [5] loss 0.56637, dsc 0.43363\n",
      "Batch train [6] loss 0.55137, dsc 0.44863\n",
      "Batch train [7] loss 0.52370, dsc 0.47630\n",
      "Batch train [8] loss 0.49645, dsc 0.50355\n",
      "Batch train [9] loss 0.54316, dsc 0.45684\n",
      "Batch train [10] loss 0.52760, dsc 0.47240\n",
      "Epoch [378] train done\n",
      "Batch eval [1] loss 0.56692, dsc 0.43308\n",
      "Batch eval [2] loss 0.59910, dsc 0.40090\n",
      "Batch eval [3] loss 0.56424, dsc 0.43576\n",
      "Batch eval [4] loss 0.61763, dsc 0.38237\n",
      "Batch eval [5] loss 0.57770, dsc 0.42230\n",
      "Epoch [378] valid done\n",
      "Epoch [378] T 1520.94s, deltaT 4.00s, loss: train 0.53733, valid 0.58512, dsc: train 0.46267, valid 0.41488\n",
      "Batch train [1] loss 0.49643, dsc 0.50357\n",
      "Batch train [2] loss 0.53292, dsc 0.46708\n",
      "Batch train [3] loss 0.55282, dsc 0.44718\n",
      "Batch train [4] loss 0.56253, dsc 0.43747\n",
      "Batch train [5] loss 0.51814, dsc 0.48186\n",
      "Batch train [6] loss 0.54266, dsc 0.45734\n",
      "Batch train [7] loss 0.54319, dsc 0.45681\n",
      "Batch train [8] loss 0.51071, dsc 0.48929\n",
      "Batch train [9] loss 0.53240, dsc 0.46760\n",
      "Batch train [10] loss 0.57335, dsc 0.42665\n",
      "Epoch [379] train done\n",
      "Batch eval [1] loss 0.56362, dsc 0.43638\n",
      "Batch eval [2] loss 0.59437, dsc 0.40563\n",
      "Batch eval [3] loss 0.56436, dsc 0.43564\n",
      "Batch eval [4] loss 0.61886, dsc 0.38114\n",
      "Batch eval [5] loss 0.57476, dsc 0.42524\n",
      "Epoch [379] valid done\n",
      "Epoch [379] T 1524.85s, deltaT 3.91s, loss: train 0.53652, valid 0.58319, dsc: train 0.46348, valid 0.41681\n",
      "Batch train [1] loss 0.52458, dsc 0.47542\n",
      "Batch train [2] loss 0.53966, dsc 0.46034\n",
      "Batch train [3] loss 0.52054, dsc 0.47946\n",
      "Batch train [4] loss 0.55614, dsc 0.44386\n",
      "Batch train [5] loss 0.54617, dsc 0.45383\n",
      "Batch train [6] loss 0.50489, dsc 0.49511\n",
      "Batch train [7] loss 0.54359, dsc 0.45641\n",
      "Batch train [8] loss 0.51870, dsc 0.48130\n",
      "Batch train [9] loss 0.53891, dsc 0.46109\n",
      "Batch train [10] loss 0.55484, dsc 0.44516\n",
      "Epoch [380] train done\n",
      "Batch eval [1] loss 0.56203, dsc 0.43797\n",
      "Batch eval [2] loss 0.59316, dsc 0.40684\n",
      "Batch eval [3] loss 0.56526, dsc 0.43474\n",
      "Batch eval [4] loss 0.61314, dsc 0.38686\n",
      "Batch eval [5] loss 0.57505, dsc 0.42495\n",
      "Epoch [380] valid done\n",
      "Epoch [380] T 1528.84s, deltaT 3.99s, loss: train 0.53480, valid 0.58173, dsc: train 0.46520, valid 0.41827\n",
      "Batch train [1] loss 0.53622, dsc 0.46378\n",
      "Batch train [2] loss 0.51978, dsc 0.48022\n",
      "Batch train [3] loss 0.53219, dsc 0.46781\n",
      "Batch train [4] loss 0.51980, dsc 0.48020\n",
      "Batch train [5] loss 0.57908, dsc 0.42092\n",
      "Batch train [6] loss 0.52061, dsc 0.47939\n",
      "Batch train [7] loss 0.56944, dsc 0.43056\n",
      "Batch train [8] loss 0.50528, dsc 0.49472\n",
      "Batch train [9] loss 0.50698, dsc 0.49302\n",
      "Batch train [10] loss 0.55436, dsc 0.44564\n",
      "Epoch [381] train done\n",
      "Batch eval [1] loss 0.55948, dsc 0.44052\n",
      "Batch eval [2] loss 0.59429, dsc 0.40571\n",
      "Batch eval [3] loss 0.56460, dsc 0.43540\n",
      "Batch eval [4] loss 0.61279, dsc 0.38721\n",
      "Batch eval [5] loss 0.57015, dsc 0.42985\n",
      "Epoch [381] valid done\n",
      "Epoch [381] T 1532.85s, deltaT 4.01s, loss: train 0.53437, valid 0.58026, dsc: train 0.46563, valid 0.41974\n",
      "Batch train [1] loss 0.54830, dsc 0.45170\n",
      "Batch train [2] loss 0.51043, dsc 0.48957\n",
      "Batch train [3] loss 0.51375, dsc 0.48625\n",
      "Batch train [4] loss 0.52620, dsc 0.47380\n",
      "Batch train [5] loss 0.55502, dsc 0.44498\n",
      "Batch train [6] loss 0.52195, dsc 0.47805\n",
      "Batch train [7] loss 0.54423, dsc 0.45577\n",
      "Batch train [8] loss 0.52729, dsc 0.47271\n",
      "Batch train [9] loss 0.54333, dsc 0.45667\n",
      "Batch train [10] loss 0.53777, dsc 0.46223\n",
      "Epoch [382] train done\n",
      "Batch eval [1] loss 0.56521, dsc 0.43479\n",
      "Batch eval [2] loss 0.60196, dsc 0.39804\n",
      "Batch eval [3] loss 0.56631, dsc 0.43369\n",
      "Batch eval [4] loss 0.61314, dsc 0.38686\n",
      "Batch eval [5] loss 0.57855, dsc 0.42145\n",
      "Epoch [382] valid done\n",
      "Epoch [382] T 1536.87s, deltaT 4.01s, loss: train 0.53283, valid 0.58503, dsc: train 0.46717, valid 0.41497\n",
      "Batch train [1] loss 0.53809, dsc 0.46191\n",
      "Batch train [2] loss 0.54180, dsc 0.45820\n",
      "Batch train [3] loss 0.51796, dsc 0.48204\n",
      "Batch train [4] loss 0.49226, dsc 0.50774\n",
      "Batch train [5] loss 0.54909, dsc 0.45091\n",
      "Batch train [6] loss 0.54966, dsc 0.45034\n",
      "Batch train [7] loss 0.58618, dsc 0.41382\n",
      "Batch train [8] loss 0.51329, dsc 0.48671\n",
      "Batch train [9] loss 0.52622, dsc 0.47378\n",
      "Batch train [10] loss 0.51013, dsc 0.48987\n",
      "Epoch [383] train done\n",
      "Batch eval [1] loss 0.56280, dsc 0.43720\n",
      "Batch eval [2] loss 0.60008, dsc 0.39992\n",
      "Batch eval [3] loss 0.56454, dsc 0.43546\n",
      "Batch eval [4] loss 0.61617, dsc 0.38383\n",
      "Batch eval [5] loss 0.58061, dsc 0.41939\n",
      "Epoch [383] valid done\n",
      "Epoch [383] T 1540.90s, deltaT 4.03s, loss: train 0.53247, valid 0.58484, dsc: train 0.46753, valid 0.41516\n",
      "Batch train [1] loss 0.51992, dsc 0.48008\n",
      "Batch train [2] loss 0.57725, dsc 0.42275\n",
      "Batch train [3] loss 0.51032, dsc 0.48968\n",
      "Batch train [4] loss 0.51581, dsc 0.48419\n",
      "Batch train [5] loss 0.53831, dsc 0.46169\n",
      "Batch train [6] loss 0.53679, dsc 0.46321\n",
      "Batch train [7] loss 0.52267, dsc 0.47733\n",
      "Batch train [8] loss 0.52778, dsc 0.47222\n",
      "Batch train [9] loss 0.52123, dsc 0.47877\n",
      "Batch train [10] loss 0.53531, dsc 0.46469\n",
      "Epoch [384] train done\n",
      "Batch eval [1] loss 0.56190, dsc 0.43810\n",
      "Batch eval [2] loss 0.60151, dsc 0.39849\n",
      "Batch eval [3] loss 0.57054, dsc 0.42946\n",
      "Batch eval [4] loss 0.62119, dsc 0.37881\n",
      "Batch eval [5] loss 0.58348, dsc 0.41652\n",
      "Epoch [384] valid done\n",
      "Epoch [384] T 1544.93s, deltaT 4.03s, loss: train 0.53054, valid 0.58773, dsc: train 0.46946, valid 0.41227\n",
      "Batch train [1] loss 0.53690, dsc 0.46310\n",
      "Batch train [2] loss 0.49577, dsc 0.50423\n",
      "Batch train [3] loss 0.54349, dsc 0.45651\n",
      "Batch train [4] loss 0.55044, dsc 0.44956\n",
      "Batch train [5] loss 0.50821, dsc 0.49179\n",
      "Batch train [6] loss 0.53416, dsc 0.46584\n",
      "Batch train [7] loss 0.51904, dsc 0.48096\n",
      "Batch train [8] loss 0.51719, dsc 0.48281\n",
      "Batch train [9] loss 0.53096, dsc 0.46904\n",
      "Batch train [10] loss 0.56688, dsc 0.43312\n",
      "Epoch [385] train done\n",
      "Batch eval [1] loss 0.56891, dsc 0.43109\n",
      "Batch eval [2] loss 0.60222, dsc 0.39778\n",
      "Batch eval [3] loss 0.56898, dsc 0.43102\n",
      "Batch eval [4] loss 0.61838, dsc 0.38162\n",
      "Batch eval [5] loss 0.58366, dsc 0.41634\n",
      "Epoch [385] valid done\n",
      "Epoch [385] T 1548.92s, deltaT 3.99s, loss: train 0.53030, valid 0.58843, dsc: train 0.46970, valid 0.41157\n",
      "Batch train [1] loss 0.55581, dsc 0.44419\n",
      "Batch train [2] loss 0.49278, dsc 0.50722\n",
      "Batch train [3] loss 0.52585, dsc 0.47415\n",
      "Batch train [4] loss 0.52754, dsc 0.47246\n",
      "Batch train [5] loss 0.50907, dsc 0.49093\n",
      "Batch train [6] loss 0.53252, dsc 0.46748\n",
      "Batch train [7] loss 0.52457, dsc 0.47543\n",
      "Batch train [8] loss 0.56600, dsc 0.43400\n",
      "Batch train [9] loss 0.51170, dsc 0.48830\n",
      "Batch train [10] loss 0.54744, dsc 0.45256\n",
      "Epoch [386] train done\n",
      "Batch eval [1] loss 0.56412, dsc 0.43588\n",
      "Batch eval [2] loss 0.59903, dsc 0.40097\n",
      "Batch eval [3] loss 0.57326, dsc 0.42674\n",
      "Batch eval [4] loss 0.62036, dsc 0.37964\n",
      "Batch eval [5] loss 0.58298, dsc 0.41702\n",
      "Epoch [386] valid done\n",
      "Epoch [386] T 1552.89s, deltaT 3.97s, loss: train 0.52933, valid 0.58795, dsc: train 0.47067, valid 0.41205\n",
      "Batch train [1] loss 0.54837, dsc 0.45163\n",
      "Batch train [2] loss 0.50665, dsc 0.49335\n",
      "Batch train [3] loss 0.53116, dsc 0.46884\n",
      "Batch train [4] loss 0.51879, dsc 0.48121\n",
      "Batch train [5] loss 0.53017, dsc 0.46983\n",
      "Batch train [6] loss 0.51389, dsc 0.48611\n",
      "Batch train [7] loss 0.52510, dsc 0.47490\n",
      "Batch train [8] loss 0.52378, dsc 0.47622\n",
      "Batch train [9] loss 0.56225, dsc 0.43775\n",
      "Batch train [10] loss 0.51700, dsc 0.48300\n",
      "Epoch [387] train done\n",
      "Batch eval [1] loss 0.55921, dsc 0.44079\n",
      "Batch eval [2] loss 0.59539, dsc 0.40461\n",
      "Batch eval [3] loss 0.55823, dsc 0.44177\n",
      "Batch eval [4] loss 0.60824, dsc 0.39176\n",
      "Batch eval [5] loss 0.56848, dsc 0.43152\n",
      "Epoch [387] valid done\n",
      "Epoch [387] T 1556.93s, deltaT 4.04s, loss: train 0.52772, valid 0.57791, dsc: train 0.47228, valid 0.42209\n",
      "Batch train [1] loss 0.54428, dsc 0.45572\n",
      "Batch train [2] loss 0.51652, dsc 0.48348\n",
      "Batch train [3] loss 0.50749, dsc 0.49251\n",
      "Batch train [4] loss 0.47615, dsc 0.52385\n",
      "Batch train [5] loss 0.55966, dsc 0.44034\n",
      "Batch train [6] loss 0.53428, dsc 0.46572\n",
      "Batch train [7] loss 0.51653, dsc 0.48347\n",
      "Batch train [8] loss 0.54173, dsc 0.45827\n",
      "Batch train [9] loss 0.50918, dsc 0.49082\n",
      "Batch train [10] loss 0.57904, dsc 0.42096\n",
      "Epoch [388] train done\n",
      "Batch eval [1] loss 0.57641, dsc 0.42359\n",
      "Batch eval [2] loss 0.60551, dsc 0.39449\n",
      "Batch eval [3] loss 0.56496, dsc 0.43504\n",
      "Batch eval [4] loss 0.61507, dsc 0.38492\n",
      "Batch eval [5] loss 0.58052, dsc 0.41948\n",
      "Epoch [388] valid done\n",
      "Epoch [388] T 1560.96s, deltaT 4.03s, loss: train 0.52848, valid 0.58849, dsc: train 0.47152, valid 0.41151\n",
      "Batch train [1] loss 0.54519, dsc 0.45481\n",
      "Batch train [2] loss 0.52869, dsc 0.47131\n",
      "Batch train [3] loss 0.54343, dsc 0.45657\n",
      "Batch train [4] loss 0.50720, dsc 0.49280\n",
      "Batch train [5] loss 0.52253, dsc 0.47747\n",
      "Batch train [6] loss 0.52037, dsc 0.47963\n",
      "Batch train [7] loss 0.52503, dsc 0.47497\n",
      "Batch train [8] loss 0.52176, dsc 0.47824\n",
      "Batch train [9] loss 0.51039, dsc 0.48961\n",
      "Batch train [10] loss 0.53631, dsc 0.46369\n",
      "Epoch [389] train done\n",
      "Batch eval [1] loss 0.56783, dsc 0.43217\n",
      "Batch eval [2] loss 0.58951, dsc 0.41049\n",
      "Batch eval [3] loss 0.56312, dsc 0.43688\n",
      "Batch eval [4] loss 0.60951, dsc 0.39049\n",
      "Batch eval [5] loss 0.56949, dsc 0.43051\n",
      "Epoch [389] valid done\n",
      "Epoch [389] T 1564.96s, deltaT 4.00s, loss: train 0.52609, valid 0.57989, dsc: train 0.47391, valid 0.42011\n",
      "Batch train [1] loss 0.54002, dsc 0.45998\n",
      "Batch train [2] loss 0.51635, dsc 0.48365\n",
      "Batch train [3] loss 0.54979, dsc 0.45021\n",
      "Batch train [4] loss 0.54048, dsc 0.45952\n",
      "Batch train [5] loss 0.55128, dsc 0.44872\n",
      "Batch train [6] loss 0.50779, dsc 0.49221\n",
      "Batch train [7] loss 0.48912, dsc 0.51088\n",
      "Batch train [8] loss 0.53981, dsc 0.46019\n",
      "Batch train [9] loss 0.50337, dsc 0.49663\n",
      "Batch train [10] loss 0.51814, dsc 0.48186\n",
      "Epoch [390] train done\n",
      "Batch eval [1] loss 0.55657, dsc 0.44343\n",
      "Batch eval [2] loss 0.58479, dsc 0.41521\n",
      "Batch eval [3] loss 0.54717, dsc 0.45283\n",
      "Batch eval [4] loss 0.60860, dsc 0.39140\n",
      "Batch eval [5] loss 0.56504, dsc 0.43496\n",
      "Epoch [390] valid done\n",
      "Epoch [390] T 1568.99s, deltaT 4.03s, loss: train 0.52561, valid 0.57243, dsc: train 0.47439, valid 0.42757\n",
      "Batch train [1] loss 0.53577, dsc 0.46423\n",
      "Batch train [2] loss 0.50810, dsc 0.49190\n",
      "Batch train [3] loss 0.55778, dsc 0.44222\n",
      "Batch train [4] loss 0.51689, dsc 0.48311\n",
      "Batch train [5] loss 0.52363, dsc 0.47637\n",
      "Batch train [6] loss 0.53107, dsc 0.46893\n",
      "Batch train [7] loss 0.53487, dsc 0.46513\n",
      "Batch train [8] loss 0.49806, dsc 0.50194\n",
      "Batch train [9] loss 0.54510, dsc 0.45490\n",
      "Batch train [10] loss 0.49235, dsc 0.50765\n",
      "Epoch [391] train done\n",
      "Batch eval [1] loss 0.55989, dsc 0.44011\n",
      "Batch eval [2] loss 0.58867, dsc 0.41133\n",
      "Batch eval [3] loss 0.55938, dsc 0.44062\n",
      "Batch eval [4] loss 0.60549, dsc 0.39451\n",
      "Batch eval [5] loss 0.56330, dsc 0.43670\n",
      "Epoch [391] valid done\n",
      "Epoch [391] T 1572.94s, deltaT 3.94s, loss: train 0.52436, valid 0.57534, dsc: train 0.47564, valid 0.42466\n",
      "Batch train [1] loss 0.52292, dsc 0.47708\n",
      "Batch train [2] loss 0.50476, dsc 0.49524\n",
      "Batch train [3] loss 0.49060, dsc 0.50940\n",
      "Batch train [4] loss 0.56140, dsc 0.43860\n",
      "Batch train [5] loss 0.54591, dsc 0.45409\n",
      "Batch train [6] loss 0.50185, dsc 0.49815\n",
      "Batch train [7] loss 0.50886, dsc 0.49114\n",
      "Batch train [8] loss 0.53976, dsc 0.46024\n",
      "Batch train [9] loss 0.53728, dsc 0.46272\n",
      "Batch train [10] loss 0.51715, dsc 0.48285\n",
      "Epoch [392] train done\n",
      "Batch eval [1] loss 0.56039, dsc 0.43961\n",
      "Batch eval [2] loss 0.58955, dsc 0.41045\n",
      "Batch eval [3] loss 0.55557, dsc 0.44443\n",
      "Batch eval [4] loss 0.60864, dsc 0.39136\n",
      "Batch eval [5] loss 0.56989, dsc 0.43011\n",
      "Epoch [392] valid done\n",
      "Epoch [392] T 1576.92s, deltaT 3.98s, loss: train 0.52305, valid 0.57681, dsc: train 0.47695, valid 0.42319\n",
      "Batch train [1] loss 0.55998, dsc 0.44002\n",
      "Batch train [2] loss 0.54262, dsc 0.45738\n",
      "Batch train [3] loss 0.53647, dsc 0.46353\n",
      "Batch train [4] loss 0.48055, dsc 0.51945\n",
      "Batch train [5] loss 0.50906, dsc 0.49094\n",
      "Batch train [6] loss 0.51209, dsc 0.48791\n",
      "Batch train [7] loss 0.52134, dsc 0.47866\n",
      "Batch train [8] loss 0.52183, dsc 0.47817\n",
      "Batch train [9] loss 0.48652, dsc 0.51348\n",
      "Batch train [10] loss 0.55252, dsc 0.44748\n",
      "Epoch [393] train done\n",
      "Batch eval [1] loss 0.55000, dsc 0.45000\n",
      "Batch eval [2] loss 0.58756, dsc 0.41244\n",
      "Batch eval [3] loss 0.55638, dsc 0.44362\n",
      "Batch eval [4] loss 0.60944, dsc 0.39056\n",
      "Batch eval [5] loss 0.56193, dsc 0.43807\n",
      "Epoch [393] valid done\n",
      "Epoch [393] T 1580.91s, deltaT 4.00s, loss: train 0.52230, valid 0.57306, dsc: train 0.47770, valid 0.42694\n",
      "Batch train [1] loss 0.55565, dsc 0.44435\n",
      "Batch train [2] loss 0.49461, dsc 0.50539\n",
      "Batch train [3] loss 0.55150, dsc 0.44850\n",
      "Batch train [4] loss 0.48036, dsc 0.51964\n",
      "Batch train [5] loss 0.53691, dsc 0.46309\n",
      "Batch train [6] loss 0.52091, dsc 0.47909\n",
      "Batch train [7] loss 0.52148, dsc 0.47852\n",
      "Batch train [8] loss 0.51364, dsc 0.48636\n",
      "Batch train [9] loss 0.51241, dsc 0.48759\n",
      "Batch train [10] loss 0.51816, dsc 0.48184\n",
      "Epoch [394] train done\n",
      "Batch eval [1] loss 0.54805, dsc 0.45195\n",
      "Batch eval [2] loss 0.59206, dsc 0.40794\n",
      "Batch eval [3] loss 0.54887, dsc 0.45113\n",
      "Batch eval [4] loss 0.60879, dsc 0.39121\n",
      "Batch eval [5] loss 0.56504, dsc 0.43496\n",
      "Epoch [394] valid done\n",
      "Epoch [394] T 1584.90s, deltaT 3.98s, loss: train 0.52056, valid 0.57256, dsc: train 0.47944, valid 0.42744\n",
      "Batch train [1] loss 0.50597, dsc 0.49403\n",
      "Batch train [2] loss 0.50468, dsc 0.49532\n",
      "Batch train [3] loss 0.49483, dsc 0.50517\n",
      "Batch train [4] loss 0.49697, dsc 0.50303\n",
      "Batch train [5] loss 0.52541, dsc 0.47459\n",
      "Batch train [6] loss 0.53518, dsc 0.46482\n",
      "Batch train [7] loss 0.53980, dsc 0.46020\n",
      "Batch train [8] loss 0.52391, dsc 0.47609\n",
      "Batch train [9] loss 0.54499, dsc 0.45501\n",
      "Batch train [10] loss 0.51825, dsc 0.48175\n",
      "Epoch [395] train done\n",
      "Batch eval [1] loss 0.55197, dsc 0.44803\n",
      "Batch eval [2] loss 0.59680, dsc 0.40320\n",
      "Batch eval [3] loss 0.55771, dsc 0.44229\n",
      "Batch eval [4] loss 0.60946, dsc 0.39054\n",
      "Batch eval [5] loss 0.57000, dsc 0.43000\n",
      "Epoch [395] valid done\n",
      "Epoch [395] T 1588.92s, deltaT 4.02s, loss: train 0.51900, valid 0.57719, dsc: train 0.48100, valid 0.42281\n",
      "Batch train [1] loss 0.52572, dsc 0.47428\n",
      "Batch train [2] loss 0.51184, dsc 0.48816\n",
      "Batch train [3] loss 0.50959, dsc 0.49041\n",
      "Batch train [4] loss 0.52754, dsc 0.47246\n",
      "Batch train [5] loss 0.53891, dsc 0.46109\n",
      "Batch train [6] loss 0.47807, dsc 0.52193\n",
      "Batch train [7] loss 0.52650, dsc 0.47350\n",
      "Batch train [8] loss 0.49760, dsc 0.50240\n",
      "Batch train [9] loss 0.53118, dsc 0.46882\n",
      "Batch train [10] loss 0.53351, dsc 0.46649\n",
      "Epoch [396] train done\n",
      "Batch eval [1] loss 0.55514, dsc 0.44486\n",
      "Batch eval [2] loss 0.58188, dsc 0.41812\n",
      "Batch eval [3] loss 0.55249, dsc 0.44751\n",
      "Batch eval [4] loss 0.60981, dsc 0.39019\n",
      "Batch eval [5] loss 0.55898, dsc 0.44102\n",
      "Epoch [396] valid done\n",
      "Epoch [396] T 1592.95s, deltaT 4.03s, loss: train 0.51804, valid 0.57166, dsc: train 0.48196, valid 0.42834\n",
      "Batch train [1] loss 0.53741, dsc 0.46259\n",
      "Batch train [2] loss 0.48857, dsc 0.51143\n",
      "Batch train [3] loss 0.51094, dsc 0.48906\n",
      "Batch train [4] loss 0.52637, dsc 0.47363\n",
      "Batch train [5] loss 0.50665, dsc 0.49335\n",
      "Batch train [6] loss 0.52822, dsc 0.47178\n",
      "Batch train [7] loss 0.50834, dsc 0.49166\n",
      "Batch train [8] loss 0.52010, dsc 0.47990\n",
      "Batch train [9] loss 0.52003, dsc 0.47997\n",
      "Batch train [10] loss 0.52359, dsc 0.47641\n",
      "Epoch [397] train done\n",
      "Batch eval [1] loss 0.54316, dsc 0.45684\n",
      "Batch eval [2] loss 0.58074, dsc 0.41926\n",
      "Batch eval [3] loss 0.54952, dsc 0.45048\n",
      "Batch eval [4] loss 0.60397, dsc 0.39603\n",
      "Batch eval [5] loss 0.55777, dsc 0.44223\n",
      "Epoch [397] valid done\n",
      "Epoch [397] T 1596.92s, deltaT 3.96s, loss: train 0.51702, valid 0.56703, dsc: train 0.48298, valid 0.43297\n",
      "Batch train [1] loss 0.50783, dsc 0.49217\n",
      "Batch train [2] loss 0.50267, dsc 0.49733\n",
      "Batch train [3] loss 0.51375, dsc 0.48625\n",
      "Batch train [4] loss 0.48524, dsc 0.51476\n",
      "Batch train [5] loss 0.50174, dsc 0.49826\n",
      "Batch train [6] loss 0.52099, dsc 0.47901\n",
      "Batch train [7] loss 0.49848, dsc 0.50152\n",
      "Batch train [8] loss 0.52228, dsc 0.47772\n",
      "Batch train [9] loss 0.58344, dsc 0.41656\n",
      "Batch train [10] loss 0.53584, dsc 0.46416\n",
      "Epoch [398] train done\n",
      "Batch eval [1] loss 0.55370, dsc 0.44630\n",
      "Batch eval [2] loss 0.59385, dsc 0.40615\n",
      "Batch eval [3] loss 0.55160, dsc 0.44840\n",
      "Batch eval [4] loss 0.60524, dsc 0.39476\n",
      "Batch eval [5] loss 0.56588, dsc 0.43412\n",
      "Epoch [398] valid done\n",
      "Epoch [398] T 1600.97s, deltaT 4.05s, loss: train 0.51723, valid 0.57406, dsc: train 0.48277, valid 0.42594\n",
      "Batch train [1] loss 0.50513, dsc 0.49487\n",
      "Batch train [2] loss 0.51628, dsc 0.48372\n",
      "Batch train [3] loss 0.53903, dsc 0.46097\n",
      "Batch train [4] loss 0.53156, dsc 0.46844\n",
      "Batch train [5] loss 0.49096, dsc 0.50904\n",
      "Batch train [6] loss 0.50193, dsc 0.49807\n",
      "Batch train [7] loss 0.52753, dsc 0.47247\n",
      "Batch train [8] loss 0.51053, dsc 0.48947\n",
      "Batch train [9] loss 0.51262, dsc 0.48738\n",
      "Batch train [10] loss 0.51876, dsc 0.48124\n",
      "Epoch [399] train done\n",
      "Batch eval [1] loss 0.54446, dsc 0.45554\n",
      "Batch eval [2] loss 0.58092, dsc 0.41908\n",
      "Batch eval [3] loss 0.55328, dsc 0.44672\n",
      "Batch eval [4] loss 0.60609, dsc 0.39391\n",
      "Batch eval [5] loss 0.55713, dsc 0.44287\n",
      "Epoch [399] valid done\n",
      "Epoch [399] T 1604.90s, deltaT 3.93s, loss: train 0.51543, valid 0.56837, dsc: train 0.48457, valid 0.43163\n",
      "Batch train [1] loss 0.54847, dsc 0.45153\n",
      "Batch train [2] loss 0.48925, dsc 0.51075\n",
      "Batch train [3] loss 0.50261, dsc 0.49739\n",
      "Batch train [4] loss 0.51865, dsc 0.48135\n",
      "Batch train [5] loss 0.49799, dsc 0.50201\n",
      "Batch train [6] loss 0.54641, dsc 0.45359\n",
      "Batch train [7] loss 0.49987, dsc 0.50013\n",
      "Batch train [8] loss 0.49883, dsc 0.50117\n",
      "Batch train [9] loss 0.52305, dsc 0.47695\n",
      "Batch train [10] loss 0.51641, dsc 0.48359\n",
      "Epoch [400] train done\n",
      "Batch eval [1] loss 0.54782, dsc 0.45218\n",
      "Batch eval [2] loss 0.58258, dsc 0.41742\n",
      "Batch eval [3] loss 0.55688, dsc 0.44312\n",
      "Batch eval [4] loss 0.59833, dsc 0.40167\n",
      "Batch eval [5] loss 0.56565, dsc 0.43435\n",
      "Epoch [400] valid done\n",
      "Epoch [400] T 1608.88s, deltaT 3.98s, loss: train 0.51415, valid 0.57025, dsc: train 0.48585, valid 0.42975\n",
      "Batch train [1] loss 0.44926, dsc 0.55074\n",
      "Batch train [2] loss 0.50065, dsc 0.49935\n",
      "Batch train [3] loss 0.52124, dsc 0.47876\n",
      "Batch train [4] loss 0.51331, dsc 0.48669\n",
      "Batch train [5] loss 0.54520, dsc 0.45480\n",
      "Batch train [6] loss 0.51004, dsc 0.48996\n",
      "Batch train [7] loss 0.52083, dsc 0.47917\n",
      "Batch train [8] loss 0.53592, dsc 0.46408\n",
      "Batch train [9] loss 0.54556, dsc 0.45444\n",
      "Batch train [10] loss 0.49848, dsc 0.50152\n",
      "Epoch [401] train done\n",
      "Batch eval [1] loss 0.54937, dsc 0.45063\n",
      "Batch eval [2] loss 0.57919, dsc 0.42081\n",
      "Batch eval [3] loss 0.54363, dsc 0.45637\n",
      "Batch eval [4] loss 0.60087, dsc 0.39913\n",
      "Batch eval [5] loss 0.55921, dsc 0.44079\n",
      "Epoch [401] valid done\n",
      "Epoch [401] T 1612.79s, deltaT 3.91s, loss: train 0.51405, valid 0.56645, dsc: train 0.48595, valid 0.43355\n",
      "Batch train [1] loss 0.49274, dsc 0.50726\n",
      "Batch train [2] loss 0.51434, dsc 0.48566\n",
      "Batch train [3] loss 0.52604, dsc 0.47396\n",
      "Batch train [4] loss 0.50435, dsc 0.49565\n",
      "Batch train [5] loss 0.54373, dsc 0.45627\n",
      "Batch train [6] loss 0.49846, dsc 0.50154\n",
      "Batch train [7] loss 0.51582, dsc 0.48418\n",
      "Batch train [8] loss 0.50920, dsc 0.49080\n",
      "Batch train [9] loss 0.50939, dsc 0.49061\n",
      "Batch train [10] loss 0.51049, dsc 0.48951\n",
      "Epoch [402] train done\n",
      "Batch eval [1] loss 0.53903, dsc 0.46097\n",
      "Batch eval [2] loss 0.57920, dsc 0.42080\n",
      "Batch eval [3] loss 0.54450, dsc 0.45550\n",
      "Batch eval [4] loss 0.59725, dsc 0.40275\n",
      "Batch eval [5] loss 0.55269, dsc 0.44731\n",
      "Epoch [402] valid done\n",
      "Epoch [402] T 1616.76s, deltaT 3.96s, loss: train 0.51246, valid 0.56253, dsc: train 0.48754, valid 0.43747\n",
      "Batch train [1] loss 0.53636, dsc 0.46364\n",
      "Batch train [2] loss 0.51771, dsc 0.48229\n",
      "Batch train [3] loss 0.51489, dsc 0.48511\n",
      "Batch train [4] loss 0.47851, dsc 0.52149\n",
      "Batch train [5] loss 0.53928, dsc 0.46072\n",
      "Batch train [6] loss 0.52238, dsc 0.47762\n",
      "Batch train [7] loss 0.46252, dsc 0.53748\n",
      "Batch train [8] loss 0.55247, dsc 0.44753\n",
      "Batch train [9] loss 0.50487, dsc 0.49513\n",
      "Batch train [10] loss 0.49754, dsc 0.50246\n",
      "Epoch [403] train done\n",
      "Batch eval [1] loss 0.53911, dsc 0.46089\n",
      "Batch eval [2] loss 0.57548, dsc 0.42452\n",
      "Batch eval [3] loss 0.55248, dsc 0.44752\n",
      "Batch eval [4] loss 0.59968, dsc 0.40032\n",
      "Batch eval [5] loss 0.55700, dsc 0.44300\n",
      "Epoch [403] valid done\n",
      "Epoch [403] T 1620.78s, deltaT 4.02s, loss: train 0.51265, valid 0.56475, dsc: train 0.48735, valid 0.43525\n",
      "Batch train [1] loss 0.52698, dsc 0.47302\n",
      "Batch train [2] loss 0.53088, dsc 0.46912\n",
      "Batch train [3] loss 0.47632, dsc 0.52368\n",
      "Batch train [4] loss 0.53806, dsc 0.46194\n",
      "Batch train [5] loss 0.47329, dsc 0.52671\n",
      "Batch train [6] loss 0.52713, dsc 0.47287\n",
      "Batch train [7] loss 0.52894, dsc 0.47106\n",
      "Batch train [8] loss 0.48969, dsc 0.51031\n",
      "Batch train [9] loss 0.47403, dsc 0.52597\n",
      "Batch train [10] loss 0.54582, dsc 0.45418\n",
      "Epoch [404] train done\n",
      "Batch eval [1] loss 0.54215, dsc 0.45785\n",
      "Batch eval [2] loss 0.58134, dsc 0.41866\n",
      "Batch eval [3] loss 0.54930, dsc 0.45070\n",
      "Batch eval [4] loss 0.60668, dsc 0.39332\n",
      "Batch eval [5] loss 0.56213, dsc 0.43787\n",
      "Epoch [404] valid done\n",
      "Epoch [404] T 1624.79s, deltaT 4.01s, loss: train 0.51111, valid 0.56832, dsc: train 0.48889, valid 0.43168\n",
      "Batch train [1] loss 0.55193, dsc 0.44807\n",
      "Batch train [2] loss 0.50518, dsc 0.49482\n",
      "Batch train [3] loss 0.50434, dsc 0.49566\n",
      "Batch train [4] loss 0.47915, dsc 0.52085\n",
      "Batch train [5] loss 0.52786, dsc 0.47214\n",
      "Batch train [6] loss 0.50588, dsc 0.49412\n",
      "Batch train [7] loss 0.49695, dsc 0.50305\n",
      "Batch train [8] loss 0.50996, dsc 0.49004\n",
      "Batch train [9] loss 0.49322, dsc 0.50678\n",
      "Batch train [10] loss 0.51609, dsc 0.48391\n",
      "Epoch [405] train done\n",
      "Batch eval [1] loss 0.54303, dsc 0.45697\n",
      "Batch eval [2] loss 0.57283, dsc 0.42717\n",
      "Batch eval [3] loss 0.53946, dsc 0.46054\n",
      "Batch eval [4] loss 0.59884, dsc 0.40116\n",
      "Batch eval [5] loss 0.55237, dsc 0.44763\n",
      "Epoch [405] valid done\n",
      "Epoch [405] T 1628.80s, deltaT 4.01s, loss: train 0.50906, valid 0.56131, dsc: train 0.49094, valid 0.43869\n",
      "Batch train [1] loss 0.49209, dsc 0.50791\n",
      "Batch train [2] loss 0.51693, dsc 0.48307\n",
      "Batch train [3] loss 0.47370, dsc 0.52630\n",
      "Batch train [4] loss 0.50610, dsc 0.49390\n",
      "Batch train [5] loss 0.50713, dsc 0.49287\n",
      "Batch train [6] loss 0.50765, dsc 0.49235\n",
      "Batch train [7] loss 0.52954, dsc 0.47046\n",
      "Batch train [8] loss 0.53484, dsc 0.46516\n",
      "Batch train [9] loss 0.49686, dsc 0.50314\n",
      "Batch train [10] loss 0.51558, dsc 0.48442\n",
      "Epoch [406] train done\n",
      "Batch eval [1] loss 0.55558, dsc 0.44442\n",
      "Batch eval [2] loss 0.58362, dsc 0.41638\n",
      "Batch eval [3] loss 0.54847, dsc 0.45153\n",
      "Batch eval [4] loss 0.59705, dsc 0.40295\n",
      "Batch eval [5] loss 0.56088, dsc 0.43912\n",
      "Epoch [406] valid done\n",
      "Epoch [406] T 1632.73s, deltaT 3.93s, loss: train 0.50804, valid 0.56912, dsc: train 0.49196, valid 0.43088\n",
      "Batch train [1] loss 0.53379, dsc 0.46621\n",
      "Batch train [2] loss 0.51927, dsc 0.48073\n",
      "Batch train [3] loss 0.50573, dsc 0.49427\n",
      "Batch train [4] loss 0.49857, dsc 0.50143\n",
      "Batch train [5] loss 0.49096, dsc 0.50904\n",
      "Batch train [6] loss 0.52697, dsc 0.47303\n",
      "Batch train [7] loss 0.54981, dsc 0.45019\n",
      "Batch train [8] loss 0.48483, dsc 0.51517\n",
      "Batch train [9] loss 0.48209, dsc 0.51791\n",
      "Batch train [10] loss 0.48389, dsc 0.51611\n",
      "Epoch [407] train done\n",
      "Batch eval [1] loss 0.54146, dsc 0.45854\n",
      "Batch eval [2] loss 0.57497, dsc 0.42503\n",
      "Batch eval [3] loss 0.53828, dsc 0.46172\n",
      "Batch eval [4] loss 0.59932, dsc 0.40068\n",
      "Batch eval [5] loss 0.55404, dsc 0.44596\n",
      "Epoch [407] valid done\n",
      "Epoch [407] T 1636.77s, deltaT 4.04s, loss: train 0.50759, valid 0.56161, dsc: train 0.49241, valid 0.43839\n",
      "Batch train [1] loss 0.50118, dsc 0.49882\n",
      "Batch train [2] loss 0.47567, dsc 0.52433\n",
      "Batch train [3] loss 0.48391, dsc 0.51609\n",
      "Batch train [4] loss 0.54740, dsc 0.45260\n",
      "Batch train [5] loss 0.52729, dsc 0.47271\n",
      "Batch train [6] loss 0.51034, dsc 0.48966\n",
      "Batch train [7] loss 0.49292, dsc 0.50708\n",
      "Batch train [8] loss 0.49821, dsc 0.50179\n",
      "Batch train [9] loss 0.51457, dsc 0.48543\n",
      "Batch train [10] loss 0.51167, dsc 0.48833\n",
      "Epoch [408] train done\n",
      "Batch eval [1] loss 0.54633, dsc 0.45367\n",
      "Batch eval [2] loss 0.57665, dsc 0.42335\n",
      "Batch eval [3] loss 0.54489, dsc 0.45511\n",
      "Batch eval [4] loss 0.59635, dsc 0.40365\n",
      "Batch eval [5] loss 0.55680, dsc 0.44320\n",
      "Epoch [408] valid done\n",
      "Epoch [408] T 1640.86s, deltaT 4.09s, loss: train 0.50632, valid 0.56420, dsc: train 0.49368, valid 0.43580\n",
      "Batch train [1] loss 0.52776, dsc 0.47224\n",
      "Batch train [2] loss 0.47396, dsc 0.52604\n",
      "Batch train [3] loss 0.47352, dsc 0.52648\n",
      "Batch train [4] loss 0.50982, dsc 0.49018\n",
      "Batch train [5] loss 0.49874, dsc 0.50126\n",
      "Batch train [6] loss 0.51390, dsc 0.48610\n",
      "Batch train [7] loss 0.51477, dsc 0.48523\n",
      "Batch train [8] loss 0.50528, dsc 0.49472\n",
      "Batch train [9] loss 0.53871, dsc 0.46129\n",
      "Batch train [10] loss 0.49687, dsc 0.50313\n",
      "Epoch [409] train done\n",
      "Batch eval [1] loss 0.53999, dsc 0.46001\n",
      "Batch eval [2] loss 0.57545, dsc 0.42455\n",
      "Batch eval [3] loss 0.54146, dsc 0.45854\n",
      "Batch eval [4] loss 0.59269, dsc 0.40731\n",
      "Batch eval [5] loss 0.55228, dsc 0.44772\n",
      "Epoch [409] valid done\n",
      "Epoch [409] T 1644.86s, deltaT 4.00s, loss: train 0.50533, valid 0.56037, dsc: train 0.49467, valid 0.43963\n",
      "Batch train [1] loss 0.51310, dsc 0.48690\n",
      "Batch train [2] loss 0.51277, dsc 0.48723\n",
      "Batch train [3] loss 0.51833, dsc 0.48167\n",
      "Batch train [4] loss 0.49678, dsc 0.50322\n",
      "Batch train [5] loss 0.54732, dsc 0.45268\n",
      "Batch train [6] loss 0.49294, dsc 0.50706\n",
      "Batch train [7] loss 0.47824, dsc 0.52176\n",
      "Batch train [8] loss 0.49390, dsc 0.50610\n",
      "Batch train [9] loss 0.49554, dsc 0.50446\n",
      "Batch train [10] loss 0.49726, dsc 0.50274\n",
      "Epoch [410] train done\n",
      "Batch eval [1] loss 0.53488, dsc 0.46512\n",
      "Batch eval [2] loss 0.57067, dsc 0.42933\n",
      "Batch eval [3] loss 0.53647, dsc 0.46353\n",
      "Batch eval [4] loss 0.58524, dsc 0.41476\n",
      "Batch eval [5] loss 0.54176, dsc 0.45824\n",
      "Epoch [410] valid done\n",
      "Epoch [410] T 1648.75s, deltaT 3.89s, loss: train 0.50462, valid 0.55380, dsc: train 0.49538, valid 0.44619\n",
      "Batch train [1] loss 0.49107, dsc 0.50893\n",
      "Batch train [2] loss 0.49432, dsc 0.50568\n",
      "Batch train [3] loss 0.52320, dsc 0.47680\n",
      "Batch train [4] loss 0.50308, dsc 0.49692\n",
      "Batch train [5] loss 0.45988, dsc 0.54012\n",
      "Batch train [6] loss 0.50523, dsc 0.49477\n",
      "Batch train [7] loss 0.52622, dsc 0.47378\n",
      "Batch train [8] loss 0.50195, dsc 0.49805\n",
      "Batch train [9] loss 0.52780, dsc 0.47220\n",
      "Batch train [10] loss 0.50889, dsc 0.49111\n",
      "Epoch [411] train done\n",
      "Batch eval [1] loss 0.54485, dsc 0.45515\n",
      "Batch eval [2] loss 0.57128, dsc 0.42872\n",
      "Batch eval [3] loss 0.54475, dsc 0.45525\n",
      "Batch eval [4] loss 0.58537, dsc 0.41463\n",
      "Batch eval [5] loss 0.55187, dsc 0.44813\n",
      "Epoch [411] valid done\n",
      "Epoch [411] T 1652.69s, deltaT 3.94s, loss: train 0.50416, valid 0.55962, dsc: train 0.49584, valid 0.44038\n",
      "Batch train [1] loss 0.49552, dsc 0.50448\n",
      "Batch train [2] loss 0.49305, dsc 0.50695\n",
      "Batch train [3] loss 0.50128, dsc 0.49872\n",
      "Batch train [4] loss 0.47749, dsc 0.52251\n",
      "Batch train [5] loss 0.53073, dsc 0.46927\n",
      "Batch train [6] loss 0.54984, dsc 0.45016\n",
      "Batch train [7] loss 0.48382, dsc 0.51618\n",
      "Batch train [8] loss 0.47022, dsc 0.52978\n",
      "Batch train [9] loss 0.51828, dsc 0.48172\n",
      "Batch train [10] loss 0.51438, dsc 0.48562\n",
      "Epoch [412] train done\n",
      "Batch eval [1] loss 0.53823, dsc 0.46177\n",
      "Batch eval [2] loss 0.57319, dsc 0.42681\n",
      "Batch eval [3] loss 0.54564, dsc 0.45436\n",
      "Batch eval [4] loss 0.59265, dsc 0.40735\n",
      "Batch eval [5] loss 0.54909, dsc 0.45091\n",
      "Epoch [412] valid done\n",
      "Epoch [412] T 1656.67s, deltaT 3.98s, loss: train 0.50346, valid 0.55976, dsc: train 0.49654, valid 0.44024\n",
      "Batch train [1] loss 0.48602, dsc 0.51398\n",
      "Batch train [2] loss 0.45777, dsc 0.54223\n",
      "Batch train [3] loss 0.46841, dsc 0.53159\n",
      "Batch train [4] loss 0.50837, dsc 0.49163\n",
      "Batch train [5] loss 0.51599, dsc 0.48401\n",
      "Batch train [6] loss 0.50325, dsc 0.49675\n",
      "Batch train [7] loss 0.51887, dsc 0.48113\n",
      "Batch train [8] loss 0.54091, dsc 0.45909\n",
      "Batch train [9] loss 0.49618, dsc 0.50382\n",
      "Batch train [10] loss 0.52428, dsc 0.47572\n",
      "Epoch [413] train done\n",
      "Batch eval [1] loss 0.55149, dsc 0.44851\n",
      "Batch eval [2] loss 0.58154, dsc 0.41846\n",
      "Batch eval [3] loss 0.54678, dsc 0.45322\n",
      "Batch eval [4] loss 0.59046, dsc 0.40954\n",
      "Batch eval [5] loss 0.55717, dsc 0.44283\n",
      "Epoch [413] valid done\n",
      "Epoch [413] T 1660.72s, deltaT 4.05s, loss: train 0.50200, valid 0.56549, dsc: train 0.49800, valid 0.43451\n",
      "Batch train [1] loss 0.48602, dsc 0.51398\n",
      "Batch train [2] loss 0.47672, dsc 0.52328\n",
      "Batch train [3] loss 0.47910, dsc 0.52090\n",
      "Batch train [4] loss 0.53607, dsc 0.46393\n",
      "Batch train [5] loss 0.52081, dsc 0.47919\n",
      "Batch train [6] loss 0.48401, dsc 0.51599\n",
      "Batch train [7] loss 0.48376, dsc 0.51624\n",
      "Batch train [8] loss 0.51479, dsc 0.48521\n",
      "Batch train [9] loss 0.50318, dsc 0.49682\n",
      "Batch train [10] loss 0.51361, dsc 0.48639\n",
      "Epoch [414] train done\n",
      "Batch eval [1] loss 0.54485, dsc 0.45515\n",
      "Batch eval [2] loss 0.57317, dsc 0.42683\n",
      "Batch eval [3] loss 0.54795, dsc 0.45205\n",
      "Batch eval [4] loss 0.58532, dsc 0.41468\n",
      "Batch eval [5] loss 0.54685, dsc 0.45315\n",
      "Epoch [414] valid done\n",
      "Epoch [414] T 1664.62s, deltaT 3.89s, loss: train 0.49981, valid 0.55963, dsc: train 0.50019, valid 0.44037\n",
      "Batch train [1] loss 0.46847, dsc 0.53153\n",
      "Batch train [2] loss 0.48130, dsc 0.51870\n",
      "Batch train [3] loss 0.50539, dsc 0.49461\n",
      "Batch train [4] loss 0.53426, dsc 0.46574\n",
      "Batch train [5] loss 0.49887, dsc 0.50113\n",
      "Batch train [6] loss 0.49174, dsc 0.50826\n",
      "Batch train [7] loss 0.51514, dsc 0.48486\n",
      "Batch train [8] loss 0.53522, dsc 0.46478\n",
      "Batch train [9] loss 0.48704, dsc 0.51296\n",
      "Batch train [10] loss 0.47663, dsc 0.52337\n",
      "Epoch [415] train done\n",
      "Batch eval [1] loss 0.54567, dsc 0.45433\n",
      "Batch eval [2] loss 0.57189, dsc 0.42811\n",
      "Batch eval [3] loss 0.55061, dsc 0.44939\n",
      "Batch eval [4] loss 0.58616, dsc 0.41384\n",
      "Batch eval [5] loss 0.55342, dsc 0.44658\n",
      "Epoch [415] valid done\n",
      "Epoch [415] T 1668.62s, deltaT 4.00s, loss: train 0.49940, valid 0.56155, dsc: train 0.50060, valid 0.43845\n",
      "Batch train [1] loss 0.49759, dsc 0.50241\n",
      "Batch train [2] loss 0.46780, dsc 0.53220\n",
      "Batch train [3] loss 0.51794, dsc 0.48206\n",
      "Batch train [4] loss 0.49585, dsc 0.50415\n",
      "Batch train [5] loss 0.55927, dsc 0.44073\n",
      "Batch train [6] loss 0.49395, dsc 0.50605\n",
      "Batch train [7] loss 0.46703, dsc 0.53297\n",
      "Batch train [8] loss 0.52013, dsc 0.47987\n",
      "Batch train [9] loss 0.49849, dsc 0.50151\n",
      "Batch train [10] loss 0.47020, dsc 0.52980\n",
      "Epoch [416] train done\n",
      "Batch eval [1] loss 0.54838, dsc 0.45162\n",
      "Batch eval [2] loss 0.57821, dsc 0.42179\n",
      "Batch eval [3] loss 0.54953, dsc 0.45047\n",
      "Batch eval [4] loss 0.58958, dsc 0.41042\n",
      "Batch eval [5] loss 0.55201, dsc 0.44799\n",
      "Epoch [416] valid done\n",
      "Epoch [416] T 1672.61s, deltaT 3.98s, loss: train 0.49882, valid 0.56354, dsc: train 0.50118, valid 0.43646\n",
      "Batch train [1] loss 0.48712, dsc 0.51288\n",
      "Batch train [2] loss 0.46049, dsc 0.53951\n",
      "Batch train [3] loss 0.49897, dsc 0.50103\n",
      "Batch train [4] loss 0.52787, dsc 0.47213\n",
      "Batch train [5] loss 0.50903, dsc 0.49097\n",
      "Batch train [6] loss 0.48897, dsc 0.51103\n",
      "Batch train [7] loss 0.50568, dsc 0.49432\n",
      "Batch train [8] loss 0.50811, dsc 0.49189\n",
      "Batch train [9] loss 0.48822, dsc 0.51178\n",
      "Batch train [10] loss 0.49190, dsc 0.50810\n",
      "Epoch [417] train done\n",
      "Batch eval [1] loss 0.54485, dsc 0.45515\n",
      "Batch eval [2] loss 0.57672, dsc 0.42328\n",
      "Batch eval [3] loss 0.54126, dsc 0.45874\n",
      "Batch eval [4] loss 0.58600, dsc 0.41400\n",
      "Batch eval [5] loss 0.55035, dsc 0.44965\n",
      "Epoch [417] valid done\n",
      "Epoch [417] T 1676.60s, deltaT 4.00s, loss: train 0.49664, valid 0.55984, dsc: train 0.50336, valid 0.44016\n",
      "Batch train [1] loss 0.49604, dsc 0.50396\n",
      "Batch train [2] loss 0.52361, dsc 0.47639\n",
      "Batch train [3] loss 0.51078, dsc 0.48922\n",
      "Batch train [4] loss 0.48695, dsc 0.51305\n",
      "Batch train [5] loss 0.50959, dsc 0.49041\n",
      "Batch train [6] loss 0.48700, dsc 0.51300\n",
      "Batch train [7] loss 0.49264, dsc 0.50736\n",
      "Batch train [8] loss 0.46496, dsc 0.53504\n",
      "Batch train [9] loss 0.48140, dsc 0.51860\n",
      "Batch train [10] loss 0.51066, dsc 0.48934\n",
      "Epoch [418] train done\n",
      "Batch eval [1] loss 0.53443, dsc 0.46557\n",
      "Batch eval [2] loss 0.57191, dsc 0.42809\n",
      "Batch eval [3] loss 0.53809, dsc 0.46191\n",
      "Batch eval [4] loss 0.58707, dsc 0.41293\n",
      "Batch eval [5] loss 0.54840, dsc 0.45160\n",
      "Epoch [418] valid done\n",
      "Epoch [418] T 1680.55s, deltaT 3.95s, loss: train 0.49636, valid 0.55598, dsc: train 0.50364, valid 0.44402\n",
      "Batch train [1] loss 0.51826, dsc 0.48174\n",
      "Batch train [2] loss 0.49616, dsc 0.50384\n",
      "Batch train [3] loss 0.50877, dsc 0.49123\n",
      "Batch train [4] loss 0.49257, dsc 0.50743\n",
      "Batch train [5] loss 0.50416, dsc 0.49584\n",
      "Batch train [6] loss 0.50287, dsc 0.49713\n",
      "Batch train [7] loss 0.48572, dsc 0.51428\n",
      "Batch train [8] loss 0.47916, dsc 0.52084\n",
      "Batch train [9] loss 0.47120, dsc 0.52880\n",
      "Batch train [10] loss 0.48731, dsc 0.51269\n",
      "Epoch [419] train done\n",
      "Batch eval [1] loss 0.52778, dsc 0.47222\n",
      "Batch eval [2] loss 0.56755, dsc 0.43245\n",
      "Batch eval [3] loss 0.52988, dsc 0.47012\n",
      "Batch eval [4] loss 0.58678, dsc 0.41322\n",
      "Batch eval [5] loss 0.54157, dsc 0.45843\n",
      "Epoch [419] valid done\n",
      "Epoch [419] T 1684.51s, deltaT 3.95s, loss: train 0.49462, valid 0.55071, dsc: train 0.50538, valid 0.44929\n",
      "Batch train [1] loss 0.48843, dsc 0.51157\n",
      "Batch train [2] loss 0.49617, dsc 0.50383\n",
      "Batch train [3] loss 0.50126, dsc 0.49874\n",
      "Batch train [4] loss 0.52755, dsc 0.47245\n",
      "Batch train [5] loss 0.48286, dsc 0.51714\n",
      "Batch train [6] loss 0.48674, dsc 0.51326\n",
      "Batch train [7] loss 0.48368, dsc 0.51632\n",
      "Batch train [8] loss 0.47078, dsc 0.52922\n",
      "Batch train [9] loss 0.48391, dsc 0.51609\n",
      "Batch train [10] loss 0.52112, dsc 0.47888\n",
      "Epoch [420] train done\n",
      "Batch eval [1] loss 0.53227, dsc 0.46773\n",
      "Batch eval [2] loss 0.56862, dsc 0.43138\n",
      "Batch eval [3] loss 0.53932, dsc 0.46068\n",
      "Batch eval [4] loss 0.58173, dsc 0.41827\n",
      "Batch eval [5] loss 0.54888, dsc 0.45112\n",
      "Epoch [420] valid done\n",
      "Epoch [420] T 1688.49s, deltaT 3.98s, loss: train 0.49425, valid 0.55416, dsc: train 0.50575, valid 0.44584\n",
      "Batch train [1] loss 0.49735, dsc 0.50265\n",
      "Batch train [2] loss 0.49395, dsc 0.50605\n",
      "Batch train [3] loss 0.47023, dsc 0.52977\n",
      "Batch train [4] loss 0.46066, dsc 0.53934\n",
      "Batch train [5] loss 0.49147, dsc 0.50853\n",
      "Batch train [6] loss 0.52622, dsc 0.47378\n",
      "Batch train [7] loss 0.49620, dsc 0.50380\n",
      "Batch train [8] loss 0.49818, dsc 0.50182\n",
      "Batch train [9] loss 0.50843, dsc 0.49157\n",
      "Batch train [10] loss 0.49163, dsc 0.50837\n",
      "Epoch [421] train done\n",
      "Batch eval [1] loss 0.53984, dsc 0.46016\n",
      "Batch eval [2] loss 0.57460, dsc 0.42540\n",
      "Batch eval [3] loss 0.53935, dsc 0.46065\n",
      "Batch eval [4] loss 0.58303, dsc 0.41697\n",
      "Batch eval [5] loss 0.54859, dsc 0.45141\n",
      "Epoch [421] valid done\n",
      "Epoch [421] T 1692.51s, deltaT 4.01s, loss: train 0.49343, valid 0.55708, dsc: train 0.50657, valid 0.44292\n",
      "Batch train [1] loss 0.48829, dsc 0.51171\n",
      "Batch train [2] loss 0.51939, dsc 0.48061\n",
      "Batch train [3] loss 0.48070, dsc 0.51930\n",
      "Batch train [4] loss 0.48839, dsc 0.51161\n",
      "Batch train [5] loss 0.55058, dsc 0.44942\n",
      "Batch train [6] loss 0.47587, dsc 0.52413\n",
      "Batch train [7] loss 0.48293, dsc 0.51707\n",
      "Batch train [8] loss 0.44514, dsc 0.55486\n",
      "Batch train [9] loss 0.50981, dsc 0.49019\n",
      "Batch train [10] loss 0.48617, dsc 0.51383\n",
      "Epoch [422] train done\n",
      "Batch eval [1] loss 0.54050, dsc 0.45950\n",
      "Batch eval [2] loss 0.57438, dsc 0.42562\n",
      "Batch eval [3] loss 0.54021, dsc 0.45979\n",
      "Batch eval [4] loss 0.58654, dsc 0.41346\n",
      "Batch eval [5] loss 0.55182, dsc 0.44818\n",
      "Epoch [422] valid done\n",
      "Epoch [422] T 1696.51s, deltaT 4.01s, loss: train 0.49273, valid 0.55869, dsc: train 0.50727, valid 0.44131\n",
      "Batch train [1] loss 0.48062, dsc 0.51938\n",
      "Batch train [2] loss 0.48917, dsc 0.51083\n",
      "Batch train [3] loss 0.46370, dsc 0.53630\n",
      "Batch train [4] loss 0.50996, dsc 0.49004\n",
      "Batch train [5] loss 0.49103, dsc 0.50897\n",
      "Batch train [6] loss 0.49992, dsc 0.50008\n",
      "Batch train [7] loss 0.48595, dsc 0.51405\n",
      "Batch train [8] loss 0.47613, dsc 0.52387\n",
      "Batch train [9] loss 0.49238, dsc 0.50762\n",
      "Batch train [10] loss 0.52095, dsc 0.47905\n",
      "Epoch [423] train done\n",
      "Batch eval [1] loss 0.54034, dsc 0.45966\n",
      "Batch eval [2] loss 0.57914, dsc 0.42086\n",
      "Batch eval [3] loss 0.54005, dsc 0.45995\n",
      "Batch eval [4] loss 0.58657, dsc 0.41343\n",
      "Batch eval [5] loss 0.54889, dsc 0.45111\n",
      "Epoch [423] valid done\n",
      "Epoch [423] T 1700.51s, deltaT 4.00s, loss: train 0.49098, valid 0.55900, dsc: train 0.50902, valid 0.44100\n",
      "Batch train [1] loss 0.47962, dsc 0.52038\n",
      "Batch train [2] loss 0.48410, dsc 0.51590\n",
      "Batch train [3] loss 0.49484, dsc 0.50516\n",
      "Batch train [4] loss 0.49512, dsc 0.50488\n",
      "Batch train [5] loss 0.50650, dsc 0.49350\n",
      "Batch train [6] loss 0.47149, dsc 0.52851\n",
      "Batch train [7] loss 0.49556, dsc 0.50444\n",
      "Batch train [8] loss 0.48182, dsc 0.51818\n",
      "Batch train [9] loss 0.49128, dsc 0.50872\n",
      "Batch train [10] loss 0.49672, dsc 0.50328\n",
      "Epoch [424] train done\n",
      "Batch eval [1] loss 0.54020, dsc 0.45980\n",
      "Batch eval [2] loss 0.57626, dsc 0.42374\n",
      "Batch eval [3] loss 0.53485, dsc 0.46515\n",
      "Batch eval [4] loss 0.58520, dsc 0.41480\n",
      "Batch eval [5] loss 0.54332, dsc 0.45668\n",
      "Epoch [424] valid done\n",
      "Epoch [424] T 1704.52s, deltaT 4.00s, loss: train 0.48970, valid 0.55597, dsc: train 0.51030, valid 0.44403\n",
      "Batch train [1] loss 0.49378, dsc 0.50622\n",
      "Batch train [2] loss 0.49613, dsc 0.50387\n",
      "Batch train [3] loss 0.49540, dsc 0.50460\n",
      "Batch train [4] loss 0.50630, dsc 0.49370\n",
      "Batch train [5] loss 0.47362, dsc 0.52638\n",
      "Batch train [6] loss 0.47576, dsc 0.52424\n",
      "Batch train [7] loss 0.50592, dsc 0.49408\n",
      "Batch train [8] loss 0.47377, dsc 0.52623\n",
      "Batch train [9] loss 0.47764, dsc 0.52236\n",
      "Batch train [10] loss 0.49508, dsc 0.50492\n",
      "Epoch [425] train done\n",
      "Batch eval [1] loss 0.53384, dsc 0.46616\n",
      "Batch eval [2] loss 0.56680, dsc 0.43320\n",
      "Batch eval [3] loss 0.53683, dsc 0.46317\n",
      "Batch eval [4] loss 0.58238, dsc 0.41762\n",
      "Batch eval [5] loss 0.53638, dsc 0.46362\n",
      "Epoch [425] valid done\n",
      "Epoch [425] T 1708.51s, deltaT 3.99s, loss: train 0.48934, valid 0.55125, dsc: train 0.51066, valid 0.44875\n",
      "Batch train [1] loss 0.50671, dsc 0.49329\n",
      "Batch train [2] loss 0.44996, dsc 0.55004\n",
      "Batch train [3] loss 0.47228, dsc 0.52772\n",
      "Batch train [4] loss 0.48448, dsc 0.51552\n",
      "Batch train [5] loss 0.48854, dsc 0.51146\n",
      "Batch train [6] loss 0.48920, dsc 0.51080\n",
      "Batch train [7] loss 0.49145, dsc 0.50855\n",
      "Batch train [8] loss 0.52871, dsc 0.47129\n",
      "Batch train [9] loss 0.50511, dsc 0.49489\n",
      "Batch train [10] loss 0.47654, dsc 0.52346\n",
      "Epoch [426] train done\n",
      "Batch eval [1] loss 0.53967, dsc 0.46033\n",
      "Batch eval [2] loss 0.57357, dsc 0.42643\n",
      "Batch eval [3] loss 0.54124, dsc 0.45876\n",
      "Batch eval [4] loss 0.58233, dsc 0.41767\n",
      "Batch eval [5] loss 0.54342, dsc 0.45658\n",
      "Epoch [426] valid done\n",
      "Epoch [426] T 1712.54s, deltaT 4.03s, loss: train 0.48930, valid 0.55605, dsc: train 0.51070, valid 0.44395\n",
      "Batch train [1] loss 0.51759, dsc 0.48241\n",
      "Batch train [2] loss 0.47460, dsc 0.52540\n",
      "Batch train [3] loss 0.51245, dsc 0.48755\n",
      "Batch train [4] loss 0.46667, dsc 0.53333\n",
      "Batch train [5] loss 0.47209, dsc 0.52791\n",
      "Batch train [6] loss 0.50123, dsc 0.49877\n",
      "Batch train [7] loss 0.47577, dsc 0.52423\n",
      "Batch train [8] loss 0.48479, dsc 0.51521\n",
      "Batch train [9] loss 0.49801, dsc 0.50199\n",
      "Batch train [10] loss 0.47163, dsc 0.52837\n",
      "Epoch [427] train done\n",
      "Batch eval [1] loss 0.53009, dsc 0.46991\n",
      "Batch eval [2] loss 0.56382, dsc 0.43618\n",
      "Batch eval [3] loss 0.53748, dsc 0.46252\n",
      "Batch eval [4] loss 0.57979, dsc 0.42021\n",
      "Batch eval [5] loss 0.53908, dsc 0.46092\n",
      "Epoch [427] valid done\n",
      "Epoch [427] T 1716.57s, deltaT 4.03s, loss: train 0.48748, valid 0.55006, dsc: train 0.51252, valid 0.44994\n",
      "Batch train [1] loss 0.48831, dsc 0.51169\n",
      "Batch train [2] loss 0.46982, dsc 0.53018\n",
      "Batch train [3] loss 0.45919, dsc 0.54081\n",
      "Batch train [4] loss 0.48863, dsc 0.51137\n",
      "Batch train [5] loss 0.50726, dsc 0.49274\n",
      "Batch train [6] loss 0.50673, dsc 0.49327\n",
      "Batch train [7] loss 0.48796, dsc 0.51204\n",
      "Batch train [8] loss 0.48014, dsc 0.51986\n",
      "Batch train [9] loss 0.50830, dsc 0.49170\n",
      "Batch train [10] loss 0.47026, dsc 0.52974\n",
      "Epoch [428] train done\n",
      "Batch eval [1] loss 0.52561, dsc 0.47439\n",
      "Batch eval [2] loss 0.55880, dsc 0.44120\n",
      "Batch eval [3] loss 0.52482, dsc 0.47518\n",
      "Batch eval [4] loss 0.57600, dsc 0.42400\n",
      "Batch eval [5] loss 0.53372, dsc 0.46628\n",
      "Epoch [428] valid done\n",
      "Epoch [428] T 1720.54s, deltaT 3.97s, loss: train 0.48666, valid 0.54379, dsc: train 0.51334, valid 0.45621\n",
      "Batch train [1] loss 0.48526, dsc 0.51474\n",
      "Batch train [2] loss 0.50222, dsc 0.49778\n",
      "Batch train [3] loss 0.45215, dsc 0.54785\n",
      "Batch train [4] loss 0.48897, dsc 0.51103\n",
      "Batch train [5] loss 0.47513, dsc 0.52487\n",
      "Batch train [6] loss 0.48365, dsc 0.51635\n",
      "Batch train [7] loss 0.46336, dsc 0.53664\n",
      "Batch train [8] loss 0.47895, dsc 0.52105\n",
      "Batch train [9] loss 0.51167, dsc 0.48833\n",
      "Batch train [10] loss 0.51357, dsc 0.48643\n",
      "Epoch [429] train done\n",
      "Batch eval [1] loss 0.53231, dsc 0.46769\n",
      "Batch eval [2] loss 0.56384, dsc 0.43616\n",
      "Batch eval [3] loss 0.52939, dsc 0.47061\n",
      "Batch eval [4] loss 0.57823, dsc 0.42178\n",
      "Batch eval [5] loss 0.53750, dsc 0.46250\n",
      "Epoch [429] valid done\n",
      "Epoch [429] T 1724.50s, deltaT 3.96s, loss: train 0.48549, valid 0.54825, dsc: train 0.51451, valid 0.45175\n",
      "Batch train [1] loss 0.48872, dsc 0.51128\n",
      "Batch train [2] loss 0.45009, dsc 0.54991\n",
      "Batch train [3] loss 0.45852, dsc 0.54148\n",
      "Batch train [4] loss 0.47308, dsc 0.52692\n",
      "Batch train [5] loss 0.45693, dsc 0.54307\n",
      "Batch train [6] loss 0.50710, dsc 0.49290\n",
      "Batch train [7] loss 0.50662, dsc 0.49338\n",
      "Batch train [8] loss 0.51418, dsc 0.48582\n",
      "Batch train [9] loss 0.49729, dsc 0.50271\n",
      "Batch train [10] loss 0.50350, dsc 0.49650\n",
      "Epoch [430] train done\n",
      "Batch eval [1] loss 0.53837, dsc 0.46163\n",
      "Batch eval [2] loss 0.56936, dsc 0.43064\n",
      "Batch eval [3] loss 0.53416, dsc 0.46584\n",
      "Batch eval [4] loss 0.58336, dsc 0.41664\n",
      "Batch eval [5] loss 0.54708, dsc 0.45292\n",
      "Epoch [430] valid done\n",
      "Epoch [430] T 1728.47s, deltaT 3.97s, loss: train 0.48560, valid 0.55447, dsc: train 0.51440, valid 0.44553\n",
      "Batch train [1] loss 0.53996, dsc 0.46004\n",
      "Batch train [2] loss 0.43849, dsc 0.56151\n",
      "Batch train [3] loss 0.46543, dsc 0.53457\n",
      "Batch train [4] loss 0.51489, dsc 0.48511\n",
      "Batch train [5] loss 0.50711, dsc 0.49289\n",
      "Batch train [6] loss 0.46215, dsc 0.53785\n",
      "Batch train [7] loss 0.46532, dsc 0.53468\n",
      "Batch train [8] loss 0.49874, dsc 0.50126\n",
      "Batch train [9] loss 0.46435, dsc 0.53565\n",
      "Batch train [10] loss 0.49839, dsc 0.50161\n",
      "Epoch [431] train done\n",
      "Batch eval [1] loss 0.53038, dsc 0.46962\n",
      "Batch eval [2] loss 0.57164, dsc 0.42836\n",
      "Batch eval [3] loss 0.53596, dsc 0.46404\n",
      "Batch eval [4] loss 0.58037, dsc 0.41963\n",
      "Batch eval [5] loss 0.54514, dsc 0.45486\n",
      "Epoch [431] valid done\n",
      "Epoch [431] T 1732.46s, deltaT 3.99s, loss: train 0.48548, valid 0.55270, dsc: train 0.51452, valid 0.44730\n",
      "Batch train [1] loss 0.52488, dsc 0.47512\n",
      "Batch train [2] loss 0.45067, dsc 0.54933\n",
      "Batch train [3] loss 0.49626, dsc 0.50374\n",
      "Batch train [4] loss 0.48578, dsc 0.51422\n",
      "Batch train [5] loss 0.47606, dsc 0.52394\n",
      "Batch train [6] loss 0.49003, dsc 0.50997\n",
      "Batch train [7] loss 0.48432, dsc 0.51568\n",
      "Batch train [8] loss 0.49300, dsc 0.50700\n",
      "Batch train [9] loss 0.46691, dsc 0.53309\n",
      "Batch train [10] loss 0.46796, dsc 0.53204\n",
      "Epoch [432] train done\n",
      "Batch eval [1] loss 0.53089, dsc 0.46911\n",
      "Batch eval [2] loss 0.56494, dsc 0.43506\n",
      "Batch eval [3] loss 0.53301, dsc 0.46699\n",
      "Batch eval [4] loss 0.58082, dsc 0.41918\n",
      "Batch eval [5] loss 0.54131, dsc 0.45869\n",
      "Epoch [432] valid done\n",
      "Epoch [432] T 1736.48s, deltaT 4.02s, loss: train 0.48359, valid 0.55019, dsc: train 0.51641, valid 0.44981\n",
      "Batch train [1] loss 0.48178, dsc 0.51822\n",
      "Batch train [2] loss 0.47606, dsc 0.52394\n",
      "Batch train [3] loss 0.45148, dsc 0.54852\n",
      "Batch train [4] loss 0.45795, dsc 0.54205\n",
      "Batch train [5] loss 0.49368, dsc 0.50632\n",
      "Batch train [6] loss 0.49213, dsc 0.50787\n",
      "Batch train [7] loss 0.45706, dsc 0.54294\n",
      "Batch train [8] loss 0.53643, dsc 0.46357\n",
      "Batch train [9] loss 0.48273, dsc 0.51727\n",
      "Batch train [10] loss 0.49663, dsc 0.50337\n",
      "Epoch [433] train done\n",
      "Batch eval [1] loss 0.53433, dsc 0.46567\n",
      "Batch eval [2] loss 0.56302, dsc 0.43698\n",
      "Batch eval [3] loss 0.53934, dsc 0.46066\n",
      "Batch eval [4] loss 0.57557, dsc 0.42443\n",
      "Batch eval [5] loss 0.54125, dsc 0.45875\n",
      "Epoch [433] valid done\n",
      "Epoch [433] T 1740.52s, deltaT 4.03s, loss: train 0.48259, valid 0.55070, dsc: train 0.51741, valid 0.44930\n",
      "Batch train [1] loss 0.51425, dsc 0.48575\n",
      "Batch train [2] loss 0.47592, dsc 0.52408\n",
      "Batch train [3] loss 0.46219, dsc 0.53781\n",
      "Batch train [4] loss 0.47924, dsc 0.52076\n",
      "Batch train [5] loss 0.49768, dsc 0.50232\n",
      "Batch train [6] loss 0.47146, dsc 0.52854\n",
      "Batch train [7] loss 0.45252, dsc 0.54748\n",
      "Batch train [8] loss 0.47910, dsc 0.52090\n",
      "Batch train [9] loss 0.48710, dsc 0.51290\n",
      "Batch train [10] loss 0.49505, dsc 0.50495\n",
      "Epoch [434] train done\n",
      "Batch eval [1] loss 0.52358, dsc 0.47642\n",
      "Batch eval [2] loss 0.55645, dsc 0.44355\n",
      "Batch eval [3] loss 0.51834, dsc 0.48166\n",
      "Batch eval [4] loss 0.56874, dsc 0.43126\n",
      "Batch eval [5] loss 0.53622, dsc 0.46378\n",
      "Epoch [434] valid done\n",
      "Epoch [434] T 1744.55s, deltaT 4.04s, loss: train 0.48145, valid 0.54067, dsc: train 0.51855, valid 0.45933\n",
      "Batch train [1] loss 0.49485, dsc 0.50515\n",
      "Batch train [2] loss 0.45891, dsc 0.54109\n",
      "Batch train [3] loss 0.48274, dsc 0.51726\n",
      "Batch train [4] loss 0.50113, dsc 0.49887\n",
      "Batch train [5] loss 0.49031, dsc 0.50969\n",
      "Batch train [6] loss 0.48995, dsc 0.51005\n",
      "Batch train [7] loss 0.48448, dsc 0.51552\n",
      "Batch train [8] loss 0.46368, dsc 0.53632\n",
      "Batch train [9] loss 0.46127, dsc 0.53873\n",
      "Batch train [10] loss 0.47191, dsc 0.52809\n",
      "Epoch [435] train done\n",
      "Batch eval [1] loss 0.53118, dsc 0.46882\n",
      "Batch eval [2] loss 0.56582, dsc 0.43418\n",
      "Batch eval [3] loss 0.52664, dsc 0.47336\n",
      "Batch eval [4] loss 0.57634, dsc 0.42366\n",
      "Batch eval [5] loss 0.54111, dsc 0.45889\n",
      "Epoch [435] valid done\n",
      "Epoch [435] T 1748.53s, deltaT 3.98s, loss: train 0.47992, valid 0.54822, dsc: train 0.52008, valid 0.45178\n",
      "Batch train [1] loss 0.47401, dsc 0.52599\n",
      "Batch train [2] loss 0.51566, dsc 0.48434\n",
      "Batch train [3] loss 0.48989, dsc 0.51011\n",
      "Batch train [4] loss 0.47548, dsc 0.52452\n",
      "Batch train [5] loss 0.46001, dsc 0.53999\n",
      "Batch train [6] loss 0.48373, dsc 0.51627\n",
      "Batch train [7] loss 0.51091, dsc 0.48909\n",
      "Batch train [8] loss 0.48582, dsc 0.51418\n",
      "Batch train [9] loss 0.46289, dsc 0.53711\n",
      "Batch train [10] loss 0.44133, dsc 0.55867\n",
      "Epoch [436] train done\n",
      "Batch eval [1] loss 0.53630, dsc 0.46370\n",
      "Batch eval [2] loss 0.56947, dsc 0.43053\n",
      "Batch eval [3] loss 0.53460, dsc 0.46540\n",
      "Batch eval [4] loss 0.56992, dsc 0.43008\n",
      "Batch eval [5] loss 0.53801, dsc 0.46199\n",
      "Epoch [436] valid done\n",
      "Epoch [436] T 1752.58s, deltaT 4.05s, loss: train 0.47997, valid 0.54966, dsc: train 0.52003, valid 0.45034\n",
      "Batch train [1] loss 0.46453, dsc 0.53547\n",
      "Batch train [2] loss 0.47096, dsc 0.52904\n",
      "Batch train [3] loss 0.47698, dsc 0.52302\n",
      "Batch train [4] loss 0.50731, dsc 0.49269\n",
      "Batch train [5] loss 0.48597, dsc 0.51403\n",
      "Batch train [6] loss 0.49823, dsc 0.50177\n",
      "Batch train [7] loss 0.45933, dsc 0.54067\n",
      "Batch train [8] loss 0.48262, dsc 0.51738\n",
      "Batch train [9] loss 0.45995, dsc 0.54005\n",
      "Batch train [10] loss 0.47497, dsc 0.52503\n",
      "Epoch [437] train done\n",
      "Batch eval [1] loss 0.52392, dsc 0.47608\n",
      "Batch eval [2] loss 0.55612, dsc 0.44388\n",
      "Batch eval [3] loss 0.52998, dsc 0.47002\n",
      "Batch eval [4] loss 0.57030, dsc 0.42970\n",
      "Batch eval [5] loss 0.52632, dsc 0.47368\n",
      "Epoch [437] valid done\n",
      "Epoch [437] T 1756.59s, deltaT 4.00s, loss: train 0.47808, valid 0.54133, dsc: train 0.52192, valid 0.45867\n",
      "Batch train [1] loss 0.52663, dsc 0.47337\n",
      "Batch train [2] loss 0.48133, dsc 0.51867\n",
      "Batch train [3] loss 0.50548, dsc 0.49452\n",
      "Batch train [4] loss 0.47628, dsc 0.52372\n",
      "Batch train [5] loss 0.45962, dsc 0.54038\n",
      "Batch train [6] loss 0.45315, dsc 0.54685\n",
      "Batch train [7] loss 0.45758, dsc 0.54242\n",
      "Batch train [8] loss 0.48234, dsc 0.51766\n",
      "Batch train [9] loss 0.47229, dsc 0.52771\n",
      "Batch train [10] loss 0.46307, dsc 0.53693\n",
      "Epoch [438] train done\n",
      "Batch eval [1] loss 0.51764, dsc 0.48236\n",
      "Batch eval [2] loss 0.55980, dsc 0.44020\n",
      "Batch eval [3] loss 0.51766, dsc 0.48234\n",
      "Batch eval [4] loss 0.57368, dsc 0.42632\n",
      "Batch eval [5] loss 0.53380, dsc 0.46620\n",
      "Epoch [438] valid done\n",
      "Epoch [438] T 1760.61s, deltaT 4.03s, loss: train 0.47778, valid 0.54052, dsc: train 0.52222, valid 0.45948\n",
      "Batch train [1] loss 0.47122, dsc 0.52878\n",
      "Batch train [2] loss 0.49607, dsc 0.50393\n",
      "Batch train [3] loss 0.48138, dsc 0.51862\n",
      "Batch train [4] loss 0.47447, dsc 0.52553\n",
      "Batch train [5] loss 0.46179, dsc 0.53821\n",
      "Batch train [6] loss 0.48413, dsc 0.51587\n",
      "Batch train [7] loss 0.50271, dsc 0.49729\n",
      "Batch train [8] loss 0.46826, dsc 0.53174\n",
      "Batch train [9] loss 0.46048, dsc 0.53952\n",
      "Batch train [10] loss 0.46677, dsc 0.53323\n",
      "Epoch [439] train done\n",
      "Batch eval [1] loss 0.51834, dsc 0.48166\n",
      "Batch eval [2] loss 0.55386, dsc 0.44614\n",
      "Batch eval [3] loss 0.52046, dsc 0.47954\n",
      "Batch eval [4] loss 0.56953, dsc 0.43047\n",
      "Batch eval [5] loss 0.53435, dsc 0.46565\n",
      "Epoch [439] valid done\n",
      "Epoch [439] T 1764.56s, deltaT 3.95s, loss: train 0.47673, valid 0.53931, dsc: train 0.52327, valid 0.46069\n",
      "Batch train [1] loss 0.45663, dsc 0.54337\n",
      "Batch train [2] loss 0.50281, dsc 0.49719\n",
      "Batch train [3] loss 0.46761, dsc 0.53239\n",
      "Batch train [4] loss 0.44333, dsc 0.55667\n",
      "Batch train [5] loss 0.49809, dsc 0.50191\n",
      "Batch train [6] loss 0.47902, dsc 0.52098\n",
      "Batch train [7] loss 0.48439, dsc 0.51561\n",
      "Batch train [8] loss 0.47658, dsc 0.52342\n",
      "Batch train [9] loss 0.47784, dsc 0.52216\n",
      "Batch train [10] loss 0.47221, dsc 0.52779\n",
      "Epoch [440] train done\n",
      "Batch eval [1] loss 0.52554, dsc 0.47446\n",
      "Batch eval [2] loss 0.55845, dsc 0.44155\n",
      "Batch eval [3] loss 0.52298, dsc 0.47702\n",
      "Batch eval [4] loss 0.57163, dsc 0.42837\n",
      "Batch eval [5] loss 0.52878, dsc 0.47122\n",
      "Epoch [440] valid done\n",
      "Epoch [440] T 1768.56s, deltaT 4.00s, loss: train 0.47585, valid 0.54147, dsc: train 0.52415, valid 0.45853\n",
      "Batch train [1] loss 0.47396, dsc 0.52604\n",
      "Batch train [2] loss 0.52473, dsc 0.47527\n",
      "Batch train [3] loss 0.48457, dsc 0.51543\n",
      "Batch train [4] loss 0.47636, dsc 0.52364\n",
      "Batch train [5] loss 0.46399, dsc 0.53601\n",
      "Batch train [6] loss 0.48673, dsc 0.51327\n",
      "Batch train [7] loss 0.44954, dsc 0.55046\n",
      "Batch train [8] loss 0.46995, dsc 0.53005\n",
      "Batch train [9] loss 0.44003, dsc 0.55997\n",
      "Batch train [10] loss 0.48356, dsc 0.51644\n",
      "Epoch [441] train done\n",
      "Batch eval [1] loss 0.52045, dsc 0.47955\n",
      "Batch eval [2] loss 0.56511, dsc 0.43489\n",
      "Batch eval [3] loss 0.52965, dsc 0.47035\n",
      "Batch eval [4] loss 0.56920, dsc 0.43080\n",
      "Batch eval [5] loss 0.53418, dsc 0.46582\n",
      "Epoch [441] valid done\n",
      "Epoch [441] T 1772.53s, deltaT 3.97s, loss: train 0.47534, valid 0.54372, dsc: train 0.52466, valid 0.45628\n",
      "Batch train [1] loss 0.47899, dsc 0.52101\n",
      "Batch train [2] loss 0.47981, dsc 0.52019\n",
      "Batch train [3] loss 0.49106, dsc 0.50894\n",
      "Batch train [4] loss 0.48662, dsc 0.51338\n",
      "Batch train [5] loss 0.47829, dsc 0.52171\n",
      "Batch train [6] loss 0.47564, dsc 0.52436\n",
      "Batch train [7] loss 0.44079, dsc 0.55921\n",
      "Batch train [8] loss 0.47479, dsc 0.52521\n",
      "Batch train [9] loss 0.45248, dsc 0.54752\n",
      "Batch train [10] loss 0.48028, dsc 0.51972\n",
      "Epoch [442] train done\n",
      "Batch eval [1] loss 0.52314, dsc 0.47686\n",
      "Batch eval [2] loss 0.55467, dsc 0.44533\n",
      "Batch eval [3] loss 0.53801, dsc 0.46199\n",
      "Batch eval [4] loss 0.56686, dsc 0.43314\n",
      "Batch eval [5] loss 0.53690, dsc 0.46310\n",
      "Epoch [442] valid done\n",
      "Epoch [442] T 1776.52s, deltaT 3.99s, loss: train 0.47388, valid 0.54392, dsc: train 0.52612, valid 0.45608\n",
      "Batch train [1] loss 0.45991, dsc 0.54009\n",
      "Batch train [2] loss 0.48817, dsc 0.51183\n",
      "Batch train [3] loss 0.50410, dsc 0.49590\n",
      "Batch train [4] loss 0.45303, dsc 0.54697\n",
      "Batch train [5] loss 0.46627, dsc 0.53373\n",
      "Batch train [6] loss 0.49314, dsc 0.50686\n",
      "Batch train [7] loss 0.47621, dsc 0.52379\n",
      "Batch train [8] loss 0.49932, dsc 0.50068\n",
      "Batch train [9] loss 0.46422, dsc 0.53578\n",
      "Batch train [10] loss 0.43341, dsc 0.56659\n",
      "Epoch [443] train done\n",
      "Batch eval [1] loss 0.51989, dsc 0.48011\n",
      "Batch eval [2] loss 0.55942, dsc 0.44058\n",
      "Batch eval [3] loss 0.52109, dsc 0.47891\n",
      "Batch eval [4] loss 0.57218, dsc 0.42782\n",
      "Batch eval [5] loss 0.53240, dsc 0.46760\n",
      "Epoch [443] valid done\n",
      "Epoch [443] T 1780.59s, deltaT 4.06s, loss: train 0.47378, valid 0.54100, dsc: train 0.52622, valid 0.45900\n",
      "Batch train [1] loss 0.44692, dsc 0.55308\n",
      "Batch train [2] loss 0.47157, dsc 0.52843\n",
      "Batch train [3] loss 0.47258, dsc 0.52742\n",
      "Batch train [4] loss 0.45783, dsc 0.54217\n",
      "Batch train [5] loss 0.48101, dsc 0.51899\n",
      "Batch train [6] loss 0.45591, dsc 0.54409\n",
      "Batch train [7] loss 0.48936, dsc 0.51064\n",
      "Batch train [8] loss 0.46486, dsc 0.53514\n",
      "Batch train [9] loss 0.50157, dsc 0.49843\n",
      "Batch train [10] loss 0.48207, dsc 0.51793\n",
      "Epoch [444] train done\n",
      "Batch eval [1] loss 0.52054, dsc 0.47946\n",
      "Batch eval [2] loss 0.55478, dsc 0.44522\n",
      "Batch eval [3] loss 0.52522, dsc 0.47478\n",
      "Batch eval [4] loss 0.56532, dsc 0.43468\n",
      "Batch eval [5] loss 0.52596, dsc 0.47404\n",
      "Epoch [444] valid done\n",
      "Epoch [444] T 1784.60s, deltaT 4.01s, loss: train 0.47237, valid 0.53836, dsc: train 0.52763, valid 0.46164\n",
      "Batch train [1] loss 0.44986, dsc 0.55014\n",
      "Batch train [2] loss 0.43805, dsc 0.56195\n",
      "Batch train [3] loss 0.49666, dsc 0.50334\n",
      "Batch train [4] loss 0.48690, dsc 0.51310\n",
      "Batch train [5] loss 0.47004, dsc 0.52996\n",
      "Batch train [6] loss 0.47720, dsc 0.52280\n",
      "Batch train [7] loss 0.48487, dsc 0.51513\n",
      "Batch train [8] loss 0.46743, dsc 0.53257\n",
      "Batch train [9] loss 0.46185, dsc 0.53815\n",
      "Batch train [10] loss 0.48446, dsc 0.51554\n",
      "Epoch [445] train done\n",
      "Batch eval [1] loss 0.51541, dsc 0.48459\n",
      "Batch eval [2] loss 0.55190, dsc 0.44810\n",
      "Batch eval [3] loss 0.51498, dsc 0.48502\n",
      "Batch eval [4] loss 0.56557, dsc 0.43443\n",
      "Batch eval [5] loss 0.52800, dsc 0.47200\n",
      "Epoch [445] valid done\n",
      "Epoch [445] T 1788.62s, deltaT 4.02s, loss: train 0.47173, valid 0.53517, dsc: train 0.52827, valid 0.46483\n",
      "Batch train [1] loss 0.44360, dsc 0.55640\n",
      "Batch train [2] loss 0.45923, dsc 0.54077\n",
      "Batch train [3] loss 0.47547, dsc 0.52453\n",
      "Batch train [4] loss 0.46580, dsc 0.53420\n",
      "Batch train [5] loss 0.43430, dsc 0.56570\n",
      "Batch train [6] loss 0.48638, dsc 0.51362\n",
      "Batch train [7] loss 0.52337, dsc 0.47663\n",
      "Batch train [8] loss 0.48844, dsc 0.51156\n",
      "Batch train [9] loss 0.46193, dsc 0.53807\n",
      "Batch train [10] loss 0.47979, dsc 0.52021\n",
      "Epoch [446] train done\n",
      "Batch eval [1] loss 0.51662, dsc 0.48338\n",
      "Batch eval [2] loss 0.55808, dsc 0.44192\n",
      "Batch eval [3] loss 0.51627, dsc 0.48373\n",
      "Batch eval [4] loss 0.56680, dsc 0.43320\n",
      "Batch eval [5] loss 0.52339, dsc 0.47661\n",
      "Epoch [446] valid done\n",
      "Epoch [446] T 1792.64s, deltaT 4.02s, loss: train 0.47183, valid 0.53623, dsc: train 0.52817, valid 0.46377\n",
      "Batch train [1] loss 0.46831, dsc 0.53169\n",
      "Batch train [2] loss 0.46051, dsc 0.53949\n",
      "Batch train [3] loss 0.45725, dsc 0.54275\n",
      "Batch train [4] loss 0.47688, dsc 0.52312\n",
      "Batch train [5] loss 0.48911, dsc 0.51089\n",
      "Batch train [6] loss 0.47273, dsc 0.52727\n",
      "Batch train [7] loss 0.47657, dsc 0.52343\n",
      "Batch train [8] loss 0.48214, dsc 0.51786\n",
      "Batch train [9] loss 0.47295, dsc 0.52705\n",
      "Batch train [10] loss 0.44474, dsc 0.55526\n",
      "Epoch [447] train done\n",
      "Batch eval [1] loss 0.50911, dsc 0.49089\n",
      "Batch eval [2] loss 0.55678, dsc 0.44322\n",
      "Batch eval [3] loss 0.51542, dsc 0.48458\n",
      "Batch eval [4] loss 0.56511, dsc 0.43489\n",
      "Batch eval [5] loss 0.52675, dsc 0.47325\n",
      "Epoch [447] valid done\n",
      "Epoch [447] T 1796.65s, deltaT 4.01s, loss: train 0.47012, valid 0.53464, dsc: train 0.52988, valid 0.46536\n",
      "Batch train [1] loss 0.44727, dsc 0.55273\n",
      "Batch train [2] loss 0.43752, dsc 0.56248\n",
      "Batch train [3] loss 0.48266, dsc 0.51734\n",
      "Batch train [4] loss 0.50101, dsc 0.49899\n",
      "Batch train [5] loss 0.47832, dsc 0.52168\n",
      "Batch train [6] loss 0.46638, dsc 0.53362\n",
      "Batch train [7] loss 0.50997, dsc 0.49003\n",
      "Batch train [8] loss 0.43989, dsc 0.56011\n",
      "Batch train [9] loss 0.51697, dsc 0.48303\n",
      "Batch train [10] loss 0.42969, dsc 0.57031\n",
      "Epoch [448] train done\n",
      "Batch eval [1] loss 0.52000, dsc 0.48000\n",
      "Batch eval [2] loss 0.55540, dsc 0.44460\n",
      "Batch eval [3] loss 0.52210, dsc 0.47790\n",
      "Batch eval [4] loss 0.56419, dsc 0.43581\n",
      "Batch eval [5] loss 0.52721, dsc 0.47279\n",
      "Epoch [448] valid done\n",
      "Epoch [448] T 1800.69s, deltaT 4.04s, loss: train 0.47097, valid 0.53778, dsc: train 0.52903, valid 0.46222\n",
      "Batch train [1] loss 0.45736, dsc 0.54264\n",
      "Batch train [2] loss 0.46145, dsc 0.53855\n",
      "Batch train [3] loss 0.47464, dsc 0.52536\n",
      "Batch train [4] loss 0.48715, dsc 0.51285\n",
      "Batch train [5] loss 0.47513, dsc 0.52487\n",
      "Batch train [6] loss 0.44187, dsc 0.55813\n",
      "Batch train [7] loss 0.46141, dsc 0.53859\n",
      "Batch train [8] loss 0.48898, dsc 0.51102\n",
      "Batch train [9] loss 0.46945, dsc 0.53055\n",
      "Batch train [10] loss 0.45647, dsc 0.54353\n",
      "Epoch [449] train done\n",
      "Batch eval [1] loss 0.50909, dsc 0.49091\n",
      "Batch eval [2] loss 0.55099, dsc 0.44901\n",
      "Batch eval [3] loss 0.51487, dsc 0.48513\n",
      "Batch eval [4] loss 0.56297, dsc 0.43703\n",
      "Batch eval [5] loss 0.52546, dsc 0.47454\n",
      "Epoch [449] valid done\n",
      "Epoch [449] T 1804.70s, deltaT 4.00s, loss: train 0.46739, valid 0.53268, dsc: train 0.53261, valid 0.46732\n",
      "Batch train [1] loss 0.45141, dsc 0.54859\n",
      "Batch train [2] loss 0.47046, dsc 0.52954\n",
      "Batch train [3] loss 0.47052, dsc 0.52948\n",
      "Batch train [4] loss 0.45657, dsc 0.54343\n",
      "Batch train [5] loss 0.45061, dsc 0.54939\n",
      "Batch train [6] loss 0.44391, dsc 0.55609\n",
      "Batch train [7] loss 0.47815, dsc 0.52185\n",
      "Batch train [8] loss 0.47825, dsc 0.52175\n",
      "Batch train [9] loss 0.47632, dsc 0.52368\n",
      "Batch train [10] loss 0.49072, dsc 0.50928\n",
      "Epoch [450] train done\n",
      "Batch eval [1] loss 0.51948, dsc 0.48052\n",
      "Batch eval [2] loss 0.56028, dsc 0.43972\n",
      "Batch eval [3] loss 0.52605, dsc 0.47395\n",
      "Batch eval [4] loss 0.57020, dsc 0.42980\n",
      "Batch eval [5] loss 0.53186, dsc 0.46814\n",
      "Epoch [450] valid done\n",
      "Epoch [450] T 1808.70s, deltaT 4.01s, loss: train 0.46669, valid 0.54157, dsc: train 0.53331, valid 0.45843\n",
      "Batch train [1] loss 0.49739, dsc 0.50261\n",
      "Batch train [2] loss 0.47933, dsc 0.52067\n",
      "Batch train [3] loss 0.46363, dsc 0.53637\n",
      "Batch train [4] loss 0.49362, dsc 0.50638\n",
      "Batch train [5] loss 0.44732, dsc 0.55268\n",
      "Batch train [6] loss 0.45977, dsc 0.54023\n",
      "Batch train [7] loss 0.44147, dsc 0.55853\n",
      "Batch train [8] loss 0.44441, dsc 0.55559\n",
      "Batch train [9] loss 0.49310, dsc 0.50690\n",
      "Batch train [10] loss 0.44448, dsc 0.55552\n",
      "Epoch [451] train done\n",
      "Batch eval [1] loss 0.51842, dsc 0.48158\n",
      "Batch eval [2] loss 0.55928, dsc 0.44072\n",
      "Batch eval [3] loss 0.52569, dsc 0.47431\n",
      "Batch eval [4] loss 0.56605, dsc 0.43395\n",
      "Batch eval [5] loss 0.52837, dsc 0.47163\n",
      "Epoch [451] valid done\n",
      "Epoch [451] T 1812.71s, deltaT 4.01s, loss: train 0.46645, valid 0.53956, dsc: train 0.53355, valid 0.46044\n",
      "Batch train [1] loss 0.46684, dsc 0.53316\n",
      "Batch train [2] loss 0.45425, dsc 0.54575\n",
      "Batch train [3] loss 0.52001, dsc 0.47999\n",
      "Batch train [4] loss 0.45281, dsc 0.54719\n",
      "Batch train [5] loss 0.46066, dsc 0.53934\n",
      "Batch train [6] loss 0.44431, dsc 0.55569\n",
      "Batch train [7] loss 0.46053, dsc 0.53947\n",
      "Batch train [8] loss 0.47429, dsc 0.52571\n",
      "Batch train [9] loss 0.48421, dsc 0.51579\n",
      "Batch train [10] loss 0.44163, dsc 0.55837\n",
      "Epoch [452] train done\n",
      "Batch eval [1] loss 0.51544, dsc 0.48456\n",
      "Batch eval [2] loss 0.55460, dsc 0.44540\n",
      "Batch eval [3] loss 0.52756, dsc 0.47244\n",
      "Batch eval [4] loss 0.56471, dsc 0.43529\n",
      "Batch eval [5] loss 0.52598, dsc 0.47402\n",
      "Epoch [452] valid done\n",
      "Epoch [452] T 1816.67s, deltaT 3.96s, loss: train 0.46595, valid 0.53766, dsc: train 0.53405, valid 0.46234\n",
      "Batch train [1] loss 0.48219, dsc 0.51781\n",
      "Batch train [2] loss 0.46342, dsc 0.53658\n",
      "Batch train [3] loss 0.48528, dsc 0.51472\n",
      "Batch train [4] loss 0.47076, dsc 0.52924\n",
      "Batch train [5] loss 0.45107, dsc 0.54893\n",
      "Batch train [6] loss 0.44014, dsc 0.55986\n",
      "Batch train [7] loss 0.48052, dsc 0.51948\n",
      "Batch train [8] loss 0.44080, dsc 0.55920\n",
      "Batch train [9] loss 0.45723, dsc 0.54277\n",
      "Batch train [10] loss 0.47973, dsc 0.52027\n",
      "Epoch [453] train done\n",
      "Batch eval [1] loss 0.51620, dsc 0.48380\n",
      "Batch eval [2] loss 0.55555, dsc 0.44445\n",
      "Batch eval [3] loss 0.51367, dsc 0.48633\n",
      "Batch eval [4] loss 0.55716, dsc 0.44284\n",
      "Batch eval [5] loss 0.52588, dsc 0.47412\n",
      "Epoch [453] valid done\n",
      "Epoch [453] T 1820.66s, deltaT 3.98s, loss: train 0.46512, valid 0.53369, dsc: train 0.53488, valid 0.46631\n",
      "Batch train [1] loss 0.45990, dsc 0.54010\n",
      "Batch train [2] loss 0.47794, dsc 0.52206\n",
      "Batch train [3] loss 0.46933, dsc 0.53067\n",
      "Batch train [4] loss 0.43261, dsc 0.56739\n",
      "Batch train [5] loss 0.45893, dsc 0.54107\n",
      "Batch train [6] loss 0.47449, dsc 0.52551\n",
      "Batch train [7] loss 0.44815, dsc 0.55185\n",
      "Batch train [8] loss 0.46178, dsc 0.53822\n",
      "Batch train [9] loss 0.47149, dsc 0.52851\n",
      "Batch train [10] loss 0.47863, dsc 0.52137\n",
      "Epoch [454] train done\n",
      "Batch eval [1] loss 0.51482, dsc 0.48518\n",
      "Batch eval [2] loss 0.54333, dsc 0.45667\n",
      "Batch eval [3] loss 0.50989, dsc 0.49011\n",
      "Batch eval [4] loss 0.56125, dsc 0.43875\n",
      "Batch eval [5] loss 0.52473, dsc 0.47527\n",
      "Epoch [454] valid done\n",
      "Epoch [454] T 1824.66s, deltaT 4.00s, loss: train 0.46333, valid 0.53080, dsc: train 0.53667, valid 0.46920\n",
      "Batch train [1] loss 0.44936, dsc 0.55064\n",
      "Batch train [2] loss 0.44891, dsc 0.55109\n",
      "Batch train [3] loss 0.48444, dsc 0.51556\n",
      "Batch train [4] loss 0.48252, dsc 0.51748\n",
      "Batch train [5] loss 0.45168, dsc 0.54832\n",
      "Batch train [6] loss 0.44592, dsc 0.55408\n",
      "Batch train [7] loss 0.48343, dsc 0.51657\n",
      "Batch train [8] loss 0.48944, dsc 0.51056\n",
      "Batch train [9] loss 0.44281, dsc 0.55719\n",
      "Batch train [10] loss 0.45031, dsc 0.54969\n",
      "Epoch [455] train done\n",
      "Batch eval [1] loss 0.50661, dsc 0.49339\n",
      "Batch eval [2] loss 0.54252, dsc 0.45748\n",
      "Batch eval [3] loss 0.51161, dsc 0.48839\n",
      "Batch eval [4] loss 0.56084, dsc 0.43916\n",
      "Batch eval [5] loss 0.52288, dsc 0.47712\n",
      "Epoch [455] valid done\n",
      "Epoch [455] T 1828.65s, deltaT 4.00s, loss: train 0.46288, valid 0.52889, dsc: train 0.53712, valid 0.47111\n",
      "Batch train [1] loss 0.46627, dsc 0.53373\n",
      "Batch train [2] loss 0.46382, dsc 0.53618\n",
      "Batch train [3] loss 0.45589, dsc 0.54411\n",
      "Batch train [4] loss 0.44669, dsc 0.55331\n",
      "Batch train [5] loss 0.44168, dsc 0.55832\n",
      "Batch train [6] loss 0.43675, dsc 0.56325\n",
      "Batch train [7] loss 0.44126, dsc 0.55874\n",
      "Batch train [8] loss 0.46364, dsc 0.53636\n",
      "Batch train [9] loss 0.50548, dsc 0.49452\n",
      "Batch train [10] loss 0.50274, dsc 0.49726\n",
      "Epoch [456] train done\n",
      "Batch eval [1] loss 0.51537, dsc 0.48463\n",
      "Batch eval [2] loss 0.54790, dsc 0.45210\n",
      "Batch eval [3] loss 0.50723, dsc 0.49277\n",
      "Batch eval [4] loss 0.56127, dsc 0.43873\n",
      "Batch eval [5] loss 0.52098, dsc 0.47902\n",
      "Epoch [456] valid done\n",
      "Epoch [456] T 1832.66s, deltaT 4.00s, loss: train 0.46242, valid 0.53055, dsc: train 0.53758, valid 0.46945\n",
      "Batch train [1] loss 0.45897, dsc 0.54103\n",
      "Batch train [2] loss 0.48461, dsc 0.51539\n",
      "Batch train [3] loss 0.47652, dsc 0.52348\n",
      "Batch train [4] loss 0.48760, dsc 0.51240\n",
      "Batch train [5] loss 0.47003, dsc 0.52997\n",
      "Batch train [6] loss 0.47519, dsc 0.52481\n",
      "Batch train [7] loss 0.44640, dsc 0.55360\n",
      "Batch train [8] loss 0.43240, dsc 0.56760\n",
      "Batch train [9] loss 0.44815, dsc 0.55185\n",
      "Batch train [10] loss 0.43484, dsc 0.56516\n",
      "Epoch [457] train done\n",
      "Batch eval [1] loss 0.50690, dsc 0.49310\n",
      "Batch eval [2] loss 0.53697, dsc 0.46303\n",
      "Batch eval [3] loss 0.50001, dsc 0.49999\n",
      "Batch eval [4] loss 0.55544, dsc 0.44456\n",
      "Batch eval [5] loss 0.51796, dsc 0.48204\n",
      "Epoch [457] valid done\n",
      "Epoch [457] T 1836.60s, deltaT 3.95s, loss: train 0.46147, valid 0.52346, dsc: train 0.53853, valid 0.47654\n",
      "Batch train [1] loss 0.47374, dsc 0.52626\n",
      "Batch train [2] loss 0.45435, dsc 0.54565\n",
      "Batch train [3] loss 0.45674, dsc 0.54326\n",
      "Batch train [4] loss 0.45190, dsc 0.54810\n",
      "Batch train [5] loss 0.46014, dsc 0.53986\n",
      "Batch train [6] loss 0.44706, dsc 0.55294\n",
      "Batch train [7] loss 0.47361, dsc 0.52639\n",
      "Batch train [8] loss 0.43469, dsc 0.56531\n",
      "Batch train [9] loss 0.46145, dsc 0.53855\n",
      "Batch train [10] loss 0.48134, dsc 0.51866\n",
      "Epoch [458] train done\n",
      "Batch eval [1] loss 0.51029, dsc 0.48971\n",
      "Batch eval [2] loss 0.53621, dsc 0.46379\n",
      "Batch eval [3] loss 0.50074, dsc 0.49926\n",
      "Batch eval [4] loss 0.55752, dsc 0.44248\n",
      "Batch eval [5] loss 0.51309, dsc 0.48691\n",
      "Epoch [458] valid done\n",
      "Epoch [458] T 1840.61s, deltaT 4.01s, loss: train 0.45950, valid 0.52357, dsc: train 0.54050, valid 0.47643\n",
      "Batch train [1] loss 0.44078, dsc 0.55922\n",
      "Batch train [2] loss 0.42065, dsc 0.57935\n",
      "Batch train [3] loss 0.48732, dsc 0.51268\n",
      "Batch train [4] loss 0.47755, dsc 0.52245\n",
      "Batch train [5] loss 0.48494, dsc 0.51506\n",
      "Batch train [6] loss 0.47101, dsc 0.52899\n",
      "Batch train [7] loss 0.47236, dsc 0.52764\n",
      "Batch train [8] loss 0.43921, dsc 0.56079\n",
      "Batch train [9] loss 0.47086, dsc 0.52914\n",
      "Batch train [10] loss 0.43369, dsc 0.56631\n",
      "Epoch [459] train done\n",
      "Batch eval [1] loss 0.50760, dsc 0.49240\n",
      "Batch eval [2] loss 0.53940, dsc 0.46060\n",
      "Batch eval [3] loss 0.50460, dsc 0.49540\n",
      "Batch eval [4] loss 0.55836, dsc 0.44164\n",
      "Batch eval [5] loss 0.51644, dsc 0.48356\n",
      "Epoch [459] valid done\n",
      "Epoch [459] T 1844.64s, deltaT 4.03s, loss: train 0.45984, valid 0.52528, dsc: train 0.54016, valid 0.47472\n",
      "Batch train [1] loss 0.44481, dsc 0.55519\n",
      "Batch train [2] loss 0.46238, dsc 0.53762\n",
      "Batch train [3] loss 0.48531, dsc 0.51469\n",
      "Batch train [4] loss 0.44894, dsc 0.55106\n",
      "Batch train [5] loss 0.43567, dsc 0.56433\n",
      "Batch train [6] loss 0.47298, dsc 0.52702\n",
      "Batch train [7] loss 0.46694, dsc 0.53306\n",
      "Batch train [8] loss 0.44786, dsc 0.55214\n",
      "Batch train [9] loss 0.45793, dsc 0.54207\n",
      "Batch train [10] loss 0.45559, dsc 0.54441\n",
      "Epoch [460] train done\n",
      "Batch eval [1] loss 0.50648, dsc 0.49352\n",
      "Batch eval [2] loss 0.53664, dsc 0.46336\n",
      "Batch eval [3] loss 0.50884, dsc 0.49116\n",
      "Batch eval [4] loss 0.55555, dsc 0.44445\n",
      "Batch eval [5] loss 0.51963, dsc 0.48037\n",
      "Epoch [460] valid done\n",
      "Epoch [460] T 1848.72s, deltaT 4.07s, loss: train 0.45784, valid 0.52543, dsc: train 0.54216, valid 0.47457\n",
      "Batch train [1] loss 0.46976, dsc 0.53024\n",
      "Batch train [2] loss 0.46445, dsc 0.53555\n",
      "Batch train [3] loss 0.45916, dsc 0.54084\n",
      "Batch train [4] loss 0.46636, dsc 0.53364\n",
      "Batch train [5] loss 0.47548, dsc 0.52452\n",
      "Batch train [6] loss 0.43328, dsc 0.56672\n",
      "Batch train [7] loss 0.47485, dsc 0.52515\n",
      "Batch train [8] loss 0.42940, dsc 0.57060\n",
      "Batch train [9] loss 0.45761, dsc 0.54239\n",
      "Batch train [10] loss 0.44194, dsc 0.55806\n",
      "Epoch [461] train done\n",
      "Batch eval [1] loss 0.50474, dsc 0.49526\n",
      "Batch eval [2] loss 0.53469, dsc 0.46531\n",
      "Batch eval [3] loss 0.50516, dsc 0.49484\n",
      "Batch eval [4] loss 0.55375, dsc 0.44625\n",
      "Batch eval [5] loss 0.51511, dsc 0.48489\n",
      "Epoch [461] valid done\n",
      "Epoch [461] T 1852.68s, deltaT 3.96s, loss: train 0.45723, valid 0.52269, dsc: train 0.54277, valid 0.47731\n",
      "Batch train [1] loss 0.48233, dsc 0.51767\n",
      "Batch train [2] loss 0.44481, dsc 0.55519\n",
      "Batch train [3] loss 0.46520, dsc 0.53480\n",
      "Batch train [4] loss 0.43070, dsc 0.56930\n",
      "Batch train [5] loss 0.47909, dsc 0.52091\n",
      "Batch train [6] loss 0.45400, dsc 0.54600\n",
      "Batch train [7] loss 0.44506, dsc 0.55494\n",
      "Batch train [8] loss 0.45044, dsc 0.54956\n",
      "Batch train [9] loss 0.44972, dsc 0.55028\n",
      "Batch train [10] loss 0.46332, dsc 0.53668\n",
      "Epoch [462] train done\n",
      "Batch eval [1] loss 0.50506, dsc 0.49494\n",
      "Batch eval [2] loss 0.53105, dsc 0.46895\n",
      "Batch eval [3] loss 0.49768, dsc 0.50232\n",
      "Batch eval [4] loss 0.55170, dsc 0.44830\n",
      "Batch eval [5] loss 0.51291, dsc 0.48709\n",
      "Epoch [462] valid done\n",
      "Epoch [462] T 1856.73s, deltaT 4.04s, loss: train 0.45647, valid 0.51968, dsc: train 0.54353, valid 0.48032\n",
      "Batch train [1] loss 0.49245, dsc 0.50755\n",
      "Batch train [2] loss 0.45488, dsc 0.54512\n",
      "Batch train [3] loss 0.44836, dsc 0.55164\n",
      "Batch train [4] loss 0.45689, dsc 0.54311\n",
      "Batch train [5] loss 0.45089, dsc 0.54911\n",
      "Batch train [6] loss 0.45146, dsc 0.54854\n",
      "Batch train [7] loss 0.47138, dsc 0.52862\n",
      "Batch train [8] loss 0.43446, dsc 0.56554\n",
      "Batch train [9] loss 0.44583, dsc 0.55417\n",
      "Batch train [10] loss 0.44916, dsc 0.55084\n",
      "Epoch [463] train done\n",
      "Batch eval [1] loss 0.50024, dsc 0.49976\n",
      "Batch eval [2] loss 0.52913, dsc 0.47087\n",
      "Batch eval [3] loss 0.50604, dsc 0.49396\n",
      "Batch eval [4] loss 0.55183, dsc 0.44817\n",
      "Batch eval [5] loss 0.51029, dsc 0.48971\n",
      "Epoch [463] valid done\n",
      "Epoch [463] T 1860.76s, deltaT 4.04s, loss: train 0.45558, valid 0.51951, dsc: train 0.54442, valid 0.48049\n",
      "Batch train [1] loss 0.47229, dsc 0.52771\n",
      "Batch train [2] loss 0.44025, dsc 0.55975\n",
      "Batch train [3] loss 0.43897, dsc 0.56103\n",
      "Batch train [4] loss 0.47074, dsc 0.52926\n",
      "Batch train [5] loss 0.48386, dsc 0.51614\n",
      "Batch train [6] loss 0.44966, dsc 0.55034\n",
      "Batch train [7] loss 0.43419, dsc 0.56581\n",
      "Batch train [8] loss 0.46726, dsc 0.53274\n",
      "Batch train [9] loss 0.43303, dsc 0.56697\n",
      "Batch train [10] loss 0.45719, dsc 0.54281\n",
      "Epoch [464] train done\n",
      "Batch eval [1] loss 0.51236, dsc 0.48764\n",
      "Batch eval [2] loss 0.53593, dsc 0.46407\n",
      "Batch eval [3] loss 0.50359, dsc 0.49641\n",
      "Batch eval [4] loss 0.55161, dsc 0.44839\n",
      "Batch eval [5] loss 0.51786, dsc 0.48214\n",
      "Epoch [464] valid done\n",
      "Epoch [464] T 1864.76s, deltaT 3.99s, loss: train 0.45474, valid 0.52427, dsc: train 0.54526, valid 0.47573\n",
      "Batch train [1] loss 0.45549, dsc 0.54451\n",
      "Batch train [2] loss 0.45541, dsc 0.54459\n",
      "Batch train [3] loss 0.47341, dsc 0.52659\n",
      "Batch train [4] loss 0.44800, dsc 0.55200\n",
      "Batch train [5] loss 0.43681, dsc 0.56319\n",
      "Batch train [6] loss 0.47358, dsc 0.52642\n",
      "Batch train [7] loss 0.45755, dsc 0.54245\n",
      "Batch train [8] loss 0.44804, dsc 0.55196\n",
      "Batch train [9] loss 0.43539, dsc 0.56461\n",
      "Batch train [10] loss 0.44603, dsc 0.55397\n",
      "Epoch [465] train done\n",
      "Batch eval [1] loss 0.51818, dsc 0.48182\n",
      "Batch eval [2] loss 0.55297, dsc 0.44703\n",
      "Batch eval [3] loss 0.51269, dsc 0.48731\n",
      "Batch eval [4] loss 0.55441, dsc 0.44559\n",
      "Batch eval [5] loss 0.52228, dsc 0.47772\n",
      "Epoch [465] valid done\n",
      "Epoch [465] T 1868.77s, deltaT 4.02s, loss: train 0.45297, valid 0.53211, dsc: train 0.54703, valid 0.46789\n",
      "Batch train [1] loss 0.43797, dsc 0.56203\n",
      "Batch train [2] loss 0.47265, dsc 0.52735\n",
      "Batch train [3] loss 0.43673, dsc 0.56327\n",
      "Batch train [4] loss 0.47603, dsc 0.52397\n",
      "Batch train [5] loss 0.44099, dsc 0.55901\n",
      "Batch train [6] loss 0.46354, dsc 0.53646\n",
      "Batch train [7] loss 0.46090, dsc 0.53910\n",
      "Batch train [8] loss 0.43537, dsc 0.56463\n",
      "Batch train [9] loss 0.43055, dsc 0.56945\n",
      "Batch train [10] loss 0.47551, dsc 0.52449\n",
      "Epoch [466] train done\n",
      "Batch eval [1] loss 0.51136, dsc 0.48864\n",
      "Batch eval [2] loss 0.54186, dsc 0.45814\n",
      "Batch eval [3] loss 0.50519, dsc 0.49481\n",
      "Batch eval [4] loss 0.55367, dsc 0.44633\n",
      "Batch eval [5] loss 0.51114, dsc 0.48886\n",
      "Epoch [466] valid done\n",
      "Epoch [466] T 1872.75s, deltaT 3.97s, loss: train 0.45302, valid 0.52465, dsc: train 0.54698, valid 0.47535\n",
      "Batch train [1] loss 0.44313, dsc 0.55687\n",
      "Batch train [2] loss 0.48039, dsc 0.51961\n",
      "Batch train [3] loss 0.44856, dsc 0.55144\n",
      "Batch train [4] loss 0.46091, dsc 0.53909\n",
      "Batch train [5] loss 0.42014, dsc 0.57986\n",
      "Batch train [6] loss 0.47104, dsc 0.52896\n",
      "Batch train [7] loss 0.43411, dsc 0.56589\n",
      "Batch train [8] loss 0.45339, dsc 0.54661\n",
      "Batch train [9] loss 0.44035, dsc 0.55965\n",
      "Batch train [10] loss 0.46629, dsc 0.53371\n",
      "Epoch [467] train done\n",
      "Batch eval [1] loss 0.51834, dsc 0.48166\n",
      "Batch eval [2] loss 0.53975, dsc 0.46025\n",
      "Batch eval [3] loss 0.52034, dsc 0.47966\n",
      "Batch eval [4] loss 0.54799, dsc 0.45201\n",
      "Batch eval [5] loss 0.51444, dsc 0.48556\n",
      "Epoch [467] valid done\n",
      "Epoch [467] T 1876.74s, deltaT 3.99s, loss: train 0.45183, valid 0.52817, dsc: train 0.54817, valid 0.47183\n",
      "Batch train [1] loss 0.47800, dsc 0.52200\n",
      "Batch train [2] loss 0.43366, dsc 0.56634\n",
      "Batch train [3] loss 0.46373, dsc 0.53627\n",
      "Batch train [4] loss 0.45630, dsc 0.54370\n",
      "Batch train [5] loss 0.43606, dsc 0.56394\n",
      "Batch train [6] loss 0.46691, dsc 0.53309\n",
      "Batch train [7] loss 0.46379, dsc 0.53621\n",
      "Batch train [8] loss 0.43633, dsc 0.56367\n",
      "Batch train [9] loss 0.44900, dsc 0.55100\n",
      "Batch train [10] loss 0.43161, dsc 0.56839\n",
      "Epoch [468] train done\n",
      "Batch eval [1] loss 0.51300, dsc 0.48700\n",
      "Batch eval [2] loss 0.53508, dsc 0.46492\n",
      "Batch eval [3] loss 0.51360, dsc 0.48640\n",
      "Batch eval [4] loss 0.54739, dsc 0.45261\n",
      "Batch eval [5] loss 0.51312, dsc 0.48688\n",
      "Epoch [468] valid done\n",
      "Epoch [468] T 1880.77s, deltaT 4.03s, loss: train 0.45154, valid 0.52444, dsc: train 0.54846, valid 0.47556\n",
      "Batch train [1] loss 0.44778, dsc 0.55222\n",
      "Batch train [2] loss 0.43918, dsc 0.56082\n",
      "Batch train [3] loss 0.45662, dsc 0.54338\n",
      "Batch train [4] loss 0.44881, dsc 0.55119\n",
      "Batch train [5] loss 0.45646, dsc 0.54354\n",
      "Batch train [6] loss 0.46146, dsc 0.53854\n",
      "Batch train [7] loss 0.42784, dsc 0.57216\n",
      "Batch train [8] loss 0.43736, dsc 0.56264\n",
      "Batch train [9] loss 0.47188, dsc 0.52812\n",
      "Batch train [10] loss 0.45906, dsc 0.54094\n",
      "Epoch [469] train done\n",
      "Batch eval [1] loss 0.50408, dsc 0.49592\n",
      "Batch eval [2] loss 0.52996, dsc 0.47004\n",
      "Batch eval [3] loss 0.50338, dsc 0.49662\n",
      "Batch eval [4] loss 0.54869, dsc 0.45131\n",
      "Batch eval [5] loss 0.50772, dsc 0.49228\n",
      "Epoch [469] valid done\n",
      "Epoch [469] T 1884.71s, deltaT 3.95s, loss: train 0.45065, valid 0.51877, dsc: train 0.54935, valid 0.48123\n",
      "Batch train [1] loss 0.42623, dsc 0.57377\n",
      "Batch train [2] loss 0.46595, dsc 0.53405\n",
      "Batch train [3] loss 0.44815, dsc 0.55185\n",
      "Batch train [4] loss 0.45178, dsc 0.54822\n",
      "Batch train [5] loss 0.44829, dsc 0.55171\n",
      "Batch train [6] loss 0.42570, dsc 0.57430\n",
      "Batch train [7] loss 0.45875, dsc 0.54125\n",
      "Batch train [8] loss 0.46489, dsc 0.53511\n",
      "Batch train [9] loss 0.46629, dsc 0.53371\n",
      "Batch train [10] loss 0.44030, dsc 0.55970\n",
      "Epoch [470] train done\n",
      "Batch eval [1] loss 0.50269, dsc 0.49731\n",
      "Batch eval [2] loss 0.52739, dsc 0.47261\n",
      "Batch eval [3] loss 0.49404, dsc 0.50596\n",
      "Batch eval [4] loss 0.54770, dsc 0.45230\n",
      "Batch eval [5] loss 0.50592, dsc 0.49408\n",
      "Epoch [470] valid done\n",
      "Epoch [470] T 1888.71s, deltaT 3.99s, loss: train 0.44963, valid 0.51555, dsc: train 0.55037, valid 0.48445\n",
      "Batch train [1] loss 0.46284, dsc 0.53716\n",
      "Batch train [2] loss 0.46295, dsc 0.53705\n",
      "Batch train [3] loss 0.44614, dsc 0.55386\n",
      "Batch train [4] loss 0.43747, dsc 0.56253\n",
      "Batch train [5] loss 0.42835, dsc 0.57165\n",
      "Batch train [6] loss 0.45874, dsc 0.54126\n",
      "Batch train [7] loss 0.43990, dsc 0.56010\n",
      "Batch train [8] loss 0.44539, dsc 0.55461\n",
      "Batch train [9] loss 0.44984, dsc 0.55016\n",
      "Batch train [10] loss 0.46083, dsc 0.53917\n",
      "Epoch [471] train done\n",
      "Batch eval [1] loss 0.50757, dsc 0.49243\n",
      "Batch eval [2] loss 0.53809, dsc 0.46191\n",
      "Batch eval [3] loss 0.50591, dsc 0.49409\n",
      "Batch eval [4] loss 0.54422, dsc 0.45578\n",
      "Batch eval [5] loss 0.51272, dsc 0.48728\n",
      "Epoch [471] valid done\n",
      "Epoch [471] T 1892.68s, deltaT 3.98s, loss: train 0.44925, valid 0.52170, dsc: train 0.55075, valid 0.47830\n",
      "Batch train [1] loss 0.43923, dsc 0.56077\n",
      "Batch train [2] loss 0.45714, dsc 0.54286\n",
      "Batch train [3] loss 0.47965, dsc 0.52035\n",
      "Batch train [4] loss 0.42065, dsc 0.57935\n",
      "Batch train [5] loss 0.43246, dsc 0.56754\n",
      "Batch train [6] loss 0.45061, dsc 0.54939\n",
      "Batch train [7] loss 0.42942, dsc 0.57058\n",
      "Batch train [8] loss 0.44441, dsc 0.55559\n",
      "Batch train [9] loss 0.43731, dsc 0.56269\n",
      "Batch train [10] loss 0.49164, dsc 0.50836\n",
      "Epoch [472] train done\n",
      "Batch eval [1] loss 0.51151, dsc 0.48849\n",
      "Batch eval [2] loss 0.53462, dsc 0.46538\n",
      "Batch eval [3] loss 0.50865, dsc 0.49135\n",
      "Batch eval [4] loss 0.54978, dsc 0.45022\n",
      "Batch eval [5] loss 0.51281, dsc 0.48719\n",
      "Epoch [472] valid done\n",
      "Epoch [472] T 1896.65s, deltaT 3.97s, loss: train 0.44825, valid 0.52347, dsc: train 0.55175, valid 0.47653\n",
      "Batch train [1] loss 0.46536, dsc 0.53464\n",
      "Batch train [2] loss 0.44305, dsc 0.55695\n",
      "Batch train [3] loss 0.44835, dsc 0.55165\n",
      "Batch train [4] loss 0.42776, dsc 0.57224\n",
      "Batch train [5] loss 0.44419, dsc 0.55581\n",
      "Batch train [6] loss 0.45958, dsc 0.54042\n",
      "Batch train [7] loss 0.45171, dsc 0.54829\n",
      "Batch train [8] loss 0.42823, dsc 0.57177\n",
      "Batch train [9] loss 0.45110, dsc 0.54890\n",
      "Batch train [10] loss 0.45261, dsc 0.54739\n",
      "Epoch [473] train done\n",
      "Batch eval [1] loss 0.51866, dsc 0.48134\n",
      "Batch eval [2] loss 0.54228, dsc 0.45772\n",
      "Batch eval [3] loss 0.51687, dsc 0.48313\n",
      "Batch eval [4] loss 0.54474, dsc 0.45526\n",
      "Batch eval [5] loss 0.51556, dsc 0.48444\n",
      "Epoch [473] valid done\n",
      "Epoch [473] T 1900.66s, deltaT 4.00s, loss: train 0.44719, valid 0.52762, dsc: train 0.55281, valid 0.47238\n",
      "Batch train [1] loss 0.41926, dsc 0.58074\n",
      "Batch train [2] loss 0.48052, dsc 0.51948\n",
      "Batch train [3] loss 0.45994, dsc 0.54006\n",
      "Batch train [4] loss 0.43661, dsc 0.56339\n",
      "Batch train [5] loss 0.44627, dsc 0.55373\n",
      "Batch train [6] loss 0.43366, dsc 0.56634\n",
      "Batch train [7] loss 0.45524, dsc 0.54476\n",
      "Batch train [8] loss 0.45002, dsc 0.54998\n",
      "Batch train [9] loss 0.42760, dsc 0.57240\n",
      "Batch train [10] loss 0.45549, dsc 0.54451\n",
      "Epoch [474] train done\n",
      "Batch eval [1] loss 0.50733, dsc 0.49267\n",
      "Batch eval [2] loss 0.52763, dsc 0.47237\n",
      "Batch eval [3] loss 0.49650, dsc 0.50350\n",
      "Batch eval [4] loss 0.54501, dsc 0.45499\n",
      "Batch eval [5] loss 0.50460, dsc 0.49540\n",
      "Epoch [474] valid done\n",
      "Epoch [474] T 1904.68s, deltaT 4.03s, loss: train 0.44646, valid 0.51622, dsc: train 0.55354, valid 0.48378\n",
      "Batch train [1] loss 0.45148, dsc 0.54852\n",
      "Batch train [2] loss 0.44021, dsc 0.55979\n",
      "Batch train [3] loss 0.42339, dsc 0.57661\n",
      "Batch train [4] loss 0.40961, dsc 0.59039\n",
      "Batch train [5] loss 0.44448, dsc 0.55552\n",
      "Batch train [6] loss 0.45577, dsc 0.54423\n",
      "Batch train [7] loss 0.42274, dsc 0.57726\n",
      "Batch train [8] loss 0.49481, dsc 0.50519\n",
      "Batch train [9] loss 0.45039, dsc 0.54961\n",
      "Batch train [10] loss 0.47689, dsc 0.52311\n",
      "Epoch [475] train done\n",
      "Batch eval [1] loss 0.50755, dsc 0.49245\n",
      "Batch eval [2] loss 0.52938, dsc 0.47062\n",
      "Batch eval [3] loss 0.50102, dsc 0.49898\n",
      "Batch eval [4] loss 0.54413, dsc 0.45587\n",
      "Batch eval [5] loss 0.49948, dsc 0.50052\n",
      "Epoch [475] valid done\n",
      "Epoch [475] T 1908.65s, deltaT 3.96s, loss: train 0.44698, valid 0.51631, dsc: train 0.55302, valid 0.48369\n",
      "Batch train [1] loss 0.45132, dsc 0.54868\n",
      "Batch train [2] loss 0.47351, dsc 0.52649\n",
      "Batch train [3] loss 0.47547, dsc 0.52453\n",
      "Batch train [4] loss 0.43756, dsc 0.56244\n",
      "Batch train [5] loss 0.43243, dsc 0.56757\n",
      "Batch train [6] loss 0.42761, dsc 0.57239\n",
      "Batch train [7] loss 0.42319, dsc 0.57681\n",
      "Batch train [8] loss 0.44847, dsc 0.55153\n",
      "Batch train [9] loss 0.40695, dsc 0.59305\n",
      "Batch train [10] loss 0.47642, dsc 0.52358\n",
      "Epoch [476] train done\n",
      "Batch eval [1] loss 0.50180, dsc 0.49820\n",
      "Batch eval [2] loss 0.53053, dsc 0.46947\n",
      "Batch eval [3] loss 0.49550, dsc 0.50450\n",
      "Batch eval [4] loss 0.54234, dsc 0.45766\n",
      "Batch eval [5] loss 0.50489, dsc 0.49511\n",
      "Epoch [476] valid done\n",
      "Epoch [476] T 1912.54s, deltaT 3.89s, loss: train 0.44529, valid 0.51501, dsc: train 0.55471, valid 0.48499\n",
      "Batch train [1] loss 0.42849, dsc 0.57151\n",
      "Batch train [2] loss 0.45745, dsc 0.54255\n",
      "Batch train [3] loss 0.46429, dsc 0.53571\n",
      "Batch train [4] loss 0.45055, dsc 0.54945\n",
      "Batch train [5] loss 0.43489, dsc 0.56511\n",
      "Batch train [6] loss 0.42289, dsc 0.57711\n",
      "Batch train [7] loss 0.44047, dsc 0.55953\n",
      "Batch train [8] loss 0.44179, dsc 0.55821\n",
      "Batch train [9] loss 0.45251, dsc 0.54749\n",
      "Batch train [10] loss 0.44741, dsc 0.55259\n",
      "Epoch [477] train done\n",
      "Batch eval [1] loss 0.49736, dsc 0.50264\n",
      "Batch eval [2] loss 0.52783, dsc 0.47217\n",
      "Batch eval [3] loss 0.50584, dsc 0.49416\n",
      "Batch eval [4] loss 0.54722, dsc 0.45278\n",
      "Batch eval [5] loss 0.50054, dsc 0.49946\n",
      "Epoch [477] valid done\n",
      "Epoch [477] T 1916.53s, deltaT 3.99s, loss: train 0.44408, valid 0.51576, dsc: train 0.55592, valid 0.48424\n",
      "Batch train [1] loss 0.47449, dsc 0.52551\n",
      "Batch train [2] loss 0.43325, dsc 0.56675\n",
      "Batch train [3] loss 0.45298, dsc 0.54702\n",
      "Batch train [4] loss 0.41387, dsc 0.58613\n",
      "Batch train [5] loss 0.46236, dsc 0.53764\n",
      "Batch train [6] loss 0.43234, dsc 0.56766\n",
      "Batch train [7] loss 0.41451, dsc 0.58549\n",
      "Batch train [8] loss 0.45277, dsc 0.54723\n",
      "Batch train [9] loss 0.47609, dsc 0.52391\n",
      "Batch train [10] loss 0.42707, dsc 0.57293\n",
      "Epoch [478] train done\n",
      "Batch eval [1] loss 0.49955, dsc 0.50045\n",
      "Batch eval [2] loss 0.53291, dsc 0.46709\n",
      "Batch eval [3] loss 0.51027, dsc 0.48973\n",
      "Batch eval [4] loss 0.54408, dsc 0.45592\n",
      "Batch eval [5] loss 0.51069, dsc 0.48931\n",
      "Epoch [478] valid done\n",
      "Epoch [478] T 1920.56s, deltaT 4.03s, loss: train 0.44397, valid 0.51950, dsc: train 0.55603, valid 0.48050\n",
      "Batch train [1] loss 0.47231, dsc 0.52769\n",
      "Batch train [2] loss 0.45921, dsc 0.54079\n",
      "Batch train [3] loss 0.43933, dsc 0.56067\n",
      "Batch train [4] loss 0.42561, dsc 0.57439\n",
      "Batch train [5] loss 0.41524, dsc 0.58476\n",
      "Batch train [6] loss 0.42558, dsc 0.57442\n",
      "Batch train [7] loss 0.44870, dsc 0.55130\n",
      "Batch train [8] loss 0.42668, dsc 0.57332\n",
      "Batch train [9] loss 0.44623, dsc 0.55377\n",
      "Batch train [10] loss 0.47169, dsc 0.52831\n",
      "Epoch [479] train done\n",
      "Batch eval [1] loss 0.49227, dsc 0.50773\n",
      "Batch eval [2] loss 0.52265, dsc 0.47735\n",
      "Batch eval [3] loss 0.48663, dsc 0.51337\n",
      "Batch eval [4] loss 0.53835, dsc 0.46165\n",
      "Batch eval [5] loss 0.49738, dsc 0.50262\n",
      "Epoch [479] valid done\n",
      "Epoch [479] T 1924.52s, deltaT 3.96s, loss: train 0.44306, valid 0.50746, dsc: train 0.55694, valid 0.49254\n",
      "Batch train [1] loss 0.44557, dsc 0.55443\n",
      "Batch train [2] loss 0.41853, dsc 0.58147\n",
      "Batch train [3] loss 0.45376, dsc 0.54624\n",
      "Batch train [4] loss 0.44031, dsc 0.55969\n",
      "Batch train [5] loss 0.44161, dsc 0.55839\n",
      "Batch train [6] loss 0.42405, dsc 0.57595\n",
      "Batch train [7] loss 0.46031, dsc 0.53969\n",
      "Batch train [8] loss 0.44228, dsc 0.55772\n",
      "Batch train [9] loss 0.41731, dsc 0.58269\n",
      "Batch train [10] loss 0.47237, dsc 0.52763\n",
      "Epoch [480] train done\n",
      "Batch eval [1] loss 0.48961, dsc 0.51039\n",
      "Batch eval [2] loss 0.52882, dsc 0.47118\n",
      "Batch eval [3] loss 0.49308, dsc 0.50692\n",
      "Batch eval [4] loss 0.54132, dsc 0.45868\n",
      "Batch eval [5] loss 0.49994, dsc 0.50006\n",
      "Epoch [480] valid done\n",
      "Epoch [480] T 1928.45s, deltaT 3.93s, loss: train 0.44161, valid 0.51055, dsc: train 0.55839, valid 0.48945\n",
      "Batch train [1] loss 0.43283, dsc 0.56717\n",
      "Batch train [2] loss 0.46777, dsc 0.53223\n",
      "Batch train [3] loss 0.45705, dsc 0.54295\n",
      "Batch train [4] loss 0.48543, dsc 0.51457\n",
      "Batch train [5] loss 0.43437, dsc 0.56563\n",
      "Batch train [6] loss 0.43521, dsc 0.56479\n",
      "Batch train [7] loss 0.42574, dsc 0.57426\n",
      "Batch train [8] loss 0.40751, dsc 0.59249\n",
      "Batch train [9] loss 0.44587, dsc 0.55413\n",
      "Batch train [10] loss 0.42386, dsc 0.57614\n",
      "Epoch [481] train done\n",
      "Batch eval [1] loss 0.48652, dsc 0.51348\n",
      "Batch eval [2] loss 0.52252, dsc 0.47748\n",
      "Batch eval [3] loss 0.49003, dsc 0.50997\n",
      "Batch eval [4] loss 0.54067, dsc 0.45933\n",
      "Batch eval [5] loss 0.50002, dsc 0.49998\n",
      "Epoch [481] valid done\n",
      "Epoch [481] T 1932.41s, deltaT 3.95s, loss: train 0.44156, valid 0.50795, dsc: train 0.55844, valid 0.49205\n",
      "Batch train [1] loss 0.40402, dsc 0.59598\n",
      "Batch train [2] loss 0.42633, dsc 0.57367\n",
      "Batch train [3] loss 0.46258, dsc 0.53742\n",
      "Batch train [4] loss 0.44456, dsc 0.55544\n",
      "Batch train [5] loss 0.42262, dsc 0.57738\n",
      "Batch train [6] loss 0.47669, dsc 0.52331\n",
      "Batch train [7] loss 0.45244, dsc 0.54756\n",
      "Batch train [8] loss 0.43549, dsc 0.56451\n",
      "Batch train [9] loss 0.45133, dsc 0.54867\n",
      "Batch train [10] loss 0.43294, dsc 0.56706\n",
      "Epoch [482] train done\n",
      "Batch eval [1] loss 0.49329, dsc 0.50671\n",
      "Batch eval [2] loss 0.52716, dsc 0.47284\n",
      "Batch eval [3] loss 0.48878, dsc 0.51122\n",
      "Batch eval [4] loss 0.54207, dsc 0.45793\n",
      "Batch eval [5] loss 0.50447, dsc 0.49553\n",
      "Epoch [482] valid done\n",
      "Epoch [482] T 1936.42s, deltaT 4.01s, loss: train 0.44090, valid 0.51115, dsc: train 0.55910, valid 0.48885\n",
      "Batch train [1] loss 0.46686, dsc 0.53314\n",
      "Batch train [2] loss 0.41819, dsc 0.58181\n",
      "Batch train [3] loss 0.44015, dsc 0.55985\n",
      "Batch train [4] loss 0.41968, dsc 0.58032\n",
      "Batch train [5] loss 0.43886, dsc 0.56114\n",
      "Batch train [6] loss 0.42915, dsc 0.57085\n",
      "Batch train [7] loss 0.44735, dsc 0.55265\n",
      "Batch train [8] loss 0.42829, dsc 0.57171\n",
      "Batch train [9] loss 0.47676, dsc 0.52324\n",
      "Batch train [10] loss 0.43240, dsc 0.56760\n",
      "Epoch [483] train done\n",
      "Batch eval [1] loss 0.49279, dsc 0.50721\n",
      "Batch eval [2] loss 0.52894, dsc 0.47106\n",
      "Batch eval [3] loss 0.48892, dsc 0.51108\n",
      "Batch eval [4] loss 0.54080, dsc 0.45920\n",
      "Batch eval [5] loss 0.50429, dsc 0.49571\n",
      "Epoch [483] valid done\n",
      "Epoch [483] T 1940.44s, deltaT 4.02s, loss: train 0.43977, valid 0.51115, dsc: train 0.56023, valid 0.48885\n",
      "Batch train [1] loss 0.43763, dsc 0.56237\n",
      "Batch train [2] loss 0.41741, dsc 0.58259\n",
      "Batch train [3] loss 0.44798, dsc 0.55202\n",
      "Batch train [4] loss 0.44832, dsc 0.55168\n",
      "Batch train [5] loss 0.47673, dsc 0.52327\n",
      "Batch train [6] loss 0.45103, dsc 0.54897\n",
      "Batch train [7] loss 0.44254, dsc 0.55746\n",
      "Batch train [8] loss 0.41455, dsc 0.58545\n",
      "Batch train [9] loss 0.42990, dsc 0.57010\n",
      "Batch train [10] loss 0.41989, dsc 0.58011\n",
      "Epoch [484] train done\n",
      "Batch eval [1] loss 0.48841, dsc 0.51159\n",
      "Batch eval [2] loss 0.52577, dsc 0.47423\n",
      "Batch eval [3] loss 0.48545, dsc 0.51455\n",
      "Batch eval [4] loss 0.53724, dsc 0.46276\n",
      "Batch eval [5] loss 0.49904, dsc 0.50096\n",
      "Epoch [484] valid done\n",
      "Epoch [484] T 1944.48s, deltaT 4.04s, loss: train 0.43860, valid 0.50718, dsc: train 0.56140, valid 0.49282\n",
      "Batch train [1] loss 0.46748, dsc 0.53252\n",
      "Batch train [2] loss 0.44162, dsc 0.55838\n",
      "Batch train [3] loss 0.43503, dsc 0.56497\n",
      "Batch train [4] loss 0.43607, dsc 0.56393\n",
      "Batch train [5] loss 0.42111, dsc 0.57889\n",
      "Batch train [6] loss 0.41877, dsc 0.58123\n",
      "Batch train [7] loss 0.43743, dsc 0.56257\n",
      "Batch train [8] loss 0.46701, dsc 0.53299\n",
      "Batch train [9] loss 0.42781, dsc 0.57219\n",
      "Batch train [10] loss 0.42838, dsc 0.57162\n",
      "Epoch [485] train done\n",
      "Batch eval [1] loss 0.48606, dsc 0.51394\n",
      "Batch eval [2] loss 0.51683, dsc 0.48317\n",
      "Batch eval [3] loss 0.48770, dsc 0.51230\n",
      "Batch eval [4] loss 0.54030, dsc 0.45970\n",
      "Batch eval [5] loss 0.49755, dsc 0.50245\n",
      "Epoch [485] valid done\n",
      "Epoch [485] T 1948.49s, deltaT 4.01s, loss: train 0.43807, valid 0.50569, dsc: train 0.56193, valid 0.49431\n",
      "Batch train [1] loss 0.43272, dsc 0.56728\n",
      "Batch train [2] loss 0.45007, dsc 0.54993\n",
      "Batch train [3] loss 0.42850, dsc 0.57150\n",
      "Batch train [4] loss 0.42600, dsc 0.57400\n",
      "Batch train [5] loss 0.45735, dsc 0.54265\n",
      "Batch train [6] loss 0.40243, dsc 0.59757\n",
      "Batch train [7] loss 0.44218, dsc 0.55782\n",
      "Batch train [8] loss 0.46420, dsc 0.53580\n",
      "Batch train [9] loss 0.43529, dsc 0.56471\n",
      "Batch train [10] loss 0.43896, dsc 0.56104\n",
      "Epoch [486] train done\n",
      "Batch eval [1] loss 0.49198, dsc 0.50802\n",
      "Batch eval [2] loss 0.52211, dsc 0.47789\n",
      "Batch eval [3] loss 0.49326, dsc 0.50674\n",
      "Batch eval [4] loss 0.53696, dsc 0.46304\n",
      "Batch eval [5] loss 0.49522, dsc 0.50478\n",
      "Epoch [486] valid done\n",
      "Epoch [486] T 1952.49s, deltaT 4.00s, loss: train 0.43777, valid 0.50791, dsc: train 0.56223, valid 0.49209\n",
      "Batch train [1] loss 0.46482, dsc 0.53518\n",
      "Batch train [2] loss 0.42270, dsc 0.57730\n",
      "Batch train [3] loss 0.40572, dsc 0.59428\n",
      "Batch train [4] loss 0.45433, dsc 0.54567\n",
      "Batch train [5] loss 0.45569, dsc 0.54431\n",
      "Batch train [6] loss 0.45175, dsc 0.54825\n",
      "Batch train [7] loss 0.41719, dsc 0.58281\n",
      "Batch train [8] loss 0.42426, dsc 0.57574\n",
      "Batch train [9] loss 0.44046, dsc 0.55954\n",
      "Batch train [10] loss 0.43131, dsc 0.56869\n",
      "Epoch [487] train done\n",
      "Batch eval [1] loss 0.49410, dsc 0.50590\n",
      "Batch eval [2] loss 0.52029, dsc 0.47971\n",
      "Batch eval [3] loss 0.48285, dsc 0.51715\n",
      "Batch eval [4] loss 0.53673, dsc 0.46327\n",
      "Batch eval [5] loss 0.49824, dsc 0.50176\n",
      "Epoch [487] valid done\n",
      "Epoch [487] T 1956.50s, deltaT 4.01s, loss: train 0.43682, valid 0.50644, dsc: train 0.56318, valid 0.49356\n",
      "Batch train [1] loss 0.44586, dsc 0.55414\n",
      "Batch train [2] loss 0.40233, dsc 0.59767\n",
      "Batch train [3] loss 0.44980, dsc 0.55020\n",
      "Batch train [4] loss 0.42321, dsc 0.57679\n",
      "Batch train [5] loss 0.40765, dsc 0.59235\n",
      "Batch train [6] loss 0.45643, dsc 0.54357\n",
      "Batch train [7] loss 0.45744, dsc 0.54256\n",
      "Batch train [8] loss 0.46634, dsc 0.53366\n",
      "Batch train [9] loss 0.41294, dsc 0.58706\n",
      "Batch train [10] loss 0.43902, dsc 0.56098\n",
      "Epoch [488] train done\n",
      "Batch eval [1] loss 0.50257, dsc 0.49743\n",
      "Batch eval [2] loss 0.52113, dsc 0.47887\n",
      "Batch eval [3] loss 0.48620, dsc 0.51380\n",
      "Batch eval [4] loss 0.53555, dsc 0.46445\n",
      "Batch eval [5] loss 0.50256, dsc 0.49744\n",
      "Epoch [488] valid done\n",
      "Epoch [488] T 1960.46s, deltaT 3.96s, loss: train 0.43610, valid 0.50960, dsc: train 0.56390, valid 0.49040\n",
      "Batch train [1] loss 0.43339, dsc 0.56661\n",
      "Batch train [2] loss 0.45207, dsc 0.54793\n",
      "Batch train [3] loss 0.43874, dsc 0.56126\n",
      "Batch train [4] loss 0.40089, dsc 0.59911\n",
      "Batch train [5] loss 0.42946, dsc 0.57054\n",
      "Batch train [6] loss 0.42585, dsc 0.57415\n",
      "Batch train [7] loss 0.45609, dsc 0.54391\n",
      "Batch train [8] loss 0.43085, dsc 0.56915\n",
      "Batch train [9] loss 0.41410, dsc 0.58590\n",
      "Batch train [10] loss 0.46307, dsc 0.53693\n",
      "Epoch [489] train done\n",
      "Batch eval [1] loss 0.48636, dsc 0.51364\n",
      "Batch eval [2] loss 0.52145, dsc 0.47855\n",
      "Batch eval [3] loss 0.48380, dsc 0.51620\n",
      "Batch eval [4] loss 0.53361, dsc 0.46639\n",
      "Batch eval [5] loss 0.49828, dsc 0.50172\n",
      "Epoch [489] valid done\n",
      "Epoch [489] T 1964.38s, deltaT 3.92s, loss: train 0.43445, valid 0.50470, dsc: train 0.56555, valid 0.49530\n",
      "Batch train [1] loss 0.44365, dsc 0.55635\n",
      "Batch train [2] loss 0.41297, dsc 0.58703\n",
      "Batch train [3] loss 0.40386, dsc 0.59614\n",
      "Batch train [4] loss 0.46550, dsc 0.53450\n",
      "Batch train [5] loss 0.41534, dsc 0.58466\n",
      "Batch train [6] loss 0.46050, dsc 0.53950\n",
      "Batch train [7] loss 0.45059, dsc 0.54941\n",
      "Batch train [8] loss 0.42513, dsc 0.57487\n",
      "Batch train [9] loss 0.43532, dsc 0.56468\n",
      "Batch train [10] loss 0.42437, dsc 0.57563\n",
      "Epoch [490] train done\n",
      "Batch eval [1] loss 0.48844, dsc 0.51156\n",
      "Batch eval [2] loss 0.52009, dsc 0.47991\n",
      "Batch eval [3] loss 0.48805, dsc 0.51195\n",
      "Batch eval [4] loss 0.53397, dsc 0.46603\n",
      "Batch eval [5] loss 0.49336, dsc 0.50664\n",
      "Epoch [490] valid done\n",
      "Epoch [490] T 1968.37s, deltaT 3.99s, loss: train 0.43372, valid 0.50478, dsc: train 0.56628, valid 0.49522\n",
      "Batch train [1] loss 0.41349, dsc 0.58651\n",
      "Batch train [2] loss 0.41191, dsc 0.58809\n",
      "Batch train [3] loss 0.46311, dsc 0.53689\n",
      "Batch train [4] loss 0.47857, dsc 0.52143\n",
      "Batch train [5] loss 0.43343, dsc 0.56657\n",
      "Batch train [6] loss 0.41682, dsc 0.58318\n",
      "Batch train [7] loss 0.44368, dsc 0.55632\n",
      "Batch train [8] loss 0.45145, dsc 0.54855\n",
      "Batch train [9] loss 0.40585, dsc 0.59415\n",
      "Batch train [10] loss 0.41946, dsc 0.58054\n",
      "Epoch [491] train done\n",
      "Batch eval [1] loss 0.49285, dsc 0.50715\n",
      "Batch eval [2] loss 0.52260, dsc 0.47740\n",
      "Batch eval [3] loss 0.49140, dsc 0.50860\n",
      "Batch eval [4] loss 0.53927, dsc 0.46073\n",
      "Batch eval [5] loss 0.49626, dsc 0.50374\n",
      "Epoch [491] valid done\n",
      "Epoch [491] T 1972.40s, deltaT 4.03s, loss: train 0.43378, valid 0.50847, dsc: train 0.56622, valid 0.49153\n",
      "Batch train [1] loss 0.40680, dsc 0.59320\n",
      "Batch train [2] loss 0.41445, dsc 0.58555\n",
      "Batch train [3] loss 0.40778, dsc 0.59222\n",
      "Batch train [4] loss 0.40992, dsc 0.59008\n",
      "Batch train [5] loss 0.46331, dsc 0.53669\n",
      "Batch train [6] loss 0.41649, dsc 0.58351\n",
      "Batch train [7] loss 0.45396, dsc 0.54604\n",
      "Batch train [8] loss 0.44000, dsc 0.56000\n",
      "Batch train [9] loss 0.48082, dsc 0.51918\n",
      "Batch train [10] loss 0.43565, dsc 0.56435\n",
      "Epoch [492] train done\n",
      "Batch eval [1] loss 0.50216, dsc 0.49784\n",
      "Batch eval [2] loss 0.52859, dsc 0.47141\n",
      "Batch eval [3] loss 0.49273, dsc 0.50727\n",
      "Batch eval [4] loss 0.53633, dsc 0.46367\n",
      "Batch eval [5] loss 0.49893, dsc 0.50107\n",
      "Epoch [492] valid done\n",
      "Epoch [492] T 1976.44s, deltaT 4.04s, loss: train 0.43292, valid 0.51175, dsc: train 0.56708, valid 0.48825\n",
      "Batch train [1] loss 0.45863, dsc 0.54137\n",
      "Batch train [2] loss 0.42869, dsc 0.57131\n",
      "Batch train [3] loss 0.46326, dsc 0.53674\n",
      "Batch train [4] loss 0.41370, dsc 0.58630\n",
      "Batch train [5] loss 0.44963, dsc 0.55037\n",
      "Batch train [6] loss 0.42438, dsc 0.57562\n",
      "Batch train [7] loss 0.41653, dsc 0.58347\n",
      "Batch train [8] loss 0.42623, dsc 0.57377\n",
      "Batch train [9] loss 0.39993, dsc 0.60007\n",
      "Batch train [10] loss 0.43296, dsc 0.56704\n",
      "Epoch [493] train done\n",
      "Batch eval [1] loss 0.49104, dsc 0.50896\n",
      "Batch eval [2] loss 0.52305, dsc 0.47695\n",
      "Batch eval [3] loss 0.49335, dsc 0.50665\n",
      "Batch eval [4] loss 0.53252, dsc 0.46748\n",
      "Batch eval [5] loss 0.49411, dsc 0.50589\n",
      "Epoch [493] valid done\n",
      "Epoch [493] T 1980.40s, deltaT 3.96s, loss: train 0.43139, valid 0.50682, dsc: train 0.56861, valid 0.49318\n",
      "Batch train [1] loss 0.43879, dsc 0.56121\n",
      "Batch train [2] loss 0.41114, dsc 0.58886\n",
      "Batch train [3] loss 0.39867, dsc 0.60133\n",
      "Batch train [4] loss 0.42701, dsc 0.57299\n",
      "Batch train [5] loss 0.44652, dsc 0.55348\n",
      "Batch train [6] loss 0.43226, dsc 0.56774\n",
      "Batch train [7] loss 0.44608, dsc 0.55392\n",
      "Batch train [8] loss 0.41849, dsc 0.58151\n",
      "Batch train [9] loss 0.45077, dsc 0.54923\n",
      "Batch train [10] loss 0.43217, dsc 0.56783\n",
      "Epoch [494] train done\n",
      "Batch eval [1] loss 0.48715, dsc 0.51285\n",
      "Batch eval [2] loss 0.51964, dsc 0.48036\n",
      "Batch eval [3] loss 0.49179, dsc 0.50821\n",
      "Batch eval [4] loss 0.53287, dsc 0.46713\n",
      "Batch eval [5] loss 0.50222, dsc 0.49778\n",
      "Epoch [494] valid done\n",
      "Epoch [494] T 1984.47s, deltaT 4.07s, loss: train 0.43019, valid 0.50674, dsc: train 0.56981, valid 0.49326\n",
      "Batch train [1] loss 0.45068, dsc 0.54932\n",
      "Batch train [2] loss 0.40247, dsc 0.59753\n",
      "Batch train [3] loss 0.45343, dsc 0.54657\n",
      "Batch train [4] loss 0.43612, dsc 0.56388\n",
      "Batch train [5] loss 0.44358, dsc 0.55642\n",
      "Batch train [6] loss 0.40767, dsc 0.59233\n",
      "Batch train [7] loss 0.41929, dsc 0.58071\n",
      "Batch train [8] loss 0.40296, dsc 0.59704\n",
      "Batch train [9] loss 0.45054, dsc 0.54946\n",
      "Batch train [10] loss 0.43151, dsc 0.56849\n",
      "Epoch [495] train done\n",
      "Batch eval [1] loss 0.49031, dsc 0.50969\n",
      "Batch eval [2] loss 0.51396, dsc 0.48604\n",
      "Batch eval [3] loss 0.49953, dsc 0.50047\n",
      "Batch eval [4] loss 0.53340, dsc 0.46660\n",
      "Batch eval [5] loss 0.49164, dsc 0.50836\n",
      "Epoch [495] valid done\n",
      "Epoch [495] T 1988.56s, deltaT 4.09s, loss: train 0.42983, valid 0.50577, dsc: train 0.57017, valid 0.49423\n",
      "Batch train [1] loss 0.41601, dsc 0.58399\n",
      "Batch train [2] loss 0.41592, dsc 0.58408\n",
      "Batch train [3] loss 0.44322, dsc 0.55678\n",
      "Batch train [4] loss 0.43469, dsc 0.56531\n",
      "Batch train [5] loss 0.42995, dsc 0.57005\n",
      "Batch train [6] loss 0.43309, dsc 0.56691\n",
      "Batch train [7] loss 0.40339, dsc 0.59661\n",
      "Batch train [8] loss 0.40924, dsc 0.59076\n",
      "Batch train [9] loss 0.44863, dsc 0.55137\n",
      "Batch train [10] loss 0.44780, dsc 0.55220\n",
      "Epoch [496] train done\n",
      "Batch eval [1] loss 0.48175, dsc 0.51825\n",
      "Batch eval [2] loss 0.52199, dsc 0.47801\n",
      "Batch eval [3] loss 0.49180, dsc 0.50820\n",
      "Batch eval [4] loss 0.53385, dsc 0.46615\n",
      "Batch eval [5] loss 0.49040, dsc 0.50960\n",
      "Epoch [496] valid done\n",
      "Epoch [496] T 1992.78s, deltaT 4.22s, loss: train 0.42819, valid 0.50396, dsc: train 0.57181, valid 0.49604\n",
      "Batch train [1] loss 0.42506, dsc 0.57494\n",
      "Batch train [2] loss 0.40668, dsc 0.59332\n",
      "Batch train [3] loss 0.41167, dsc 0.58833\n",
      "Batch train [4] loss 0.47054, dsc 0.52946\n",
      "Batch train [5] loss 0.42268, dsc 0.57732\n",
      "Batch train [6] loss 0.42650, dsc 0.57350\n",
      "Batch train [7] loss 0.43513, dsc 0.56487\n",
      "Batch train [8] loss 0.41646, dsc 0.58354\n",
      "Batch train [9] loss 0.42617, dsc 0.57383\n",
      "Batch train [10] loss 0.43552, dsc 0.56448\n",
      "Epoch [497] train done\n",
      "Batch eval [1] loss 0.48023, dsc 0.51977\n",
      "Batch eval [2] loss 0.51732, dsc 0.48268\n",
      "Batch eval [3] loss 0.48590, dsc 0.51410\n",
      "Batch eval [4] loss 0.52993, dsc 0.47007\n",
      "Batch eval [5] loss 0.48376, dsc 0.51624\n",
      "Epoch [497] valid done\n",
      "Epoch [497] T 1996.90s, deltaT 4.12s, loss: train 0.42764, valid 0.49943, dsc: train 0.57236, valid 0.50057\n",
      "Batch train [1] loss 0.40341, dsc 0.59659\n",
      "Batch train [2] loss 0.45798, dsc 0.54202\n",
      "Batch train [3] loss 0.42404, dsc 0.57596\n",
      "Batch train [4] loss 0.42941, dsc 0.57059\n",
      "Batch train [5] loss 0.39966, dsc 0.60034\n",
      "Batch train [6] loss 0.40614, dsc 0.59386\n",
      "Batch train [7] loss 0.44522, dsc 0.55478\n",
      "Batch train [8] loss 0.41892, dsc 0.58108\n",
      "Batch train [9] loss 0.47473, dsc 0.52527\n",
      "Batch train [10] loss 0.42230, dsc 0.57770\n",
      "Epoch [498] train done\n",
      "Batch eval [1] loss 0.48619, dsc 0.51381\n",
      "Batch eval [2] loss 0.51765, dsc 0.48235\n",
      "Batch eval [3] loss 0.48875, dsc 0.51125\n",
      "Batch eval [4] loss 0.53231, dsc 0.46769\n",
      "Batch eval [5] loss 0.49201, dsc 0.50799\n",
      "Epoch [498] valid done\n",
      "Epoch [498] T 2000.91s, deltaT 4.00s, loss: train 0.42818, valid 0.50338, dsc: train 0.57182, valid 0.49662\n",
      "Batch train [1] loss 0.43934, dsc 0.56066\n",
      "Batch train [2] loss 0.41742, dsc 0.58258\n",
      "Batch train [3] loss 0.47109, dsc 0.52891\n",
      "Batch train [4] loss 0.42157, dsc 0.57843\n",
      "Batch train [5] loss 0.45111, dsc 0.54889\n",
      "Batch train [6] loss 0.39266, dsc 0.60734\n",
      "Batch train [7] loss 0.41517, dsc 0.58483\n",
      "Batch train [8] loss 0.39961, dsc 0.60039\n",
      "Batch train [9] loss 0.44377, dsc 0.55623\n",
      "Batch train [10] loss 0.42196, dsc 0.57804\n",
      "Epoch [499] train done\n",
      "Batch eval [1] loss 0.48676, dsc 0.51324\n",
      "Batch eval [2] loss 0.51346, dsc 0.48654\n",
      "Batch eval [3] loss 0.47828, dsc 0.52172\n",
      "Batch eval [4] loss 0.52936, dsc 0.47064\n",
      "Batch eval [5] loss 0.49075, dsc 0.50925\n",
      "Epoch [499] valid done\n",
      "Epoch [499] T 2004.95s, deltaT 4.05s, loss: train 0.42737, valid 0.49972, dsc: train 0.57263, valid 0.50028\n",
      "Batch train [1] loss 0.43182, dsc 0.56818\n",
      "Batch train [2] loss 0.41791, dsc 0.58209\n",
      "Batch train [3] loss 0.43836, dsc 0.56164\n",
      "Batch train [4] loss 0.40076, dsc 0.59924\n",
      "Batch train [5] loss 0.43018, dsc 0.56982\n",
      "Batch train [6] loss 0.44353, dsc 0.55647\n",
      "Batch train [7] loss 0.43708, dsc 0.56292\n",
      "Batch train [8] loss 0.43773, dsc 0.56227\n",
      "Batch train [9] loss 0.41575, dsc 0.58425\n",
      "Batch train [10] loss 0.40542, dsc 0.59458\n",
      "Epoch [500] train done\n",
      "Batch eval [1] loss 0.48439, dsc 0.51561\n",
      "Batch eval [2] loss 0.51614, dsc 0.48386\n",
      "Batch eval [3] loss 0.47374, dsc 0.52626\n",
      "Batch eval [4] loss 0.52937, dsc 0.47063\n",
      "Batch eval [5] loss 0.49258, dsc 0.50742\n",
      "Epoch [500] valid done\n",
      "Epoch [500] T 2008.94s, deltaT 3.98s, loss: train 0.42585, valid 0.49925, dsc: train 0.57415, valid 0.50075\n",
      "Elapsed time 0:33:28\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from src.model_and_training.load_checkpoint_model_info import load_checkpoint_model_info\n",
    "\n",
    "TRAIN_LOW_MODEL=True\n",
    "\n",
    "if TRAIN_LOW_MODEL:\n",
    "    # preparing model loop params\n",
    "    low_res_model_info = prepare_model(epochs=500, \n",
    "                                       learning_rate=3e-4,\n",
    "                                       in_channels=4,\n",
    "                                       dropout_rate=0.2,\n",
    "                                       train_batch_size=4,\n",
    "                                       train_dataset=train_low_res_dataset, valid_dataset=valid_low_res_dataset, test_dataset=test_low_res_dataset)\n",
    "    show_model_info(low_res_model_info)\n",
    "\n",
    "    # running training loop\n",
    "    train_loop(low_res_model_info)\n",
    "    \n",
    "    # model export\n",
    "    low_res_model = low_res_model_info['model']\n",
    "else:\n",
    "    epoch = 110\n",
    "    log_date = datetime.datetime(year=2020, month=10, day=26, hour=16, minute=3, second=38).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_name = f'{log_date}_3d_unet'\n",
    "\n",
    "    loaded_model_info = load_checkpoint_model_info(model_name, epoch, train_low_res_dataset, valid_low_res_dataset, test_low_res_dataset)\n",
    "\n",
    "    # model export\n",
    "    low_res_model_info = loaded_model_info\n",
    "    low_res_model = loaded_model_info['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full resolution cutting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading high/full res dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA using 1x dataset\n",
      "filtering labels\n",
      "filtering labels done\n",
      "parsing dataset to numpy\n",
      "numpy parsing done\n",
      "data type: int16 int8\n",
      "data max 15.305964469909668, min -1.1480365991592407\n",
      "label max 22, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab81ce82f744279aad3df2b5448c281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9992bb9437e24115b8d32914e201ef5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset data and label shapes (1, 160, 32, 32) (1, 160, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "full_res_dataset = get_dataset(dataset_size=50, shrink_factor=1, filter_labels=filter_labels, unify_labels=False)\n",
    "full_res_dataset.to_numpy()\n",
    "full_res_dataset.show_data_type()\n",
    "preview_dataset(full_res_dataset, preview_index=0, show_hist=False)\n",
    "\n",
    "print('dataset data and label shapes', low_res_dataset.data_list[0].shape, full_res_dataset.data_list[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing low res network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting bounding box cut in full res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 40, valid_size 5, test 5, full 50\n",
      "train indices [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "valid indices [6, 13, 19, 25, 38]\n",
      "test indices [16, 26, 27, 29, 39]\n"
     ]
    }
   ],
   "source": [
    "full_res_split_dataset_obj = copy_split_dataset(full_res_dataset, low_res_split_dataset_obj)\n",
    "get_dataset_info(full_res_dataset, full_res_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### debuging cut algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved model to cpu\n",
      "CUDA Memory Usage\n",
      "GPU:        GeForce RTX 2070\n",
      "Allocated:  0.0 GB\n",
      "Cached:     0.0 GB\n",
      "Max memory: 0.4 GB\n",
      "Max Cached: 0.5 GB\n"
     ]
    }
   ],
   "source": [
    "# moving model to cpu and setting to eval mode, preventing model params changes/training\n",
    "low_res_model = low_res_model.to('cpu')\n",
    "#low_res_model.to(low_res_model_info['device'])\n",
    "low_res_model.eval()\n",
    "\n",
    "low_res_model_info['model'] = low_res_model\n",
    "torch.cuda.empty_cache()\n",
    "print('moved model to cpu')\n",
    "\n",
    "show_cuda_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug removing 35 outlier pixels from 1711\n",
      "debug box delta [15 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1648187 1648187\n",
      "debug bounding box sizes (57, 144, 160) (72, 192, 168)\n",
      "debug bounding boxes (58, 114, 176, 319, 176, 335) (51, 122, 152, 343, 172, 339)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e45a06abebb417fb19272fde5289ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=79, max=159),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168176c5683a4a7fb2146b1669ba0ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df66eea335ed448fa7791242b1cb4d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4a625d183f495aa73904d5c3be7c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_index = 32\n",
    "\n",
    "raw_low_data =  low_res_dataset.data_list[dataset_index]\n",
    "raw_low_label = low_res_dataset.label_list[dataset_index]\n",
    "raw_full_data = full_res_dataset.data_list[dataset_index]\n",
    "raw_full_label = full_res_dataset.label_list[dataset_index]\n",
    "\n",
    "tmp = get_full_res_cut(low_res_model=low_res_model, \n",
    "                       raw_low_res_data_img=raw_low_data,\n",
    "                       raw_low_res_label_img=raw_low_label,\n",
    "                       raw_full_res_data_img=raw_full_data, \n",
    "                       raw_full_res_label_img=raw_full_label,\n",
    "                       low_res_mask_threshold=0.5,\n",
    "                       desire_bounding_box_size=DESIRE_BOUNDING_BOX_SIZE, \n",
    "                       show_debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Running cut algorithm, creating cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting cut index 0\n",
      "debug removing 9 outlier pixels from 1333\n",
      "debug box delta [20 48 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1223526 1223526\n",
      "getting cut index 1\n",
      "debug removing 0 outlier pixels from 1332\n",
      "debug box delta [24 16 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1326052 1326052\n",
      "getting cut index 2\n",
      "debug removing 0 outlier pixels from 1784\n",
      "debug box delta [ 19   0 -24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1890464 1890464\n",
      "getting cut index 3\n",
      "debug removing 0 outlier pixels from 1489\n",
      "debug box delta [17 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1560217 1560217\n",
      "getting cut index 4\n",
      "debug removing 8 outlier pixels from 1440\n",
      "debug box delta [20 48 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1451227 1451227\n",
      "getting cut index 5\n",
      "debug removing 0 outlier pixels from 1325\n",
      "debug box delta [22 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1262651 1262651\n",
      "getting cut index 6\n",
      "debug removing 0 outlier pixels from 1298\n",
      "debug box delta [19 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1566938 1566938\n",
      "getting cut index 7\n",
      "debug removing 0 outlier pixels from 916\n",
      "debug box delta [30 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 869847 869847\n",
      "getting cut index 8\n",
      "debug removing 0 outlier pixels from 1404\n",
      "debug box delta [20 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1397249 1397249\n",
      "getting cut index 9\n",
      "debug removing 28 outlier pixels from 1405\n",
      "debug box delta [20 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1350330 1350330\n",
      "getting cut index 10\n",
      "debug removing 0 outlier pixels from 1572\n",
      "debug box delta [20  0 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1635868 1635868\n",
      "getting cut index 11\n",
      "debug removing 13 outlier pixels from 1324\n",
      "debug box delta [23 48 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1283062 1283062\n",
      "getting cut index 12\n",
      "debug removing 0 outlier pixels from 1455\n",
      "debug box delta [22 32 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1518406 1518406\n",
      "getting cut index 13\n",
      "debug removing 0 outlier pixels from 1320\n",
      "debug box delta [23 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1504194 1504194\n",
      "getting cut index 14\n",
      "debug removing 0 outlier pixels from 1177\n",
      "debug box delta [25 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1084254 1084254\n",
      "getting cut index 15\n",
      "debug removing 0 outlier pixels from 1244\n",
      "debug box delta [20 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1221257 1221257\n",
      "getting cut index 16\n",
      "debug removing 0 outlier pixels from 978\n",
      "debug box delta [21 64 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 945639 945639\n",
      "getting cut index 17\n",
      "debug removing 0 outlier pixels from 1458\n",
      "debug box delta [23 16 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1469035 1469035\n",
      "getting cut index 18\n",
      "debug removing 7 outlier pixels from 1342\n",
      "debug box delta [23 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1322571 1322571\n",
      "getting cut index 19\n",
      "debug removing 0 outlier pixels from 1385\n",
      "debug box delta [17 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1593516 1593516\n",
      "getting cut index 20\n",
      "debug removing 0 outlier pixels from 1332\n",
      "debug box delta [25 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1390348 1390348\n",
      "getting cut index 21\n",
      "debug removing 0 outlier pixels from 1480\n",
      "debug box delta [ 22  16 -24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1463017 1463017\n",
      "getting cut index 22\n",
      "debug removing 0 outlier pixels from 1185\n",
      "debug box delta [26 32 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1162215 1162215\n",
      "getting cut index 23\n",
      "debug removing 0 outlier pixels from 1095\n",
      "debug box delta [23 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1029805 1029805\n",
      "getting cut index 24\n",
      "debug removing 0 outlier pixels from 1627\n",
      "debug box delta [ 29 -16   8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1689537 1689537\n",
      "getting cut index 25\n",
      "debug removing 2 outlier pixels from 1223\n",
      "debug box delta [22 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1141739 1141739\n",
      "getting cut index 26\n",
      "debug removing 5 outlier pixels from 1315\n",
      "debug box delta [19 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1167835 1167835\n",
      "getting cut index 27\n",
      "debug removing 2 outlier pixels from 1553\n",
      "debug box delta [22 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1783264 1783264\n",
      "getting cut index 28\n",
      "debug removing 27 outlier pixels from 1880\n",
      "debug box delta [ 22  16 -24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1944758 1944758\n",
      "getting cut index 29\n",
      "debug removing 19 outlier pixels from 1309\n",
      "debug box delta [19 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1583396 1583396\n",
      "getting cut index 30\n",
      "debug removing 11 outlier pixels from 1321\n",
      "debug box delta [18 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1248609 1248609\n",
      "getting cut index 31\n",
      "debug removing 0 outlier pixels from 1073\n",
      "debug box delta [25 48 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 947124 947124\n",
      "getting cut index 32\n",
      "debug removing 35 outlier pixels from 1711\n",
      "debug box delta [15 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1648187 1648187\n",
      "getting cut index 33\n",
      "debug removing 0 outlier pixels from 1256\n",
      "debug box delta [20 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1214697 1214697\n",
      "getting cut index 34\n",
      "debug removing 0 outlier pixels from 1449\n",
      "debug box delta [24 16 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1445951 1445951\n",
      "getting cut index 35\n",
      "debug removing 50 outlier pixels from 1881\n",
      "debug box delta [ 14  48 -24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1963068 1963068\n",
      "getting cut index 36\n",
      "debug removing 0 outlier pixels from 1312\n",
      "debug box delta [26 16  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1241941 1241941\n",
      "getting cut index 37\n",
      "debug removing 0 outlier pixels from 1376\n",
      "debug box delta [19 32 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1298886 1298886\n",
      "getting cut index 38\n",
      "debug removing 0 outlier pixels from 1378\n",
      "debug box delta [21 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1731533 1731533\n",
      "getting cut index 39\n",
      "debug removing 11 outlier pixels from 1096\n",
      "debug box delta [26 48  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1067335 1067335\n",
      "getting cut index 40\n",
      "debug removing 0 outlier pixels from 1257\n",
      "debug box delta [26 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1311715 1311715\n",
      "getting cut index 41\n",
      "debug removing 0 outlier pixels from 1509\n",
      "debug box delta [18 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1411792 1411792\n",
      "getting cut index 42\n",
      "debug removing 11 outlier pixels from 1059\n",
      "debug box delta [27 48 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 951804 951804\n",
      "getting cut index 43\n",
      "debug removing 0 outlier pixels from 1129\n",
      "debug box delta [31 48 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1024831 1024831\n",
      "getting cut index 44\n",
      "debug removing 0 outlier pixels from 1655\n",
      "debug box delta [13 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1763923 1763923\n",
      "getting cut index 45\n",
      "debug removing 0 outlier pixels from 1137\n",
      "debug box delta [27 32 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1115633 1115633\n",
      "getting cut index 46\n",
      "debug removing 0 outlier pixels from 1580\n",
      "debug box delta [21 32 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1670156 1670156\n",
      "getting cut index 47\n",
      "debug removing 0 outlier pixels from 1389\n",
      "debug box delta [20 32  8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1413179 1413179\n",
      "getting cut index 48\n",
      "debug removing 0 outlier pixels from 948\n",
      "debug box delta [27 64 24]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 781269 781269\n",
      "getting cut index 49\n",
      "debug removing 0 outlier pixels from 1699\n",
      "debug box delta [19 16 -8]\n",
      "debug, Does cut and original label contain the same amount of pixels? True 1756965 1756965\n"
     ]
    }
   ],
   "source": [
    "cut_full_res_dataset = full_res_dataset.copy(copy_lists=False)\n",
    "cut_full_res_dataset = get_cut_lists(low_res_model, low_res_dataset, full_res_dataset, cut_full_res_dataset, low_res_mask_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 192, 168) (72, 192, 168)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cut_full_res_dataset.set_output_label(None)\n",
    "debug_data, debug_label = cut_full_res_dataset[48]\n",
    "debug_data = debug_data[0] # channel dimension\n",
    "slice_index = 30\n",
    "print(debug_data.shape, debug_label.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "plt.subplot(1, 2, 1).set_title('data')\n",
    "plt.imshow(debug_data[slice_index], cmap=\"gray\")\n",
    "\n",
    "plt.subplot(1, 2, 2).set_title('label')\n",
    "plt.imshow(debug_label[slice_index], cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### reviewing full res and cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type: int16 int8\n",
      "\n",
      "full res shape (1, 160, 512, 512) (1, 160, 512, 512)\n",
      "cut full res shape (1, 72, 192, 168) (1, 72, 192, 168)\n",
      "\n",
      "dataset RAM sizes in GB 5.859375 0.32444000244140625\n",
      "single item RAM in GB 0.0390625 0.078125\n",
      "\n",
      "data max 3071, min -1024\n",
      "label max 22, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a764301de6143b2bcfd0d20799995ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0a7c74ce924738bfcff0769895cbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cut_full_res_dataset.show_data_type()\n",
    "print()\n",
    "print('full res shape', full_res_dataset[0][0].shape, full_res_dataset[1][0].shape)\n",
    "print('cut full res shape', cut_full_res_dataset[0][0].shape, cut_full_res_dataset[1][0].shape)\n",
    "print()\n",
    "print('dataset RAM sizes in GB', full_res_dataset.get_data_size() / 1024**3, cut_full_res_dataset.get_data_size() / 1024**3)\n",
    "print('single item RAM in GB', full_res_dataset.label_list[0].nbytes / 1024**3, full_res_dataset.data_list[0].nbytes / 1024**3)\n",
    "print()\n",
    "preview_dataset(cut_full_res_dataset, max_slices=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Full resolution cut model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cut_full_res_dataset.set_output_label([OARS_LABELS.EYE_L, OARS_LABELS.EYE_R, OARS_LABELS.LENS_L, OARS_LABELS.LENS_R])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cut_full_res_dataset.__getitem__(0)[1][47])\n",
    "plt.show()\n",
    "\n",
    "cut_split_dataset_obj = copy_split_dataset(cut_full_res_dataset, low_res_split_dataset_obj)\n",
    "get_dataset_info(cut_full_res_dataset, cut_split_dataset_obj)\n",
    "\n",
    "cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(\n",
    "    'train_dataset', 'valid_dataset', 'test_dataset')(cut_split_dataset_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Cut Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cut_model_info = prepare_model(epochs=50,\n",
    "                               learning_rate=5e-4,\n",
    "                               in_channels=8,\n",
    "                               dropout_rate=0.2,\n",
    "                               train_batch_size=2,\n",
    "                               train_dataset=cut_train_dataset, valid_dataset=cut_valid_dataset, test_dataset=cut_test_dataset)\n",
    "show_model_info(cut_model_info)\n",
    "\n",
    "train_loop(cut_train_loop_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cuda_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "160x128x128 = 2_621_440 \\\n",
    "72x198x168 = 2_395_008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_slices = cut_train_dataset[0][1].shape[0]\n",
    "\n",
    "display(Markdown(\"### Train Eval\"))\n",
    "show_model_dataset_pred_preview(cut_model_info, cut_train_dataset, max_slices=max_slices, default_slice=49)\n",
    "\n",
    "# display(Markdown(\"### Valid Eval\"))\n",
    "# show_model_dataset_pred_preview(cut_model_info, cut_valid_dataset, max_slices=max_slices, default_slice=53)\n",
    "\n",
    "# display(Markdown(\"### Test Eval\"))\n",
    "# eval_image_dataset(test_dataset, 78, 'test_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing label prediction comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, rescaled_preds = get_rescaled_preds(cut_model_info[\"model\"], cut_full_res_dataset, cut_model_info[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_train_idx = low_res_split_dataset_obj['train_dataset'].indices[1]\n",
    "rnd_valid_idx = low_res_split_dataset_obj['valid_dataset'].indices[1]\n",
    "\n",
    "print(f'Train index {rnd_train_idx}')\n",
    "compare_prediction_with_ground_true(cut_full_res_dataset, rescaled_preds, pred_threshold=0.5, dataset_index=rnd_train_idx, default_slice=44)\n",
    "print(f'Valid index {rnd_valid_idx}')\n",
    "compare_prediction_with_ground_true(cut_full_res_dataset, rescaled_preds, pred_threshold=0.5, dataset_index=rnd_valid_idx, default_slice=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
