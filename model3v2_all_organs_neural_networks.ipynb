{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n",
      "Dataset biggest bounding box wihtout spinal cord [56, 177, 156]\n",
      "Cut target size [72, 192, 168]\n",
      "Done Init\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/dp_tomastik/code')\n",
    "    !bash \"/content/drive/My Drive/dp_tomastik/code/scripts/install_libs.sh\"\n",
    "    \n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "from torchio import RandomAffine, Compose, ZNormalization\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from src.helpers import preview_3d_image\n",
    "from src.helpers import show_cuda_usage, preview_model_dataset_pred, preview_dataset\n",
    "from src.helpers import get_threshold_info_df, get_rescaled_preds\n",
    "from src.helpers import compare_prediction_with_ground_true, compare_one_prediction_with_ground_true\n",
    "from src.helpers import get_img_outliers_pixels, get_raw_with_prediction\n",
    "from src.helpers import get_rescaled_pred\n",
    "from src.helpers import get_transformed_label_np, create_regis_trans_list, trans_list\n",
    "\n",
    "from src.dataset import HaNOarsDataset, transform_input_with_registration, get_norm_transform\n",
    "from src.dataset import get_full_res_cut, get_cut_lists, OARS_LABELS, get_dataset, get_dataset_info, get_dataset_transform\n",
    "from src.dataset import split_dataset, copy_split_dataset\n",
    "\n",
    "from src.model_and_training import prepare_model, train_loop, show_model_info, load_checkpoint_model_info\n",
    "from src.model_and_training import iterate_model_v3v2\n",
    "from src.model_and_training.getters.get_device import get_device\n",
    "from src.model_and_training.architectures.unet_architecture_v3v2 import UNetV3v2\n",
    "\n",
    "from src.consts import DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "  \n",
    "torch.manual_seed(20)\n",
    "logging.basicConfig(filename='logs/model3v2_all_organs_jupyter.log', level=logging.DEBUG)\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading precourse neural network with datasets:\n",
    "- loading fullres dataset (512x512)\n",
    "- loading lowres dataset (32x32)\n",
    "- loading precourse model\n",
    "- parsing dataset to create cut dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cut dataset\n"
     ]
    }
   ],
   "source": [
    "CALCULATE_DATASET = False\n",
    "data_path = './data/HaN_OAR_cut_72_192_168'\n",
    "\n",
    "if CALCULATE_DATASET:\n",
    "    datasets_params = ['train_dataset', 'valid_dataset', 'test_dataset']\n",
    "    filter_labels = OARS_LABELS.OARS_LABELS_LIST\n",
    "    if OARS_LABELS.SPINAL_CORD in filter_labels:\n",
    "        filter_labels.remove(OARS_LABELS.SPINAL_CORD)\n",
    "\n",
    "    # low res\n",
    "    low_res_dataset = get_dataset(dataset_size=50, shrink_factor=16, filter_labels=filter_labels, unify_labels=True)\n",
    "    low_res_dataset.dilatate_labels(repeat=1)\n",
    "    low_res_dataset.to_numpy()\n",
    "    low_res_split_dataset_obj = split_dataset(low_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "    train_low_res_dataset, valid_low_res_dataset, test_low_res_dataset = itemgetter(*datasets_params)(low_res_split_dataset_obj)\n",
    "\n",
    "    # full res\n",
    "    full_res_dataset = get_dataset(dataset_size=50, shrink_factor=1, filter_labels=filter_labels, unify_labels=False)\n",
    "    full_res_dataset.to_numpy()\n",
    "    full_res_split_dataset_obj = copy_split_dataset(full_res_dataset, low_res_split_dataset_obj)\n",
    "\n",
    "    # low res model - precourse model\n",
    "    epoch = 500\n",
    "    log_date = datetime.datetime(year=2020, month=10, day=27, hour=11, minute=45, second=30).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_name = f'{log_date}_3d_unet_PRECOURSE'\n",
    "\n",
    "    low_res_model_info = load_checkpoint_model_info(model_name, epoch, train_low_res_dataset, valid_low_res_dataset, test_low_res_dataset)\n",
    "    show_model_info(low_res_model_info)\n",
    "\n",
    "    # moving low res to gpu\n",
    "    low_res_model_info['device'] = get_device()\n",
    "    # low_res_model_info['device'] = 'cpu'\n",
    "    low_res_model_info['model'] = low_res_model_info['model'].to(low_res_model_info['device'])\n",
    "    low_res_model_info['model'].eval()\n",
    "\n",
    "    # cut res\n",
    "    cut_full_res_dataset = full_res_dataset.copy(copy_lists=False)\n",
    "    cut_full_res_dataset = get_cut_lists(low_res_model_info['model'],\n",
    "                                         low_res_model_info['device'],\n",
    "                                         low_res_dataset, \n",
    "                                         full_res_dataset, \n",
    "                                         cut_full_res_dataset, \n",
    "                                         low_res_mask_threshold=0.5)\n",
    "    cut_full_res_dataset.set_output_label(None)\n",
    "    cut_split_dataset_obj = copy_split_dataset(cut_full_res_dataset, low_res_split_dataset_obj)\n",
    "    cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*datasets_params)(cut_split_dataset_obj)\n",
    "\n",
    "    # moving low res model to cpu\n",
    "    low_res_model_info['device'] = 'cpu'\n",
    "    low_res_model_info['model'] = low_res_model_info['model'].to(low_res_model_info['device'])\n",
    "    \n",
    "    # saving parsed data\n",
    "    cut_full_res_dataset.save_to_file(data_path)\n",
    "else:\n",
    "    print('loading cut dataset')\n",
    "    cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "    cut_full_res_dataset.load_from_file(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 72, 192, 168), (1, 72, 192, 168))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_full_res_dataset.data_list[0].shape, cut_full_res_dataset.label_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low_res_dataset not loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    get_dataset_info(low_res_dataset, low_res_split_dataset_obj)\n",
    "except NameError:\n",
    "    print('low_res_dataset not loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from src.dataset.dataset_transforms import get_dataset_transform\n",
    "from src.dataset.transform_input import transform_input\n",
    "\n",
    "\n",
    "def preview_dataset(dataset, preview_index=0, show_hist=False, use_transform=False):\n",
    "    data, label = dataset.get_raw_item_with_label_filter(preview_index)  # equivalent dataset[preview_index]\n",
    "    if use_transform:\n",
    "        transform = get_dataset_transform()\n",
    "        data, label = transform_input(data, label, transform)\n",
    "\n",
    "    max_channels = label.shape[0]\n",
    "    max_slices = label.shape[1]\n",
    "\n",
    "    print(f'data max {data.max()}, min {data.min()}')\n",
    "    print(f'label max {label.max()}, min {label.min()}')\n",
    "\n",
    "    def f(slice_index, label_channel):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(data[0, slice_index], cmap=\"gray\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(label[label_channel, slice_index])\n",
    "        plt.show()\n",
    "\n",
    "        if show_hist:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(data.flatten(), 128)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(label.flatten(), 128)\n",
    "            plt.show()\n",
    "\n",
    "    sliceSlider = widgets.IntSlider(min=0, max=max_slices - 1, step=1, value=(max_slices - 1) / 2)\n",
    "    labelChannelSlider = widgets.IntSlider(min=0, max=max_channels - 1, step=1, value=(max_channels - 1) / 2)\n",
    "    ui = widgets.VBox([widgets.HBox([sliceSlider, labelChannelSlider])])\n",
    "    out = widgets.interactive_output(f, {'slice_index': sliceSlider, 'label_channel': labelChannelSlider})\n",
    "    # noinspection PyTypeChecker\n",
    "    display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data max 3071, min -1024\n",
      "label max 22, min 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d69952a61740f3a175b8f37172e8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71), IntSlider(value=0, max=0))),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582baf38f93f45028bcbeac7456e6677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_dataset(cut_full_res_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding registration to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas shape (136, 120, 219) (136, 120, 219) (136, 120, 219) (136, 120, 219)\n"
     ]
    }
   ],
   "source": [
    "atlas_ri_sitk = sitk.ReadImage('./data/PDDCA-1.2-atlas/probabilistic_atlas/RI.mhd')\n",
    "atlas_brainstem_map_sitk = sitk.ReadImage('./data/PDDCA-1.2-atlas/probabilistic_atlas/brain_stem_map.mhd')\n",
    "atlas_left_parotid_map_sitk = sitk.ReadImage('./data/PDDCA-1.2-atlas/probabilistic_atlas/left_parotid_map.mhd')\n",
    "atlas_right_parotid_map_sitk = sitk.ReadImage('./data/PDDCA-1.2-atlas/probabilistic_atlas/right_parotid_map.mhd')\n",
    "\n",
    "atlas_ri = sitk.GetArrayFromImage(atlas_ri_sitk)\n",
    "atlas_brainstem_map = sitk.GetArrayFromImage(atlas_brainstem_map_sitk)\n",
    "atlas_left_parotid_map = sitk.GetArrayFromImage(atlas_left_parotid_map_sitk)\n",
    "atlas_right_parotid_map = sitk.GetArrayFromImage(atlas_right_parotid_map_sitk)\n",
    "\n",
    "print('Atlas shape', atlas_ri.shape, atlas_brainstem_map.shape, atlas_left_parotid_map.shape, atlas_right_parotid_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas input (136, 120, 219) (136, 120, 219)\n"
     ]
    }
   ],
   "source": [
    "# resampling atlas to higher spacing in slices\n",
    "atlas_resampler = sitk.ResampleImageFilter()\n",
    "atlas_resampler.SetReferenceImage(atlas_ri_sitk)\n",
    "atlas_resampler.SetInterpolator(sitk.sitkLinear)\n",
    "new_spacing = atlas_ri_sitk.GetSpacing()\n",
    "new_spacing = (new_spacing[0], new_spacing[1], 5.75)\n",
    "atlas_resampler.SetOutputSpacing(new_spacing)\n",
    "\n",
    "atlas_input_data_sitk = atlas_resampler.Execute(atlas_ri_sitk)\n",
    "atlas_input_label_sitk = atlas_resampler.Execute(atlas_right_parotid_map_sitk)\n",
    "\n",
    "# parsing to numpy\n",
    "atlas_input_data_np = sitk.GetArrayFromImage(atlas_input_data_sitk)\n",
    "atlas_input_label_np = sitk.GetArrayFromImage(atlas_input_label_sitk)\n",
    "atlas_input_np = (atlas_input_data_np, atlas_input_label_np)\n",
    "\n",
    "print('Atlas input', atlas_input_data_np.shape, atlas_input_label_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "showing registration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9765625, 0.9765625, 3.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_full_res_dataset.spacing_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas and dataset spacing (2.389269, 2.5155, 3.216912) (1.1796875, 1.1796875, 3.0)\n"
     ]
    }
   ],
   "source": [
    "print('Atlas and dataset spacing', atlas_ri_sitk.GetSpacing(), cut_full_res_dataset.spacing_list[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db27cf41d8ba4483b5010bb6a1a9175c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=67, max=135),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c5929555f44f3e98450898e0636732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2581d015b85a4b4c8e46ee6275c3e52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=67, max=135),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ad776b6a8e4b05a0f99c6a16106a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdd57089e5647a9aa32b0ff330188fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=67, max=135),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9217997786274a9bbbac9d969b4bedd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769e9924b7e343f5a8d49d7092106761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=67, max=135),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc72e02cd6984d2a80e46929fa8bb493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_3d_image(atlas_ri, figsize=(5,5))\n",
    "preview_3d_image(atlas_brainstem_map, figsize=(5,5))\n",
    "preview_3d_image(atlas_left_parotid_map, figsize=(5,5))\n",
    "preview_3d_image(atlas_right_parotid_map, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 72, 192, 168), (1, 72, 192, 168))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_full_res_dataset[0][0].shape, cut_full_res_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef65f2153b94c87a4921c256a5575ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb08cfcad774ad4b7ca4f0512699d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_3d_image(dataset_input[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 211 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 212\n",
      "  Metric value: -0.7815303303309206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb4c15bf51b42ab83f254834ddb7561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117dc188251b438fb561f5f2832333c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_input = cut_full_res_dataset.get_raw_item_with_label_filter(0)\n",
    "tmp = get_transformed_label_np(dataset_input, atlas_input_np, numberOfIterations=2500, show=False, preview=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95828b53bea54f13b186dbf3da1aaf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=67, max=135),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15373e0a8d224df5824758e5b95e0039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_3d_image(atlas_brainstem_map_sitk + atlas_left_parotid_map_sitk + atlas_right_parotid_map_sitk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "registering whole dataset to probabilistic atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 211 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 212\n",
      "  Metric value: -0.7815303303309206\n",
      "Registration done for index: 0\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 281 iterations. Gradient magnitude (9.97276e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 282\n",
      "  Metric value: -0.6014645117096772\n",
      "Registration done for index: 1\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 2058 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 2059\n",
      "  Metric value: -0.6610746228301189\n",
      "Registration done for index: 2\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 161 iterations. Gradient magnitude (9.94478e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 162\n",
      "  Metric value: -0.2669261666228794\n",
      "Registration done for index: 3\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 201 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 202\n",
      "  Metric value: -0.7210860775625451\n",
      "Registration done for index: 4\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 166 iterations. Gradient magnitude (1.16838e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 167\n",
      "  Metric value: -0.7874581873085426\n",
      "Registration done for index: 5\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 266 iterations. Gradient magnitude (9.82137e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 267\n",
      "  Metric value: -0.7601182356035199\n",
      "Registration done for index: 6\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 334 iterations. Gradient magnitude (9.96977e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 335\n",
      "  Metric value: -0.6915655183830122\n",
      "Registration done for index: 7\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 188 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 189\n",
      "  Metric value: -0.7537508130154776\n",
      "Registration done for index: 8\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 456 iterations. Gradient magnitude (9.91057e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 457\n",
      "  Metric value: -0.5872656665292584\n",
      "Registration done for index: 9\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 820 iterations. Gradient magnitude (9.8661e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 821\n",
      "  Metric value: -0.18697826567421777\n",
      "Registration done for index: 10\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 307 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 308\n",
      "  Metric value: -0.7656709451120806\n",
      "Registration done for index: 11\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 313 iterations. Gradient magnitude (9.74811e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 314\n",
      "  Metric value: -0.7025748464307477\n",
      "Registration done for index: 12\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 214 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 215\n",
      "  Metric value: -0.7710484688374576\n",
      "Registration done for index: 13\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 333 iterations. Gradient magnitude (9.91985e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 334\n",
      "  Metric value: -0.7411645841937088\n",
      "Registration done for index: 14\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 297 iterations. Gradient magnitude (9.95224e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 298\n",
      "  Metric value: -0.7366814068484687\n",
      "Registration done for index: 15\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 314 iterations. Gradient magnitude (9.86077e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 315\n",
      "  Metric value: -0.6666355988847281\n",
      "Registration done for index: 16\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 347 iterations. Gradient magnitude (6.9013e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 348\n",
      "  Metric value: -0.2789751934746737\n",
      "Registration done for index: 17\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 242 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 243\n",
      "  Metric value: -0.7609893641614983\n",
      "Registration done for index: 18\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 219 iterations. Gradient magnitude (9.85687e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 220\n",
      "  Metric value: -0.26646131030294234\n",
      "Registration done for index: 19\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 223 iterations. Gradient magnitude (9.63014e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 224\n",
      "  Metric value: -0.6997638477257715\n",
      "Registration done for index: 20\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 90 iterations. Gradient magnitude (9.92311e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 91\n",
      "  Metric value: -0.288535259428533\n",
      "Registration done for index: 21\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 265 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 266\n",
      "  Metric value: -0.7584059698798865\n",
      "Registration done for index: 22\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 507 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 508\n",
      "  Metric value: -0.693598764907098\n",
      "Registration done for index: 23\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 325 iterations. Gradient magnitude (9.87395e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 326\n",
      "  Metric value: -0.5803733763077836\n",
      "Registration done for index: 24\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 451 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 452\n",
      "  Metric value: -0.6867803205204814\n",
      "Registration done for index: 25\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 183 iterations. Gradient magnitude (9.98997e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 184\n",
      "  Metric value: -0.739038362748137\n",
      "Registration done for index: 26\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 267 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 268\n",
      "  Metric value: -0.5951104410526568\n",
      "Registration done for index: 27\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 446 iterations. Gradient magnitude (7.87904e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 447\n",
      "  Metric value: -0.5876499587658994\n",
      "Registration done for index: 28\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 243 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 244\n",
      "  Metric value: -0.7614268487816905\n",
      "Registration done for index: 29\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 349 iterations. Gradient magnitude (9.83306e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 350\n",
      "  Metric value: -0.7409905212436272\n",
      "Registration done for index: 30\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 339 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 340\n",
      "  Metric value: -0.7485782334580384\n",
      "Registration done for index: 31\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 168 iterations. Gradient magnitude (9.75841e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 169\n",
      "  Metric value: -0.6136457594715674\n",
      "Registration done for index: 32\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 310 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 311\n",
      "  Metric value: -0.7396554393584631\n",
      "Registration done for index: 33\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 385 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 386\n",
      "  Metric value: -0.6803058173542698\n",
      "Registration done for index: 34\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 86 iterations. Gradient magnitude (9.98016e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 87\n",
      "  Metric value: -0.27999586446098823\n",
      "Registration done for index: 35\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 1373 iterations. Gradient magnitude (8.88365e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 1374\n",
      "  Metric value: -0.2816935960001366\n",
      "Registration done for index: 36\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 255 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 256\n",
      "  Metric value: -0.737815899034085\n",
      "Registration done for index: 37\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 204 iterations. Gradient magnitude (7.54508e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 205\n",
      "  Metric value: -0.7339974646886995\n",
      "Registration done for index: 38\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 80 iterations. Gradient magnitude (2.1739e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 81\n",
      "  Metric value: -0.6083486610302146\n",
      "Registration done for index: 39\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 90 iterations. Gradient magnitude (8.83168e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 91\n",
      "  Metric value: -0.6791317945479346\n",
      "Registration done for index: 40\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 124 iterations. Gradient magnitude (6.24126e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 125\n",
      "  Metric value: -0.6546156801625964\n",
      "Registration done for index: 41\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 252 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 253\n",
      "  Metric value: -0.8188679788616697\n",
      "Registration done for index: 42\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 380 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 381\n",
      "  Metric value: -0.7452248177988424\n",
      "Registration done for index: 43\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 143 iterations. Gradient magnitude (3.22072e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 144\n",
      "  Metric value: -0.7793379456067503\n",
      "Registration done for index: 44\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 246 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 247\n",
      "  Metric value: -0.7316242697036124\n",
      "Registration done for index: 45\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 221 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 222\n",
      "  Metric value: -0.7076706529878807\n",
      "Registration done for index: 46\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Step too small after 292 iterations. Current step (6.10352e-05) is less than minimum step (0.0001).\n",
      "  Iteration: 293\n",
      "  Metric value: -0.7614264326260196\n",
      "Registration done for index: 47\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Gradient magnitude tolerance met after 376 iterations. Gradient magnitude (9.86908e-07) is less than gradient magnitude tolerance (1e-06).\n",
      "  Iteration: 377\n",
      "  Metric value: -0.6885390410041868\n",
      "Registration done for index: 48\n",
      "Optimizer stop condition: RegularStepGradientDescentOptimizerv4: Maximum number of iterations (3000) exceeded.\n",
      "  Iteration: 3000\n",
      "  Metric value: -0.5572952166482759\n",
      "Registration done for index: 49\n"
     ]
    }
   ],
   "source": [
    "CALC_REGISTRATION = False\n",
    "if CALC_REGISTRATION:\n",
    "    regis_trans_list = create_regis_trans_list(cut_full_res_dataset, atlas_input_data_np, numberOfIterations=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating brainstem\n",
      "Atlas input (136, 120, 219) (136, 120, 219)\n",
      "Transform done for index: 0\n",
      "Transform done for index: 1\n",
      "Transform done for index: 2\n",
      "Transform done for index: 3\n",
      "Transform done for index: 4\n",
      "Transform done for index: 5\n",
      "Transform done for index: 6\n",
      "Transform done for index: 7\n",
      "Transform done for index: 8\n",
      "Transform done for index: 9\n",
      "Transform done for index: 10\n",
      "Transform done for index: 11\n",
      "Transform done for index: 12\n",
      "Transform done for index: 13\n",
      "Transform done for index: 14\n",
      "Transform done for index: 15\n",
      "Transform done for index: 16\n",
      "Transform done for index: 17\n",
      "Transform done for index: 18\n",
      "Transform done for index: 19\n",
      "Transform done for index: 20\n",
      "Transform done for index: 21\n",
      "Transform done for index: 22\n",
      "Transform done for index: 23\n",
      "Transform done for index: 24\n",
      "Transform done for index: 25\n",
      "Transform done for index: 26\n",
      "Transform done for index: 27\n",
      "Transform done for index: 28\n",
      "Transform done for index: 29\n",
      "Transform done for index: 30\n",
      "Transform done for index: 31\n",
      "Transform done for index: 32\n",
      "Transform done for index: 33\n",
      "Transform done for index: 34\n",
      "Transform done for index: 35\n",
      "Transform done for index: 36\n",
      "Transform done for index: 37\n",
      "Transform done for index: 38\n",
      "Transform done for index: 39\n",
      "Transform done for index: 40\n",
      "Transform done for index: 41\n",
      "Transform done for index: 42\n",
      "Transform done for index: 43\n",
      "Transform done for index: 44\n",
      "Transform done for index: 45\n",
      "Transform done for index: 46\n",
      "Transform done for index: 47\n",
      "Transform done for index: 48\n",
      "Transform done for index: 49\n",
      "cut_full_res_dataset.is_numpy, None\n",
      "Calculating left_parotid\n",
      "Atlas input (136, 120, 219) (136, 120, 219)\n",
      "Transform done for index: 0\n",
      "Transform done for index: 1\n",
      "Transform done for index: 2\n",
      "Transform done for index: 3\n",
      "Transform done for index: 4\n",
      "Transform done for index: 5\n",
      "Transform done for index: 6\n",
      "Transform done for index: 7\n",
      "Transform done for index: 8\n",
      "Transform done for index: 9\n",
      "Transform done for index: 10\n",
      "Transform done for index: 11\n",
      "Transform done for index: 12\n",
      "Transform done for index: 13\n",
      "Transform done for index: 14\n",
      "Transform done for index: 15\n",
      "Transform done for index: 16\n",
      "Transform done for index: 17\n",
      "Transform done for index: 18\n",
      "Transform done for index: 19\n",
      "Transform done for index: 20\n",
      "Transform done for index: 21\n",
      "Transform done for index: 22\n",
      "Transform done for index: 23\n",
      "Transform done for index: 24\n",
      "Transform done for index: 25\n",
      "Transform done for index: 26\n",
      "Transform done for index: 27\n",
      "Transform done for index: 28\n",
      "Transform done for index: 29\n",
      "Transform done for index: 30\n",
      "Transform done for index: 31\n",
      "Transform done for index: 32\n",
      "Transform done for index: 33\n",
      "Transform done for index: 34\n",
      "Transform done for index: 35\n",
      "Transform done for index: 36\n",
      "Transform done for index: 37\n",
      "Transform done for index: 38\n",
      "Transform done for index: 39\n",
      "Transform done for index: 40\n",
      "Transform done for index: 41\n",
      "Transform done for index: 42\n",
      "Transform done for index: 43\n",
      "Transform done for index: 44\n",
      "Transform done for index: 45\n",
      "Transform done for index: 46\n",
      "Transform done for index: 47\n",
      "Transform done for index: 48\n",
      "Transform done for index: 49\n",
      "cut_full_res_dataset.is_numpy, None\n",
      "Calculating right_parotid\n",
      "Atlas input (136, 120, 219) (136, 120, 219)\n",
      "Transform done for index: 0\n",
      "Transform done for index: 1\n",
      "Transform done for index: 2\n",
      "Transform done for index: 3\n",
      "Transform done for index: 4\n",
      "Transform done for index: 5\n",
      "Transform done for index: 6\n",
      "Transform done for index: 7\n",
      "Transform done for index: 8\n",
      "Transform done for index: 9\n",
      "Transform done for index: 10\n",
      "Transform done for index: 11\n",
      "Transform done for index: 12\n",
      "Transform done for index: 13\n",
      "Transform done for index: 14\n",
      "Transform done for index: 15\n",
      "Transform done for index: 16\n",
      "Transform done for index: 17\n",
      "Transform done for index: 18\n",
      "Transform done for index: 19\n",
      "Transform done for index: 20\n",
      "Transform done for index: 21\n",
      "Transform done for index: 22\n",
      "Transform done for index: 23\n",
      "Transform done for index: 24\n",
      "Transform done for index: 25\n",
      "Transform done for index: 26\n",
      "Transform done for index: 27\n",
      "Transform done for index: 28\n",
      "Transform done for index: 29\n",
      "Transform done for index: 30\n",
      "Transform done for index: 31\n",
      "Transform done for index: 32\n",
      "Transform done for index: 33\n",
      "Transform done for index: 34\n",
      "Transform done for index: 35\n",
      "Transform done for index: 36\n",
      "Transform done for index: 37\n",
      "Transform done for index: 38\n",
      "Transform done for index: 39\n",
      "Transform done for index: 40\n",
      "Transform done for index: 41\n",
      "Transform done for index: 42\n",
      "Transform done for index: 43\n",
      "Transform done for index: 44\n",
      "Transform done for index: 45\n",
      "Transform done for index: 46\n",
      "Transform done for index: 47\n",
      "Transform done for index: 48\n",
      "Transform done for index: 49\n",
      "cut_full_res_dataset.is_numpy, None\n",
      "Calculating parotids\n",
      "Atlas input (136, 120, 219) (136, 120, 219)\n",
      "Transform done for index: 0\n",
      "Transform done for index: 1\n",
      "Transform done for index: 2\n",
      "Transform done for index: 3\n",
      "Transform done for index: 4\n",
      "Transform done for index: 5\n",
      "Transform done for index: 6\n",
      "Transform done for index: 7\n",
      "Transform done for index: 8\n",
      "Transform done for index: 9\n",
      "Transform done for index: 10\n",
      "Transform done for index: 11\n",
      "Transform done for index: 12\n",
      "Transform done for index: 13\n",
      "Transform done for index: 14\n",
      "Transform done for index: 15\n",
      "Transform done for index: 16\n",
      "Transform done for index: 17\n",
      "Transform done for index: 18\n",
      "Transform done for index: 19\n",
      "Transform done for index: 20\n",
      "Transform done for index: 21\n",
      "Transform done for index: 22\n",
      "Transform done for index: 23\n",
      "Transform done for index: 24\n",
      "Transform done for index: 25\n",
      "Transform done for index: 26\n",
      "Transform done for index: 27\n",
      "Transform done for index: 28\n",
      "Transform done for index: 29\n",
      "Transform done for index: 30\n",
      "Transform done for index: 31\n",
      "Transform done for index: 32\n",
      "Transform done for index: 33\n",
      "Transform done for index: 34\n",
      "Transform done for index: 35\n",
      "Transform done for index: 36\n",
      "Transform done for index: 37\n",
      "Transform done for index: 38\n",
      "Transform done for index: 39\n",
      "Transform done for index: 40\n",
      "Transform done for index: 41\n",
      "Transform done for index: 42\n",
      "Transform done for index: 43\n",
      "Transform done for index: 44\n",
      "Transform done for index: 45\n",
      "Transform done for index: 46\n",
      "Transform done for index: 47\n",
      "Transform done for index: 48\n",
      "Transform done for index: 49\n",
      "cut_full_res_dataset.is_numpy, None\n",
      "Calculating all_maps\n",
      "Atlas input (136, 120, 219) (136, 120, 219)\n",
      "Transform done for index: 0\n",
      "Transform done for index: 1\n",
      "Transform done for index: 2\n",
      "Transform done for index: 3\n",
      "Transform done for index: 4\n",
      "Transform done for index: 5\n",
      "Transform done for index: 6\n",
      "Transform done for index: 7\n",
      "Transform done for index: 8\n",
      "Transform done for index: 9\n",
      "Transform done for index: 10\n",
      "Transform done for index: 11\n",
      "Transform done for index: 12\n",
      "Transform done for index: 13\n",
      "Transform done for index: 14\n",
      "Transform done for index: 15\n",
      "Transform done for index: 16\n",
      "Transform done for index: 17\n",
      "Transform done for index: 18\n",
      "Transform done for index: 19\n",
      "Transform done for index: 20\n",
      "Transform done for index: 21\n",
      "Transform done for index: 22\n",
      "Transform done for index: 23\n",
      "Transform done for index: 24\n",
      "Transform done for index: 25\n",
      "Transform done for index: 26\n",
      "Transform done for index: 27\n",
      "Transform done for index: 28\n",
      "Transform done for index: 29\n",
      "Transform done for index: 30\n",
      "Transform done for index: 31\n",
      "Transform done for index: 32\n",
      "Transform done for index: 33\n",
      "Transform done for index: 34\n",
      "Transform done for index: 35\n",
      "Transform done for index: 36\n",
      "Transform done for index: 37\n",
      "Transform done for index: 38\n",
      "Transform done for index: 39\n",
      "Transform done for index: 40\n",
      "Transform done for index: 41\n",
      "Transform done for index: 42\n",
      "Transform done for index: 43\n",
      "Transform done for index: 44\n",
      "Transform done for index: 45\n",
      "Transform done for index: 46\n",
      "Transform done for index: 47\n",
      "Transform done for index: 48\n",
      "Transform done for index: 49\n",
      "cut_full_res_dataset.is_numpy, None\n"
     ]
    }
   ],
   "source": [
    "if CALC_REGISTRATION:\n",
    "    cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "    cut_full_res_dataset.load_from_file('./data/HaN_OAR_cut_72_192_168')\n",
    "\n",
    "    # atlas_brainstem_map_sitk, atlas_left_parotid_map_sitk, atlas_right_parotid_map_sitk\n",
    "    atlas_input_data_sitk = atlas_resampler.Execute(atlas_ri_sitk)\n",
    "    atlas_input_labels = list()\n",
    "    atlas_input_labels.append(('brainstem', atlas_brainstem_map_sitk))\n",
    "    atlas_input_labels.append(('left_parotid', atlas_left_parotid_map_sitk))\n",
    "    atlas_input_labels.append(('right_parotid', atlas_right_parotid_map_sitk))\n",
    "    atlas_input_labels.append(('parotids', atlas_left_parotid_map_sitk + atlas_right_parotid_map_sitk))\n",
    "    atlas_input_labels.append(('all_maps', atlas_brainstem_map_sitk + atlas_left_parotid_map_sitk + atlas_right_parotid_map_sitk))\n",
    "    \n",
    "    for atlas_name, atlas_label_map in atlas_input_labels:\n",
    "        print(f'Calculating {atlas_name}')\n",
    "        atlas_input_label_sitk = atlas_resampler.Execute(atlas_label_map)\n",
    "\n",
    "        # parsing to numpy\n",
    "        atlas_input_data_np = sitk.GetArrayFromImage(atlas_input_data_sitk)\n",
    "        atlas_input_label_np = sitk.GetArrayFromImage(atlas_input_label_sitk)\n",
    "        atlas_input_np = (atlas_input_data_np, atlas_input_label_np)\n",
    "        print('Atlas input', atlas_input_data_np.shape, atlas_input_label_np.shape)\n",
    "\n",
    "        # registration_list = create_regis_list(cut_full_res_dataset, atlas_input_np, numberOfIterations=3000)\n",
    "        atlas_input_np = (atlas_input_data_np, atlas_input_label_np)\n",
    "        registration_list = trans_list(cut_full_res_dataset, atlas_input_np, regis_trans_list)\n",
    "\n",
    "        cut_full_res_dataset.data_list = registration_list\n",
    "        cut_full_res_dataset.save_to_file(f'./data/HaN_OAR_cut_{atlas_name}_reg')\n",
    "\n",
    "        print(f'cut_full_res_dataset.is_numpy, {cut_full_res_dataset.output_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8935f7fda1d41478d75b23cd781fccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f40809d03754c2fb35262a04bd3fd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a4c72c953a4201a1342b52a2c12fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d2e948c742448d87f817a115f1d82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccc6a3bd2de47ae8961e04b4d19ad0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=35, max=71),)),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220e51823bca47a5bea4b6107fb40908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_3d_image(registration_list[0][0], figsize=(10, 10))\n",
    "preview_3d_image(registration_list[0][1], figsize=(10, 10))\n",
    "\n",
    "preview_3d_image(cut_full_res_dataset.label_list[1][0] == OARS_LABELS.PAROTID_GLAND_R, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cut dataset\n"
     ]
    }
   ],
   "source": [
    "print('loading cut dataset')\n",
    "data_path = './data/HaN_OAR_cut_brainstem_reg'\n",
    "l_parotid_cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "l_parotid_cut_full_res_dataset.load_from_file(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview_3d_image(l_parotid_cut_full_res_dataset[0][0][0], figsize=(5, 5))\n",
    "# preview_3d_image(l_parotid_cut_full_res_dataset[0][0][1], figsize=(5, 5))\n",
    "# preview_3d_image(l_parotid_cut_full_res_dataset[0][1], figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training all organs models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking list used for training single models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels['SPINAL_CORD']\n",
    "\n",
    "tmp_list = list(filter_labels.items())\n",
    "labels_list = [tmp_list[10]]\n",
    "for OAR_KEY, OAR_VALUE in labels_list:\n",
    "    print(f\"{OAR_KEY}, {OAR_VALUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_MODELS = False\n",
    "if TRAIN_MODELS:\n",
    "    for OAR_KEY, OAR_VALUE in labels_list:\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        log_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        model_name = f'{log_date}_3d_unet_lowres_model3v2_{OAR_KEY}'\n",
    "\n",
    "        print(f'Training model with dataset label \\'{OAR_KEY}\\', value \\'{OAR_VALUE}\\'')\n",
    "        print(f'folder \\'{model_name}\\'')\n",
    "        cut_model_info = prepare_model(epochs=75,\n",
    "                                       learning_rate=3e-4,\n",
    "                                       in_channels=8,\n",
    "                                       input_data_channels=1,\n",
    "                                       output_label_channels=1,\n",
    "                                       dropout_rate=0.2,\n",
    "                                       train_batch_size=1,\n",
    "                                       model_name=model_name,\n",
    "                                       train_dataset=cut_train_dataset, \n",
    "                                       valid_dataset=cut_valid_dataset, \n",
    "                                       test_dataset=cut_test_dataset,\n",
    "                                       model_class=UNetV3v2)\n",
    "        show_model_info(cut_model_info)\n",
    "        print('\\n\\n')\n",
    "        train_loop(cut_model_info, iterate_model_fn=iterate_model_v3v2)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # clearing memory\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels['SPINAL_CORD']\n",
    "\n",
    "tmp_list = list(filter_labels.items())\n",
    "labels_list = [tmp_list[5], tmp_list[6], tmp_list[7], tmp_list[10], tmp_list[11], tmp_list[12]]\n",
    "for OAR_KEY, OAR_VALUE in labels_list:\n",
    "    print(f\"{OAR_KEY}, {OAR_VALUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_models(oar_key):\n",
    "    possible_models = [folder_name for folder_name in os.listdir('./models') if oar_key in folder_name]    \n",
    "    \n",
    "    return possible_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading models to CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "for OAR_KEY, OAR_VALUE in labels_list:\n",
    "    epoch = 75\n",
    "    possible_models = get_possible_models(f\"model3v2_{OAR_KEY}\")\n",
    "    if len(possible_models) <= 0:\n",
    "        print(f'{OAR_KEY} Model: No avaiable model')\n",
    "        continue\n",
    "\n",
    "    model_name = possible_models[0]\n",
    "    print(f'{OAR_KEY} Model: Loading model {model_name}')\n",
    "\n",
    "    # loading model checkpoint\n",
    "    cut_model_info = load_checkpoint_model_info(model_name, epoch, cut_train_dataset, cut_valid_dataset, cut_test_dataset, model_class=UNetV3v2)\n",
    "\n",
    "    # moving model to cpu/cuda with eval mode\n",
    "    cut_model_info['device'] = 'cpu'\n",
    "    cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "    cut_model_info['model'].eval()\n",
    "    cut_model_info['model'].disable_tensorboard_writing = True\n",
    "    \n",
    "    models[OAR_KEY] = cut_model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Eval vs Train Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_full_res_dataset.set_output_label(OARS_LABELS.PITUITARY)\n",
    "cut_model_info = models['PITUITARY']\n",
    "cut_model_info['device'] = get_device()\n",
    "cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "cut_model_info['model'].disable_tensorboard_writing = True\n",
    "    \n",
    "model, model_name, optimizer, criterion = itemgetter('model', 'model_name', 'optimizer', 'criterion')(cut_model_info)\n",
    "epochs, device, tensorboard_writer = itemgetter('epochs', 'device', 'tensorboard_writer')(cut_model_info)\n",
    "train_dataloader, valid_dataloader, test_dataloader = itemgetter('train_dataloader',\n",
    "                                                                 'valid_dataloader',\n",
    "                                                                 'test_dataloader')(cut_model_info)\n",
    "model.actual_epoch = 100\n",
    "\n",
    "valid_loss, valid_dsc = iterate_model_v3v2(valid_dataloader, model, optimizer, criterion, device, is_eval=True)\n",
    "print(valid_loss, valid_dsc)\n",
    "\n",
    "cut_model_info['model'].disable_tensorboard_writing = True\n",
    "cut_model_info['device'] = 'cpu'\n",
    "cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_DSC_INFO = True\n",
    "if SHOW_DSC_INFO:\n",
    "    info_per_organs_df = {}\n",
    "    models_info = list()\n",
    "    for OAR_KEY, OAR_VALUE in labels_list:\n",
    "        if OAR_KEY not in models:\n",
    "            print(f'{OAR_KEY} Model: No avaiable model')\n",
    "            continue\n",
    "\n",
    "        # getting model to gpu\n",
    "        cut_model_info = models[OAR_KEY]\n",
    "        cut_model_info['device'] = get_device()\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "        cut_model_info['model'].eval()\n",
    "        cut_model_info['model'].disable_tensorboard_writing = True\n",
    "\n",
    "        # preparing dataset for comparison\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "\n",
    "        # calculating dsc predictions        \n",
    "        info_df, preds, rescaled_preds = get_threshold_info_df(\n",
    "                                    model=cut_model_info['model'], \n",
    "                                    dataset=cut_full_res_dataset, \n",
    "                                    device=cut_model_info['device'], \n",
    "                                    train_indices=cut_train_dataset.indices, \n",
    "                                    valid_indices=cut_valid_dataset.indices, \n",
    "                                    test_indices=cut_test_dataset.indices,\n",
    "                                    step=0.5,\n",
    "                                    transform_input_fn=transform_input_with_registration)\n",
    "        info_per_organs_df[OAR_KEY] = info_df\n",
    "\n",
    "        # moving model back to cpu\n",
    "        cut_model_info['device'] = 'cpu'\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "\n",
    "        # parsing data\n",
    "        best_threshold_col = 'thres_rescaled_dsc_0.50'\n",
    "        train_tmp_df = info_df[info_df['is_train']][best_threshold_col]\n",
    "        valid_tmp_df = info_df[info_df['is_valid']][best_threshold_col]\n",
    "        train_dsc = train_tmp_df.mean()\n",
    "        valid_dsc = valid_tmp_df.mean()\n",
    "        print(f'{OAR_KEY} Model: DSC train {round(train_dsc, 4)} valid {round(valid_dsc, 4)}')\n",
    "\n",
    "        models_info.append({\n",
    "            'oar_key': OAR_KEY,\n",
    "            'model_name': model_name,\n",
    "            # Train\n",
    "            'train_dsc_mean': train_dsc,\n",
    "            'train_dsc_std': train_tmp_df.std(),\n",
    "            'train_dsc_median': train_tmp_df.median(),\n",
    "            'train_dsc_min': train_tmp_df.min(),\n",
    "            'train_dsc_max': train_tmp_df.max(),\n",
    "            # Valid\n",
    "            'valid_dsc_mean': valid_dsc,\n",
    "            'valid_dsc_std': valid_tmp_df.std(),\n",
    "            'valid_dsc_median': valid_tmp_df.median(),\n",
    "            'valid_dsc_min': valid_tmp_df.min(),\n",
    "            'valid_dsc_max': valid_tmp_df.max(),\n",
    "            # Both\n",
    "            'train_valid_mean_delta': train_dsc - valid_dsc\n",
    "        })\n",
    "\n",
    "    models_info_df = pd.DataFrame(models_info)\n",
    "    \n",
    "    tmp_df = models_info_df[['oar_key', 'train_dsc_mean', 'train_dsc_std', 'valid_dsc_mean', 'valid_dsc_std']].copy()\n",
    "    tmp_df['train_dsc_mean'] = (tmp_df['train_dsc_mean'] * 100).round(2)\n",
    "    tmp_df['valid_dsc_mean'] = (tmp_df['valid_dsc_mean'] * 100).round(2)\n",
    "    tmp_df['train_dsc_std'] = (tmp_df['train_dsc_std'] * 100).round(2)\n",
    "    tmp_df['valid_dsc_std'] = (tmp_df['valid_dsc_std'] * 100).round(2)\n",
    "    \n",
    "    display(tmp_df.mean().round(2))\n",
    "    display(tmp_df.round(2))\n",
    "    display(tmp_df.sort_values(by=['train_dsc_std']).round(2))\n",
    "    display(models_info_df.sort_values(by=['train_dsc_mean']).drop(columns=['model_name']).round(2))\n",
    "    display(models_info_df.sort_values(by=['train_valid_mean_delta']).drop(columns=['model_name']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_DSC_INFO:\n",
    "    tmp_column = 'is_train'\n",
    "    \n",
    "    try:\n",
    "        print('OARS_LABELS.PAROTID_GLAND_R')\n",
    "        tmp_df = info_per_organs_df[OARS_LABELS.OARS_LABELS_R_DICT[OARS_LABELS.PAROTID_GLAND_R]]\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:   \n",
    "        print('OARS_LABELS.PAROTID_GLAND_L')\n",
    "        tmp_df = info_per_organs_df[OARS_LABELS.OARS_LABELS_R_DICT[OARS_LABELS.PAROTID_GLAND_L]]\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try: \n",
    "        print('OARS_LABELS.OPT_NERVE_L')\n",
    "        tmp_df = info_per_organs_df[OARS_LABELS.OARS_LABELS_R_DICT[OARS_LABELS.OPT_NERVE_L]]\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try: \n",
    "        print('OARS_LABELS.PITUITARY')\n",
    "        tmp_df = info_per_organs_df[OARS_LABELS.OARS_LABELS_R_DICT[OARS_LABELS.PITUITARY]]\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions merging and checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels_dict = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels_dict['SPINAL_CORD']\n",
    "\n",
    "cut_full_res_dataset.set_output_label(filter_labels_dict)\n",
    "preview_dataset(cut_full_res_dataset, preview_index=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSE_CUT_DATASET = True\n",
    "if PARSE_CUT_DATASET:\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    prediction_threshold = 0.5\n",
    "    output_label_items = list(cut_full_res_dataset.output_label.items())[:]\n",
    "    cut_dataset_predictions = defaultdict(lambda: defaultdict(lambda: np.zeros(cut_full_res_dataset[0][0][0].shape)))\n",
    "    \n",
    "    # for each label\n",
    "    for label_index, val in enumerate(output_label_items[:]):\n",
    "        OAR_KEY, OAR_VALUE = val\n",
    "        # loading model\n",
    "        if OAR_KEY not in models:\n",
    "            print(f'{label_index+1}/{len(output_label_items)}: {OAR_KEY} Model: No avaiable model')\n",
    "            continue\n",
    "        print(f'{label_index+1}/{len(output_label_items)}: {OAR_KEY} Model: got model {datetime.datetime.now()}')\n",
    "\n",
    "        # getting model to gpu\n",
    "        cut_model_info = models[OAR_KEY]\n",
    "        cut_model_info['device'] = get_device()\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "        cut_model_info['model'].eval()\n",
    "        cut_model_info['model'].disable_tensorboard_writing = True\n",
    "\n",
    "        # for label in whole dataset\n",
    "        for index in range(len(cut_full_res_dataset)):\n",
    "            prediction, rescaled_pred = get_rescaled_pred(cut_model_info['model'], cut_full_res_dataset, \n",
    "                                                          cut_model_info['device'], index, use_only_one_dimension=False)\n",
    "    \n",
    "            cut_dataset_predictions[index][OAR_VALUE] = prediction[0]\n",
    "            # extended_cut_full_res_dataset.data_list[index][label_index + 1] = prediction\n",
    "            # extended_cut_full_res_dataset.data_list[index][label_index + 1] = ((rescaled_pred > prediction_threshold) * 1).astype(np.int8)\n",
    "\n",
    "        # moving model back to cpu\n",
    "        cut_model_info['device'] = 'cpu'\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARSE_CUT_DATASET:\n",
    "    def custom_preview_dataset(dataset, predictions, preview_index=0, show_hist=False, use_transform=False):\n",
    "        data, label = dataset.get_raw_item_with_label_filter(preview_index)  # equivalent dataset[preview_index]\n",
    "        if use_transform:\n",
    "            transform = get_dataset_transform()\n",
    "            data, label = transform_input(data, label, transform)\n",
    "\n",
    "        prediction = predictions[preview_index]\n",
    "        max_channels = label.shape[0]\n",
    "        max_slices = label.shape[1]\n",
    "\n",
    "        print(f'data max {data.max()}, min {data.min()}')\n",
    "        print(f'label max {label.max()}, min {label.min()}')\n",
    "\n",
    "        def f(slice_index, label_channel):\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(data[0, slice_index], cmap=\"gray\")\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(prediction[label_channel+1][slice_index])\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(label[label_channel, slice_index])\n",
    "            plt.show()\n",
    "\n",
    "            if show_hist:\n",
    "                plt.figure(figsize=(20, 10))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.hist(data.flatten(), 128)\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.hist(label.flatten(), 128)\n",
    "                plt.show()\n",
    "\n",
    "        sliceSlider = widgets.IntSlider(min=0, max=max_slices - 1, step=1, value=(max_slices - 1) / 2)\n",
    "        labelChannelSlider = widgets.IntSlider(min=0, max=max_channels - 1, step=1, value=(max_channels - 1) / 2)\n",
    "        ui = widgets.VBox([widgets.HBox([sliceSlider, labelChannelSlider])])\n",
    "        out = widgets.interactive_output(f, {'slice_index': sliceSlider, 'label_channel': labelChannelSlider})\n",
    "        # noinspection PyTypeChecker\n",
    "        display(ui, out)\n",
    "    \n",
    "    index = cut_valid_dataset.indices[3]\n",
    "    index = 35\n",
    "    custom_preview_dataset(cut_full_res_dataset, cut_dataset_predictions, preview_index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: refactor\n",
    "\n",
    "MERGE_PREDICTIONS = False\n",
    "if MERGE_PREDICTIONS:\n",
    "    merged_predictions = [None] * len(extended_cut_full_res_dataset)\n",
    "    for index in range(len(extended_cut_full_res_dataset)):\n",
    "        # print(f\"{index+1}/{len(extended_cut_full_res_dataset)}: Merging predictions to single label\")\n",
    "        data, label = extended_cut_full_res_dataset.get_raw_item_with_label_filter(index)\n",
    "\n",
    "        new_data = np.zeros(data[0].shape, dtype=np.int16)\n",
    "        for i in range(1, 22):\n",
    "            new_data += data[i]\n",
    "\n",
    "        merged_predictions[index] = new_data\n",
    "    print('Merging done')\n",
    "\n",
    "    # checking how many masks are overlapping\n",
    "    for i, tmp_merged in enumerate(merged_predictions):\n",
    "        display(f'scan id {i}: {np.where(tmp_merged == 1)[0].shape[0]}, {np.where(tmp_merged == 2)[0].shape[0]}, {np.where(tmp_merged == 3)[0].shape[0]}, {np.where(tmp_merged == 4)[0].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: refactor\n",
    "\n",
    "from src.losses import calc_dscm\n",
    "\n",
    "def custom_preview_dataset2(dataset, preview_index=0, use_transform=False):\n",
    "    data, label = dataset.get_raw_item_with_label_filter(preview_index)\n",
    "    if use_transform:\n",
    "        transform = get_dataset_transform()\n",
    "        data, label = transform_input(data, label, transform)\n",
    "        \n",
    "    cut_data, cut_label = cut_full_res_dataset.get_raw_item_with_label_filter(preview_index)\n",
    "    max_channels = label.shape[0]\n",
    "    max_slices = label.shape[1]\n",
    "    \n",
    "    print(f'data max {data.max()}, min {data.min()}')\n",
    "    print(f'label max {label.max()}, min {label.min()}')\n",
    "    print(f'{data.shape}, {cut_data.shape}, {label.shape}, {cut_label.shape}')\n",
    "    print(f'{data.dtype}, {cut_data.dtype}, {label.dtype}, {cut_label.dtype}')\n",
    "    tmp_merged = merged_predictions[preview_index]\n",
    "    print(f'{np.where(tmp_merged == 1)[0].shape},{np.where(tmp_merged == 2)[0].shape},{np.where(tmp_merged == 3)[0].shape},{np.where(tmp_merged == 4)[0].shape}')\n",
    "\n",
    "    def f(slice_index, label_channel):\n",
    "        print(f'{OARS_LABELS.OARS_LABELS_R_DICT[label_channel+1]}')\n",
    "        tmp_tensor_label = torch.tensor(label[label_channel])\n",
    "        tmp_tensor_prediciton = torch.tensor(data[label_channel+1])\n",
    "        tmp_dsc = calc_dsc(tmp_tensor_label, tmp_tensor_prediciton)\n",
    "        print(f'dsc {tmp_dsc}')\n",
    "\n",
    "        plt.figure(figsize=(30, 20))\n",
    "\n",
    "        plt.subplot(2, 3, 1).title.set_text('data')\n",
    "        plt.imshow(cut_data[0, slice_index], cmap=\"gray\")\n",
    "        plt.subplot(2, 3, 2).title.set_text('label')\n",
    "        plt.imshow(label[label_channel, slice_index])\n",
    "        plt.subplot(2, 3, 3).title.set_text('prediciton')\n",
    "        plt.imshow(data[label_channel+1, slice_index])\n",
    "        # print(data.shape, np.sum(data[label_channel+1]), np.unique(data[1])[-1])\n",
    "        print(f'slices with values > 0', (np.where(data[label_channel+1] > 0))[0])\n",
    "        \n",
    "        plt.subplot(2, 3, 4).title.set_text('merged prediction labels')\n",
    "        plt.imshow(tmp_merged[slice_index], vmin=0, vmax=np.unique(tmp_merged)[-1])\n",
    "        plt.subplot(2, 3, 5).title.set_text('merged labels ')\n",
    "        plt.imshow(np.sum(label, axis=0)[slice_index])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    sliceSlider = widgets.IntSlider(min=0, max=max_slices - 1, step=1, value=(max_slices - 1) / 2)\n",
    "    labelChannelSlider = widgets.IntSlider(min=0, max=max_channels - 1, step=1, value=(max_channels - 1) / 2)\n",
    "    ui = widgets.VBox([widgets.HBox([sliceSlider, labelChannelSlider])])\n",
    "    out = widgets.interactive_output(f, {'slice_index': sliceSlider, 'label_channel': labelChannelSlider})\n",
    "    # noinspection PyTypeChecker\n",
    "    display(ui, out)\n",
    "\n",
    "custom_preview_dataset2(extended_cut_full_res_dataset, preview_index=35, use_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
