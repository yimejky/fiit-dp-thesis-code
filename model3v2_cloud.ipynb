{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/dp_tomastik/code')\n",
    "    !bash \"/content/drive/My Drive/dp_tomastik/code/scripts/install_libs.sh\"\n",
    "    \n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "from torchio import RandomAffine, Compose, ZNormalization\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from src.helpers import preview_3d_image\n",
    "from src.helpers import show_cuda_usage, preview_model_dataset_pred, preview_dataset\n",
    "from src.helpers import get_threshold_info_df, get_rescaled_preds\n",
    "from src.helpers import compare_prediction_with_ground_true, compare_one_prediction_with_ground_true\n",
    "from src.helpers import get_img_outliers_pixels, get_raw_with_prediction\n",
    "from src.helpers import get_rescaled_pred\n",
    "from src.helpers import get_transformed_label_np, create_regis_trans_list, trans_list\n",
    "\n",
    "from src.dataset import HaNOarsDataset, transform_input_with_registration, get_norm_transform\n",
    "from src.dataset import get_full_res_cut, get_cut_lists, OARS_LABELS, get_dataset, get_dataset_info, get_dataset_transform\n",
    "from src.dataset import split_dataset, copy_split_dataset\n",
    "\n",
    "from src.model_and_training import prepare_model, train_loop, show_model_info, load_checkpoint_model_info\n",
    "from src.model_and_training import iterate_model_v3v2\n",
    "from src.model_and_training.getters.get_device import get_device\n",
    "from src.model_and_training.architectures.unet_architecture_v3v2 import UNetV3v2\n",
    "\n",
    "from src.consts import DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "  \n",
    "torch.manual_seed(20)\n",
    "logging.basicConfig(filename='logs/model3v2_all_organs_jupyter.log', level=logging.DEBUG)\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_models(oar_key):\n",
    "    possible_models = [folder_name for folder_name in os.listdir('./models') if oar_key in folder_name]    \n",
    "    \n",
    "    return possible_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training all organs models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting random indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'./data/HaN_OAR_cut_all_maps_reg'\n",
    "example_cut_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "example_cut_dataset.load_from_file(data_path)\n",
    "example_cut_dataset_obj = split_dataset(example_cut_dataset, train_size=40, valid_size=5, test_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing dataset input example\n",
    "# data_path = f'./data/HaN_OAR_cut_left_parotid_reg'\n",
    "# cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "# cut_full_res_dataset.load_from_file(data_path)\n",
    "# cut_full_res_dataset.set_output_label(OARS_LABELS.PAROTID_GLAND_R)\n",
    "                                     \n",
    "# preview_3d_image(cut_full_res_dataset[0][0][0], figsize=(4,4))\n",
    "# preview_3d_image(cut_full_res_dataset[0][0][1], figsize=(4,4))\n",
    "# preview_3d_image(cut_full_res_dataset[0][1], figsize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels['SPINAL_CORD']\n",
    "\n",
    "tmp_list = list(filter_labels.items())\n",
    "labels_list = list()\n",
    "\n",
    "# creating registration dataset and organ segmentation pairs\n",
    "# dataset use inverted labeling of left and right\n",
    "# labels_list.append(('left_parotid', *tmp_list[12]))\n",
    "# labels_list.append(('right_parotid', *tmp_list[11]))\n",
    "# labels_list.append(('brainstem', *tmp_list[10]))\n",
    "# labels_list.append(('parotids', *tmp_list[10]))\n",
    "# labels_list.append(('all_maps', *tmp_list[10]))\n",
    "\n",
    "for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "    print(f\"{DATASET_REG_NAME}, {OAR_KEY}, {OAR_VALUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_MODELS = False\n",
    "if TRAIN_MODELS:\n",
    "    for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "        # loading dataset\n",
    "        data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "        cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "        cut_full_res_dataset.load_from_file(data_path)\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        \n",
    "        cut_full_res_dataset_obj = copy_split_dataset(cut_full_res_dataset, example_cut_dataset_obj)\n",
    "        cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "\n",
    "        # preparing model name\n",
    "        log_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        model_name = f'{log_date}_3d_unet_lowres_model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg'\n",
    "\n",
    "        print(f'Training model with dataset label \\'{OAR_KEY}\\', value \\'{OAR_VALUE}\\'')\n",
    "        print(f'folder \\'{model_name}\\'')\n",
    "        cut_model_info = prepare_model(epochs=175,\n",
    "                                       learning_rate=3e-4,\n",
    "                                       in_channels=8,\n",
    "                                       input_data_channels=1,\n",
    "                                       output_label_channels=1,\n",
    "                                       dropout_rate=0.2,\n",
    "                                       train_batch_size=2,\n",
    "                                       model_name=model_name,\n",
    "                                       train_dataset=cut_train_dataset, \n",
    "                                       valid_dataset=cut_valid_dataset, \n",
    "                                       test_dataset=cut_test_dataset,\n",
    "                                       model_class=UNetV3v2)\n",
    "        show_model_info(cut_model_info)\n",
    "        print('\\n\\n')\n",
    "        train_loop(cut_model_info, iterate_model_fn=iterate_model_v3v2)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # clearing memory\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_and_training.getters.get_loaders import get_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RETRAIN_MODELS = False\n",
    "if RETRAIN_MODELS:\n",
    "    for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "        # loading dataset\n",
    "        data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "        cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "        cut_full_res_dataset.load_from_file(data_path)\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        \n",
    "        cut_full_res_dataset_obj = copy_split_dataset(cut_full_res_dataset, example_cut_dataset_obj)\n",
    "        cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "\n",
    "        # preparing model name\n",
    "        log_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        model_name = f'{log_date}_3d_unet_lowres_model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg'\n",
    "\n",
    "        print(f'Training model with dataset label \\'{OAR_KEY}\\', value \\'{OAR_VALUE}\\'')\n",
    "        print(f'folder \\'{model_name}\\'')\n",
    "        \n",
    "        # getting possible models\n",
    "        possible_models = get_possible_models(f\"model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg\")\n",
    "        if len(possible_models) <= 0:\n",
    "            print(f'{OAR_KEY} Model: No avaiable model')\n",
    "            continue\n",
    "\n",
    "        model_name = possible_models[0]\n",
    "        print(f'Model: Loading model {model_name}')\n",
    "\n",
    "        # loading model checkpoint\n",
    "        epoch = 175\n",
    "        cut_model_info = load_checkpoint_model_info(model_name, epoch, cut_train_dataset, cut_valid_dataset, cut_test_dataset, model_class=UNetV3v2)\n",
    "        cut_model_info['epochs'] = 175\n",
    "\n",
    "        # train_batch_size = 1\n",
    "        # train_dataloader, valid_dataloader, test_dataloader = get_loaders(train_batch_size, cut_train_dataset, cut_valid_dataset, cut_test_dataset)\n",
    "        # cut_model_info[\"train_dataloader\"] = train_dataloader\n",
    "        # cut_model_info[\"valid_dataloader\"] = valid_dataloader\n",
    "        # cut_model_info[\"test_dataloader\"] = test_dataloader\n",
    "        \n",
    "        show_model_info(cut_model_info)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        # training\n",
    "        train_loop(cut_model_info, iterate_model_fn=iterate_model_v3v2, start_epoch=epoch)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # clearing memory\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels['SPINAL_CORD']\n",
    "\n",
    "tmp_list = list(filter_labels.items())\n",
    "labels_list = list()\n",
    "\n",
    "# creating registration dataset and organ segmentation pairs\n",
    "# dataset use inverted labeling of left and right\n",
    "labels_list.append(('left_parotid', *tmp_list[12]))\n",
    "labels_list.append(('right_parotid', *tmp_list[11]))\n",
    "\n",
    "labels_list.append(('brainstem', *tmp_list[10]))\n",
    "labels_list.append(('parotids', *tmp_list[10]))\n",
    "labels_list.append(('all_maps', *tmp_list[10]))\n",
    "\n",
    "for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "    print(f\"{DATASET_REG_NAME}, {OAR_KEY}, {OAR_VALUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading models to CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "    model_reg_name = f'{OAR_KEY}-{DATASET_REG_NAME}'\n",
    "    \n",
    "    # dataset loading\n",
    "    data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "    cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "    cut_full_res_dataset.load_from_file(data_path)\n",
    "    cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "    \n",
    "    cut_full_res_dataset_obj = copy_split_dataset(cut_full_res_dataset, example_cut_dataset_obj)\n",
    "    cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "    \n",
    "    # cut_full_res_dataset_obj = split_dataset(cut_full_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "    # cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "\n",
    "    possible_models = get_possible_models(f\"model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg\")\n",
    "    if len(possible_models) <= 0:\n",
    "        print(f'{OAR_KEY} Model: No avaiable model')\n",
    "        continue\n",
    "\n",
    "    model_name = possible_models[0]\n",
    "    print(f'{model_reg_name} Model: Loading model {model_name}')\n",
    "\n",
    "    # loading model checkpoint\n",
    "    epoch = 175\n",
    "    cut_model_info = load_checkpoint_model_info(model_name, epoch, cut_train_dataset, cut_valid_dataset, cut_test_dataset, model_class=UNetV3v2)\n",
    "\n",
    "    # moving model to cpu/cuda with eval mode\n",
    "    cut_model_info['device'] = 'cpu'\n",
    "    cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "    cut_model_info['model'].eval()\n",
    "    cut_model_info['model'].disable_tensorboard_writing = True\n",
    "    \n",
    "    models[model_reg_name] = cut_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Eval vs Train Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(train_dataloader):\n",
    "#     if i == 0:\n",
    "#         print(data[0][0][0].shape)\n",
    "#         preview_3d_image(data[0][0][0], figsize=(4, 4))\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_full_res_dataset.set_output_label(OARS_LABELS.PITUITARY)\n",
    "cut_model_info = models['PITUITARY-all_maps'] # list(models.keys())\n",
    "cut_model_info['device'] = get_device()\n",
    "cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "cut_model_info['model'].disable_tensorboard_writing = True\n",
    "    \n",
    "model, model_name, optimizer, criterion = itemgetter('model', 'model_name', 'optimizer', 'criterion')(cut_model_info)\n",
    "epochs, device, tensorboard_writer = itemgetter('epochs', 'device', 'tensorboard_writer')(cut_model_info)\n",
    "train_dataloader, valid_dataloader, test_dataloader = itemgetter('train_dataloader',\n",
    "                                                                 'valid_dataloader',\n",
    "                                                                 'test_dataloader')(cut_model_info)\n",
    "# model.actual_epoch = 100\n",
    "# valid_loss, valid_dsc = iterate_model_v3v2(train_dataloader, model, optimizer, criterion, device, is_eval=True)\n",
    "# print(valid_loss, valid_dsc)\n",
    "\n",
    "cut_model_info['model'].disable_tensorboard_writing = True\n",
    "cut_model_info['device'] = 'cpu'\n",
    "cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = get_rescaled_pred(cut_model_info['model'], cut_full_res_dataset, 'cpu', 0, transform_input_fn=transform_input_with_registration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_DSC_INFO = True\n",
    "if SHOW_DSC_INFO:\n",
    "    info_per_organs_df = {}\n",
    "    models_info = list()\n",
    "    for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "        model_name = f'{OAR_KEY}-{DATASET_REG_NAME}'\n",
    "        \n",
    "        if model_name not in models:\n",
    "            print(f'{model_name} Model: No avaiable model')\n",
    "            continue\n",
    "\n",
    "        # getting model to gpu\n",
    "        cut_model_info = models[model_name]\n",
    "        cut_model_info['device'] = get_device()\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "        cut_model_info['model'].eval()\n",
    "        cut_model_info['model'].disable_tensorboard_writing = True\n",
    "\n",
    "        # preparing dataset for comparison\n",
    "        # dataset loading\n",
    "        data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "        cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "        cut_full_res_dataset.load_from_file(data_path)\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        \n",
    "        cut_full_res_dataset_obj = copy_split_dataset(cut_full_res_dataset, example_cut_dataset_obj)\n",
    "        cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "        # preview_3d_image(cut_train_dataset[0][0][0], figsize=(5, 5))\n",
    "        # preview_3d_image(cut_train_dataset[0][0][0], figsize=(5, 5))\n",
    "        \n",
    "        # calculating dsc predictions        \n",
    "        info_df, preds, rescaled_preds = get_threshold_info_df(\n",
    "                                    model=cut_model_info['model'], \n",
    "                                    dataset=cut_full_res_dataset, \n",
    "                                    device=cut_model_info['device'], \n",
    "                                    train_indices=cut_train_dataset.indices, \n",
    "                                    valid_indices=cut_valid_dataset.indices, \n",
    "                                    test_indices=cut_test_dataset.indices,\n",
    "                                    step=0.5,\n",
    "                                    transform_input_fn=transform_input_with_registration)\n",
    "        info_per_organs_df[model_name] = info_df\n",
    "\n",
    "        # moving model back to cpu\n",
    "        cut_model_info['device'] = 'cpu'\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "\n",
    "        # parsing data\n",
    "        best_threshold_col = 'thres_rescaled_dsc_0.50'\n",
    "        train_tmp_df = info_df[info_df['is_train']][best_threshold_col]\n",
    "        valid_tmp_df = info_df[info_df['is_valid']][best_threshold_col]\n",
    "        train_dsc = train_tmp_df.mean()\n",
    "        valid_dsc = valid_tmp_df.mean()\n",
    "        print(f'{model_name} Model: DSC train {round(train_dsc, 4)} valid {round(valid_dsc, 4)}')\n",
    "\n",
    "        models_info.append({\n",
    "            'oar_key': OAR_KEY,\n",
    "            'model_name': model_name,\n",
    "            # Train\n",
    "            'train_dsc_mean': train_dsc,\n",
    "            'train_dsc_std': train_tmp_df.std(),\n",
    "            'train_dsc_median': train_tmp_df.median(),\n",
    "            'train_dsc_min': train_tmp_df.min(),\n",
    "            'train_dsc_max': train_tmp_df.max(),\n",
    "            # Valid\n",
    "            'valid_dsc_mean': valid_dsc,\n",
    "            'valid_dsc_std': valid_tmp_df.std(),\n",
    "            'valid_dsc_median': valid_tmp_df.median(),\n",
    "            'valid_dsc_min': valid_tmp_df.min(),\n",
    "            'valid_dsc_max': valid_tmp_df.max(),\n",
    "            # Both\n",
    "            'train_valid_mean_delta': train_dsc - valid_dsc\n",
    "        })\n",
    "\n",
    "    models_info_df = pd.DataFrame(models_info)\n",
    "    \n",
    "    tmp_df = models_info_df[['model_name', 'train_dsc_mean', 'train_dsc_std', 'valid_dsc_mean', 'valid_dsc_std']].copy()\n",
    "    tmp_df['train_dsc_mean'] = (tmp_df['train_dsc_mean'] * 100).round(2)\n",
    "    tmp_df['valid_dsc_mean'] = (tmp_df['valid_dsc_mean'] * 100).round(2)\n",
    "    tmp_df['train_dsc_std'] = (tmp_df['train_dsc_std'] * 100).round(2)\n",
    "    tmp_df['valid_dsc_std'] = (tmp_df['valid_dsc_std'] * 100).round(2)\n",
    "    \n",
    "    display(tmp_df.mean().round(2))\n",
    "    display(tmp_df.round(2))\n",
    "    display(tmp_df.sort_values(by=['train_dsc_std']).round(2))\n",
    "    display(models_info_df.sort_values(by=['train_dsc_mean']).round(2))\n",
    "    display(models_info_df.sort_values(by=['train_valid_mean_delta']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_DSC_INFO:\n",
    "    tmp_column = 'is_train'\n",
    "    \n",
    "    try:\n",
    "        print('OARS_LABELS.PAROTID_GLAND_R')\n",
    "        tmp_df = info_per_organs_df['PAROTID_GLAND_L-right_parotid']\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:   \n",
    "        print('OARS_LABELS.PAROTID_GLAND_L')\n",
    "        tmp_df = info_per_organs_df['PITUITARY-brainstem Model']\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try: \n",
    "        print('OARS_LABELS.OPT_NERVE_L')\n",
    "        tmp_df = info_per_organs_df['PITUITARY-parotids']\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try: \n",
    "        print('OARS_LABELS.PITUITARY')\n",
    "        tmp_df = info_per_organs_df['PITUITARY-all_maps']\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
