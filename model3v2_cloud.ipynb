{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n",
      "Dataset biggest bounding box wihtout spinal cord [56, 177, 156]\n",
      "Cut target size [72, 192, 168]\n",
      "Done Init\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/dp_tomastik/code')\n",
    "    !bash \"/content/drive/My Drive/dp_tomastik/code/scripts/install_libs.sh\"\n",
    "    \n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "from torchio import RandomAffine, Compose, ZNormalization\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from src.helpers import preview_3d_image\n",
    "from src.helpers import show_cuda_usage, preview_model_dataset_pred, preview_dataset\n",
    "from src.helpers import get_threshold_info_df, get_rescaled_preds\n",
    "from src.helpers import compare_prediction_with_ground_true, compare_one_prediction_with_ground_true\n",
    "from src.helpers import get_img_outliers_pixels, get_raw_with_prediction\n",
    "from src.helpers import get_rescaled_pred\n",
    "from src.helpers import get_transformed_label_np, create_regis_trans_list, trans_list\n",
    "\n",
    "from src.dataset import HaNOarsDataset, transform_input_with_registration, get_norm_transform\n",
    "from src.dataset import get_full_res_cut, get_cut_lists, OARS_LABELS, get_dataset, get_dataset_info, get_dataset_transform\n",
    "from src.dataset import split_dataset, copy_split_dataset\n",
    "\n",
    "from src.model_and_training import prepare_model, train_loop, show_model_info, load_checkpoint_model_info\n",
    "from src.model_and_training import iterate_model_v3v2\n",
    "from src.model_and_training.getters.get_device import get_device\n",
    "from src.model_and_training.architectures.unet_architecture_v3v2 import UNetV3v2\n",
    "\n",
    "from src.consts import DATASET_MAX_BOUNDING_BOX, DESIRE_BOUNDING_BOX_SIZE\n",
    "  \n",
    "torch.manual_seed(20)\n",
    "logging.basicConfig(filename='logs/model3v2_all_organs_jupyter.log', level=logging.DEBUG)\n",
    "\n",
    "print('Dataset biggest bounding box wihtout spinal cord', DATASET_MAX_BOUNDING_BOX)\n",
    "print('Cut target size', DESIRE_BOUNDING_BOX_SIZE)\n",
    "print('Done Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training all organs models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing dataset input example\n",
    "# data_path = f'./data/HaN_OAR_cut_left_parotid_reg'\n",
    "# cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "# cut_full_res_dataset.load_from_file(data_path)\n",
    "# cut_full_res_dataset.set_output_label(OARS_LABELS.PAROTID_GLAND_R)\n",
    "                                     \n",
    "# preview_3d_image(cut_full_res_dataset[0][0][0], figsize=(4,4))\n",
    "# preview_3d_image(cut_full_res_dataset[0][0][1], figsize=(4,4))\n",
    "# preview_3d_image(cut_full_res_dataset[0][1], figsize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_parotid, PAROTID_GLAND_R, 13\n",
      "right_parotid, PAROTID_GLAND_L, 12\n",
      "brainstem, PITUITARY, 11\n",
      "parotids, PITUITARY, 11\n",
      "all_maps, PITUITARY, 11\n"
     ]
    }
   ],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels['SPINAL_CORD']\n",
    "\n",
    "tmp_list = list(filter_labels.items())\n",
    "labels_list = list()\n",
    "\n",
    "# creating registration dataset and organ segmentation pairs\n",
    "# dataset use inverted labeling of left and right\n",
    "labels_list.append(('left_parotid', *tmp_list[12]))\n",
    "labels_list.append(('right_parotid', *tmp_list[11]))\n",
    "\n",
    "labels_list.append(('brainstem', *tmp_list[10]))\n",
    "labels_list.append(('parotids', *tmp_list[10]))\n",
    "labels_list.append(('all_maps', *tmp_list[10]))\n",
    "\n",
    "for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "    print(f\"{DATASET_REG_NAME}, {OAR_KEY}, {OAR_VALUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_MODELS = True\n",
    "if TRAIN_MODELS:\n",
    "    for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "        # loading dataset\n",
    "        data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "        cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "        cut_full_res_dataset.load_from_file(data_path)\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        \n",
    "        cut_full_res_dataset_obj = split_dataset(cut_full_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "        cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "        \n",
    "        # preparing model name\n",
    "        log_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        model_name = f'{log_date}_3d_unet_lowres_model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg'\n",
    "\n",
    "        print(f'Training model with dataset label \\'{OAR_KEY}\\', value \\'{OAR_VALUE}\\'')\n",
    "        print(f'folder \\'{model_name}\\'')\n",
    "        cut_model_info = prepare_model(epochs=100,\n",
    "                                       learning_rate=3e-4,\n",
    "                                       in_channels=8,\n",
    "                                       input_data_channels=1,\n",
    "                                       output_label_channels=1,\n",
    "                                       dropout_rate=0.2,\n",
    "                                       train_batch_size=2,\n",
    "                                       model_name=model_name,\n",
    "                                       train_dataset=cut_train_dataset, \n",
    "                                       valid_dataset=cut_valid_dataset, \n",
    "                                       test_dataset=cut_test_dataset,\n",
    "                                       model_class=UNetV3v2)\n",
    "        show_model_info(cut_model_info)\n",
    "        print('\\n\\n')\n",
    "        train_loop(cut_model_info, iterate_model_fn=iterate_model_v3v2)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # clearing memory\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels['SPINAL_CORD']\n",
    "\n",
    "tmp_list = list(filter_labels.items())\n",
    "labels_list = list()\n",
    "\n",
    "# creating registration dataset and organ segmentation pairs\n",
    "# dataset use inverted labeling of left and right\n",
    "labels_list.append(('left_parotid', *tmp_list[12]))\n",
    "labels_list.append(('right_parotid', *tmp_list[11]))\n",
    "\n",
    "labels_list.append(('brainstem', *tmp_list[10]))\n",
    "labels_list.append(('parotids', *tmp_list[10]))\n",
    "labels_list.append(('all_maps', *tmp_list[10]))\n",
    "\n",
    "for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "    print(f\"{DATASET_REG_NAME}, {OAR_KEY}, {OAR_VALUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_models(oar_key):\n",
    "    possible_models = [folder_name for folder_name in os.listdir('./models') if oar_key in folder_name]    \n",
    "    \n",
    "    return possible_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading models to CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "    model_reg_name = f'{OAR_KEY}-{DATASET_REG_NAME}'\n",
    "    \n",
    "    # dataset loading\n",
    "    data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "    cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "    cut_full_res_dataset.load_from_file(data_path)\n",
    "    cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "    cut_full_res_dataset_obj = split_dataset(cut_full_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "    cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "    \n",
    "    epoch = 1\n",
    "    possible_models = get_possible_models(f\"model3v2__cloud-{OAR_KEY}-{DATASET_REG_NAME}_reg\")\n",
    "    if len(possible_models) <= 0:\n",
    "        print(f'{OAR_KEY} Model: No avaiable model')\n",
    "        continue\n",
    "\n",
    "    model_name = possible_models[0]\n",
    "    print(f'{model_reg_name} Model: Loading model {model_name}')\n",
    "\n",
    "    # loading model checkpoint\n",
    "    cut_model_info = load_checkpoint_model_info(model_name, epoch, cut_train_dataset, cut_valid_dataset, cut_test_dataset, model_class=UNetV3v2)\n",
    "\n",
    "    # moving model to cpu/cuda with eval mode\n",
    "    cut_model_info['device'] = 'cpu'\n",
    "    cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "    cut_model_info['model'].eval()\n",
    "    cut_model_info['model'].disable_tensorboard_writing = True\n",
    "    \n",
    "    models[model_reg_name] = cut_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Eval vs Train Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_full_res_dataset.set_output_label(OARS_LABELS.PITUITARY)\n",
    "cut_model_info = models[list(models.keys())[0]]\n",
    "cut_model_info['device'] = get_device()\n",
    "cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "cut_model_info['model'].disable_tensorboard_writing = True\n",
    "    \n",
    "model, model_name, optimizer, criterion = itemgetter('model', 'model_name', 'optimizer', 'criterion')(cut_model_info)\n",
    "epochs, device, tensorboard_writer = itemgetter('epochs', 'device', 'tensorboard_writer')(cut_model_info)\n",
    "train_dataloader, valid_dataloader, test_dataloader = itemgetter('train_dataloader',\n",
    "                                                                 'valid_dataloader',\n",
    "                                                                 'test_dataloader')(cut_model_info)\n",
    "model.actual_epoch = 100\n",
    "\n",
    "valid_loss, valid_dsc = iterate_model_v3v2(valid_dataloader, model, optimizer, criterion, device, is_eval=True)\n",
    "print(valid_loss, valid_dsc)\n",
    "\n",
    "cut_model_info['model'].disable_tensorboard_writing = True\n",
    "cut_model_info['device'] = 'cpu'\n",
    "cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_DSC_INFO = True\n",
    "if SHOW_DSC_INFO:\n",
    "    info_per_organs_df = {}\n",
    "    models_info = list()\n",
    "    for DATASET_REG_NAME, OAR_KEY, OAR_VALUE in labels_list:\n",
    "        model_name = f'{OAR_KEY}-{DATASET_REG_NAME}'\n",
    "        \n",
    "        if model_name not in models:\n",
    "            print(f'{model_name} Model: No avaiable model')\n",
    "            continue\n",
    "\n",
    "        # getting model to gpu\n",
    "        cut_model_info = models[model_name]\n",
    "        cut_model_info['device'] = get_device()\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "        cut_model_info['model'].eval()\n",
    "        cut_model_info['model'].disable_tensorboard_writing = True\n",
    "\n",
    "        # preparing dataset for comparison\n",
    "        # dataset loading\n",
    "        data_path = f'./data/HaN_OAR_cut_{DATASET_REG_NAME}_reg'\n",
    "        cut_full_res_dataset = HaNOarsDataset(data_path, size=50, load_images=False)\n",
    "        cut_full_res_dataset.load_from_file(data_path)\n",
    "        cut_full_res_dataset.set_output_label(OAR_VALUE)\n",
    "        cut_full_res_dataset_obj = split_dataset(cut_full_res_dataset, train_size=40, valid_size=5, test_size=5)\n",
    "        cut_train_dataset, cut_valid_dataset, cut_test_dataset = itemgetter(*['train_dataset', 'valid_dataset', 'test_dataset'])(cut_full_res_dataset_obj)\n",
    "    \n",
    "\n",
    "        # calculating dsc predictions        \n",
    "        info_df, preds, rescaled_preds = get_threshold_info_df(\n",
    "                                    model=cut_model_info['model'], \n",
    "                                    dataset=cut_full_res_dataset, \n",
    "                                    device=cut_model_info['device'], \n",
    "                                    train_indices=cut_train_dataset.indices, \n",
    "                                    valid_indices=cut_valid_dataset.indices, \n",
    "                                    test_indices=cut_test_dataset.indices,\n",
    "                                    step=0.5,\n",
    "                                    transform_input_fn=transform_input_with_registration)\n",
    "        info_per_organs_df[model_name] = info_df\n",
    "\n",
    "        # moving model back to cpu\n",
    "        cut_model_info['device'] = 'cpu'\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "\n",
    "        # parsing data\n",
    "        best_threshold_col = 'thres_rescaled_dsc_0.50'\n",
    "        train_tmp_df = info_df[info_df['is_train']][best_threshold_col]\n",
    "        valid_tmp_df = info_df[info_df['is_valid']][best_threshold_col]\n",
    "        train_dsc = train_tmp_df.mean()\n",
    "        valid_dsc = valid_tmp_df.mean()\n",
    "        print(f'{model_name} Model: DSC train {round(train_dsc, 4)} valid {round(valid_dsc, 4)}')\n",
    "\n",
    "        models_info.append({\n",
    "            'oar_key': OAR_KEY,\n",
    "            'model_name': model_name,\n",
    "            # Train\n",
    "            'train_dsc_mean': train_dsc,\n",
    "            'train_dsc_std': train_tmp_df.std(),\n",
    "            'train_dsc_median': train_tmp_df.median(),\n",
    "            'train_dsc_min': train_tmp_df.min(),\n",
    "            'train_dsc_max': train_tmp_df.max(),\n",
    "            # Valid\n",
    "            'valid_dsc_mean': valid_dsc,\n",
    "            'valid_dsc_std': valid_tmp_df.std(),\n",
    "            'valid_dsc_median': valid_tmp_df.median(),\n",
    "            'valid_dsc_min': valid_tmp_df.min(),\n",
    "            'valid_dsc_max': valid_tmp_df.max(),\n",
    "            # Both\n",
    "            'train_valid_mean_delta': train_dsc - valid_dsc\n",
    "        })\n",
    "\n",
    "    models_info_df = pd.DataFrame(models_info)\n",
    "    \n",
    "    tmp_df = models_info_df[['oar_key', 'train_dsc_mean', 'train_dsc_std', 'valid_dsc_mean', 'valid_dsc_std']].copy()\n",
    "    tmp_df['train_dsc_mean'] = (tmp_df['train_dsc_mean'] * 100).round(2)\n",
    "    tmp_df['valid_dsc_mean'] = (tmp_df['valid_dsc_mean'] * 100).round(2)\n",
    "    tmp_df['train_dsc_std'] = (tmp_df['train_dsc_std'] * 100).round(2)\n",
    "    tmp_df['valid_dsc_std'] = (tmp_df['valid_dsc_std'] * 100).round(2)\n",
    "    \n",
    "    display(tmp_df.mean().round(2))\n",
    "    display(tmp_df.round(2))\n",
    "    display(tmp_df.sort_values(by=['train_dsc_std']).round(2))\n",
    "    display(models_info_df.sort_values(by=['train_dsc_mean']).drop(columns=['model_name']).round(2))\n",
    "    display(models_info_df.sort_values(by=['train_valid_mean_delta']).drop(columns=['model_name']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_DSC_INFO:\n",
    "    tmp_column = 'is_train'\n",
    "    \n",
    "    try:\n",
    "        print('OARS_LABELS.PAROTID_GLAND_R')\n",
    "        tmp_df = info_per_organs_df[OARS_LABELS.OARS_LABELS_R_DICT[OARS_LABELS.PAROTID_GLAND_R]]\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:   \n",
    "        print('OARS_LABELS.PAROTID_GLAND_L')\n",
    "        tmp_df = info_per_organs_df[OARS_LABELS.OARS_LABELS_R_DICT[OARS_LABELS.PAROTID_GLAND_L]]\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try: \n",
    "        print('OARS_LABELS.OPT_NERVE_L')\n",
    "        tmp_df = info_per_organs_df[OARS_LABELS.OARS_LABELS_R_DICT[OARS_LABELS.OPT_NERVE_L]]\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try: \n",
    "        print('OARS_LABELS.PITUITARY')\n",
    "        tmp_df = info_per_organs_df[OARS_LABELS.OARS_LABELS_R_DICT[OARS_LABELS.PITUITARY]]\n",
    "        display(tmp_df[tmp_df[tmp_column]].sort_values(by='thres_rescaled_dsc_0.50'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions merging and checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels_dict = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels_dict['SPINAL_CORD']\n",
    "\n",
    "cut_full_res_dataset.set_output_label(filter_labels_dict)\n",
    "preview_dataset(cut_full_res_dataset, preview_index=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSE_CUT_DATASET = True\n",
    "if PARSE_CUT_DATASET:\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    prediction_threshold = 0.5\n",
    "    cut_dataset_predictions = defaultdict(lambda: defaultdict(lambda: np.zeros(cut_full_res_dataset[0][0][0].shape)))\n",
    "    \n",
    "    # for each label\n",
    "    for label_index, val in enumerate(labels_list[:]):\n",
    "        DATASET_REG_NAME, OAR_KEY, OAR_VALUE = val\n",
    "        model_name = f'{OAR_KEY}-{DATASET_REG_NAME}'\n",
    "        \n",
    "        # loading model\n",
    "        if model_name not in models:\n",
    "            print(f'{label_index+1}/{len(labels_list)}: {model_name} Model: No avaiable model')\n",
    "            continue\n",
    "        print(f'{label_index+1}/{len(labels_list)}: {model_name} Model: got model {datetime.datetime.now()}')\n",
    "\n",
    "        # getting model to gpu\n",
    "        cut_model_info = models[model_name]\n",
    "        cut_model_info['device'] = get_device()\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])\n",
    "        cut_model_info['model'].eval()\n",
    "        cut_model_info['model'].disable_tensorboard_writing = True\n",
    "\n",
    "        # for label in whole dataset\n",
    "        for index in range(len(cut_full_res_dataset)):\n",
    "            prediction, rescaled_pred = get_rescaled_pred(cut_model_info['model'], cut_full_res_dataset, \n",
    "                                                          cut_model_info['device'], index, use_only_one_dimension=False)\n",
    "    \n",
    "            cut_dataset_predictions[index][OAR_VALUE] = prediction[0]\n",
    "            # extended_cut_full_res_dataset.data_list[index][label_index + 1] = prediction\n",
    "            # extended_cut_full_res_dataset.data_list[index][label_index + 1] = ((rescaled_pred > prediction_threshold) * 1).astype(np.int8)\n",
    "\n",
    "        # moving model back to cpu\n",
    "        cut_model_info['device'] = 'cpu'\n",
    "        cut_model_info['model'] = cut_model_info['model'].to(cut_model_info['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels_dict = OARS_LABELS.OARS_LABELS_DICT\n",
    "if 'SPINAL_CORD' in filter_labels:\n",
    "    del filter_labels_dict['SPINAL_CORD']\n",
    "\n",
    "cut_full_res_dataset.set_output_label(filter_labels_dict)\n",
    "preview_dataset(cut_full_res_dataset, preview_index=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARSE_CUT_DATASET:\n",
    "    def custom_preview_dataset(dataset, predictions, preview_index=0, show_hist=False, use_transform=False):\n",
    "        data, label = dataset.get_raw_item_with_label_filter(preview_index)  # equivalent dataset[preview_index]\n",
    "        if use_transform:\n",
    "            transform = get_dataset_transform()\n",
    "            data, label = transform_input(data, label, transform)\n",
    "\n",
    "        prediction = predictions[preview_index]\n",
    "        max_channels = label.shape[0]\n",
    "        max_slices = label.shape[1]\n",
    "\n",
    "        print(f'data max {data.max()}, min {data.min()}')\n",
    "        print(f'label max {label.max()}, min {label.min()}')\n",
    "        print(f'data {data.shape}, label {label.shape}')\n",
    "\n",
    "        def f(slice_index, label_channel):\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(data[0, slice_index], cmap=\"gray\")\n",
    "            plt.title('data')\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(prediction[label_channel+1][slice_index])\n",
    "            plt.title(f'prediction {label_channel}')\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(label[label_channel, slice_index])\n",
    "            plt.title(f'label {label_channel}')\n",
    "            plt.show()\n",
    "\n",
    "            if show_hist:\n",
    "                plt.figure(figsize=(20, 10))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.hist(data.flatten(), 128)\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.hist(label.flatten(), 128)\n",
    "                plt.show()\n",
    "\n",
    "        sliceSlider = widgets.IntSlider(min=0, max=max_slices - 1, step=1, value=(max_slices - 1) / 2)\n",
    "        labelChannelSlider = widgets.IntSlider(min=0, max=max_channels - 1, step=1, value=(max_channels - 1) / 2)\n",
    "        ui = widgets.VBox([widgets.HBox([sliceSlider, labelChannelSlider])])\n",
    "        out = widgets.interactive_output(f, {'slice_index': sliceSlider, 'label_channel': labelChannelSlider})\n",
    "        # noinspection PyTypeChecker\n",
    "        display(ui, out)\n",
    "    \n",
    "    index = cut_valid_dataset.indices[3]\n",
    "    index = 35\n",
    "    custom_preview_dataset(cut_full_res_dataset, cut_dataset_predictions, preview_index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
